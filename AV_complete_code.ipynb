{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CJOFvANYYsH"
      },
      "source": [
        "# Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XoB-W-vEExO",
        "outputId": "94a67cae-30db-4e37-85f1-31d15e917a2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AFEW Loading"
      ],
      "metadata": {
        "id": "Z48iDD555dTg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CRpS3gBx82zp"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Percorso della cartella dove sono memorizzati i file zip nel tuo Google Drive\n",
        "zip_folder_path = '/content/drive/MyDrive/AFEW'\n",
        "\n",
        "# Percorso dove vuoi estrarre i file\n",
        "extraction_path = '/content/dataset/'\n",
        "\n",
        "for i in range(1, 13):\n",
        "    zip_file_path = os.path.join(zip_folder_path, f\"{str(i).zfill(2)}.zip\")\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extraction_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Face Extraction e Frame Resizing"
      ],
      "metadata": {
        "id": "5tREC1ql5hep"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4KjFq8iA8Kdk"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "def load_and_preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Converti in RGB\n",
        "    image = image / 255.0  # Normalizza i valori dei pixel\n",
        "    image = image.astype(np.float32)\n",
        "    return image\n",
        "\n",
        "def crop_face(image, landmarks, expansion_factor=0.2):\n",
        "    x_coordinates = [int(landmark[0]) for landmark in landmarks]\n",
        "    y_coordinates = [int(landmark[1]) for landmark in landmarks]\n",
        "\n",
        "    x_min, x_max = min(x_coordinates), max(x_coordinates)\n",
        "    y_min, y_max = min(y_coordinates), max(y_coordinates)\n",
        "\n",
        "    # Calcolo dell'intervallo\n",
        "    x_range = x_max - x_min\n",
        "    y_range = y_max - y_min\n",
        "\n",
        "    # Aggiustamento per l'expansion factor\n",
        "    x_min = max(x_min - int(x_range * expansion_factor), 0)\n",
        "    x_max = min(x_max + int(x_range * expansion_factor), image.shape[1])\n",
        "    y_min = max(y_min - int(y_range * expansion_factor), 0)\n",
        "    y_max = min(y_max + int(y_range * expansion_factor), image.shape[0])\n",
        "\n",
        "    # Verifica che le coordinate di ritaglio siano valide\n",
        "    if x_min >= x_max or y_min >= y_max:\n",
        "        print(f\"Errore di calcolo del ritaglio: x_min={x_min}, x_max={x_max}, y_min={y_min}, y_max={y_max}\")\n",
        "        return None\n",
        "\n",
        "    cropped_face = image[y_min:y_max, x_min:x_max]\n",
        "    return cropped_face\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "l-6mS7z98MrI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "def process_video_folder(folder_path, folder_name, save_path):\n",
        "    images, labels = [], []\n",
        "\n",
        "    json_path = os.path.join(folder_path, f'{folder_name}.json')\n",
        "    with open(json_path, 'r') as file:\n",
        "        json_data = json.load(file)\n",
        "\n",
        "    for frame_id, frame_info in json_data['frames'].items():\n",
        "        image_path = os.path.join(folder_path, f\"{frame_id}.png\")\n",
        "\n",
        "        image = load_and_preprocess_image(image_path)\n",
        "\n",
        "        landmarks = frame_info['landmarks']\n",
        "        cropped_face = crop_face(image, landmarks)\n",
        "        cropped_face = (cropped_face * 255).astype(np.uint8)\n",
        "        #resize img to 224,224\n",
        "        # Salvataggio dell'immagine ritagliata\n",
        "        resized_cropped_face = cv2.resize(cropped_face, (224,224))  # Ridimensiona l'immagine\n",
        "        save_folder = os.path.join(save_path, folder_name)\n",
        "        os.makedirs(save_folder, exist_ok=True)  # Crea la cartella se non esiste\n",
        "        save_image_path = os.path.join(save_folder, f\"{frame_id}.png\")\n",
        "        cv2.imwrite(save_image_path, cv2.cvtColor(resized_cropped_face, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        images.append(resized_cropped_face)\n",
        "        labels.append((frame_info['valence'], frame_info['arousal']))\n",
        "\n",
        "    return images, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VJziz7j8Ods",
        "outputId": "fbada2b0-7ca0-4737-846e-36b30e352990"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import gc\n",
        "# Pulizia della memoria\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ubqb1Vt98G13",
        "outputId": "e40c76ed-b462-4c8e-d41d-43d8a0990bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cartelle processate: 600/601\n",
            "Elaborazione completata.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# Percorso dove salverai i dati ridimensionati\n",
        "save_path = 'dataset_resized'\n",
        "\n",
        "# Percorso dove hai estratto i dati\n",
        "extraction_path = 'dataset'\n",
        "\n",
        "processed_folders = 0\n",
        "total_folders = len(os.listdir(extraction_path))\n",
        "\n",
        "X, Y = [], []\n",
        "\n",
        "for folder_name in os.listdir(extraction_path):\n",
        "    folder_path = os.path.join(extraction_path, folder_name)\n",
        "    if os.path.isdir(folder_path):\n",
        "        images, labels = process_video_folder(folder_path, folder_name, save_path)\n",
        "        X.extend(images)\n",
        "        Y.extend(labels)\n",
        "        processed_folders += 1\n",
        "        sys.stdout.write(f\"\\rCartelle processate: {processed_folders}/{total_folders}\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "print(\"\\nElaborazione completata.\")\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute Action Unit and Key Frames Extraction"
      ],
      "metadata": {
        "id": "J1PofxlU5x3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q py-feat"
      ],
      "metadata": {
        "id": "rD590FZ96rWM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7baa607-4f1f-4cf4-892b-01e8f84e08c3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.4/381.4 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from feat import Detector\n",
        "\n",
        "detector = Detector(\n",
        "    face_model=\"retinaface\",\n",
        "    landmark_model=\"mobilefacenet\",\n",
        "    au_model='xgb',\n",
        "    emotion_model=\"resmasknet\",\n",
        "    facepose_model=\"img2pose\",\n",
        "    device='auto'\n",
        ")\n",
        "\n",
        "detector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2srIxuUbUkj",
        "outputId": "c30ff04c-9094-4fa8-f764-7995898a48e1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1786259/1786259 [00:00<00:00, 9541091.23it/s]\n",
            "100%|██████████| 12281146/12281146 [00:00<00:00, 32227602.65it/s]\n",
            "100%|██████████| 965545/965545 [00:00<00:00, 6181176.28it/s]\n",
            "100%|██████████| 33635572/33635572 [00:00<00:00, 50616122.78it/s]\n",
            "100%|██████████| 130263/130263 [00:00<00:00, 1689955.53it/s]\n",
            "100%|██████████| 45886076/45886076 [00:00<00:00, 58620632.01it/s]\n",
            "100%|██████████| 130263/130263 [00:00<00:00, 1678110.65it/s]\n",
            "100%|██████████| 53851068/53851068 [00:00<00:00, 64324478.01it/s]\n",
            "100%|██████████| 130263/130263 [00:00<00:00, 1749760.20it/s]\n",
            "100%|██████████| 166975/166975 [00:00<00:00, 2245282.13it/s]\n",
            "100%|██████████| 530776/530776 [00:00<00:00, 4520633.92it/s]\n",
            "100%|██████████| 493515/493515 [00:00<00:00, 3983288.92it/s]\n",
            "100%|██████████| 207163/207163 [00:00<00:00, 2247781.33it/s]\n",
            "100%|██████████| 1151853/1151853 [00:00<00:00, 6440212.01it/s]\n",
            "100%|██████████| 572282/572282 [00:00<00:00, 4370544.12it/s]\n",
            "100%|██████████| 330487/330487 [00:00<00:00, 2726075.30it/s]\n",
            "100%|██████████| 335449/335449 [00:00<00:00, 2808333.50it/s]\n",
            "100%|██████████| 586893/586893 [00:00<00:00, 4090938.44it/s]\n",
            "100%|██████████| 206931/206931 [00:00<00:00, 2423069.89it/s]\n",
            "100%|██████████| 689547/689547 [00:00<00:00, 4385843.20it/s]\n",
            "100%|██████████| 583629/583629 [00:00<00:00, 4108250.43it/s]\n",
            "100%|██████████| 207077/207077 [00:00<00:00, 2465731.02it/s]\n",
            "100%|██████████| 256865/256865 [00:00<00:00, 2612236.43it/s]\n",
            "100%|██████████| 1079268/1079268 [00:00<00:00, 6673907.66it/s]\n",
            "100%|██████████| 1952633/1952633 [00:00<00:00, 9405730.54it/s]\n",
            "100%|██████████| 312240/312240 [00:00<00:00, 2627509.34it/s]\n",
            "100%|██████████| 524139/524139 [00:00<00:00, 4134993.62it/s]\n",
            "100%|██████████| 77744/77744 [00:00<00:00, 1759106.05it/s]\n",
            "100%|██████████| 551634217/551634217 [00:10<00:00, 53624849.11it/s]\n",
            "100%|██████████| 448/448 [00:00<00:00, 191524.63it/s]\n",
            "100%|██████████| 944/944 [00:00<00:00, 712510.88it/s]\n",
            "100%|██████████| 169843940/169843940 [00:02<00:00, 60111438.99it/s]\n",
            "100%|██████████| 176/176 [00:00<00:00, 293401.23it/s]\n",
            "100%|██████████| 176/176 [00:00<00:00, 164592.53it/s]\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 96.3MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "feat.detector.Detector(face_model=retinaface, landmark_model=mobilefacenet, au_model=xgb, emotion_model=resmasknet, facepose_model=img2pose)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Percorso della cartella principale\n",
        "root_folder = '/content/dataset_resized/'\n",
        "\n",
        "# Inizializza il conteggio dei frame come None per il confronto\n",
        "min_frames = None\n",
        "\n",
        "# Elenco di tutte le sottocartelle\n",
        "subfolders = [f.path for f in os.scandir(root_folder) if f.is_dir()]\n",
        "\n",
        "# Calcola il numero minimo di frame in tutte le sottocartelle\n",
        "for subfolder in subfolders:\n",
        "    # Lista dei file di frame nella sottocartella\n",
        "    frame_files = [f for f in os.listdir(subfolder) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "    num_frames = len(frame_files)\n",
        "\n",
        "    # Aggiorna il conteggio minimo se necessario\n",
        "    if min_frames is None or num_frames < min_frames:\n",
        "        min_frames = num_frames\n",
        "\n",
        "min_frames\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6ZS2gSBEDIn",
        "outputId": "4a87d9e5-22ab-48bd-eb41-668b7263d031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def count_total_frames(folder_path):\n",
        "    \"\"\"\n",
        "    Conta il numero totale di frames nelle sottocartelle di un dato percorso.\n",
        "\n",
        "    :param folder_path: Percorso della cartella principale contenente le sottocartelle.\n",
        "    :return: Numero totale di frames.\n",
        "    \"\"\"\n",
        "    total_frames = 0\n",
        "\n",
        "    # Itera sulle sottocartelle\n",
        "    for subfolder in os.listdir(folder_path):\n",
        "        subfolder_path = os.path.join(folder_path, subfolder)\n",
        "        if os.path.isdir(subfolder_path):\n",
        "            # Conta i file (frame) nella sottocartella\n",
        "            frames = [f for f in os.listdir(subfolder_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "            total_frames += len(frames)\n",
        "\n",
        "    return total_frames\n",
        "\n",
        "# Esempio di utilizzo della funzione\n",
        "folder_path = '/content/dataset/'  # Modifica con il tuo percorso\n",
        "total_frames = count_total_frames(folder_path)\n",
        "total_frames\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yve-cTxJk6MD",
        "outputId": "e7e1123b-03af-4a91-d743-7b00bd6a8f6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30051"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from feat import Detector\n",
        "\n",
        "detector = Detector(au_model='xgb')\n",
        "\n",
        "def select_key_frames(subfolder_path, max_images, json_data):\n",
        "    \"\"\"\n",
        "    Seleziona i key-frames per variazione delle AU,\n",
        "    mantenendo l'ordine cronologico dei frame.\n",
        "    \"\"\"\n",
        "    frame_ids = sorted(os.listdir(subfolder_path))  # Ordina i frame per nome\n",
        "    au_variations = []\n",
        "    frame_data = []\n",
        "\n",
        "    for filename in frame_ids:\n",
        "        if filename.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
        "            frame_id = filename.split('.')[0]\n",
        "            image_path = os.path.join(subfolder_path, filename)\n",
        "\n",
        "            try:\n",
        "                # Ottieni i valori di arousal e valence per l'immagine corrente\n",
        "                arousal = json_data['frames'][frame_id]['arousal']\n",
        "                valence = json_data['frames'][frame_id]['valence']\n",
        "\n",
        "                # Esegui la detection delle AU\n",
        "                prediction = detector.detect_image(image_path)\n",
        "                au_values = np.array(list(prediction.aus.iloc[0]))\n",
        "                au_variations.append(np.std(au_values))  # Variazione come deviazione standard\n",
        "\n",
        "                frame_data.append((image_path, arousal, valence))\n",
        "            except Exception as e:\n",
        "                print(f\"Errore durante l'elaborazione di {image_path}: {e}\")\n",
        "\n",
        "    # Seleziona gli indici dei frame con le maggiori variazioni\n",
        "    sorted_indices = np.argsort(au_variations)\n",
        "    key_frame_indices = sorted_indices[-max_images:] if max_images <= len(sorted_indices) else sorted_indices\n",
        "    key_frames_ordered = sorted(key_frame_indices)  # Mantiene l'ordine cronologico\n",
        "\n",
        "    return [frame_data[i] for i in key_frames_ordered]\n",
        "\n"
      ],
      "metadata": {
        "id": "oNxix3UKkLtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_images_with_key_frame_extraction(folder_path, max_folders=600, max_images_per_folder=10, start_folder_index=0):\n",
        "    \"\"\"\n",
        "    Processa le immagini in un dato percorso per estrarre le Action Units (AU)\n",
        "    e i valori di arousal e valence, a partire da una specifica cartella.\n",
        "\n",
        "    :param folder_path: Percorso della cartella contenente le immagini.\n",
        "    :param max_folders: Numero massimo di sotto-cartelle da processare.\n",
        "    :param max_images_per_folder: Numero massimo di immagini da processare per sotto-cartella.\n",
        "    :param start_folder_index: Indice della cartella da cui iniziare l'elaborazione.\n",
        "    :return: DataFrame con i valori delle AU, arousal e valence per ogni immagine processata.\n",
        "    \"\"\"\n",
        "    aus_results = []\n",
        "\n",
        "    print(f\"Elaborazione della cartella: {folder_path}\")\n",
        "    sorted_folders = sorted(os.listdir(folder_path))\n",
        "    for i, folder_name in enumerate(sorted_folders):\n",
        "        if i < start_folder_index:\n",
        "            continue\n",
        "        if i >= start_folder_index + max_folders:\n",
        "            break\n",
        "\n",
        "        subfolder_path = os.path.join(folder_path, folder_name)\n",
        "        json_path = os.path.join('/content/dataset/'+folder_name+'/', f'{folder_name}.json')\n",
        "\n",
        "        print(f\"Elaborazione della sotto-cartella: {subfolder_path}\")\n",
        "        if not os.path.isdir(subfolder_path) or not os.path.exists(json_path):\n",
        "            print(f\"Cartella o file JSON non trovati: {subfolder_path}, {json_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(json_path, 'r') as file:\n",
        "            json_data = json.load(file)\n",
        "\n",
        "        key_frames = select_key_frames(subfolder_path, max_images_per_folder, json_data)\n",
        "\n",
        "        for image_path, arousal, valence in key_frames:\n",
        "            try:\n",
        "                # Esegui la detection delle AU\n",
        "                prediction = detector.detect_image(image_path)\n",
        "                au_values = prediction.aus.iloc[0].to_dict()\n",
        "\n",
        "                # Aggiungi image_path all'inizio del dizionario\n",
        "                au_values = {'image_path': image_path, **au_values}\n",
        "\n",
        "                au_values.update({'arousal': arousal, 'valence': valence})\n",
        "                aus_results.append(au_values)\n",
        "            except Exception as e:\n",
        "                print(f\"Errore durante l'elaborazione di {image_path}: {e}\")\n",
        "\n",
        "    return pd.DataFrame(aus_results)"
      ],
      "metadata": {
        "id": "dhozHiDigPDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Percorso della cartella principale del dataset\n",
        "dataset_path = \"/content/dataset_resized/\"\n",
        "\n",
        "# Processa le immagini nel dataset\n",
        "# Puoi modificare 'max_folders' in base al numero di sotto-cartelle che vuoi processare\n",
        "aus_data = process_images_with_key_frame_extraction(dataset_path,200,start_folder_index=0)\n",
        "\n",
        "# Stampa i primi record per avere un'anteprima dei dati\n",
        "print(aus_data.head())\n",
        "\n",
        "# Ora 'aus_data' è un DataFrame che puoi utilizzare per ulteriori analisi o per addestrare un modello\n"
      ],
      "metadata": {
        "id": "A5vo1FOFjW7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aus_data.shape"
      ],
      "metadata": {
        "id": "OqauUlHr8Gih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3764b28-c30c-4162-b280-e2e36c317eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 23)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aus_data_cleaned = aus_data.dropna()\n",
        "\n",
        "# Verifica le dimensioni del DataFrame dopo la pulizia\n",
        "print(aus_data_cleaned.shape)"
      ],
      "metadata": {
        "id": "J-I100PB8DjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3163b3a-3255-401c-cebd-60968d24b7f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2000, 23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aus_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "lyuNxgZ1n3n3",
        "outputId": "d1c67021-ac89-42f2-d632-c751fadf2aa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               image_path      AU01      AU02      AU04  \\\n",
              "0  /content/dataset_resized/001/00000.png  0.441251  0.343375  0.440618   \n",
              "1  /content/dataset_resized/001/00001.png  0.421926  0.363765  0.434397   \n",
              "2  /content/dataset_resized/001/00007.png  0.492545  0.382224  0.621281   \n",
              "3  /content/dataset_resized/001/00024.png  0.391621  0.282561  0.718449   \n",
              "4  /content/dataset_resized/001/00026.png  0.376173  0.305542  0.397151   \n",
              "\n",
              "       AU05      AU06  AU07      AU09      AU10  AU11  ...      AU17  AU20  \\\n",
              "0  0.351839  0.209998   0.0  0.230523  0.001349   0.0  ...  0.430306   1.0   \n",
              "1  0.351787  0.203124   0.0  0.224019  0.008139   0.0  ...  0.433068   1.0   \n",
              "2  0.444887  0.186932   0.0  0.223182  0.000403   0.0  ...  0.482991   1.0   \n",
              "3  0.403961  0.195292   0.0  0.253391  0.033507   1.0  ...  0.424992   1.0   \n",
              "4  0.406158  0.196051   1.0  0.300231  0.001653   0.0  ...  0.424217   1.0   \n",
              "\n",
              "       AU23      AU24      AU25      AU26      AU28      AU43  arousal  \\\n",
              "0  0.272994  0.341583  0.955296  0.763431  0.071433  0.135054      5.0   \n",
              "1  0.267938  0.344917  0.986915  0.800393  0.100110  0.152723      5.0   \n",
              "2  0.181584  0.279610  0.928026  0.693113  0.099127  0.141085      5.0   \n",
              "3  0.480082  0.409115  0.932144  0.593253  0.088416  0.163365      5.0   \n",
              "4  0.324522  0.376909  0.952130  0.631469  0.109218  0.128785      5.0   \n",
              "\n",
              "   valence  \n",
              "0      0.0  \n",
              "1      0.0  \n",
              "2      0.0  \n",
              "3      0.0  \n",
              "4      0.0  \n",
              "\n",
              "[5 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6d9626ba-ac12-4cc3-a629-d548c295605d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>AU01</th>\n",
              "      <th>AU02</th>\n",
              "      <th>AU04</th>\n",
              "      <th>AU05</th>\n",
              "      <th>AU06</th>\n",
              "      <th>AU07</th>\n",
              "      <th>AU09</th>\n",
              "      <th>AU10</th>\n",
              "      <th>AU11</th>\n",
              "      <th>...</th>\n",
              "      <th>AU17</th>\n",
              "      <th>AU20</th>\n",
              "      <th>AU23</th>\n",
              "      <th>AU24</th>\n",
              "      <th>AU25</th>\n",
              "      <th>AU26</th>\n",
              "      <th>AU28</th>\n",
              "      <th>AU43</th>\n",
              "      <th>arousal</th>\n",
              "      <th>valence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/dataset_resized/001/00000.png</td>\n",
              "      <td>0.441251</td>\n",
              "      <td>0.343375</td>\n",
              "      <td>0.440618</td>\n",
              "      <td>0.351839</td>\n",
              "      <td>0.209998</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.230523</td>\n",
              "      <td>0.001349</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.430306</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.272994</td>\n",
              "      <td>0.341583</td>\n",
              "      <td>0.955296</td>\n",
              "      <td>0.763431</td>\n",
              "      <td>0.071433</td>\n",
              "      <td>0.135054</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/dataset_resized/001/00001.png</td>\n",
              "      <td>0.421926</td>\n",
              "      <td>0.363765</td>\n",
              "      <td>0.434397</td>\n",
              "      <td>0.351787</td>\n",
              "      <td>0.203124</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.224019</td>\n",
              "      <td>0.008139</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.433068</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.267938</td>\n",
              "      <td>0.344917</td>\n",
              "      <td>0.986915</td>\n",
              "      <td>0.800393</td>\n",
              "      <td>0.100110</td>\n",
              "      <td>0.152723</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/dataset_resized/001/00007.png</td>\n",
              "      <td>0.492545</td>\n",
              "      <td>0.382224</td>\n",
              "      <td>0.621281</td>\n",
              "      <td>0.444887</td>\n",
              "      <td>0.186932</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.223182</td>\n",
              "      <td>0.000403</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.482991</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.181584</td>\n",
              "      <td>0.279610</td>\n",
              "      <td>0.928026</td>\n",
              "      <td>0.693113</td>\n",
              "      <td>0.099127</td>\n",
              "      <td>0.141085</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/dataset_resized/001/00024.png</td>\n",
              "      <td>0.391621</td>\n",
              "      <td>0.282561</td>\n",
              "      <td>0.718449</td>\n",
              "      <td>0.403961</td>\n",
              "      <td>0.195292</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.253391</td>\n",
              "      <td>0.033507</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.424992</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.480082</td>\n",
              "      <td>0.409115</td>\n",
              "      <td>0.932144</td>\n",
              "      <td>0.593253</td>\n",
              "      <td>0.088416</td>\n",
              "      <td>0.163365</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/dataset_resized/001/00026.png</td>\n",
              "      <td>0.376173</td>\n",
              "      <td>0.305542</td>\n",
              "      <td>0.397151</td>\n",
              "      <td>0.406158</td>\n",
              "      <td>0.196051</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.300231</td>\n",
              "      <td>0.001653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.424217</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.324522</td>\n",
              "      <td>0.376909</td>\n",
              "      <td>0.952130</td>\n",
              "      <td>0.631469</td>\n",
              "      <td>0.109218</td>\n",
              "      <td>0.128785</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 23 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d9626ba-ac12-4cc3-a629-d548c295605d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6d9626ba-ac12-4cc3-a629-d548c295605d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6d9626ba-ac12-4cc3-a629-d548c295605d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ed6c9dca-93c2-4946-8a8c-5c9a833b476e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ed6c9dca-93c2-4946-8a8c-5c9a833b476e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ed6c9dca-93c2-4946-8a8c-5c9a833b476e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcoliamo la media dei valori di arousal e valence\n",
        "media_arousal = aus_data['arousal'].mean()\n",
        "media_valence = aus_data['valence'].mean()\n",
        "\n",
        "media_arousal, media_valence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws1d3F88o6Ea",
        "outputId": "32cfe7f0-4d9c-4f6e-aff7-11710a4c1d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.94, -0.4625)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(aus_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4ejGyEDD21A",
        "outputId": "b4069c44-d033-4c9d-99c0-8af19c93fdf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6000"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aus_data.to_csv('afew_keyframes.csv', index=False)"
      ],
      "metadata": {
        "id": "ZNJ37Gym7XnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Combination of Landmarks"
      ],
      "metadata": {
        "id": "1K8QmUDx7DFc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vWHzhjs7d4wf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "cfc580fb-3957-47d2-f11d-98122fe75bf7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               image_path      AU01      AU02      AU04  \\\n",
              "0  /content/dataset_resized/401/00015.png  0.311357  0.135927  0.798931   \n",
              "1  /content/dataset_resized/401/00016.png  0.291428  0.148411  0.881824   \n",
              "2  /content/dataset_resized/401/00018.png  0.340948  0.113658  0.831665   \n",
              "3  /content/dataset_resized/401/00031.png  0.291201  0.127205  0.756636   \n",
              "4  /content/dataset_resized/401/00046.png  0.360045  0.199449  0.859479   \n",
              "\n",
              "       AU05      AU06  AU07      AU09      AU10  AU11  ...      AU17  AU20  \\\n",
              "0  0.328358  0.113616   1.0  0.493366  0.029253   0.0  ...  0.257176   1.0   \n",
              "1  0.365231  0.104657   1.0  0.360925  0.062726   0.0  ...  0.327704   1.0   \n",
              "2  0.340745  0.084021   0.0  0.353916  0.018393   0.0  ...  0.309095   1.0   \n",
              "3  0.326717  0.118116   0.0  0.433923  0.005935   0.0  ...  0.297202   1.0   \n",
              "4  0.252883  0.122171   1.0  0.447879  0.063832   0.0  ...  0.340059   1.0   \n",
              "\n",
              "       AU23      AU24      AU25      AU26      AU28      AU43  arousal  \\\n",
              "0  0.719543  0.268569  0.993881  0.885669  0.501640  0.786895      1.0   \n",
              "1  0.655670  0.382354  0.990813  0.953209  0.437312  0.711133      1.0   \n",
              "2  0.407399  0.346851  0.978948  0.869907  0.266384  0.741665      1.0   \n",
              "3  0.264385  0.346684  0.999072  0.740485  0.712587  0.701903      2.0   \n",
              "4  0.582021  0.231151  0.994853  0.659481  0.780542  0.535933      2.0   \n",
              "\n",
              "   valence  \n",
              "0     -1.0  \n",
              "1     -1.0  \n",
              "2     -1.0  \n",
              "3     -1.0  \n",
              "4     -1.0  \n",
              "\n",
              "[5 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-112ec6cd-a49d-4dab-8c28-47f45415ba91\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>AU01</th>\n",
              "      <th>AU02</th>\n",
              "      <th>AU04</th>\n",
              "      <th>AU05</th>\n",
              "      <th>AU06</th>\n",
              "      <th>AU07</th>\n",
              "      <th>AU09</th>\n",
              "      <th>AU10</th>\n",
              "      <th>AU11</th>\n",
              "      <th>...</th>\n",
              "      <th>AU17</th>\n",
              "      <th>AU20</th>\n",
              "      <th>AU23</th>\n",
              "      <th>AU24</th>\n",
              "      <th>AU25</th>\n",
              "      <th>AU26</th>\n",
              "      <th>AU28</th>\n",
              "      <th>AU43</th>\n",
              "      <th>arousal</th>\n",
              "      <th>valence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/dataset_resized/401/00015.png</td>\n",
              "      <td>0.311357</td>\n",
              "      <td>0.135927</td>\n",
              "      <td>0.798931</td>\n",
              "      <td>0.328358</td>\n",
              "      <td>0.113616</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.493366</td>\n",
              "      <td>0.029253</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.257176</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.719543</td>\n",
              "      <td>0.268569</td>\n",
              "      <td>0.993881</td>\n",
              "      <td>0.885669</td>\n",
              "      <td>0.501640</td>\n",
              "      <td>0.786895</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/dataset_resized/401/00016.png</td>\n",
              "      <td>0.291428</td>\n",
              "      <td>0.148411</td>\n",
              "      <td>0.881824</td>\n",
              "      <td>0.365231</td>\n",
              "      <td>0.104657</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.360925</td>\n",
              "      <td>0.062726</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.327704</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.655670</td>\n",
              "      <td>0.382354</td>\n",
              "      <td>0.990813</td>\n",
              "      <td>0.953209</td>\n",
              "      <td>0.437312</td>\n",
              "      <td>0.711133</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/dataset_resized/401/00018.png</td>\n",
              "      <td>0.340948</td>\n",
              "      <td>0.113658</td>\n",
              "      <td>0.831665</td>\n",
              "      <td>0.340745</td>\n",
              "      <td>0.084021</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.353916</td>\n",
              "      <td>0.018393</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.309095</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.407399</td>\n",
              "      <td>0.346851</td>\n",
              "      <td>0.978948</td>\n",
              "      <td>0.869907</td>\n",
              "      <td>0.266384</td>\n",
              "      <td>0.741665</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/dataset_resized/401/00031.png</td>\n",
              "      <td>0.291201</td>\n",
              "      <td>0.127205</td>\n",
              "      <td>0.756636</td>\n",
              "      <td>0.326717</td>\n",
              "      <td>0.118116</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.433923</td>\n",
              "      <td>0.005935</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.297202</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.264385</td>\n",
              "      <td>0.346684</td>\n",
              "      <td>0.999072</td>\n",
              "      <td>0.740485</td>\n",
              "      <td>0.712587</td>\n",
              "      <td>0.701903</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/dataset_resized/401/00046.png</td>\n",
              "      <td>0.360045</td>\n",
              "      <td>0.199449</td>\n",
              "      <td>0.859479</td>\n",
              "      <td>0.252883</td>\n",
              "      <td>0.122171</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.447879</td>\n",
              "      <td>0.063832</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.340059</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.582021</td>\n",
              "      <td>0.231151</td>\n",
              "      <td>0.994853</td>\n",
              "      <td>0.659481</td>\n",
              "      <td>0.780542</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 23 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-112ec6cd-a49d-4dab-8c28-47f45415ba91')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-112ec6cd-a49d-4dab-8c28-47f45415ba91 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-112ec6cd-a49d-4dab-8c28-47f45415ba91');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-671dea20-bbe7-4e98-a39e-34349bce29d4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-671dea20-bbe7-4e98-a39e-34349bce29d4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-671dea20-bbe7-4e98-a39e-34349bce29d4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# Percorso del file CSV\n",
        "file_path = 'data/afew_keyframes.csv'\n",
        "\n",
        "# Carica il DataFrame dal file CSV\n",
        "aus_data = pd.read_csv(file_path)\n",
        "\n",
        "aus_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrOzhcPh67QL",
        "outputId": "1765ba62-1782-40f0-bdd1-b06c53176315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0faa7dc697b8>:39: RuntimeWarning: divide by zero encountered in long_scalars\n",
            "  sum_x = sum_x + land_x[i] * (1/land_x[j])\n",
            "<ipython-input-8-0faa7dc697b8>:39: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  sum_x = sum_x + land_x[i] * (1/land_x[j])\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "columns_names = [f'Landmark_x_{i}' for i in range(68)]\n",
        "columns_names1 = [f'Landmark_y_{i}' for i in range(68)]\n",
        "\n",
        "all_columns_names = columns_names + columns_names1\n",
        "\n",
        "df = pd.DataFrame(columns=all_columns_names)\n",
        "\n",
        "for index, row in aus_data.iterrows():\n",
        "\n",
        "  directory_path, file_name = os.path.split(row['image_path'])\n",
        "  directory_path = directory_path.replace('_resized', '')\n",
        "  json_name = directory_path.split('/')[3]\n",
        "  file_name = file_name.split('.')[0]\n",
        "\n",
        "  json_path = f'{directory_path}/{json_name}.json'\n",
        "  with open(json_path, 'r') as file:\n",
        "      json_data = json.load(file)\n",
        "\n",
        "  landmarks = json_data['frames'][file_name]['landmarks']\n",
        "  #print(f\"Frame ID: {frame_id}\")\n",
        "\n",
        "  vec_comb_lin_x = []\n",
        "  vec_comb_lin_y = []\n",
        "\n",
        "  land_x = np.array([int(x) for x, _ in landmarks])\n",
        "  land_y = np.array([int(y) for _, y in landmarks])\n",
        "      #print(f\"Landmark {idx + 1}: ({x}, {y})\")\n",
        "\n",
        "  for i in range(68):\n",
        "    sum_x = 0\n",
        "    sum_y = 0\n",
        "\n",
        "    for j in range(68):\n",
        "      sum_x = sum_x + land_x[i] * (1/land_x[j])\n",
        "      sum_y = sum_y + land_y[i] * (1/land_y[j])\n",
        "\n",
        "    vec_comb_lin_x.append(sum_x)\n",
        "    vec_comb_lin_y.append(sum_y)\n",
        "\n",
        "  all_landmark = vec_comb_lin_x + vec_comb_lin_y\n",
        "\n",
        "  new_df = pd.DataFrame([all_landmark], columns=df.columns)\n",
        "\n",
        "  # Concatenate the original DataFrame and the new DataFrame\n",
        "  df = pd.concat([df, new_df], ignore_index=True)\n",
        "\n",
        "  #return pd.DataFrame(aus_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "UqZeeCFXqHGX",
        "outputId": "7503bd68-0c05-48f3-d4fe-353d6943251d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Landmark_x_0  Landmark_x_1  Landmark_x_2  Landmark_x_3  Landmark_x_4  \\\n",
              "0        50.012623     50.229128     51.311652     53.043691     55.858254   \n",
              "1        50.213808     50.430246     51.512440     53.243951     56.057656   \n",
              "2        50.123587     50.340572     51.425498     52.944394     55.765202   \n",
              "3        49.159634     49.612718     50.518886     52.557765     55.729354   \n",
              "4        48.385258     48.621284     49.565386     51.217566     54.285899   \n",
              "...            ...           ...           ...           ...           ...   \n",
              "5995     57.384650     57.384650     58.197464     59.660528     61.286156   \n",
              "5996     57.507875     57.507875     58.317845     59.775792     61.557726   \n",
              "5997     56.870898     56.870898     57.647823     59.201673     61.066292   \n",
              "5998     56.737361     56.586464     57.190053     58.548128     60.509792   \n",
              "5999     56.501308     56.351039     57.102386     58.755350     61.009392   \n",
              "\n",
              "      Landmark_x_5  Landmark_x_6  Landmark_x_7  Landmark_x_8  Landmark_x_9  \\\n",
              "0        59.322332     62.786410     66.683498     70.580585     73.611653   \n",
              "1        59.520677     63.200137     67.096036     70.991935     73.805640   \n",
              "2        58.802995     62.491744     66.397478     70.303212     73.341005   \n",
              "3        59.354028     63.205243     67.509543     71.587301     74.758890   \n",
              "4        57.826284     62.074746     66.559233     71.279746     75.056156   \n",
              "...            ...           ...           ...           ...           ...   \n",
              "5995     63.074346     65.025099     67.788666     71.527609     75.266552   \n",
              "5996     63.177666     65.121594     67.713498     71.277367     75.165223   \n",
              "5997     63.241682     65.261686     67.747846     71.010930     74.429400   \n",
              "5998     62.622353     64.584018     66.998373     69.865421     72.883366   \n",
              "5999     63.113164     64.916397     67.170439     69.875288     72.580138   \n",
              "\n",
              "      ...  Landmark_y_58  Landmark_y_59  Landmark_y_60  Landmark_y_61  \\\n",
              "0     ...      88.867585      87.158593      82.886113      80.322625   \n",
              "1     ...      88.436996      87.015179      82.749730      80.474823   \n",
              "2     ...      88.126393      86.700399      82.707618      80.996426   \n",
              "3     ...      88.725191      87.013451      82.734101      80.166491   \n",
              "4     ...      87.698894      86.544962      83.083163      81.063781   \n",
              "...   ...            ...            ...            ...            ...   \n",
              "5995  ...      92.540042      90.953641      86.987640      84.608038   \n",
              "5996  ...      91.264913      90.215891      86.806569      84.970781   \n",
              "5997  ...      91.284954      89.711076      86.825632      86.038693   \n",
              "5998  ...      94.327038      92.114087      87.688185      85.751853   \n",
              "5999  ...      95.423319      92.926778      87.933698      85.714551   \n",
              "\n",
              "      Landmark_y_62  Landmark_y_63  Landmark_y_64  Landmark_y_65  \\\n",
              "0         80.037793      79.752961      81.177121      85.164769   \n",
              "1         80.190460      79.906096      81.043550      84.740273   \n",
              "2         80.711227      80.426028      81.281624      84.418810   \n",
              "3         80.166491      79.595911      80.737071      84.731131   \n",
              "4         81.063781      80.486814      81.352264      83.948613   \n",
              "...             ...            ...            ...            ...   \n",
              "5995      84.343638      83.814838      84.079238      88.309640   \n",
              "5996      84.708525      84.184014      83.921759      87.331080   \n",
              "5997      86.038693      85.514066      84.989440      87.350258   \n",
              "5998      85.751853      85.475234      86.028472      90.454374   \n",
              "5999      85.714551      85.437157      86.269337      91.262418   \n",
              "\n",
              "      Landmark_y_66  Landmark_y_67  \n",
              "0         85.734433      85.734433  \n",
              "1         85.593363      85.593363  \n",
              "2         85.274406      85.274406  \n",
              "3         85.587001      85.587001  \n",
              "4         84.525579      84.525579  \n",
              "...             ...            ...  \n",
              "5995      89.102840      89.102840  \n",
              "5996      88.117847      88.117847  \n",
              "5997      87.874884      87.874884  \n",
              "5998      91.007612      90.730993  \n",
              "5999      91.817205      91.539811  \n",
              "\n",
              "[6000 rows x 136 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1af4447e-85c1-4d0a-9ea9-c90bbbc2dce3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Landmark_x_0</th>\n",
              "      <th>Landmark_x_1</th>\n",
              "      <th>Landmark_x_2</th>\n",
              "      <th>Landmark_x_3</th>\n",
              "      <th>Landmark_x_4</th>\n",
              "      <th>Landmark_x_5</th>\n",
              "      <th>Landmark_x_6</th>\n",
              "      <th>Landmark_x_7</th>\n",
              "      <th>Landmark_x_8</th>\n",
              "      <th>Landmark_x_9</th>\n",
              "      <th>...</th>\n",
              "      <th>Landmark_y_58</th>\n",
              "      <th>Landmark_y_59</th>\n",
              "      <th>Landmark_y_60</th>\n",
              "      <th>Landmark_y_61</th>\n",
              "      <th>Landmark_y_62</th>\n",
              "      <th>Landmark_y_63</th>\n",
              "      <th>Landmark_y_64</th>\n",
              "      <th>Landmark_y_65</th>\n",
              "      <th>Landmark_y_66</th>\n",
              "      <th>Landmark_y_67</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50.012623</td>\n",
              "      <td>50.229128</td>\n",
              "      <td>51.311652</td>\n",
              "      <td>53.043691</td>\n",
              "      <td>55.858254</td>\n",
              "      <td>59.322332</td>\n",
              "      <td>62.786410</td>\n",
              "      <td>66.683498</td>\n",
              "      <td>70.580585</td>\n",
              "      <td>73.611653</td>\n",
              "      <td>...</td>\n",
              "      <td>88.867585</td>\n",
              "      <td>87.158593</td>\n",
              "      <td>82.886113</td>\n",
              "      <td>80.322625</td>\n",
              "      <td>80.037793</td>\n",
              "      <td>79.752961</td>\n",
              "      <td>81.177121</td>\n",
              "      <td>85.164769</td>\n",
              "      <td>85.734433</td>\n",
              "      <td>85.734433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50.213808</td>\n",
              "      <td>50.430246</td>\n",
              "      <td>51.512440</td>\n",
              "      <td>53.243951</td>\n",
              "      <td>56.057656</td>\n",
              "      <td>59.520677</td>\n",
              "      <td>63.200137</td>\n",
              "      <td>67.096036</td>\n",
              "      <td>70.991935</td>\n",
              "      <td>73.805640</td>\n",
              "      <td>...</td>\n",
              "      <td>88.436996</td>\n",
              "      <td>87.015179</td>\n",
              "      <td>82.749730</td>\n",
              "      <td>80.474823</td>\n",
              "      <td>80.190460</td>\n",
              "      <td>79.906096</td>\n",
              "      <td>81.043550</td>\n",
              "      <td>84.740273</td>\n",
              "      <td>85.593363</td>\n",
              "      <td>85.593363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50.123587</td>\n",
              "      <td>50.340572</td>\n",
              "      <td>51.425498</td>\n",
              "      <td>52.944394</td>\n",
              "      <td>55.765202</td>\n",
              "      <td>58.802995</td>\n",
              "      <td>62.491744</td>\n",
              "      <td>66.397478</td>\n",
              "      <td>70.303212</td>\n",
              "      <td>73.341005</td>\n",
              "      <td>...</td>\n",
              "      <td>88.126393</td>\n",
              "      <td>86.700399</td>\n",
              "      <td>82.707618</td>\n",
              "      <td>80.996426</td>\n",
              "      <td>80.711227</td>\n",
              "      <td>80.426028</td>\n",
              "      <td>81.281624</td>\n",
              "      <td>84.418810</td>\n",
              "      <td>85.274406</td>\n",
              "      <td>85.274406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>49.159634</td>\n",
              "      <td>49.612718</td>\n",
              "      <td>50.518886</td>\n",
              "      <td>52.557765</td>\n",
              "      <td>55.729354</td>\n",
              "      <td>59.354028</td>\n",
              "      <td>63.205243</td>\n",
              "      <td>67.509543</td>\n",
              "      <td>71.587301</td>\n",
              "      <td>74.758890</td>\n",
              "      <td>...</td>\n",
              "      <td>88.725191</td>\n",
              "      <td>87.013451</td>\n",
              "      <td>82.734101</td>\n",
              "      <td>80.166491</td>\n",
              "      <td>80.166491</td>\n",
              "      <td>79.595911</td>\n",
              "      <td>80.737071</td>\n",
              "      <td>84.731131</td>\n",
              "      <td>85.587001</td>\n",
              "      <td>85.587001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>48.385258</td>\n",
              "      <td>48.621284</td>\n",
              "      <td>49.565386</td>\n",
              "      <td>51.217566</td>\n",
              "      <td>54.285899</td>\n",
              "      <td>57.826284</td>\n",
              "      <td>62.074746</td>\n",
              "      <td>66.559233</td>\n",
              "      <td>71.279746</td>\n",
              "      <td>75.056156</td>\n",
              "      <td>...</td>\n",
              "      <td>87.698894</td>\n",
              "      <td>86.544962</td>\n",
              "      <td>83.083163</td>\n",
              "      <td>81.063781</td>\n",
              "      <td>81.063781</td>\n",
              "      <td>80.486814</td>\n",
              "      <td>81.352264</td>\n",
              "      <td>83.948613</td>\n",
              "      <td>84.525579</td>\n",
              "      <td>84.525579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>57.384650</td>\n",
              "      <td>57.384650</td>\n",
              "      <td>58.197464</td>\n",
              "      <td>59.660528</td>\n",
              "      <td>61.286156</td>\n",
              "      <td>63.074346</td>\n",
              "      <td>65.025099</td>\n",
              "      <td>67.788666</td>\n",
              "      <td>71.527609</td>\n",
              "      <td>75.266552</td>\n",
              "      <td>...</td>\n",
              "      <td>92.540042</td>\n",
              "      <td>90.953641</td>\n",
              "      <td>86.987640</td>\n",
              "      <td>84.608038</td>\n",
              "      <td>84.343638</td>\n",
              "      <td>83.814838</td>\n",
              "      <td>84.079238</td>\n",
              "      <td>88.309640</td>\n",
              "      <td>89.102840</td>\n",
              "      <td>89.102840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>57.507875</td>\n",
              "      <td>57.507875</td>\n",
              "      <td>58.317845</td>\n",
              "      <td>59.775792</td>\n",
              "      <td>61.557726</td>\n",
              "      <td>63.177666</td>\n",
              "      <td>65.121594</td>\n",
              "      <td>67.713498</td>\n",
              "      <td>71.277367</td>\n",
              "      <td>75.165223</td>\n",
              "      <td>...</td>\n",
              "      <td>91.264913</td>\n",
              "      <td>90.215891</td>\n",
              "      <td>86.806569</td>\n",
              "      <td>84.970781</td>\n",
              "      <td>84.708525</td>\n",
              "      <td>84.184014</td>\n",
              "      <td>83.921759</td>\n",
              "      <td>87.331080</td>\n",
              "      <td>88.117847</td>\n",
              "      <td>88.117847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>56.870898</td>\n",
              "      <td>56.870898</td>\n",
              "      <td>57.647823</td>\n",
              "      <td>59.201673</td>\n",
              "      <td>61.066292</td>\n",
              "      <td>63.241682</td>\n",
              "      <td>65.261686</td>\n",
              "      <td>67.747846</td>\n",
              "      <td>71.010930</td>\n",
              "      <td>74.429400</td>\n",
              "      <td>...</td>\n",
              "      <td>91.284954</td>\n",
              "      <td>89.711076</td>\n",
              "      <td>86.825632</td>\n",
              "      <td>86.038693</td>\n",
              "      <td>86.038693</td>\n",
              "      <td>85.514066</td>\n",
              "      <td>84.989440</td>\n",
              "      <td>87.350258</td>\n",
              "      <td>87.874884</td>\n",
              "      <td>87.874884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>56.737361</td>\n",
              "      <td>56.586464</td>\n",
              "      <td>57.190053</td>\n",
              "      <td>58.548128</td>\n",
              "      <td>60.509792</td>\n",
              "      <td>62.622353</td>\n",
              "      <td>64.584018</td>\n",
              "      <td>66.998373</td>\n",
              "      <td>69.865421</td>\n",
              "      <td>72.883366</td>\n",
              "      <td>...</td>\n",
              "      <td>94.327038</td>\n",
              "      <td>92.114087</td>\n",
              "      <td>87.688185</td>\n",
              "      <td>85.751853</td>\n",
              "      <td>85.751853</td>\n",
              "      <td>85.475234</td>\n",
              "      <td>86.028472</td>\n",
              "      <td>90.454374</td>\n",
              "      <td>91.007612</td>\n",
              "      <td>90.730993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>56.501308</td>\n",
              "      <td>56.351039</td>\n",
              "      <td>57.102386</td>\n",
              "      <td>58.755350</td>\n",
              "      <td>61.009392</td>\n",
              "      <td>63.113164</td>\n",
              "      <td>64.916397</td>\n",
              "      <td>67.170439</td>\n",
              "      <td>69.875288</td>\n",
              "      <td>72.580138</td>\n",
              "      <td>...</td>\n",
              "      <td>95.423319</td>\n",
              "      <td>92.926778</td>\n",
              "      <td>87.933698</td>\n",
              "      <td>85.714551</td>\n",
              "      <td>85.714551</td>\n",
              "      <td>85.437157</td>\n",
              "      <td>86.269337</td>\n",
              "      <td>91.262418</td>\n",
              "      <td>91.817205</td>\n",
              "      <td>91.539811</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 136 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1af4447e-85c1-4d0a-9ea9-c90bbbc2dce3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1af4447e-85c1-4d0a-9ea9-c90bbbc2dce3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1af4447e-85c1-4d0a-9ea9-c90bbbc2dce3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-11040472-14b5-4bcb-bc6f-c560f2b42b17\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-11040472-14b5-4bcb-bc6f-c560f2b42b17')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-11040472-14b5-4bcb-bc6f-c560f2b42b17 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "97UznpGJJLK8"
      },
      "outputs": [],
      "source": [
        "merged_df = pd.merge(df, aus_data, left_index=True, right_index=True, how='inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "KbHJ5aE_Jeu3",
        "outputId": "9c04e2c0-aaa0-48e2-936e-362a1de55799"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Landmark_x_0  Landmark_x_1  Landmark_x_2  Landmark_x_3  Landmark_x_4  \\\n",
              "0        50.012623     50.229128     51.311652     53.043691     55.858254   \n",
              "1        50.213808     50.430246     51.512440     53.243951     56.057656   \n",
              "2        50.123587     50.340572     51.425498     52.944394     55.765202   \n",
              "3        49.159634     49.612718     50.518886     52.557765     55.729354   \n",
              "4        48.385258     48.621284     49.565386     51.217566     54.285899   \n",
              "...            ...           ...           ...           ...           ...   \n",
              "5995     57.384650     57.384650     58.197464     59.660528     61.286156   \n",
              "5996     57.507875     57.507875     58.317845     59.775792     61.557726   \n",
              "5997     56.870898     56.870898     57.647823     59.201673     61.066292   \n",
              "5998     56.737361     56.586464     57.190053     58.548128     60.509792   \n",
              "5999     56.501308     56.351039     57.102386     58.755350     61.009392   \n",
              "\n",
              "      Landmark_x_5  Landmark_x_6  Landmark_x_7  Landmark_x_8  Landmark_x_9  \\\n",
              "0        59.322332     62.786410     66.683498     70.580585     73.611653   \n",
              "1        59.520677     63.200137     67.096036     70.991935     73.805640   \n",
              "2        58.802995     62.491744     66.397478     70.303212     73.341005   \n",
              "3        59.354028     63.205243     67.509543     71.587301     74.758890   \n",
              "4        57.826284     62.074746     66.559233     71.279746     75.056156   \n",
              "...            ...           ...           ...           ...           ...   \n",
              "5995     63.074346     65.025099     67.788666     71.527609     75.266552   \n",
              "5996     63.177666     65.121594     67.713498     71.277367     75.165223   \n",
              "5997     63.241682     65.261686     67.747846     71.010930     74.429400   \n",
              "5998     62.622353     64.584018     66.998373     69.865421     72.883366   \n",
              "5999     63.113164     64.916397     67.170439     69.875288     72.580138   \n",
              "\n",
              "      ...      AU17  AU20      AU23      AU24      AU25      AU26      AU28  \\\n",
              "0     ...  0.257176   1.0  0.719543  0.268569  0.993881  0.885669  0.501640   \n",
              "1     ...  0.327704   1.0  0.655670  0.382354  0.990813  0.953209  0.437312   \n",
              "2     ...  0.309095   1.0  0.407399  0.346851  0.978948  0.869907  0.266384   \n",
              "3     ...  0.297202   1.0  0.264385  0.346684  0.999072  0.740485  0.712587   \n",
              "4     ...  0.340059   1.0  0.582021  0.231151  0.994853  0.659481  0.780542   \n",
              "...   ...       ...   ...       ...       ...       ...       ...       ...   \n",
              "5995  ...  0.539658   0.0  0.722313  0.064561  0.998532  0.746642  0.063520   \n",
              "5996  ...  0.545938   0.0  0.603435  0.156507  0.986948  0.566380  0.103323   \n",
              "5997  ...  0.508652   0.0  0.459939  0.352326  0.965488  0.837143  0.633322   \n",
              "5998  ...  0.545037   0.0  0.535025  0.066318  0.992853  0.892109  0.421667   \n",
              "5999  ...  0.421382   1.0  0.407309  0.047096  0.998882  0.789521  0.164114   \n",
              "\n",
              "          AU43  arousal  valence  \n",
              "0     0.786895      1.0     -1.0  \n",
              "1     0.711133      1.0     -1.0  \n",
              "2     0.741665      1.0     -1.0  \n",
              "3     0.701903      2.0     -1.0  \n",
              "4     0.535933      2.0     -1.0  \n",
              "...        ...      ...      ...  \n",
              "5995  0.549602      3.0     -1.0  \n",
              "5996  0.467108      3.0     -1.0  \n",
              "5997  0.098899      2.0      0.0  \n",
              "5998  0.059766      2.0      0.0  \n",
              "5999  0.124826      2.0      0.0  \n",
              "\n",
              "[6000 rows x 159 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-81fda5f8-8fdc-45b2-8fab-992b6c293864\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Landmark_x_0</th>\n",
              "      <th>Landmark_x_1</th>\n",
              "      <th>Landmark_x_2</th>\n",
              "      <th>Landmark_x_3</th>\n",
              "      <th>Landmark_x_4</th>\n",
              "      <th>Landmark_x_5</th>\n",
              "      <th>Landmark_x_6</th>\n",
              "      <th>Landmark_x_7</th>\n",
              "      <th>Landmark_x_8</th>\n",
              "      <th>Landmark_x_9</th>\n",
              "      <th>...</th>\n",
              "      <th>AU17</th>\n",
              "      <th>AU20</th>\n",
              "      <th>AU23</th>\n",
              "      <th>AU24</th>\n",
              "      <th>AU25</th>\n",
              "      <th>AU26</th>\n",
              "      <th>AU28</th>\n",
              "      <th>AU43</th>\n",
              "      <th>arousal</th>\n",
              "      <th>valence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50.012623</td>\n",
              "      <td>50.229128</td>\n",
              "      <td>51.311652</td>\n",
              "      <td>53.043691</td>\n",
              "      <td>55.858254</td>\n",
              "      <td>59.322332</td>\n",
              "      <td>62.786410</td>\n",
              "      <td>66.683498</td>\n",
              "      <td>70.580585</td>\n",
              "      <td>73.611653</td>\n",
              "      <td>...</td>\n",
              "      <td>0.257176</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.719543</td>\n",
              "      <td>0.268569</td>\n",
              "      <td>0.993881</td>\n",
              "      <td>0.885669</td>\n",
              "      <td>0.501640</td>\n",
              "      <td>0.786895</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50.213808</td>\n",
              "      <td>50.430246</td>\n",
              "      <td>51.512440</td>\n",
              "      <td>53.243951</td>\n",
              "      <td>56.057656</td>\n",
              "      <td>59.520677</td>\n",
              "      <td>63.200137</td>\n",
              "      <td>67.096036</td>\n",
              "      <td>70.991935</td>\n",
              "      <td>73.805640</td>\n",
              "      <td>...</td>\n",
              "      <td>0.327704</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.655670</td>\n",
              "      <td>0.382354</td>\n",
              "      <td>0.990813</td>\n",
              "      <td>0.953209</td>\n",
              "      <td>0.437312</td>\n",
              "      <td>0.711133</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50.123587</td>\n",
              "      <td>50.340572</td>\n",
              "      <td>51.425498</td>\n",
              "      <td>52.944394</td>\n",
              "      <td>55.765202</td>\n",
              "      <td>58.802995</td>\n",
              "      <td>62.491744</td>\n",
              "      <td>66.397478</td>\n",
              "      <td>70.303212</td>\n",
              "      <td>73.341005</td>\n",
              "      <td>...</td>\n",
              "      <td>0.309095</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.407399</td>\n",
              "      <td>0.346851</td>\n",
              "      <td>0.978948</td>\n",
              "      <td>0.869907</td>\n",
              "      <td>0.266384</td>\n",
              "      <td>0.741665</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>49.159634</td>\n",
              "      <td>49.612718</td>\n",
              "      <td>50.518886</td>\n",
              "      <td>52.557765</td>\n",
              "      <td>55.729354</td>\n",
              "      <td>59.354028</td>\n",
              "      <td>63.205243</td>\n",
              "      <td>67.509543</td>\n",
              "      <td>71.587301</td>\n",
              "      <td>74.758890</td>\n",
              "      <td>...</td>\n",
              "      <td>0.297202</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.264385</td>\n",
              "      <td>0.346684</td>\n",
              "      <td>0.999072</td>\n",
              "      <td>0.740485</td>\n",
              "      <td>0.712587</td>\n",
              "      <td>0.701903</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>48.385258</td>\n",
              "      <td>48.621284</td>\n",
              "      <td>49.565386</td>\n",
              "      <td>51.217566</td>\n",
              "      <td>54.285899</td>\n",
              "      <td>57.826284</td>\n",
              "      <td>62.074746</td>\n",
              "      <td>66.559233</td>\n",
              "      <td>71.279746</td>\n",
              "      <td>75.056156</td>\n",
              "      <td>...</td>\n",
              "      <td>0.340059</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.582021</td>\n",
              "      <td>0.231151</td>\n",
              "      <td>0.994853</td>\n",
              "      <td>0.659481</td>\n",
              "      <td>0.780542</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>57.384650</td>\n",
              "      <td>57.384650</td>\n",
              "      <td>58.197464</td>\n",
              "      <td>59.660528</td>\n",
              "      <td>61.286156</td>\n",
              "      <td>63.074346</td>\n",
              "      <td>65.025099</td>\n",
              "      <td>67.788666</td>\n",
              "      <td>71.527609</td>\n",
              "      <td>75.266552</td>\n",
              "      <td>...</td>\n",
              "      <td>0.539658</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.722313</td>\n",
              "      <td>0.064561</td>\n",
              "      <td>0.998532</td>\n",
              "      <td>0.746642</td>\n",
              "      <td>0.063520</td>\n",
              "      <td>0.549602</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>57.507875</td>\n",
              "      <td>57.507875</td>\n",
              "      <td>58.317845</td>\n",
              "      <td>59.775792</td>\n",
              "      <td>61.557726</td>\n",
              "      <td>63.177666</td>\n",
              "      <td>65.121594</td>\n",
              "      <td>67.713498</td>\n",
              "      <td>71.277367</td>\n",
              "      <td>75.165223</td>\n",
              "      <td>...</td>\n",
              "      <td>0.545938</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.603435</td>\n",
              "      <td>0.156507</td>\n",
              "      <td>0.986948</td>\n",
              "      <td>0.566380</td>\n",
              "      <td>0.103323</td>\n",
              "      <td>0.467108</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>56.870898</td>\n",
              "      <td>56.870898</td>\n",
              "      <td>57.647823</td>\n",
              "      <td>59.201673</td>\n",
              "      <td>61.066292</td>\n",
              "      <td>63.241682</td>\n",
              "      <td>65.261686</td>\n",
              "      <td>67.747846</td>\n",
              "      <td>71.010930</td>\n",
              "      <td>74.429400</td>\n",
              "      <td>...</td>\n",
              "      <td>0.508652</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.459939</td>\n",
              "      <td>0.352326</td>\n",
              "      <td>0.965488</td>\n",
              "      <td>0.837143</td>\n",
              "      <td>0.633322</td>\n",
              "      <td>0.098899</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>56.737361</td>\n",
              "      <td>56.586464</td>\n",
              "      <td>57.190053</td>\n",
              "      <td>58.548128</td>\n",
              "      <td>60.509792</td>\n",
              "      <td>62.622353</td>\n",
              "      <td>64.584018</td>\n",
              "      <td>66.998373</td>\n",
              "      <td>69.865421</td>\n",
              "      <td>72.883366</td>\n",
              "      <td>...</td>\n",
              "      <td>0.545037</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.535025</td>\n",
              "      <td>0.066318</td>\n",
              "      <td>0.992853</td>\n",
              "      <td>0.892109</td>\n",
              "      <td>0.421667</td>\n",
              "      <td>0.059766</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>56.501308</td>\n",
              "      <td>56.351039</td>\n",
              "      <td>57.102386</td>\n",
              "      <td>58.755350</td>\n",
              "      <td>61.009392</td>\n",
              "      <td>63.113164</td>\n",
              "      <td>64.916397</td>\n",
              "      <td>67.170439</td>\n",
              "      <td>69.875288</td>\n",
              "      <td>72.580138</td>\n",
              "      <td>...</td>\n",
              "      <td>0.421382</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.407309</td>\n",
              "      <td>0.047096</td>\n",
              "      <td>0.998882</td>\n",
              "      <td>0.789521</td>\n",
              "      <td>0.164114</td>\n",
              "      <td>0.124826</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 159 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-81fda5f8-8fdc-45b2-8fab-992b6c293864')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-81fda5f8-8fdc-45b2-8fab-992b6c293864 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-81fda5f8-8fdc-45b2-8fab-992b6c293864');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e7c7a32f-2494-4033-b47c-ab65b2c7c01c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e7c7a32f-2494-4033-b47c-ab65b2c7c01c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e7c7a32f-2494-4033-b47c-ab65b2c7c01c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "iT6Rl8bTNLNg"
      },
      "outputs": [],
      "source": [
        "merged_df = merged_df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sAz-LSvJy9Iy"
      },
      "outputs": [],
      "source": [
        "merged_df = merged_df.drop(['image_path', 'arousal', 'valence'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "sYQEeyDYJ32b",
        "outputId": "f1b298d2-b553-429e-ba06-883ec096442d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Landmark_x_0  Landmark_x_1  Landmark_x_2  Landmark_x_3  Landmark_x_4  \\\n",
              "0        50.012623     50.229128     51.311652     53.043691     55.858254   \n",
              "1        50.213808     50.430246     51.512440     53.243951     56.057656   \n",
              "2        50.123587     50.340572     51.425498     52.944394     55.765202   \n",
              "3        49.159634     49.612718     50.518886     52.557765     55.729354   \n",
              "4        48.385258     48.621284     49.565386     51.217566     54.285899   \n",
              "...            ...           ...           ...           ...           ...   \n",
              "5995     57.384650     57.384650     58.197464     59.660528     61.286156   \n",
              "5996     57.507875     57.507875     58.317845     59.775792     61.557726   \n",
              "5997     56.870898     56.870898     57.647823     59.201673     61.066292   \n",
              "5998     56.737361     56.586464     57.190053     58.548128     60.509792   \n",
              "5999     56.501308     56.351039     57.102386     58.755350     61.009392   \n",
              "\n",
              "      Landmark_x_5  Landmark_x_6  Landmark_x_7  Landmark_x_8  Landmark_x_9  \\\n",
              "0        59.322332     62.786410     66.683498     70.580585     73.611653   \n",
              "1        59.520677     63.200137     67.096036     70.991935     73.805640   \n",
              "2        58.802995     62.491744     66.397478     70.303212     73.341005   \n",
              "3        59.354028     63.205243     67.509543     71.587301     74.758890   \n",
              "4        57.826284     62.074746     66.559233     71.279746     75.056156   \n",
              "...            ...           ...           ...           ...           ...   \n",
              "5995     63.074346     65.025099     67.788666     71.527609     75.266552   \n",
              "5996     63.177666     65.121594     67.713498     71.277367     75.165223   \n",
              "5997     63.241682     65.261686     67.747846     71.010930     74.429400   \n",
              "5998     62.622353     64.584018     66.998373     69.865421     72.883366   \n",
              "5999     63.113164     64.916397     67.170439     69.875288     72.580138   \n",
              "\n",
              "      ...      AU14      AU15      AU17  AU20      AU23      AU24      AU25  \\\n",
              "0     ...  0.194991  0.467736  0.257176   1.0  0.719543  0.268569  0.993881   \n",
              "1     ...  0.206847  0.223989  0.327704   1.0  0.655670  0.382354  0.990813   \n",
              "2     ...  0.180410  0.385084  0.309095   1.0  0.407399  0.346851  0.978948   \n",
              "3     ...  0.203248  0.433349  0.297202   1.0  0.264385  0.346684  0.999072   \n",
              "4     ...  0.355545  0.677510  0.340059   1.0  0.582021  0.231151  0.994853   \n",
              "...   ...       ...       ...       ...   ...       ...       ...       ...   \n",
              "5995  ...  0.291543  0.753380  0.539658   0.0  0.722313  0.064561  0.998532   \n",
              "5996  ...  0.200482  0.404790  0.545938   0.0  0.603435  0.156507  0.986948   \n",
              "5997  ...  0.141673  0.341747  0.508652   0.0  0.459939  0.352326  0.965488   \n",
              "5998  ...  0.119963  0.570278  0.545037   0.0  0.535025  0.066318  0.992853   \n",
              "5999  ...  0.231830  0.700732  0.421382   1.0  0.407309  0.047096  0.998882   \n",
              "\n",
              "          AU26      AU28      AU43  \n",
              "0     0.885669  0.501640  0.786895  \n",
              "1     0.953209  0.437312  0.711133  \n",
              "2     0.869907  0.266384  0.741665  \n",
              "3     0.740485  0.712587  0.701903  \n",
              "4     0.659481  0.780542  0.535933  \n",
              "...        ...       ...       ...  \n",
              "5995  0.746642  0.063520  0.549602  \n",
              "5996  0.566380  0.103323  0.467108  \n",
              "5997  0.837143  0.633322  0.098899  \n",
              "5998  0.892109  0.421667  0.059766  \n",
              "5999  0.789521  0.164114  0.124826  \n",
              "\n",
              "[5999 rows x 156 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-983d18c1-c922-42c2-bae7-7313cc5d6968\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Landmark_x_0</th>\n",
              "      <th>Landmark_x_1</th>\n",
              "      <th>Landmark_x_2</th>\n",
              "      <th>Landmark_x_3</th>\n",
              "      <th>Landmark_x_4</th>\n",
              "      <th>Landmark_x_5</th>\n",
              "      <th>Landmark_x_6</th>\n",
              "      <th>Landmark_x_7</th>\n",
              "      <th>Landmark_x_8</th>\n",
              "      <th>Landmark_x_9</th>\n",
              "      <th>...</th>\n",
              "      <th>AU14</th>\n",
              "      <th>AU15</th>\n",
              "      <th>AU17</th>\n",
              "      <th>AU20</th>\n",
              "      <th>AU23</th>\n",
              "      <th>AU24</th>\n",
              "      <th>AU25</th>\n",
              "      <th>AU26</th>\n",
              "      <th>AU28</th>\n",
              "      <th>AU43</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50.012623</td>\n",
              "      <td>50.229128</td>\n",
              "      <td>51.311652</td>\n",
              "      <td>53.043691</td>\n",
              "      <td>55.858254</td>\n",
              "      <td>59.322332</td>\n",
              "      <td>62.786410</td>\n",
              "      <td>66.683498</td>\n",
              "      <td>70.580585</td>\n",
              "      <td>73.611653</td>\n",
              "      <td>...</td>\n",
              "      <td>0.194991</td>\n",
              "      <td>0.467736</td>\n",
              "      <td>0.257176</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.719543</td>\n",
              "      <td>0.268569</td>\n",
              "      <td>0.993881</td>\n",
              "      <td>0.885669</td>\n",
              "      <td>0.501640</td>\n",
              "      <td>0.786895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50.213808</td>\n",
              "      <td>50.430246</td>\n",
              "      <td>51.512440</td>\n",
              "      <td>53.243951</td>\n",
              "      <td>56.057656</td>\n",
              "      <td>59.520677</td>\n",
              "      <td>63.200137</td>\n",
              "      <td>67.096036</td>\n",
              "      <td>70.991935</td>\n",
              "      <td>73.805640</td>\n",
              "      <td>...</td>\n",
              "      <td>0.206847</td>\n",
              "      <td>0.223989</td>\n",
              "      <td>0.327704</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.655670</td>\n",
              "      <td>0.382354</td>\n",
              "      <td>0.990813</td>\n",
              "      <td>0.953209</td>\n",
              "      <td>0.437312</td>\n",
              "      <td>0.711133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50.123587</td>\n",
              "      <td>50.340572</td>\n",
              "      <td>51.425498</td>\n",
              "      <td>52.944394</td>\n",
              "      <td>55.765202</td>\n",
              "      <td>58.802995</td>\n",
              "      <td>62.491744</td>\n",
              "      <td>66.397478</td>\n",
              "      <td>70.303212</td>\n",
              "      <td>73.341005</td>\n",
              "      <td>...</td>\n",
              "      <td>0.180410</td>\n",
              "      <td>0.385084</td>\n",
              "      <td>0.309095</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.407399</td>\n",
              "      <td>0.346851</td>\n",
              "      <td>0.978948</td>\n",
              "      <td>0.869907</td>\n",
              "      <td>0.266384</td>\n",
              "      <td>0.741665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>49.159634</td>\n",
              "      <td>49.612718</td>\n",
              "      <td>50.518886</td>\n",
              "      <td>52.557765</td>\n",
              "      <td>55.729354</td>\n",
              "      <td>59.354028</td>\n",
              "      <td>63.205243</td>\n",
              "      <td>67.509543</td>\n",
              "      <td>71.587301</td>\n",
              "      <td>74.758890</td>\n",
              "      <td>...</td>\n",
              "      <td>0.203248</td>\n",
              "      <td>0.433349</td>\n",
              "      <td>0.297202</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.264385</td>\n",
              "      <td>0.346684</td>\n",
              "      <td>0.999072</td>\n",
              "      <td>0.740485</td>\n",
              "      <td>0.712587</td>\n",
              "      <td>0.701903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>48.385258</td>\n",
              "      <td>48.621284</td>\n",
              "      <td>49.565386</td>\n",
              "      <td>51.217566</td>\n",
              "      <td>54.285899</td>\n",
              "      <td>57.826284</td>\n",
              "      <td>62.074746</td>\n",
              "      <td>66.559233</td>\n",
              "      <td>71.279746</td>\n",
              "      <td>75.056156</td>\n",
              "      <td>...</td>\n",
              "      <td>0.355545</td>\n",
              "      <td>0.677510</td>\n",
              "      <td>0.340059</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.582021</td>\n",
              "      <td>0.231151</td>\n",
              "      <td>0.994853</td>\n",
              "      <td>0.659481</td>\n",
              "      <td>0.780542</td>\n",
              "      <td>0.535933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>57.384650</td>\n",
              "      <td>57.384650</td>\n",
              "      <td>58.197464</td>\n",
              "      <td>59.660528</td>\n",
              "      <td>61.286156</td>\n",
              "      <td>63.074346</td>\n",
              "      <td>65.025099</td>\n",
              "      <td>67.788666</td>\n",
              "      <td>71.527609</td>\n",
              "      <td>75.266552</td>\n",
              "      <td>...</td>\n",
              "      <td>0.291543</td>\n",
              "      <td>0.753380</td>\n",
              "      <td>0.539658</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.722313</td>\n",
              "      <td>0.064561</td>\n",
              "      <td>0.998532</td>\n",
              "      <td>0.746642</td>\n",
              "      <td>0.063520</td>\n",
              "      <td>0.549602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>57.507875</td>\n",
              "      <td>57.507875</td>\n",
              "      <td>58.317845</td>\n",
              "      <td>59.775792</td>\n",
              "      <td>61.557726</td>\n",
              "      <td>63.177666</td>\n",
              "      <td>65.121594</td>\n",
              "      <td>67.713498</td>\n",
              "      <td>71.277367</td>\n",
              "      <td>75.165223</td>\n",
              "      <td>...</td>\n",
              "      <td>0.200482</td>\n",
              "      <td>0.404790</td>\n",
              "      <td>0.545938</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.603435</td>\n",
              "      <td>0.156507</td>\n",
              "      <td>0.986948</td>\n",
              "      <td>0.566380</td>\n",
              "      <td>0.103323</td>\n",
              "      <td>0.467108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>56.870898</td>\n",
              "      <td>56.870898</td>\n",
              "      <td>57.647823</td>\n",
              "      <td>59.201673</td>\n",
              "      <td>61.066292</td>\n",
              "      <td>63.241682</td>\n",
              "      <td>65.261686</td>\n",
              "      <td>67.747846</td>\n",
              "      <td>71.010930</td>\n",
              "      <td>74.429400</td>\n",
              "      <td>...</td>\n",
              "      <td>0.141673</td>\n",
              "      <td>0.341747</td>\n",
              "      <td>0.508652</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.459939</td>\n",
              "      <td>0.352326</td>\n",
              "      <td>0.965488</td>\n",
              "      <td>0.837143</td>\n",
              "      <td>0.633322</td>\n",
              "      <td>0.098899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>56.737361</td>\n",
              "      <td>56.586464</td>\n",
              "      <td>57.190053</td>\n",
              "      <td>58.548128</td>\n",
              "      <td>60.509792</td>\n",
              "      <td>62.622353</td>\n",
              "      <td>64.584018</td>\n",
              "      <td>66.998373</td>\n",
              "      <td>69.865421</td>\n",
              "      <td>72.883366</td>\n",
              "      <td>...</td>\n",
              "      <td>0.119963</td>\n",
              "      <td>0.570278</td>\n",
              "      <td>0.545037</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.535025</td>\n",
              "      <td>0.066318</td>\n",
              "      <td>0.992853</td>\n",
              "      <td>0.892109</td>\n",
              "      <td>0.421667</td>\n",
              "      <td>0.059766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>56.501308</td>\n",
              "      <td>56.351039</td>\n",
              "      <td>57.102386</td>\n",
              "      <td>58.755350</td>\n",
              "      <td>61.009392</td>\n",
              "      <td>63.113164</td>\n",
              "      <td>64.916397</td>\n",
              "      <td>67.170439</td>\n",
              "      <td>69.875288</td>\n",
              "      <td>72.580138</td>\n",
              "      <td>...</td>\n",
              "      <td>0.231830</td>\n",
              "      <td>0.700732</td>\n",
              "      <td>0.421382</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.407309</td>\n",
              "      <td>0.047096</td>\n",
              "      <td>0.998882</td>\n",
              "      <td>0.789521</td>\n",
              "      <td>0.164114</td>\n",
              "      <td>0.124826</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5999 rows × 156 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-983d18c1-c922-42c2-bae7-7313cc5d6968')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-983d18c1-c922-42c2-bae7-7313cc5d6968 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-983d18c1-c922-42c2-bae7-7313cc5d6968');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a9614138-bc70-4cef-80f6-594cb6a7f14a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a9614138-bc70-4cef-80f6-594cb6a7f14a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a9614138-bc70-4cef-80f6-594cb6a7f14a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 293
        }
      ],
      "source": [
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.to_csv('landmarks.csv', index=False)"
      ],
      "metadata": {
        "id": "iQv-hD0i7vL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train our detection model"
      ],
      "metadata": {
        "id": "VZSDvupc77Uk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rLLc1DrKFTp",
        "outputId": "d2fa3d9e-91a3-44e2-c7bb-a1672d97e20b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_56\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_336 (Dense)           (None, 256)               35072     \n",
            "                                                                 \n",
            " batch_normalization_280 (B  (None, 256)               1024      \n",
            " atchNormalization)                                              \n",
            "                                                                 \n",
            " dropout_280 (Dropout)       (None, 256)               0         \n",
            "                                                                 \n",
            " dense_337 (Dense)           (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_281 (B  (None, 128)               512       \n",
            " atchNormalization)                                              \n",
            "                                                                 \n",
            " dropout_281 (Dropout)       (None, 128)               0         \n",
            "                                                                 \n",
            " dense_338 (Dense)           (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_282 (B  (None, 64)                256       \n",
            " atchNormalization)                                              \n",
            "                                                                 \n",
            " dropout_282 (Dropout)       (None, 64)                0         \n",
            "                                                                 \n",
            " dense_339 (Dense)           (None, 32)                2080      \n",
            "                                                                 \n",
            " batch_normalization_283 (B  (None, 32)                128       \n",
            " atchNormalization)                                              \n",
            "                                                                 \n",
            " dropout_283 (Dropout)       (None, 32)                0         \n",
            "                                                                 \n",
            " dense_340 (Dense)           (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_284 (B  (None, 16)                64        \n",
            " atchNormalization)                                              \n",
            "                                                                 \n",
            " dropout_284 (Dropout)       (None, 16)                0         \n",
            "                                                                 \n",
            " dense_341 (Dense)           (None, 20)                340       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 81156 (317.02 KB)\n",
            "Trainable params: 80164 (313.14 KB)\n",
            "Non-trainable params: 992 (3.88 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Nuovi parametri ottimizzati\n",
        "learning_rate_opt = 8.574491575670956e-05\n",
        "dropout_rate_opt = 0.23569115438975155\n",
        "batch_size_opt = 32\n",
        "epochs_opt = 200\n",
        "\n",
        "# Creazione del modello con i nuovi parametri\n",
        "model = models.Sequential()\n",
        "model.add(layers.InputLayer(input_shape=(136,)))\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(dropout_rate_opt))\n",
        "\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(dropout_rate_opt))\n",
        "\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(dropout_rate_opt))\n",
        "\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(dropout_rate_opt))\n",
        "\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(dropout_rate_opt))\n",
        "\n",
        "model.add(layers.Dense(20, activation='linear'))\n",
        "\n",
        "# Creazione del programma di decadimento esponenziale\n",
        "lr_schedule = ExponentialDecay(learning_rate_opt, decay_steps=10000, decay_rate=0.9, staircase=True)\n",
        "\n",
        "# Cambio dell'ottimizzatore a RMSprop con i nuovi parametri\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
        "\n",
        "# Definizione della callback di early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                               patience=12,\n",
        "                               restore_best_weights=True,\n",
        "                               verbose=1)\n",
        "\n",
        "# Compilazione del modello con il nuovo ottimizzatore\n",
        "model.compile(optimizer=optimizer, loss=tf.keras.losses.LogCosh(), metrics=['mae'])\n",
        "\n",
        "# Stampa della struttura del modello\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MHRN8KfCMcgu"
      },
      "outputs": [],
      "source": [
        "aus_names_list = merged_df.columns.to_list()[136:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "zcib0cYOcRWN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = merged_df.drop(aus_names_list, axis=1)\n",
        "y = merged_df[aus_names_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "rzpu5uJxKjOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5775531c-34eb-4842-8854-2644f6f82987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "120/120 [==============================] - 5s 13ms/step - loss: 0.4827 - mae: 0.9269 - val_loss: 1.7669 - val_mae: 2.3838\n",
            "Epoch 2/200\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4404 - mae: 0.8734 - val_loss: 0.3217 - val_mae: 0.7365\n",
            "Epoch 3/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4016 - mae: 0.8225 - val_loss: 0.2191 - val_mae: 0.5738\n",
            "Epoch 4/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3672 - mae: 0.7775 - val_loss: 0.1965 - val_mae: 0.5307\n",
            "Epoch 5/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3336 - mae: 0.7318 - val_loss: 0.2154 - val_mae: 0.5701\n",
            "Epoch 6/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3043 - mae: 0.6905 - val_loss: 0.1783 - val_mae: 0.5111\n",
            "Epoch 7/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2773 - mae: 0.6531 - val_loss: 0.1836 - val_mae: 0.5159\n",
            "Epoch 8/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2517 - mae: 0.6154 - val_loss: 0.1351 - val_mae: 0.4276\n",
            "Epoch 9/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2313 - mae: 0.5856 - val_loss: 0.1219 - val_mae: 0.4049\n",
            "Epoch 10/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2096 - mae: 0.5517 - val_loss: 0.1127 - val_mae: 0.3886\n",
            "Epoch 11/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1917 - mae: 0.5237 - val_loss: 0.0966 - val_mae: 0.3567\n",
            "Epoch 12/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1760 - mae: 0.4977 - val_loss: 0.0903 - val_mae: 0.3443\n",
            "Epoch 13/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1612 - mae: 0.4727 - val_loss: 0.0817 - val_mae: 0.3252\n",
            "Epoch 14/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1496 - mae: 0.4528 - val_loss: 0.0844 - val_mae: 0.3298\n",
            "Epoch 15/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1383 - mae: 0.4332 - val_loss: 0.0794 - val_mae: 0.3186\n",
            "Epoch 16/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1295 - mae: 0.4167 - val_loss: 0.0691 - val_mae: 0.2954\n",
            "Epoch 17/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1195 - mae: 0.3984 - val_loss: 0.0682 - val_mae: 0.2927\n",
            "Epoch 18/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1107 - mae: 0.3819 - val_loss: 0.0633 - val_mae: 0.2824\n",
            "Epoch 19/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1046 - mae: 0.3703 - val_loss: 0.0609 - val_mae: 0.2766\n",
            "Epoch 20/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0979 - mae: 0.3569 - val_loss: 0.0560 - val_mae: 0.2654\n",
            "Epoch 21/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0928 - mae: 0.3458 - val_loss: 0.0536 - val_mae: 0.2606\n",
            "Epoch 22/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0878 - mae: 0.3357 - val_loss: 0.0520 - val_mae: 0.2575\n",
            "Epoch 23/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0811 - mae: 0.3221 - val_loss: 0.0494 - val_mae: 0.2522\n",
            "Epoch 24/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0783 - mae: 0.3155 - val_loss: 0.0473 - val_mae: 0.2470\n",
            "Epoch 25/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0736 - mae: 0.3060 - val_loss: 0.0453 - val_mae: 0.2427\n",
            "Epoch 26/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0698 - mae: 0.2972 - val_loss: 0.0445 - val_mae: 0.2409\n",
            "Epoch 27/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0678 - mae: 0.2929 - val_loss: 0.0434 - val_mae: 0.2397\n",
            "Epoch 28/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0647 - mae: 0.2861 - val_loss: 0.0426 - val_mae: 0.2374\n",
            "Epoch 29/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0616 - mae: 0.2790 - val_loss: 0.0416 - val_mae: 0.2342\n",
            "Epoch 30/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0593 - mae: 0.2734 - val_loss: 0.0411 - val_mae: 0.2326\n",
            "Epoch 31/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0578 - mae: 0.2700 - val_loss: 0.0412 - val_mae: 0.2324\n",
            "Epoch 32/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0564 - mae: 0.2664 - val_loss: 0.0403 - val_mae: 0.2307\n",
            "Epoch 33/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0549 - mae: 0.2625 - val_loss: 0.0399 - val_mae: 0.2293\n",
            "Epoch 34/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0531 - mae: 0.2582 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 35/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0525 - mae: 0.2566 - val_loss: 0.0393 - val_mae: 0.2289\n",
            "Epoch 36/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0506 - mae: 0.2518 - val_loss: 0.0392 - val_mae: 0.2260\n",
            "Epoch 37/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0498 - mae: 0.2494 - val_loss: 0.0390 - val_mae: 0.2243\n",
            "Epoch 38/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0488 - mae: 0.2471 - val_loss: 0.0389 - val_mae: 0.2253\n",
            "Epoch 39/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0482 - mae: 0.2448 - val_loss: 0.0390 - val_mae: 0.2247\n",
            "Epoch 40/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0473 - mae: 0.2428 - val_loss: 0.0379 - val_mae: 0.2213\n",
            "Epoch 41/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0464 - mae: 0.2404 - val_loss: 0.0384 - val_mae: 0.2227\n",
            "Epoch 42/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0452 - mae: 0.2369 - val_loss: 0.0383 - val_mae: 0.2213\n",
            "Epoch 43/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0441 - mae: 0.2341 - val_loss: 0.0387 - val_mae: 0.2216\n",
            "Epoch 44/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0436 - mae: 0.2320 - val_loss: 0.0373 - val_mae: 0.2183\n",
            "Epoch 45/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0426 - mae: 0.2298 - val_loss: 0.0362 - val_mae: 0.2163\n",
            "Epoch 46/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0421 - mae: 0.2279 - val_loss: 0.0384 - val_mae: 0.2207\n",
            "Epoch 47/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0416 - mae: 0.2262 - val_loss: 0.0364 - val_mae: 0.2147\n",
            "Epoch 48/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0409 - mae: 0.2243 - val_loss: 0.0355 - val_mae: 0.2125\n",
            "Epoch 49/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0406 - mae: 0.2230 - val_loss: 0.0372 - val_mae: 0.2162\n",
            "Epoch 50/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0399 - mae: 0.2211 - val_loss: 0.0343 - val_mae: 0.2083\n",
            "Epoch 51/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0395 - mae: 0.2197 - val_loss: 0.0357 - val_mae: 0.2124\n",
            "Epoch 52/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0389 - mae: 0.2181 - val_loss: 0.0362 - val_mae: 0.2105\n",
            "Epoch 53/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0384 - mae: 0.2163 - val_loss: 0.0347 - val_mae: 0.2065\n",
            "Epoch 54/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0381 - mae: 0.2152 - val_loss: 0.0365 - val_mae: 0.2097\n",
            "Epoch 55/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0376 - mae: 0.2139 - val_loss: 0.0405 - val_mae: 0.2181\n",
            "Epoch 56/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0374 - mae: 0.2131 - val_loss: 0.0346 - val_mae: 0.2044\n",
            "Epoch 57/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0369 - mae: 0.2113 - val_loss: 0.0344 - val_mae: 0.2037\n",
            "Epoch 58/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0367 - mae: 0.2104 - val_loss: 0.0364 - val_mae: 0.2112\n",
            "Epoch 59/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0363 - mae: 0.2097 - val_loss: 0.0352 - val_mae: 0.2076\n",
            "Epoch 60/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0358 - mae: 0.2077 - val_loss: 0.0336 - val_mae: 0.2032\n",
            "Epoch 61/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0358 - mae: 0.2074 - val_loss: 0.0342 - val_mae: 0.2041\n",
            "Epoch 62/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0353 - mae: 0.2060 - val_loss: 0.0319 - val_mae: 0.2003\n",
            "Epoch 63/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0353 - mae: 0.2058 - val_loss: 0.0330 - val_mae: 0.2024\n",
            "Epoch 64/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0349 - mae: 0.2046 - val_loss: 0.0337 - val_mae: 0.2014\n",
            "Epoch 65/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0346 - mae: 0.2037 - val_loss: 0.0329 - val_mae: 0.2003\n",
            "Epoch 66/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0347 - mae: 0.2035 - val_loss: 0.0340 - val_mae: 0.1999\n",
            "Epoch 67/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0345 - mae: 0.2028 - val_loss: 0.0328 - val_mae: 0.1971\n",
            "Epoch 68/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0342 - mae: 0.2019 - val_loss: 0.0362 - val_mae: 0.2051\n",
            "Epoch 69/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0340 - mae: 0.2015 - val_loss: 0.0315 - val_mae: 0.1946\n",
            "Epoch 70/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0340 - mae: 0.2010 - val_loss: 0.0351 - val_mae: 0.2074\n",
            "Epoch 71/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0336 - mae: 0.1997 - val_loss: 0.0324 - val_mae: 0.2025\n",
            "Epoch 72/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0337 - mae: 0.1999 - val_loss: 0.0321 - val_mae: 0.1972\n",
            "Epoch 73/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0335 - mae: 0.1992 - val_loss: 0.0321 - val_mae: 0.1954\n",
            "Epoch 74/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0333 - mae: 0.1984 - val_loss: 0.0324 - val_mae: 0.1980\n",
            "Epoch 75/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0331 - mae: 0.1979 - val_loss: 0.0411 - val_mae: 0.2255\n",
            "Epoch 76/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0328 - mae: 0.1970 - val_loss: 0.0323 - val_mae: 0.2012\n",
            "Epoch 77/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0332 - mae: 0.1978 - val_loss: 0.0389 - val_mae: 0.2118\n",
            "Epoch 78/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0329 - mae: 0.1971 - val_loss: 0.0347 - val_mae: 0.2012\n",
            "Epoch 79/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0328 - mae: 0.1965 - val_loss: 0.0343 - val_mae: 0.2009\n",
            "Epoch 80/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0327 - mae: 0.1963 - val_loss: 0.0397 - val_mae: 0.2146\n",
            "Epoch 81/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0326 - mae: 0.1959 - val_loss: 0.0307 - val_mae: 0.1934\n",
            "Epoch 82/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0324 - mae: 0.1951 - val_loss: 0.0338 - val_mae: 0.1975\n",
            "Epoch 83/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0323 - mae: 0.1946 - val_loss: 0.0319 - val_mae: 0.1942\n",
            "Epoch 84/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0326 - mae: 0.1955 - val_loss: 0.0312 - val_mae: 0.1932\n",
            "Epoch 85/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0322 - mae: 0.1944 - val_loss: 0.0336 - val_mae: 0.1986\n",
            "Epoch 86/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0320 - mae: 0.1937 - val_loss: 0.0307 - val_mae: 0.1947\n",
            "Epoch 87/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0321 - mae: 0.1938 - val_loss: 0.0310 - val_mae: 0.1935\n",
            "Epoch 88/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0319 - mae: 0.1932 - val_loss: 0.0302 - val_mae: 0.1935\n",
            "Epoch 89/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0321 - mae: 0.1936 - val_loss: 0.0393 - val_mae: 0.2113\n",
            "Epoch 90/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0320 - mae: 0.1931 - val_loss: 0.0333 - val_mae: 0.1985\n",
            "Epoch 91/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0319 - mae: 0.1933 - val_loss: 0.0301 - val_mae: 0.1922\n",
            "Epoch 92/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0318 - mae: 0.1931 - val_loss: 0.0347 - val_mae: 0.2004\n",
            "Epoch 93/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0317 - mae: 0.1924 - val_loss: 0.0371 - val_mae: 0.2055\n",
            "Epoch 94/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0316 - mae: 0.1920 - val_loss: 0.0335 - val_mae: 0.1973\n",
            "Epoch 95/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0318 - mae: 0.1926 - val_loss: 0.0309 - val_mae: 0.1956\n",
            "Epoch 96/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0316 - mae: 0.1919 - val_loss: 0.0299 - val_mae: 0.1889\n",
            "Epoch 97/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0317 - mae: 0.1925 - val_loss: 0.0301 - val_mae: 0.1888\n",
            "Epoch 98/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0315 - mae: 0.1919 - val_loss: 0.0331 - val_mae: 0.1963\n",
            "Epoch 99/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0315 - mae: 0.1916 - val_loss: 0.0322 - val_mae: 0.1950\n",
            "Epoch 100/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0315 - mae: 0.1919 - val_loss: 0.0296 - val_mae: 0.1891\n",
            "Epoch 101/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0313 - mae: 0.1911 - val_loss: 0.0323 - val_mae: 0.1947\n",
            "Epoch 102/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0312 - mae: 0.1906 - val_loss: 0.0317 - val_mae: 0.1915\n",
            "Epoch 103/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0315 - mae: 0.1916 - val_loss: 0.0512 - val_mae: 0.2395\n",
            "Epoch 104/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0313 - mae: 0.1908 - val_loss: 0.0313 - val_mae: 0.1911\n",
            "Epoch 105/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0311 - mae: 0.1902 - val_loss: 0.0358 - val_mae: 0.2036\n",
            "Epoch 106/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0314 - mae: 0.1908 - val_loss: 0.0345 - val_mae: 0.2026\n",
            "Epoch 107/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0312 - mae: 0.1904 - val_loss: 0.0345 - val_mae: 0.1985\n",
            "Epoch 108/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0312 - mae: 0.1907 - val_loss: 0.0295 - val_mae: 0.1875\n",
            "Epoch 109/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0311 - mae: 0.1901 - val_loss: 0.0322 - val_mae: 0.1954\n",
            "Epoch 110/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0312 - mae: 0.1903 - val_loss: 0.0348 - val_mae: 0.2026\n",
            "Epoch 111/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0311 - mae: 0.1903 - val_loss: 0.0302 - val_mae: 0.1920\n",
            "Epoch 112/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0312 - mae: 0.1902 - val_loss: 0.0350 - val_mae: 0.2019\n",
            "Epoch 113/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0309 - mae: 0.1895 - val_loss: 0.0410 - val_mae: 0.2253\n",
            "Epoch 114/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0313 - mae: 0.1909 - val_loss: 0.0318 - val_mae: 0.2008\n",
            "Epoch 115/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0313 - mae: 0.1907 - val_loss: 0.0338 - val_mae: 0.1976\n",
            "Epoch 116/200\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 0.0311 - mae: 0.1902 - val_loss: 0.0354 - val_mae: 0.2024\n",
            "Epoch 117/200\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.0310 - mae: 0.1898 - val_loss: 0.0345 - val_mae: 0.1995\n",
            "Epoch 118/200\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0312 - mae: 0.1902 - val_loss: 0.0301 - val_mae: 0.1916\n",
            "Epoch 119/200\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0308 - mae: 0.1889 - val_loss: 0.0324 - val_mae: 0.1968\n",
            "Epoch 120/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0311 - mae: 0.1900 - val_loss: 0.0301 - val_mae: 0.1870\n",
            "Epoch 121/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0310 - mae: 0.1896 - val_loss: 0.0290 - val_mae: 0.1860\n",
            "Epoch 122/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0309 - mae: 0.1895 - val_loss: 0.0397 - val_mae: 0.2168\n",
            "Epoch 123/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0307 - mae: 0.1889 - val_loss: 0.0293 - val_mae: 0.1884\n",
            "Epoch 124/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0311 - mae: 0.1897 - val_loss: 0.0296 - val_mae: 0.1855\n",
            "Epoch 125/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0308 - mae: 0.1887 - val_loss: 0.0359 - val_mae: 0.2013\n",
            "Epoch 126/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0309 - mae: 0.1895 - val_loss: 0.0330 - val_mae: 0.1961\n",
            "Epoch 127/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0309 - mae: 0.1895 - val_loss: 0.0317 - val_mae: 0.1932\n",
            "Epoch 128/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0308 - mae: 0.1890 - val_loss: 0.0341 - val_mae: 0.2022\n",
            "Epoch 129/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0307 - mae: 0.1885 - val_loss: 0.0326 - val_mae: 0.1937\n",
            "Epoch 130/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0307 - mae: 0.1889 - val_loss: 0.0422 - val_mae: 0.2242\n",
            "Epoch 131/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0305 - mae: 0.1877 - val_loss: 0.0299 - val_mae: 0.1863\n",
            "Epoch 132/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0306 - mae: 0.1881 - val_loss: 0.0299 - val_mae: 0.1873\n",
            "Epoch 133/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0306 - mae: 0.1878 - val_loss: 0.0294 - val_mae: 0.1899\n",
            "Epoch 134/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0305 - mae: 0.1876 - val_loss: 0.0342 - val_mae: 0.1984\n",
            "Epoch 135/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0305 - mae: 0.1876 - val_loss: 0.0355 - val_mae: 0.2013\n",
            "Epoch 136/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0307 - mae: 0.1884 - val_loss: 0.0291 - val_mae: 0.1843\n",
            "Epoch 137/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0305 - mae: 0.1879 - val_loss: 0.0319 - val_mae: 0.1903\n",
            "Epoch 138/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0307 - mae: 0.1885 - val_loss: 0.0353 - val_mae: 0.2112\n",
            "Epoch 139/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0304 - mae: 0.1875 - val_loss: 0.0379 - val_mae: 0.2088\n",
            "Epoch 140/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0304 - mae: 0.1872 - val_loss: 0.0330 - val_mae: 0.1954\n",
            "Epoch 141/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0306 - mae: 0.1878 - val_loss: 0.0339 - val_mae: 0.2006\n",
            "Epoch 142/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0305 - mae: 0.1878 - val_loss: 0.0313 - val_mae: 0.1953\n",
            "Epoch 143/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0306 - mae: 0.1880 - val_loss: 0.0373 - val_mae: 0.2077\n",
            "Epoch 144/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0305 - mae: 0.1875 - val_loss: 0.0308 - val_mae: 0.1895\n",
            "Epoch 145/200\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0304 - mae: 0.1875 - val_loss: 0.0306 - val_mae: 0.1915\n",
            "Epoch 146/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0304 - mae: 0.1871 - val_loss: 0.0316 - val_mae: 0.1898\n",
            "Epoch 147/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0305 - mae: 0.1878 - val_loss: 0.0301 - val_mae: 0.1876\n",
            "Epoch 148/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0305 - mae: 0.1877 - val_loss: 0.0315 - val_mae: 0.1959\n",
            "Epoch 149/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0302 - mae: 0.1867 - val_loss: 0.0301 - val_mae: 0.1876\n",
            "Epoch 150/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0304 - mae: 0.1873 - val_loss: 0.0319 - val_mae: 0.1919\n",
            "Epoch 151/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0303 - mae: 0.1868 - val_loss: 0.0320 - val_mae: 0.1922\n",
            "Epoch 152/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0303 - mae: 0.1869 - val_loss: 0.0313 - val_mae: 0.1890\n",
            "Epoch 153/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0304 - mae: 0.1869 - val_loss: 0.0333 - val_mae: 0.1944\n",
            "Epoch 154/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0304 - mae: 0.1871 - val_loss: 0.0357 - val_mae: 0.2036\n",
            "Epoch 155/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0304 - mae: 0.1872 - val_loss: 0.0292 - val_mae: 0.1874\n",
            "Epoch 156/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0304 - mae: 0.1868 - val_loss: 0.0288 - val_mae: 0.1840\n",
            "Epoch 157/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0303 - mae: 0.1870 - val_loss: 0.0301 - val_mae: 0.1906\n",
            "Epoch 158/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0306 - mae: 0.1876 - val_loss: 0.0296 - val_mae: 0.1858\n",
            "Epoch 159/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0304 - mae: 0.1870 - val_loss: 0.0286 - val_mae: 0.1833\n",
            "Epoch 160/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0303 - mae: 0.1870 - val_loss: 0.0369 - val_mae: 0.2017\n",
            "Epoch 161/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0302 - mae: 0.1865 - val_loss: 0.0320 - val_mae: 0.1901\n",
            "Epoch 162/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0304 - mae: 0.1872 - val_loss: 0.0294 - val_mae: 0.1868\n",
            "Epoch 163/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0302 - mae: 0.1865 - val_loss: 0.0288 - val_mae: 0.1852\n",
            "Epoch 164/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0303 - mae: 0.1867 - val_loss: 0.0306 - val_mae: 0.1895\n",
            "Epoch 165/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0304 - mae: 0.1874 - val_loss: 0.0301 - val_mae: 0.1844\n",
            "Epoch 166/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0302 - mae: 0.1866 - val_loss: 0.0294 - val_mae: 0.1841\n",
            "Epoch 167/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0303 - mae: 0.1866 - val_loss: 0.0309 - val_mae: 0.1861\n",
            "Epoch 168/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0303 - mae: 0.1868 - val_loss: 0.0288 - val_mae: 0.1832\n",
            "Epoch 169/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0301 - mae: 0.1861 - val_loss: 0.0302 - val_mae: 0.1863\n",
            "Epoch 170/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0301 - mae: 0.1861 - val_loss: 0.0327 - val_mae: 0.1991\n",
            "Epoch 171/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0300 - mae: 0.1858 - val_loss: 0.0306 - val_mae: 0.1928\n",
            "Epoch 172/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0300 - mae: 0.1857 - val_loss: 0.0335 - val_mae: 0.1997\n",
            "Epoch 173/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0299 - mae: 0.1855 - val_loss: 0.0323 - val_mae: 0.1904\n",
            "Epoch 174/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0302 - mae: 0.1863 - val_loss: 0.0312 - val_mae: 0.1877\n",
            "Epoch 175/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0300 - mae: 0.1857 - val_loss: 0.0298 - val_mae: 0.1834\n",
            "Epoch 176/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0301 - mae: 0.1857 - val_loss: 0.0295 - val_mae: 0.1833\n",
            "Epoch 177/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0299 - mae: 0.1851 - val_loss: 0.0347 - val_mae: 0.1992\n",
            "Epoch 178/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0300 - mae: 0.1856 - val_loss: 0.0304 - val_mae: 0.1909\n",
            "Epoch 179/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0303 - mae: 0.1866 - val_loss: 0.0330 - val_mae: 0.1911\n",
            "Epoch 180/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0299 - mae: 0.1854 - val_loss: 0.0318 - val_mae: 0.1908\n",
            "Epoch 181/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0299 - mae: 0.1852 - val_loss: 0.0287 - val_mae: 0.1838\n",
            "Epoch 182/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0301 - mae: 0.1857 - val_loss: 0.0299 - val_mae: 0.1845\n",
            "Epoch 183/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0301 - mae: 0.1858 - val_loss: 0.0308 - val_mae: 0.1892\n",
            "Epoch 184/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0299 - mae: 0.1853 - val_loss: 0.0291 - val_mae: 0.1873\n",
            "Epoch 185/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0302 - mae: 0.1861 - val_loss: 0.0343 - val_mae: 0.1936\n",
            "Epoch 186/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0298 - mae: 0.1849 - val_loss: 0.0322 - val_mae: 0.1893\n",
            "Epoch 187/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0302 - mae: 0.1858 - val_loss: 0.0329 - val_mae: 0.1941\n",
            "Epoch 188/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0298 - mae: 0.1846 - val_loss: 0.0297 - val_mae: 0.1849\n",
            "Epoch 189/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0299 - mae: 0.1848 - val_loss: 0.0338 - val_mae: 0.1934\n",
            "Epoch 190/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0297 - mae: 0.1844 - val_loss: 0.0302 - val_mae: 0.1850\n",
            "Epoch 191/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0300 - mae: 0.1853 - val_loss: 0.0308 - val_mae: 0.1890\n",
            "Epoch 192/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0299 - mae: 0.1854 - val_loss: 0.0299 - val_mae: 0.1851\n",
            "Epoch 193/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0301 - mae: 0.1859 - val_loss: 0.0286 - val_mae: 0.1831\n",
            "Epoch 194/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0299 - mae: 0.1851 - val_loss: 0.0401 - val_mae: 0.2217\n",
            "Epoch 195/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0302 - mae: 0.1864 - val_loss: 0.0365 - val_mae: 0.2064\n",
            "Epoch 196/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0300 - mae: 0.1857 - val_loss: 0.0301 - val_mae: 0.1864\n",
            "Epoch 197/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0300 - mae: 0.1857 - val_loss: 0.0288 - val_mae: 0.1824\n",
            "Epoch 198/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0303 - mae: 0.1864 - val_loss: 0.0354 - val_mae: 0.2022\n",
            "Epoch 199/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0301 - mae: 0.1859 - val_loss: 0.0353 - val_mae: 0.1958\n",
            "Epoch 200/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0299 - mae: 0.1854 - val_loss: 0.0296 - val_mae: 0.1828\n",
            "38/38 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Assuming X_train and X_test are your input features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Addestramento del modello\n",
        "model.fit(X_train, y_train, epochs=epochs_opt, batch_size=batch_size_opt, validation_split=0.2)\n",
        "\n",
        "# Dopo l'addestramento, puoi utilizzare il modello per fare predizioni su nuovi dati\n",
        "# Supponendo che tu abbia dati di test: X_test\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Ora 'predictions' conterrà le previsioni del tuo modello per i dati di test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znaoGQ-INg2R",
        "outputId": "db026a8f-5837-40be-ee3e-78a26a91a958"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - 1s 5ms/step - loss: 0.0297 - mae: 0.1824\n",
            "Mean Squared Error (MSE): 0.02973225899040699\n",
            "Mean Absolute Error (MAE): 0.18241603672504425\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Mean Squared Error (MSE): {loss}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('au_pred_model.h5')"
      ],
      "metadata": {
        "id": "vC21PJ0h8z7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning Model"
      ],
      "metadata": {
        "id": "0--tc80d8NsN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VlBnZ5Vjb9M3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7631aefa-090f-42c4-dabf-973d289b6d92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.24)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.4)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.0 alembic-1.13.1 colorlog-6.8.2 optuna-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c73t3Z-4bvvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8061a0b2-8479-4ab2-c8b9-4e91ce5a839a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 17:59:30,514] A new study created in memory with name: no-name-48322fe4-15ce-4501-ab8f-8204e69d6562\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "30/30 [==============================] - 8s 60ms/step - loss: 0.1909 - mae: 0.4918 - val_loss: 8.4595 - val_mae: 9.1388\n",
            "Epoch 2/200\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0701 - mae: 0.2842 - val_loss: 1.8946 - val_mae: 2.4819\n",
            "Epoch 3/200\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0497 - mae: 0.2460 - val_loss: 1.0004 - val_mae: 1.5133\n",
            "Epoch 4/200\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0432 - mae: 0.2340 - val_loss: 0.1183 - val_mae: 0.4359\n",
            "Epoch 5/200\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0402 - mae: 0.2270 - val_loss: 0.0446 - val_mae: 0.2426\n",
            "Epoch 6/200\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0386 - mae: 0.2222 - val_loss: 0.0421 - val_mae: 0.2308\n",
            "Epoch 7/200\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0373 - mae: 0.2166 - val_loss: 0.0521 - val_mae: 0.2524\n",
            "Epoch 8/200\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0358 - mae: 0.2110 - val_loss: 0.0416 - val_mae: 0.2275\n",
            "Epoch 9/200\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0350 - mae: 0.2071 - val_loss: 0.0589 - val_mae: 0.2644\n",
            "Epoch 10/200\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.0349 - mae: 0.2066 - val_loss: 0.0867 - val_mae: 0.3336\n",
            "Epoch 11/200\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0345 - mae: 0.2050 - val_loss: 0.0698 - val_mae: 0.2953\n",
            "Epoch 12/200\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0340 - mae: 0.2033 - val_loss: 0.0545 - val_mae: 0.2561\n",
            "Epoch 13/200\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0338 - mae: 0.2021 - val_loss: 0.0454 - val_mae: 0.2302\n",
            "Epoch 14/200\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0337 - mae: 0.2020 - val_loss: 0.0559 - val_mae: 0.2609\n",
            "Epoch 15/200\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0336 - mae: 0.2008 - val_loss: 0.0339 - val_mae: 0.2068\n",
            "Epoch 16/200\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0330 - mae: 0.1990 - val_loss: 0.0560 - val_mae: 0.2615\n",
            "Epoch 17/200\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0332 - mae: 0.1997 - val_loss: 0.0688 - val_mae: 0.2915\n",
            "Epoch 18/200\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0328 - mae: 0.1981 - val_loss: 0.0352 - val_mae: 0.2061\n",
            "Epoch 19/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0327 - mae: 0.1975 - val_loss: 0.0768 - val_mae: 0.3074\n",
            "Epoch 20/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0327 - mae: 0.1976 - val_loss: 0.1359 - val_mae: 0.4279\n",
            "Epoch 21/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0324 - mae: 0.1963 - val_loss: 0.1588 - val_mae: 0.4684\n",
            "Epoch 22/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0324 - mae: 0.1960 - val_loss: 0.0592 - val_mae: 0.2665\n",
            "Epoch 23/200\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0324 - mae: 0.1958 - val_loss: 0.0478 - val_mae: 0.2355\n",
            "Epoch 24/200\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0327 - mae: 0.1969 - val_loss: 0.1088 - val_mae: 0.3509\n",
            "Epoch 25/200\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0330 - mae: 0.1978 - val_loss: 0.2245 - val_mae: 0.5357\n",
            "Epoch 26/200\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0330 - mae: 0.1980 - val_loss: 0.2609 - val_mae: 0.5829\n",
            "Epoch 27/200\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.0325 - mae: 0.1959 - val_loss: 0.2560 - val_mae: 0.5629\n",
            "Epoch 28/200\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0327 - mae: 0.1966 - val_loss: 0.4994 - val_mae: 0.8834\n",
            "Epoch 29/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0331 - mae: 0.1983 - val_loss: 0.4735 - val_mae: 0.8384\n",
            "Epoch 30/200\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0331 - mae: 0.1987 - val_loss: 0.3391 - val_mae: 0.6860\n",
            "Epoch 31/200\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0329 - mae: 0.1973 - val_loss: 0.3019 - val_mae: 0.6345\n",
            "Epoch 32/200\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0329 - mae: 0.1974 - val_loss: 0.1903 - val_mae: 0.4808\n",
            "Epoch 33/200\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0327 - mae: 0.1970 - val_loss: 0.1250 - val_mae: 0.3768\n",
            "Epoch 34/200\n",
            "30/30 [==============================] - 0s 17ms/step - loss: 0.0326 - mae: 0.1962 - val_loss: 0.2325 - val_mae: 0.5429\n",
            "Epoch 35/200\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0325 - mae: 0.1961 - val_loss: 0.3615 - val_mae: 0.6991\n",
            "Epoch 36/200\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0324 - mae: 0.1955 - val_loss: 0.1661 - val_mae: 0.4627\n",
            "Epoch 37/200\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0326 - mae: 0.1962 - val_loss: 0.4349 - val_mae: 0.7927\n",
            "Epoch 38/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0328 - mae: 0.1971 - val_loss: 0.4351 - val_mae: 0.7945\n",
            "Epoch 39/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0328 - mae: 0.1970 - val_loss: 0.4774 - val_mae: 0.8465\n",
            "Epoch 40/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0330 - mae: 0.1978 - val_loss: 0.6698 - val_mae: 1.0936\n",
            "Epoch 41/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0323 - mae: 0.1950 - val_loss: 0.4014 - val_mae: 0.7662\n",
            "Epoch 42/200\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0323 - mae: 0.1950 - val_loss: 0.2377 - val_mae: 0.5921\n",
            "Epoch 43/200\n",
            "30/30 [==============================] - 82s 3s/step - loss: 0.0325 - mae: 0.1959 - val_loss: 0.2710 - val_mae: 0.6641\n",
            "Epoch 44/200\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.0327 - mae: 0.1968 - val_loss: 0.2227 - val_mae: 0.5927\n",
            "Epoch 45/200\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0331 - mae: 0.1978 - val_loss: 0.1599 - val_mae: 0.4330\n",
            "Epoch 46/200\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.0334 - mae: 0.1993 - val_loss: 0.2944 - val_mae: 0.6214\n",
            "Epoch 47/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0331 - mae: 0.1982 - val_loss: 0.1256 - val_mae: 0.3809\n",
            "Epoch 48/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0332 - mae: 0.1987 - val_loss: 0.1923 - val_mae: 0.4827\n",
            "Epoch 49/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0331 - mae: 0.1982 - val_loss: 0.1986 - val_mae: 0.4975\n",
            "Epoch 50/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0330 - mae: 0.1979 - val_loss: 0.3885 - val_mae: 0.7385\n",
            "Epoch 51/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0328 - mae: 0.1965 - val_loss: 0.3634 - val_mae: 0.7136\n",
            "Epoch 52/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0328 - mae: 0.1972 - val_loss: 0.4372 - val_mae: 0.8032\n",
            "Epoch 53/200\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0327 - mae: 0.1968 - val_loss: 0.3744 - val_mae: 0.7232\n",
            "Epoch 54/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0325 - mae: 0.1959 - val_loss: 0.2334 - val_mae: 0.5403\n",
            "Epoch 55/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0326 - mae: 0.1962 - val_loss: 0.1090 - val_mae: 0.3609\n",
            "Epoch 56/200\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0326 - mae: 0.1963 - val_loss: 0.2326 - val_mae: 0.5404\n",
            "Epoch 57/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0326 - mae: 0.1965 - val_loss: 0.2562 - val_mae: 0.5725\n",
            "Epoch 58/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0332 - mae: 0.1981 - val_loss: 0.2722 - val_mae: 0.5978\n",
            "Epoch 59/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0334 - mae: 0.1998 - val_loss: 0.4543 - val_mae: 0.8468\n",
            "Epoch 60/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0336 - mae: 0.2001 - val_loss: 0.1741 - val_mae: 0.4619\n",
            "Epoch 61/200\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0346 - mae: 0.2046 - val_loss: 0.2620 - val_mae: 0.5876\n",
            "Epoch 62/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0345 - mae: 0.2041 - val_loss: 0.4229 - val_mae: 0.7924\n",
            "Epoch 63/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0339 - mae: 0.2015 - val_loss: 0.0456 - val_mae: 0.2282\n",
            "Epoch 64/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0337 - mae: 0.2007 - val_loss: 0.0645 - val_mae: 0.2688\n",
            "Epoch 65/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0335 - mae: 0.1998 - val_loss: 0.1683 - val_mae: 0.4544\n",
            "Epoch 66/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0340 - mae: 0.2022 - val_loss: 0.5863 - val_mae: 0.9846\n",
            "Epoch 67/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0337 - mae: 0.2007 - val_loss: 0.0555 - val_mae: 0.2507\n",
            "Epoch 68/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0338 - mae: 0.2010 - val_loss: 0.0549 - val_mae: 0.2473\n",
            "Epoch 69/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0337 - mae: 0.2008 - val_loss: 0.0925 - val_mae: 0.3353\n",
            "Epoch 70/200\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0336 - mae: 0.2006 - val_loss: 0.0494 - val_mae: 0.2351\n",
            "Epoch 71/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0336 - mae: 0.2003 - val_loss: 0.0386 - val_mae: 0.2215\n",
            "Epoch 72/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0334 - mae: 0.1995 - val_loss: 0.0490 - val_mae: 0.2406\n",
            "Epoch 73/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0335 - mae: 0.2002 - val_loss: 0.0624 - val_mae: 0.2682\n",
            "Epoch 74/200\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0337 - mae: 0.2007 - val_loss: 0.0615 - val_mae: 0.2629\n",
            "Epoch 75/200\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0336 - mae: 0.2002 - val_loss: 0.0933 - val_mae: 0.3258\n",
            "Epoch 76/200\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.0334 - mae: 0.1998 - val_loss: 0.1974 - val_mae: 0.4774\n",
            "Epoch 77/200\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0346 - mae: 0.2047 - val_loss: 0.0387 - val_mae: 0.2186\n",
            "Epoch 78/200\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0334 - mae: 0.1998 - val_loss: 0.0361 - val_mae: 0.2076\n",
            "Epoch 79/200\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0335 - mae: 0.1998 - val_loss: 0.0414 - val_mae: 0.2335\n",
            "Epoch 80/200\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0335 - mae: 0.2000 - val_loss: 0.1001 - val_mae: 0.3336\n",
            "Epoch 81/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0334 - mae: 0.1994 - val_loss: 0.0481 - val_mae: 0.2393\n",
            "Epoch 82/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0342 - mae: 0.2026 - val_loss: 0.0919 - val_mae: 0.3181\n",
            "Epoch 83/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0354 - mae: 0.2079 - val_loss: 0.1332 - val_mae: 0.4073\n",
            "Epoch 84/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0353 - mae: 0.2078 - val_loss: 0.0745 - val_mae: 0.2855\n",
            "Epoch 85/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0352 - mae: 0.2075 - val_loss: 0.1134 - val_mae: 0.3701\n",
            "Epoch 86/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0353 - mae: 0.2079 - val_loss: 0.0432 - val_mae: 0.2432\n",
            "Epoch 87/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0349 - mae: 0.2062 - val_loss: 0.1209 - val_mae: 0.3855\n",
            "Epoch 88/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0352 - mae: 0.2074 - val_loss: 0.0480 - val_mae: 0.2510\n",
            "Epoch 89/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0351 - mae: 0.2069 - val_loss: 0.0696 - val_mae: 0.2757\n",
            "Epoch 90/200\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0351 - mae: 0.2070 - val_loss: 0.0368 - val_mae: 0.2218\n",
            "Epoch 91/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0350 - mae: 0.2064 - val_loss: 0.0741 - val_mae: 0.2875\n",
            "Epoch 92/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0350 - mae: 0.2064 - val_loss: 0.0337 - val_mae: 0.2080\n",
            "Epoch 93/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0351 - mae: 0.2070 - val_loss: 0.0847 - val_mae: 0.3270\n",
            "Epoch 94/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0364 - mae: 0.2117 - val_loss: 0.0552 - val_mae: 0.2656\n",
            "Epoch 95/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0400 - val_mae: 0.2304\n",
            "Epoch 96/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0403 - val_mae: 0.2264\n",
            "Epoch 97/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2297\n",
            "Epoch 98/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0402 - val_mae: 0.2254\n",
            "Epoch 99/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0400 - val_mae: 0.2263\n",
            "Epoch 100/200\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0401 - val_mae: 0.2319\n",
            "Epoch 101/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0400 - val_mae: 0.2307\n",
            "Epoch 102/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0403 - val_mae: 0.2260\n",
            "Epoch 103/200\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0413 - val_mae: 0.2247\n",
            "Epoch 104/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0400 - val_mae: 0.2272\n",
            "Epoch 105/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2308\n",
            "Epoch 106/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0401 - val_mae: 0.2278\n",
            "Epoch 107/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0402 - val_mae: 0.2283\n",
            "Epoch 108/200\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0403 - val_mae: 0.2274\n",
            "Epoch 109/200\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0401 - val_mae: 0.2307\n",
            "Epoch 110/200\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2286\n",
            "Epoch 111/200\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0400 - val_mae: 0.2271\n",
            "Epoch 112/200\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0403 - val_mae: 0.2280\n",
            "Epoch 113/200\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0400 - val_mae: 0.2286\n",
            "Epoch 114/200\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0402 - val_mae: 0.2284\n",
            "Epoch 115/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0400 - val_mae: 0.2279\n",
            "Epoch 116/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0400 - val_mae: 0.2274\n",
            "Epoch 117/200\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2284\n",
            "Epoch 118/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 119/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 120/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0400 - val_mae: 0.2273\n",
            "Epoch 121/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0400 - val_mae: 0.2287\n",
            "Epoch 122/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0400 - val_mae: 0.2286\n",
            "Epoch 123/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0400 - val_mae: 0.2278\n",
            "Epoch 124/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0401 - val_mae: 0.2296\n",
            "Epoch 125/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2271 - val_loss: 0.0400 - val_mae: 0.2284\n",
            "Epoch 126/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 127/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 128/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2285\n",
            "Epoch 129/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 130/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 131/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2270\n",
            "Epoch 132/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 133/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 134/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2272\n",
            "Epoch 135/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 136/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 137/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0400 - val_mae: 0.2291\n",
            "Epoch 138/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2272 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 139/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2286\n",
            "Epoch 140/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 141/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2285\n",
            "Epoch 142/200\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 143/200\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2284\n",
            "Epoch 144/200\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2294\n",
            "Epoch 145/200\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 146/200\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2278\n",
            "Epoch 147/200\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 148/200\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 149/200\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0400 - val_mae: 0.2287\n",
            "Epoch 150/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 151/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0396 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2270\n",
            "Epoch 152/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2275\n",
            "Epoch 153/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 154/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 155/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2272\n",
            "Epoch 156/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2270\n",
            "Epoch 157/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2278\n",
            "Epoch 158/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 159/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2267\n",
            "Epoch 160/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 161/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 162/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 163/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 164/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 165/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 166/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2284\n",
            "Epoch 167/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 168/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 169/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 170/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2284\n",
            "Epoch 171/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2286\n",
            "Epoch 172/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2271\n",
            "Epoch 173/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 174/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2273\n",
            "Epoch 175/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2288\n",
            "Epoch 176/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 177/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2270\n",
            "Epoch 178/200\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 179/200\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2284\n",
            "Epoch 180/200\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 181/200\n",
            "30/30 [==============================] - 0s 17ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2277\n",
            "Epoch 182/200\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 183/200\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0396 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 184/200\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2291\n",
            "Epoch 185/200\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 186/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 187/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0396 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 188/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2278\n",
            "Epoch 189/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 190/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2274\n",
            "Epoch 191/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 192/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2297\n",
            "Epoch 193/200\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0396 - mae: 0.2272 - val_loss: 0.0399 - val_mae: 0.2272\n",
            "Epoch 194/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 195/200\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2285\n",
            "Epoch 196/200\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2292\n",
            "Epoch 197/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 198/200\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2272\n",
            "Epoch 199/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2265 - val_loss: 0.0398 - val_mae: 0.2289\n",
            "Epoch 200/200\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:02:23,292] Trial 0 finished with value: 0.039827607572078705 and parameters: {'learning_rate': 0.006421476326277428, 'dropout_rate': 0.36888127980912266, 'batch_size': 128, 'epochs': 200}. Best is trial 0 with value: 0.039827607572078705.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "60/60 [==============================] - 3s 14ms/step - loss: 1.0536 - mae: 1.5480 - val_loss: 0.1679 - val_mae: 0.4905\n",
            "Epoch 2/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 1.0305 - mae: 1.5180 - val_loss: 0.1900 - val_mae: 0.5216\n",
            "Epoch 3/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 1.0088 - mae: 1.4928 - val_loss: 0.1960 - val_mae: 0.5285\n",
            "Epoch 4/100\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.9874 - mae: 1.4711 - val_loss: 0.1972 - val_mae: 0.5335\n",
            "Epoch 5/100\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.9409 - mae: 1.4179 - val_loss: 0.1914 - val_mae: 0.5246\n",
            "Epoch 6/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.9450 - mae: 1.4214 - val_loss: 0.1880 - val_mae: 0.5208\n",
            "Epoch 7/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.8973 - mae: 1.3709 - val_loss: 0.1824 - val_mae: 0.5116\n",
            "Epoch 8/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.8748 - mae: 1.3423 - val_loss: 0.1750 - val_mae: 0.5006\n",
            "Epoch 9/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.8527 - mae: 1.3156 - val_loss: 0.1709 - val_mae: 0.4945\n",
            "Epoch 10/100\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.8399 - mae: 1.3030 - val_loss: 0.1649 - val_mae: 0.4838\n",
            "Epoch 11/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.8231 - mae: 1.2812 - val_loss: 0.1601 - val_mae: 0.4763\n",
            "Epoch 12/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7749 - mae: 1.2275 - val_loss: 0.1547 - val_mae: 0.4672\n",
            "Epoch 13/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.7754 - mae: 1.2251 - val_loss: 0.1502 - val_mae: 0.4588\n",
            "Epoch 14/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.7388 - mae: 1.1824 - val_loss: 0.1459 - val_mae: 0.4518\n",
            "Epoch 15/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.7418 - mae: 1.1845 - val_loss: 0.1426 - val_mae: 0.4461\n",
            "Epoch 16/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6810 - mae: 1.1157 - val_loss: 0.1360 - val_mae: 0.4332\n",
            "Epoch 17/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6816 - mae: 1.1148 - val_loss: 0.1334 - val_mae: 0.4292\n",
            "Epoch 18/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6801 - mae: 1.1097 - val_loss: 0.1299 - val_mae: 0.4231\n",
            "Epoch 19/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6439 - mae: 1.0700 - val_loss: 0.1250 - val_mae: 0.4137\n",
            "Epoch 20/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6282 - mae: 1.0495 - val_loss: 0.1219 - val_mae: 0.4080\n",
            "Epoch 21/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6017 - mae: 1.0174 - val_loss: 0.1190 - val_mae: 0.4028\n",
            "Epoch 22/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5725 - mae: 0.9801 - val_loss: 0.1154 - val_mae: 0.3954\n",
            "Epoch 23/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.5561 - mae: 0.9652 - val_loss: 0.1120 - val_mae: 0.3884\n",
            "Epoch 24/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.5403 - mae: 0.9458 - val_loss: 0.1082 - val_mae: 0.3802\n",
            "Epoch 25/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.5292 - mae: 0.9290 - val_loss: 0.1062 - val_mae: 0.3763\n",
            "Epoch 26/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5255 - mae: 0.9219 - val_loss: 0.1040 - val_mae: 0.3709\n",
            "Epoch 27/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5275 - mae: 0.9224 - val_loss: 0.1015 - val_mae: 0.3651\n",
            "Epoch 28/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.4661 - mae: 0.8520 - val_loss: 0.0986 - val_mae: 0.3592\n",
            "Epoch 29/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.4396 - mae: 0.8172 - val_loss: 0.0954 - val_mae: 0.3519\n",
            "Epoch 30/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.4538 - mae: 0.8344 - val_loss: 0.0935 - val_mae: 0.3486\n",
            "Epoch 31/100\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.4305 - mae: 0.8028 - val_loss: 0.0910 - val_mae: 0.3430\n",
            "Epoch 32/100\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.4333 - mae: 0.8053 - val_loss: 0.0892 - val_mae: 0.3388\n",
            "Epoch 33/100\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.4091 - mae: 0.7744 - val_loss: 0.0872 - val_mae: 0.3342\n",
            "Epoch 34/100\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.3842 - mae: 0.7440 - val_loss: 0.0854 - val_mae: 0.3304\n",
            "Epoch 35/100\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.3981 - mae: 0.7572 - val_loss: 0.0836 - val_mae: 0.3256\n",
            "Epoch 36/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.3686 - mae: 0.7224 - val_loss: 0.0820 - val_mae: 0.3227\n",
            "Epoch 37/100\n",
            "60/60 [==============================] - 1s 16ms/step - loss: 0.3593 - mae: 0.7097 - val_loss: 0.0803 - val_mae: 0.3185\n",
            "Epoch 38/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.3353 - mae: 0.6774 - val_loss: 0.0786 - val_mae: 0.3148\n",
            "Epoch 39/100\n",
            "60/60 [==============================] - 1s 18ms/step - loss: 0.3433 - mae: 0.6855 - val_loss: 0.0771 - val_mae: 0.3111\n",
            "Epoch 40/100\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.3205 - mae: 0.6576 - val_loss: 0.0757 - val_mae: 0.3079\n",
            "Epoch 41/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.3039 - mae: 0.6358 - val_loss: 0.0744 - val_mae: 0.3049\n",
            "Epoch 42/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.2970 - mae: 0.6251 - val_loss: 0.0730 - val_mae: 0.3018\n",
            "Epoch 43/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.2913 - mae: 0.6161 - val_loss: 0.0717 - val_mae: 0.2992\n",
            "Epoch 44/100\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.2801 - mae: 0.6006 - val_loss: 0.0706 - val_mae: 0.2968\n",
            "Epoch 45/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.2740 - mae: 0.5939 - val_loss: 0.0695 - val_mae: 0.2941\n",
            "Epoch 46/100\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.2551 - mae: 0.5697 - val_loss: 0.0686 - val_mae: 0.2919\n",
            "Epoch 47/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2502 - mae: 0.5614 - val_loss: 0.0677 - val_mae: 0.2902\n",
            "Epoch 48/100\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.2494 - mae: 0.5573 - val_loss: 0.0664 - val_mae: 0.2873\n",
            "Epoch 49/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2353 - mae: 0.5375 - val_loss: 0.0656 - val_mae: 0.2854\n",
            "Epoch 50/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2277 - mae: 0.5285 - val_loss: 0.0646 - val_mae: 0.2833\n",
            "Epoch 51/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.2125 - mae: 0.5084 - val_loss: 0.0639 - val_mae: 0.2817\n",
            "Epoch 52/100\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.2089 - mae: 0.5023 - val_loss: 0.0630 - val_mae: 0.2801\n",
            "Epoch 53/100\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.2033 - mae: 0.4932 - val_loss: 0.0622 - val_mae: 0.2786\n",
            "Epoch 54/100\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.1928 - mae: 0.4789 - val_loss: 0.0614 - val_mae: 0.2770\n",
            "Epoch 55/100\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.1909 - mae: 0.4744 - val_loss: 0.0607 - val_mae: 0.2756\n",
            "Epoch 56/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.1768 - mae: 0.4560 - val_loss: 0.0601 - val_mae: 0.2742\n",
            "Epoch 57/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.1666 - mae: 0.4404 - val_loss: 0.0593 - val_mae: 0.2727\n",
            "Epoch 58/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.1695 - mae: 0.4446 - val_loss: 0.0586 - val_mae: 0.2715\n",
            "Epoch 59/100\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.1585 - mae: 0.4283 - val_loss: 0.0579 - val_mae: 0.2700\n",
            "Epoch 60/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1592 - mae: 0.4291 - val_loss: 0.0573 - val_mae: 0.2690\n",
            "Epoch 61/100\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.1510 - mae: 0.4165 - val_loss: 0.0567 - val_mae: 0.2679\n",
            "Epoch 62/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1454 - mae: 0.4072 - val_loss: 0.0561 - val_mae: 0.2668\n",
            "Epoch 63/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1345 - mae: 0.3933 - val_loss: 0.0555 - val_mae: 0.2657\n",
            "Epoch 64/100\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.1348 - mae: 0.3937 - val_loss: 0.0551 - val_mae: 0.2651\n",
            "Epoch 65/100\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.1293 - mae: 0.3847 - val_loss: 0.0545 - val_mae: 0.2640\n",
            "Epoch 66/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.1212 - mae: 0.3736 - val_loss: 0.0540 - val_mae: 0.2630\n",
            "Epoch 67/100\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 0.1191 - mae: 0.3692 - val_loss: 0.0536 - val_mae: 0.2624\n",
            "Epoch 68/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.1158 - mae: 0.3651 - val_loss: 0.0531 - val_mae: 0.2615\n",
            "Epoch 69/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.1143 - mae: 0.3622 - val_loss: 0.0526 - val_mae: 0.2608\n",
            "Epoch 70/100\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.1073 - mae: 0.3509 - val_loss: 0.0522 - val_mae: 0.2600\n",
            "Epoch 71/100\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.1023 - mae: 0.3430 - val_loss: 0.0518 - val_mae: 0.2593\n",
            "Epoch 72/100\n",
            "60/60 [==============================] - 1s 18ms/step - loss: 0.0993 - mae: 0.3388 - val_loss: 0.0514 - val_mae: 0.2587\n",
            "Epoch 73/100\n",
            "60/60 [==============================] - 1s 19ms/step - loss: 0.0940 - mae: 0.3308 - val_loss: 0.0510 - val_mae: 0.2579\n",
            "Epoch 74/100\n",
            "60/60 [==============================] - 1s 18ms/step - loss: 0.0923 - mae: 0.3281 - val_loss: 0.0506 - val_mae: 0.2572\n",
            "Epoch 75/100\n",
            "60/60 [==============================] - 1s 16ms/step - loss: 0.0878 - mae: 0.3207 - val_loss: 0.0502 - val_mae: 0.2566\n",
            "Epoch 76/100\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0843 - mae: 0.3152 - val_loss: 0.0498 - val_mae: 0.2559\n",
            "Epoch 77/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0846 - mae: 0.3160 - val_loss: 0.0495 - val_mae: 0.2554\n",
            "Epoch 78/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0805 - mae: 0.3087 - val_loss: 0.0491 - val_mae: 0.2547\n",
            "Epoch 79/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0779 - mae: 0.3050 - val_loss: 0.0488 - val_mae: 0.2541\n",
            "Epoch 80/100\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0737 - mae: 0.2983 - val_loss: 0.0484 - val_mae: 0.2535\n",
            "Epoch 81/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0717 - mae: 0.2943 - val_loss: 0.0481 - val_mae: 0.2530\n",
            "Epoch 82/100\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0691 - mae: 0.2901 - val_loss: 0.0478 - val_mae: 0.2523\n",
            "Epoch 83/100\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0688 - mae: 0.2893 - val_loss: 0.0475 - val_mae: 0.2518\n",
            "Epoch 84/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0649 - mae: 0.2836 - val_loss: 0.0472 - val_mae: 0.2512\n",
            "Epoch 85/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0654 - mae: 0.2833 - val_loss: 0.0470 - val_mae: 0.2508\n",
            "Epoch 86/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0636 - mae: 0.2798 - val_loss: 0.0467 - val_mae: 0.2502\n",
            "Epoch 87/100\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0604 - mae: 0.2750 - val_loss: 0.0464 - val_mae: 0.2496\n",
            "Epoch 88/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0592 - mae: 0.2727 - val_loss: 0.0461 - val_mae: 0.2491\n",
            "Epoch 89/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0571 - mae: 0.2689 - val_loss: 0.0459 - val_mae: 0.2487\n",
            "Epoch 90/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0550 - mae: 0.2652 - val_loss: 0.0456 - val_mae: 0.2481\n",
            "Epoch 91/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0548 - mae: 0.2645 - val_loss: 0.0454 - val_mae: 0.2476\n",
            "Epoch 92/100\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0537 - mae: 0.2625 - val_loss: 0.0452 - val_mae: 0.2471\n",
            "Epoch 93/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0516 - mae: 0.2589 - val_loss: 0.0450 - val_mae: 0.2466\n",
            "Epoch 94/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0512 - mae: 0.2577 - val_loss: 0.0447 - val_mae: 0.2462\n",
            "Epoch 95/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0501 - mae: 0.2553 - val_loss: 0.0445 - val_mae: 0.2458\n",
            "Epoch 96/100\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0497 - mae: 0.2544 - val_loss: 0.0443 - val_mae: 0.2453\n",
            "Epoch 97/100\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.0485 - mae: 0.2523 - val_loss: 0.0441 - val_mae: 0.2449\n",
            "Epoch 98/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0477 - mae: 0.2505 - val_loss: 0.0440 - val_mae: 0.2444\n",
            "Epoch 99/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0471 - mae: 0.2493 - val_loss: 0.0438 - val_mae: 0.2440\n",
            "Epoch 100/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0463 - mae: 0.2475 - val_loss: 0.0436 - val_mae: 0.2436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:03:29,819] Trial 1 finished with value: 0.04360068589448929 and parameters: {'learning_rate': 8.137552267078367e-05, 'dropout_rate': 0.8392795998396673, 'batch_size': 64, 'epochs': 100}. Best is trial 0 with value: 0.039827607572078705.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "15/15 [==============================] - 3s 36ms/step - loss: 0.6521 - mae: 1.1050 - val_loss: 5.0864 - val_mae: 5.7619\n",
            "Epoch 2/50\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6487 - mae: 1.0996 - val_loss: 2.7166 - val_mae: 3.3539\n",
            "Epoch 3/50\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.6441 - mae: 1.0969 - val_loss: 1.5692 - val_mae: 2.1688\n",
            "Epoch 4/50\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.6409 - mae: 1.0920 - val_loss: 0.9388 - val_mae: 1.4880\n",
            "Epoch 5/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6473 - mae: 1.0998 - val_loss: 0.5466 - val_mae: 1.0434\n",
            "Epoch 6/50\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.6290 - mae: 1.0785 - val_loss: 0.3146 - val_mae: 0.7457\n",
            "Epoch 7/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6413 - mae: 1.0932 - val_loss: 0.1957 - val_mae: 0.5596\n",
            "Epoch 8/50\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.6418 - mae: 1.0918 - val_loss: 0.1437 - val_mae: 0.4641\n",
            "Epoch 9/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6329 - mae: 1.0803 - val_loss: 0.1370 - val_mae: 0.4432\n",
            "Epoch 10/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6353 - mae: 1.0835 - val_loss: 0.1456 - val_mae: 0.4501\n",
            "Epoch 11/50\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.6599 - mae: 1.1126 - val_loss: 0.1598 - val_mae: 0.4701\n",
            "Epoch 12/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6304 - mae: 1.0805 - val_loss: 0.1710 - val_mae: 0.4846\n",
            "Epoch 13/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6449 - mae: 1.0944 - val_loss: 0.1801 - val_mae: 0.4948\n",
            "Epoch 14/50\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6401 - mae: 1.0913 - val_loss: 0.1882 - val_mae: 0.5040\n",
            "Epoch 15/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6305 - mae: 1.0763 - val_loss: 0.1950 - val_mae: 0.5133\n",
            "Epoch 16/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6413 - mae: 1.0884 - val_loss: 0.2008 - val_mae: 0.5242\n",
            "Epoch 17/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6283 - mae: 1.0741 - val_loss: 0.2056 - val_mae: 0.5340\n",
            "Epoch 18/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6428 - mae: 1.0916 - val_loss: 0.2112 - val_mae: 0.5459\n",
            "Epoch 19/50\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6392 - mae: 1.0909 - val_loss: 0.2170 - val_mae: 0.5579\n",
            "Epoch 20/50\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.6355 - mae: 1.0827 - val_loss: 0.2214 - val_mae: 0.5667\n",
            "Epoch 21/50\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.6216 - mae: 1.0670 - val_loss: 0.2245 - val_mae: 0.5731\n",
            "Epoch 22/50\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.6425 - mae: 1.0902 - val_loss: 0.2266 - val_mae: 0.5774\n",
            "Epoch 23/50\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.6368 - mae: 1.0833 - val_loss: 0.2281 - val_mae: 0.5804\n",
            "Epoch 24/50\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.6401 - mae: 1.0884 - val_loss: 0.2302 - val_mae: 0.5840\n",
            "Epoch 25/50\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.6380 - mae: 1.0877 - val_loss: 0.2302 - val_mae: 0.5842\n",
            "Epoch 26/50\n",
            "15/15 [==============================] - 0s 27ms/step - loss: 0.6161 - mae: 1.0629 - val_loss: 0.2306 - val_mae: 0.5847\n",
            "Epoch 27/50\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.6174 - mae: 1.0632 - val_loss: 0.2313 - val_mae: 0.5857\n",
            "Epoch 28/50\n",
            "15/15 [==============================] - 1s 53ms/step - loss: 0.6284 - mae: 1.0751 - val_loss: 0.2312 - val_mae: 0.5854\n",
            "Epoch 29/50\n",
            "15/15 [==============================] - 1s 66ms/step - loss: 0.6285 - mae: 1.0748 - val_loss: 0.2315 - val_mae: 0.5856\n",
            "Epoch 30/50\n",
            "15/15 [==============================] - 0s 28ms/step - loss: 0.6409 - mae: 1.0887 - val_loss: 0.2317 - val_mae: 0.5859\n",
            "Epoch 31/50\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.6273 - mae: 1.0733 - val_loss: 0.2320 - val_mae: 0.5869\n",
            "Epoch 32/50\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6220 - mae: 1.0672 - val_loss: 0.2318 - val_mae: 0.5867\n",
            "Epoch 33/50\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6073 - mae: 1.0481 - val_loss: 0.2306 - val_mae: 0.5841\n",
            "Epoch 34/50\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6172 - mae: 1.0621 - val_loss: 0.2296 - val_mae: 0.5820\n",
            "Epoch 35/50\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.6162 - mae: 1.0600 - val_loss: 0.2282 - val_mae: 0.5792\n",
            "Epoch 36/50\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.6260 - mae: 1.0728 - val_loss: 0.2291 - val_mae: 0.5813\n",
            "Epoch 37/50\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.6112 - mae: 1.0534 - val_loss: 0.2273 - val_mae: 0.5778\n",
            "Epoch 38/50\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6368 - mae: 1.0875 - val_loss: 0.2281 - val_mae: 0.5793\n",
            "Epoch 39/50\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.6002 - mae: 1.0439 - val_loss: 0.2278 - val_mae: 0.5788\n",
            "Epoch 40/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6203 - mae: 1.0638 - val_loss: 0.2281 - val_mae: 0.5789\n",
            "Epoch 41/50\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6246 - mae: 1.0717 - val_loss: 0.2292 - val_mae: 0.5810\n",
            "Epoch 42/50\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.6151 - mae: 1.0595 - val_loss: 0.2289 - val_mae: 0.5806\n",
            "Epoch 43/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6076 - mae: 1.0516 - val_loss: 0.2287 - val_mae: 0.5808\n",
            "Epoch 44/50\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6095 - mae: 1.0504 - val_loss: 0.2275 - val_mae: 0.5788\n",
            "Epoch 45/50\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6211 - mae: 1.0668 - val_loss: 0.2284 - val_mae: 0.5808\n",
            "Epoch 46/50\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.6157 - mae: 1.0613 - val_loss: 0.2283 - val_mae: 0.5806\n",
            "Epoch 47/50\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5974 - mae: 1.0404 - val_loss: 0.2273 - val_mae: 0.5787\n",
            "Epoch 48/50\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6059 - mae: 1.0472 - val_loss: 0.2260 - val_mae: 0.5765\n",
            "Epoch 49/50\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6030 - mae: 1.0410 - val_loss: 0.2236 - val_mae: 0.5729\n",
            "Epoch 50/50\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6064 - mae: 1.0475 - val_loss: 0.2230 - val_mae: 0.5709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:03:47,163] Trial 2 finished with value: 0.22295750677585602 and parameters: {'learning_rate': 6.443621431943101e-06, 'dropout_rate': 0.6527680740221705, 'batch_size': 256, 'epochs': 50}. Best is trial 0 with value: 0.039827607572078705.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "30/30 [==============================] - 3s 21ms/step - loss: 0.4017 - mae: 0.7898 - val_loss: 5.6953 - val_mae: 6.3287\n",
            "Epoch 2/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1736 - mae: 0.4616 - val_loss: 1.9964 - val_mae: 2.5768\n",
            "Epoch 3/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0909 - mae: 0.3243 - val_loss: 0.4317 - val_mae: 0.8381\n",
            "Epoch 4/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0545 - mae: 0.2605 - val_loss: 0.0754 - val_mae: 0.3023\n",
            "Epoch 5/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0431 - mae: 0.2362 - val_loss: 0.0469 - val_mae: 0.2247\n",
            "Epoch 6/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0410 - mae: 0.2316 - val_loss: 0.0442 - val_mae: 0.2327\n",
            "Epoch 7/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0405 - mae: 0.2303 - val_loss: 0.0399 - val_mae: 0.2293\n",
            "Epoch 8/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0400 - mae: 0.2285 - val_loss: 0.0400 - val_mae: 0.2293\n",
            "Epoch 9/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0400 - mae: 0.2281 - val_loss: 0.0399 - val_mae: 0.2289\n",
            "Epoch 10/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0399 - mae: 0.2278 - val_loss: 0.0398 - val_mae: 0.2279\n",
            "Epoch 11/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 12/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 13/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0399 - mae: 0.2276 - val_loss: 0.0399 - val_mae: 0.2265\n",
            "Epoch 14/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0399 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2296\n",
            "Epoch 15/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0398 - mae: 0.2274 - val_loss: 0.0398 - val_mae: 0.2270\n",
            "Epoch 16/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0399 - mae: 0.2271 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 17/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0398 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 18/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0398 - mae: 0.2271 - val_loss: 0.0398 - val_mae: 0.2273\n",
            "Epoch 19/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0398 - mae: 0.2272 - val_loss: 0.0398 - val_mae: 0.2274\n",
            "Epoch 20/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0398 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 21/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0398 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2295\n",
            "Epoch 22/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 23/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0398 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 24/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2272\n",
            "Epoch 25/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 26/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 27/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 28/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 29/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2279\n",
            "Epoch 30/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 31/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 32/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2271\n",
            "Epoch 33/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2272\n",
            "Epoch 34/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0397 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 35/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 36/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0396 - mae: 0.2265 - val_loss: 0.0398 - val_mae: 0.2279\n",
            "Epoch 37/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2279\n",
            "Epoch 38/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 39/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 40/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2277\n",
            "Epoch 41/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0396 - mae: 0.2265 - val_loss: 0.0398 - val_mae: 0.2278\n",
            "Epoch 42/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2265 - val_loss: 0.0398 - val_mae: 0.2279\n",
            "Epoch 43/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 44/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 45/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0395 - mae: 0.2264 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 46/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0395 - mae: 0.2265 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 47/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 48/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0395 - mae: 0.2264 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 49/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0395 - mae: 0.2264 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 50/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0396 - mae: 0.2265 - val_loss: 0.0398 - val_mae: 0.2279\n",
            "Epoch 51/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0395 - mae: 0.2262 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 52/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0395 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2272\n",
            "Epoch 53/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0395 - mae: 0.2264 - val_loss: 0.0398 - val_mae: 0.2273\n",
            "Epoch 54/150\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0395 - mae: 0.2262 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 55/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2264 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 56/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0395 - mae: 0.2264 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 57/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0395 - mae: 0.2262 - val_loss: 0.0399 - val_mae: 0.2271\n",
            "Epoch 58/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0395 - mae: 0.2263 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 59/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0395 - mae: 0.2264 - val_loss: 0.0398 - val_mae: 0.2277\n",
            "Epoch 60/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0395 - mae: 0.2262 - val_loss: 0.0398 - val_mae: 0.2278\n",
            "Epoch 61/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0395 - mae: 0.2263 - val_loss: 0.0398 - val_mae: 0.2274\n",
            "Epoch 62/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0395 - mae: 0.2260 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 63/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0395 - mae: 0.2262 - val_loss: 0.0399 - val_mae: 0.2271\n",
            "Epoch 64/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0395 - mae: 0.2260 - val_loss: 0.0398 - val_mae: 0.2273\n",
            "Epoch 65/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0394 - mae: 0.2258 - val_loss: 0.0398 - val_mae: 0.2274\n",
            "Epoch 66/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0394 - mae: 0.2257 - val_loss: 0.0399 - val_mae: 0.2271\n",
            "Epoch 67/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0394 - mae: 0.2255 - val_loss: 0.0399 - val_mae: 0.2272\n",
            "Epoch 68/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0393 - mae: 0.2253 - val_loss: 0.0398 - val_mae: 0.2269\n",
            "Epoch 69/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0393 - mae: 0.2251 - val_loss: 0.0399 - val_mae: 0.2268\n",
            "Epoch 70/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0391 - mae: 0.2247 - val_loss: 0.0399 - val_mae: 0.2267\n",
            "Epoch 71/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0392 - mae: 0.2246 - val_loss: 0.0398 - val_mae: 0.2275\n",
            "Epoch 72/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0392 - mae: 0.2247 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 73/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0389 - mae: 0.2237 - val_loss: 0.0393 - val_mae: 0.2263\n",
            "Epoch 74/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0388 - mae: 0.2233 - val_loss: 0.0399 - val_mae: 0.2272\n",
            "Epoch 75/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0387 - mae: 0.2230 - val_loss: 0.0399 - val_mae: 0.2270\n",
            "Epoch 76/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0386 - mae: 0.2225 - val_loss: 0.0399 - val_mae: 0.2269\n",
            "Epoch 77/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0385 - mae: 0.2220 - val_loss: 0.0399 - val_mae: 0.2272\n",
            "Epoch 78/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0384 - mae: 0.2214 - val_loss: 0.0399 - val_mae: 0.2284\n",
            "Epoch 79/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0383 - mae: 0.2216 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 80/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0382 - mae: 0.2208 - val_loss: 0.0399 - val_mae: 0.2264\n",
            "Epoch 81/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0380 - mae: 0.2201 - val_loss: 0.0589 - val_mae: 0.2566\n",
            "Epoch 82/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0380 - mae: 0.2206 - val_loss: 0.0471 - val_mae: 0.2356\n",
            "Epoch 83/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0380 - mae: 0.2201 - val_loss: 0.0372 - val_mae: 0.2199\n",
            "Epoch 84/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0377 - mae: 0.2192 - val_loss: 0.0373 - val_mae: 0.2178\n",
            "Epoch 85/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0378 - mae: 0.2191 - val_loss: 0.0380 - val_mae: 0.2193\n",
            "Epoch 86/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0376 - mae: 0.2187 - val_loss: 0.0572 - val_mae: 0.2547\n",
            "Epoch 87/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0377 - mae: 0.2186 - val_loss: 0.1868 - val_mae: 0.4984\n",
            "Epoch 88/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0375 - mae: 0.2182 - val_loss: 0.1433 - val_mae: 0.4289\n",
            "Epoch 89/150\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0376 - mae: 0.2184 - val_loss: 0.1733 - val_mae: 0.4793\n",
            "Epoch 90/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0374 - mae: 0.2176 - val_loss: 0.1544 - val_mae: 0.4482\n",
            "Epoch 91/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0371 - mae: 0.2167 - val_loss: 0.1098 - val_mae: 0.3640\n",
            "Epoch 92/150\n",
            "30/30 [==============================] - 0s 17ms/step - loss: 0.0373 - mae: 0.2172 - val_loss: 0.0776 - val_mae: 0.3007\n",
            "Epoch 93/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0373 - mae: 0.2171 - val_loss: 0.0451 - val_mae: 0.2298\n",
            "Epoch 94/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0368 - mae: 0.2153 - val_loss: 0.0931 - val_mae: 0.3305\n",
            "Epoch 95/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0368 - mae: 0.2152 - val_loss: 0.0404 - val_mae: 0.2272\n",
            "Epoch 96/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0366 - mae: 0.2147 - val_loss: 0.0402 - val_mae: 0.2266\n",
            "Epoch 97/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0366 - mae: 0.2147 - val_loss: 0.0389 - val_mae: 0.2233\n",
            "Epoch 98/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0367 - mae: 0.2148 - val_loss: 0.0403 - val_mae: 0.2270\n",
            "Epoch 99/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0369 - mae: 0.2160 - val_loss: 0.0390 - val_mae: 0.2227\n",
            "Epoch 100/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0370 - mae: 0.2159 - val_loss: 0.2440 - val_mae: 0.5892\n",
            "Epoch 101/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0369 - mae: 0.2154 - val_loss: 0.1804 - val_mae: 0.4936\n",
            "Epoch 102/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0369 - mae: 0.2158 - val_loss: 0.2517 - val_mae: 0.6037\n",
            "Epoch 103/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0370 - mae: 0.2159 - val_loss: 0.3782 - val_mae: 0.7703\n",
            "Epoch 104/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0371 - mae: 0.2163 - val_loss: 0.5704 - val_mae: 1.0131\n",
            "Epoch 105/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0370 - mae: 0.2157 - val_loss: 0.4608 - val_mae: 0.8795\n",
            "Epoch 106/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0365 - mae: 0.2142 - val_loss: 0.4645 - val_mae: 0.8775\n",
            "Epoch 107/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0368 - mae: 0.2150 - val_loss: 0.2782 - val_mae: 0.6339\n",
            "Epoch 108/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0368 - mae: 0.2150 - val_loss: 0.5150 - val_mae: 0.9384\n",
            "Epoch 109/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0369 - mae: 0.2155 - val_loss: 0.5476 - val_mae: 0.9835\n",
            "Epoch 110/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0369 - mae: 0.2155 - val_loss: 0.2798 - val_mae: 0.6396\n",
            "Epoch 111/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0369 - mae: 0.2154 - val_loss: 0.3249 - val_mae: 0.7038\n",
            "Epoch 112/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0365 - mae: 0.2135 - val_loss: 0.4680 - val_mae: 0.8842\n",
            "Epoch 113/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0366 - mae: 0.2141 - val_loss: 0.3516 - val_mae: 0.7352\n",
            "Epoch 114/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0362 - mae: 0.2132 - val_loss: 0.2094 - val_mae: 0.5363\n",
            "Epoch 115/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0366 - mae: 0.2140 - val_loss: 0.3232 - val_mae: 0.7011\n",
            "Epoch 116/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0360 - mae: 0.2122 - val_loss: 0.2276 - val_mae: 0.5713\n",
            "Epoch 117/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0363 - mae: 0.2132 - val_loss: 0.3045 - val_mae: 0.6758\n",
            "Epoch 118/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0363 - mae: 0.2128 - val_loss: 0.2343 - val_mae: 0.5775\n",
            "Epoch 119/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0364 - mae: 0.2134 - val_loss: 0.2234 - val_mae: 0.5576\n",
            "Epoch 120/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0360 - mae: 0.2120 - val_loss: 0.1622 - val_mae: 0.4613\n",
            "Epoch 121/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0362 - mae: 0.2130 - val_loss: 0.1659 - val_mae: 0.4695\n",
            "Epoch 122/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0360 - mae: 0.2118 - val_loss: 0.3175 - val_mae: 0.6947\n",
            "Epoch 123/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0359 - mae: 0.2117 - val_loss: 0.1349 - val_mae: 0.4148\n",
            "Epoch 124/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0361 - mae: 0.2121 - val_loss: 0.0826 - val_mae: 0.3105\n",
            "Epoch 125/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0363 - mae: 0.2132 - val_loss: 0.1644 - val_mae: 0.4662\n",
            "Epoch 126/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0361 - mae: 0.2122 - val_loss: 0.1194 - val_mae: 0.3902\n",
            "Epoch 127/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0360 - mae: 0.2119 - val_loss: 0.0457 - val_mae: 0.2326\n",
            "Epoch 128/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0362 - mae: 0.2126 - val_loss: 0.0376 - val_mae: 0.2215\n",
            "Epoch 129/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0359 - mae: 0.2117 - val_loss: 0.0530 - val_mae: 0.2467\n",
            "Epoch 130/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0356 - mae: 0.2106 - val_loss: 0.0495 - val_mae: 0.2368\n",
            "Epoch 131/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0360 - mae: 0.2117 - val_loss: 0.0381 - val_mae: 0.2209\n",
            "Epoch 132/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0362 - mae: 0.2124 - val_loss: 0.0419 - val_mae: 0.2296\n",
            "Epoch 133/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0361 - mae: 0.2119 - val_loss: 0.0426 - val_mae: 0.2309\n",
            "Epoch 134/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0362 - mae: 0.2126 - val_loss: 0.0429 - val_mae: 0.2318\n",
            "Epoch 135/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0360 - mae: 0.2120 - val_loss: 0.0405 - val_mae: 0.2265\n",
            "Epoch 136/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0363 - mae: 0.2129 - val_loss: 0.0395 - val_mae: 0.2241\n",
            "Epoch 137/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0359 - mae: 0.2117 - val_loss: 0.0380 - val_mae: 0.2224\n",
            "Epoch 138/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0359 - mae: 0.2114 - val_loss: 0.0413 - val_mae: 0.2285\n",
            "Epoch 139/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0366 - mae: 0.2140 - val_loss: 0.0428 - val_mae: 0.2304\n",
            "Epoch 140/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0367 - mae: 0.2143 - val_loss: 0.0489 - val_mae: 0.2378\n",
            "Epoch 141/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0368 - mae: 0.2146 - val_loss: 0.0417 - val_mae: 0.2297\n",
            "Epoch 142/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0375 - mae: 0.2177 - val_loss: 0.0779 - val_mae: 0.2990\n",
            "Epoch 143/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0370 - mae: 0.2158 - val_loss: 0.0699 - val_mae: 0.2822\n",
            "Epoch 144/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0368 - mae: 0.2152 - val_loss: 0.1357 - val_mae: 0.4169\n",
            "Epoch 145/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0364 - mae: 0.2132 - val_loss: 0.0482 - val_mae: 0.2350\n",
            "Epoch 146/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0363 - mae: 0.2130 - val_loss: 0.0419 - val_mae: 0.2272\n",
            "Epoch 147/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0364 - mae: 0.2137 - val_loss: 0.1539 - val_mae: 0.4508\n",
            "Epoch 148/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.0365 - mae: 0.2139 - val_loss: 0.0985 - val_mae: 0.3501\n",
            "Epoch 149/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0362 - mae: 0.2123 - val_loss: 0.0426 - val_mae: 0.2275\n",
            "Epoch 150/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0359 - mae: 0.2114 - val_loss: 0.1556 - val_mae: 0.4530\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:05:11,670] Trial 3 finished with value: 0.15562234818935394 and parameters: {'learning_rate': 0.003628031185395325, 'dropout_rate': 0.6295512530779386, 'batch_size': 128, 'epochs': 150}. Best is trial 0 with value: 0.039827607572078705.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "60/60 [==============================] - 4s 16ms/step - loss: 0.4366 - mae: 0.8670 - val_loss: 1.8386 - val_mae: 2.4212\n",
            "Epoch 2/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.4307 - mae: 0.8586 - val_loss: 0.7065 - val_mae: 1.2005\n",
            "Epoch 3/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.4282 - mae: 0.8548 - val_loss: 0.3837 - val_mae: 0.8128\n",
            "Epoch 4/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.4232 - mae: 0.8484 - val_loss: 0.2204 - val_mae: 0.5846\n",
            "Epoch 5/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.4184 - mae: 0.8438 - val_loss: 0.1921 - val_mae: 0.5405\n",
            "Epoch 6/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.4144 - mae: 0.8376 - val_loss: 0.1993 - val_mae: 0.5514\n",
            "Epoch 7/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.4075 - mae: 0.8276 - val_loss: 0.2022 - val_mae: 0.5526\n",
            "Epoch 8/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.4012 - mae: 0.8197 - val_loss: 0.2031 - val_mae: 0.5540\n",
            "Epoch 9/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.3986 - mae: 0.8162 - val_loss: 0.2093 - val_mae: 0.5644\n",
            "Epoch 10/100\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.3940 - mae: 0.8098 - val_loss: 0.2038 - val_mae: 0.5573\n",
            "Epoch 11/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.3902 - mae: 0.8051 - val_loss: 0.2062 - val_mae: 0.5586\n",
            "Epoch 12/100\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.3864 - mae: 0.8012 - val_loss: 0.2120 - val_mae: 0.5719\n",
            "Epoch 13/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.3826 - mae: 0.7947 - val_loss: 0.2006 - val_mae: 0.5525\n",
            "Epoch 14/100\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.3797 - mae: 0.7895 - val_loss: 0.1977 - val_mae: 0.5481\n",
            "Epoch 15/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.3755 - mae: 0.7846 - val_loss: 0.1913 - val_mae: 0.5354\n",
            "Epoch 16/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.3712 - mae: 0.7783 - val_loss: 0.1932 - val_mae: 0.5384\n",
            "Epoch 17/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.3670 - mae: 0.7730 - val_loss: 0.1926 - val_mae: 0.5373\n",
            "Epoch 18/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.3622 - mae: 0.7666 - val_loss: 0.1851 - val_mae: 0.5251\n",
            "Epoch 19/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.3581 - mae: 0.7609 - val_loss: 0.1877 - val_mae: 0.5303\n",
            "Epoch 20/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.3540 - mae: 0.7553 - val_loss: 0.1866 - val_mae: 0.5286\n",
            "Epoch 21/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.3472 - mae: 0.7461 - val_loss: 0.1919 - val_mae: 0.5361\n",
            "Epoch 22/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.3442 - mae: 0.7403 - val_loss: 0.1918 - val_mae: 0.5381\n",
            "Epoch 23/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.3393 - mae: 0.7346 - val_loss: 0.1888 - val_mae: 0.5310\n",
            "Epoch 24/100\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.3371 - mae: 0.7306 - val_loss: 0.1958 - val_mae: 0.5430\n",
            "Epoch 25/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.3353 - mae: 0.7285 - val_loss: 0.1941 - val_mae: 0.5416\n",
            "Epoch 26/100\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.3330 - mae: 0.7257 - val_loss: 0.1817 - val_mae: 0.5192\n",
            "Epoch 27/100\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.3284 - mae: 0.7179 - val_loss: 0.1821 - val_mae: 0.5187\n",
            "Epoch 28/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.3259 - mae: 0.7144 - val_loss: 0.1775 - val_mae: 0.5105\n",
            "Epoch 29/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.3217 - mae: 0.7097 - val_loss: 0.1752 - val_mae: 0.5068\n",
            "Epoch 30/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.3173 - mae: 0.7033 - val_loss: 0.1838 - val_mae: 0.5188\n",
            "Epoch 31/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.3129 - mae: 0.6955 - val_loss: 0.1780 - val_mae: 0.5084\n",
            "Epoch 32/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.3081 - mae: 0.6892 - val_loss: 0.1713 - val_mae: 0.4955\n",
            "Epoch 33/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.3048 - mae: 0.6837 - val_loss: 0.1748 - val_mae: 0.5024\n",
            "Epoch 34/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.3071 - mae: 0.6884 - val_loss: 0.1799 - val_mae: 0.5087\n",
            "Epoch 35/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.3007 - mae: 0.6788 - val_loss: 0.1707 - val_mae: 0.4954\n",
            "Epoch 36/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.2982 - mae: 0.6751 - val_loss: 0.1670 - val_mae: 0.4861\n",
            "Epoch 37/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2907 - mae: 0.6645 - val_loss: 0.1634 - val_mae: 0.4832\n",
            "Epoch 38/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2909 - mae: 0.6646 - val_loss: 0.1619 - val_mae: 0.4803\n",
            "Epoch 39/100\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.2872 - mae: 0.6591 - val_loss: 0.1650 - val_mae: 0.4863\n",
            "Epoch 40/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2826 - mae: 0.6511 - val_loss: 0.1636 - val_mae: 0.4836\n",
            "Epoch 41/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.2809 - mae: 0.6490 - val_loss: 0.1579 - val_mae: 0.4761\n",
            "Epoch 42/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2794 - mae: 0.6470 - val_loss: 0.1648 - val_mae: 0.4889\n",
            "Epoch 43/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.2753 - mae: 0.6420 - val_loss: 0.1610 - val_mae: 0.4798\n",
            "Epoch 44/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2738 - mae: 0.6387 - val_loss: 0.1549 - val_mae: 0.4687\n",
            "Epoch 45/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2703 - mae: 0.6342 - val_loss: 0.1575 - val_mae: 0.4750\n",
            "Epoch 46/100\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.2686 - mae: 0.6310 - val_loss: 0.1553 - val_mae: 0.4703\n",
            "Epoch 47/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2609 - mae: 0.6203 - val_loss: 0.1586 - val_mae: 0.4739\n",
            "Epoch 48/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.2629 - mae: 0.6218 - val_loss: 0.1578 - val_mae: 0.4734\n",
            "Epoch 49/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.2589 - mae: 0.6168 - val_loss: 0.1536 - val_mae: 0.4656\n",
            "Epoch 50/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.2565 - mae: 0.6127 - val_loss: 0.1553 - val_mae: 0.4676\n",
            "Epoch 51/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.2550 - mae: 0.6112 - val_loss: 0.1513 - val_mae: 0.4620\n",
            "Epoch 52/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.2505 - mae: 0.6051 - val_loss: 0.1535 - val_mae: 0.4646\n",
            "Epoch 53/100\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.2503 - mae: 0.6040 - val_loss: 0.1516 - val_mae: 0.4602\n",
            "Epoch 54/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.2478 - mae: 0.5996 - val_loss: 0.1524 - val_mae: 0.4632\n",
            "Epoch 55/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.2455 - mae: 0.5961 - val_loss: 0.1460 - val_mae: 0.4544\n",
            "Epoch 56/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.2420 - mae: 0.5907 - val_loss: 0.1503 - val_mae: 0.4597\n",
            "Epoch 57/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.2418 - mae: 0.5897 - val_loss: 0.1380 - val_mae: 0.4404\n",
            "Epoch 58/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2408 - mae: 0.5896 - val_loss: 0.1391 - val_mae: 0.4421\n",
            "Epoch 59/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.2363 - mae: 0.5819 - val_loss: 0.1436 - val_mae: 0.4503\n",
            "Epoch 60/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2342 - mae: 0.5795 - val_loss: 0.1420 - val_mae: 0.4459\n",
            "Epoch 61/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.2321 - mae: 0.5763 - val_loss: 0.1407 - val_mae: 0.4441\n",
            "Epoch 62/100\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.2281 - mae: 0.5707 - val_loss: 0.1441 - val_mae: 0.4487\n",
            "Epoch 63/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2292 - mae: 0.5719 - val_loss: 0.1363 - val_mae: 0.4351\n",
            "Epoch 64/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2266 - mae: 0.5673 - val_loss: 0.1334 - val_mae: 0.4301\n",
            "Epoch 65/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.2229 - mae: 0.5615 - val_loss: 0.1364 - val_mae: 0.4363\n",
            "Epoch 66/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.2229 - mae: 0.5618 - val_loss: 0.1352 - val_mae: 0.4337\n",
            "Epoch 67/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.2205 - mae: 0.5578 - val_loss: 0.1334 - val_mae: 0.4302\n",
            "Epoch 68/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2208 - mae: 0.5585 - val_loss: 0.1356 - val_mae: 0.4337\n",
            "Epoch 69/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.2145 - mae: 0.5495 - val_loss: 0.1388 - val_mae: 0.4397\n",
            "Epoch 70/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.2135 - mae: 0.5484 - val_loss: 0.1364 - val_mae: 0.4368\n",
            "Epoch 71/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.2136 - mae: 0.5480 - val_loss: 0.1330 - val_mae: 0.4297\n",
            "Epoch 72/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.2111 - mae: 0.5437 - val_loss: 0.1253 - val_mae: 0.4172\n",
            "Epoch 73/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2099 - mae: 0.5419 - val_loss: 0.1266 - val_mae: 0.4174\n",
            "Epoch 74/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.2088 - mae: 0.5402 - val_loss: 0.1238 - val_mae: 0.4137\n",
            "Epoch 75/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.2082 - mae: 0.5390 - val_loss: 0.1221 - val_mae: 0.4105\n",
            "Epoch 76/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.2060 - mae: 0.5358 - val_loss: 0.1231 - val_mae: 0.4117\n",
            "Epoch 77/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.2035 - mae: 0.5308 - val_loss: 0.1251 - val_mae: 0.4146\n",
            "Epoch 78/100\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.2035 - mae: 0.5313 - val_loss: 0.1238 - val_mae: 0.4126\n",
            "Epoch 79/100\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.1976 - mae: 0.5233 - val_loss: 0.1249 - val_mae: 0.4142\n",
            "Epoch 80/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.1989 - mae: 0.5247 - val_loss: 0.1227 - val_mae: 0.4104\n",
            "Epoch 81/100\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.1973 - mae: 0.5225 - val_loss: 0.1216 - val_mae: 0.4101\n",
            "Epoch 82/100\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.1948 - mae: 0.5185 - val_loss: 0.1192 - val_mae: 0.4056\n",
            "Epoch 83/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.1960 - mae: 0.5202 - val_loss: 0.1192 - val_mae: 0.4050\n",
            "Epoch 84/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.1947 - mae: 0.5186 - val_loss: 0.1227 - val_mae: 0.4102\n",
            "Epoch 85/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.1916 - mae: 0.5126 - val_loss: 0.1198 - val_mae: 0.4042\n",
            "Epoch 86/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.1889 - mae: 0.5094 - val_loss: 0.1171 - val_mae: 0.3999\n",
            "Epoch 87/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.1884 - mae: 0.5079 - val_loss: 0.1208 - val_mae: 0.4056\n",
            "Epoch 88/100\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.1879 - mae: 0.5074 - val_loss: 0.1167 - val_mae: 0.3978\n",
            "Epoch 89/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1873 - mae: 0.5058 - val_loss: 0.1161 - val_mae: 0.3979\n",
            "Epoch 90/100\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.1847 - mae: 0.5023 - val_loss: 0.1172 - val_mae: 0.4001\n",
            "Epoch 91/100\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.1844 - mae: 0.5010 - val_loss: 0.1169 - val_mae: 0.3987\n",
            "Epoch 92/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1820 - mae: 0.4974 - val_loss: 0.1171 - val_mae: 0.4004\n",
            "Epoch 93/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1802 - mae: 0.4948 - val_loss: 0.1182 - val_mae: 0.4014\n",
            "Epoch 94/100\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1799 - mae: 0.4944 - val_loss: 0.1141 - val_mae: 0.3936\n",
            "Epoch 95/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.1779 - mae: 0.4905 - val_loss: 0.1136 - val_mae: 0.3917\n",
            "Epoch 96/100\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.1780 - mae: 0.4913 - val_loss: 0.1116 - val_mae: 0.3887\n",
            "Epoch 97/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.1765 - mae: 0.4887 - val_loss: 0.1119 - val_mae: 0.3885\n",
            "Epoch 98/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.1783 - mae: 0.4910 - val_loss: 0.1105 - val_mae: 0.3862\n",
            "Epoch 99/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.1743 - mae: 0.4854 - val_loss: 0.1123 - val_mae: 0.3873\n",
            "Epoch 100/100\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.1732 - mae: 0.4839 - val_loss: 0.1110 - val_mae: 0.3860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:06:36,882] Trial 4 finished with value: 0.11100940406322479 and parameters: {'learning_rate': 1.0992038168078248e-05, 'dropout_rate': 0.16611554221563118, 'batch_size': 64, 'epochs': 100}. Best is trial 0 with value: 0.039827607572078705.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "60/60 [==============================] - 3s 12ms/step - loss: 0.7761 - mae: 1.2506 - val_loss: 0.3897 - val_mae: 0.8316\n",
            "Epoch 2/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.7767 - mae: 1.2506 - val_loss: 0.1716 - val_mae: 0.5034\n",
            "Epoch 3/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.7837 - mae: 1.2595 - val_loss: 0.1563 - val_mae: 0.4772\n",
            "Epoch 4/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.7894 - mae: 1.2649 - val_loss: 0.1497 - val_mae: 0.4633\n",
            "Epoch 5/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7759 - mae: 1.2502 - val_loss: 0.1511 - val_mae: 0.4633\n",
            "Epoch 6/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7829 - mae: 1.2586 - val_loss: 0.1484 - val_mae: 0.4563\n",
            "Epoch 7/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.7775 - mae: 1.2514 - val_loss: 0.1501 - val_mae: 0.4595\n",
            "Epoch 8/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7730 - mae: 1.2445 - val_loss: 0.1500 - val_mae: 0.4596\n",
            "Epoch 9/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.7674 - mae: 1.2411 - val_loss: 0.1501 - val_mae: 0.4600\n",
            "Epoch 10/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.7775 - mae: 1.2512 - val_loss: 0.1505 - val_mae: 0.4603\n",
            "Epoch 11/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.7846 - mae: 1.2589 - val_loss: 0.1514 - val_mae: 0.4625\n",
            "Epoch 12/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.7707 - mae: 1.2448 - val_loss: 0.1521 - val_mae: 0.4638\n",
            "Epoch 13/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.7729 - mae: 1.2475 - val_loss: 0.1499 - val_mae: 0.4590\n",
            "Epoch 14/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.7719 - mae: 1.2458 - val_loss: 0.1485 - val_mae: 0.4566\n",
            "Epoch 15/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7767 - mae: 1.2520 - val_loss: 0.1482 - val_mae: 0.4560\n",
            "Epoch 16/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.7615 - mae: 1.2328 - val_loss: 0.1477 - val_mae: 0.4560\n",
            "Epoch 17/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7607 - mae: 1.2319 - val_loss: 0.1482 - val_mae: 0.4560\n",
            "Epoch 18/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.7574 - mae: 1.2301 - val_loss: 0.1473 - val_mae: 0.4544\n",
            "Epoch 19/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.7594 - mae: 1.2319 - val_loss: 0.1469 - val_mae: 0.4539\n",
            "Epoch 20/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7562 - mae: 1.2266 - val_loss: 0.1474 - val_mae: 0.4547\n",
            "Epoch 21/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.7600 - mae: 1.2315 - val_loss: 0.1473 - val_mae: 0.4546\n",
            "Epoch 22/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7502 - mae: 1.2214 - val_loss: 0.1458 - val_mae: 0.4522\n",
            "Epoch 23/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7586 - mae: 1.2291 - val_loss: 0.1472 - val_mae: 0.4536\n",
            "Epoch 24/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.7456 - mae: 1.2139 - val_loss: 0.1477 - val_mae: 0.4549\n",
            "Epoch 25/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7480 - mae: 1.2168 - val_loss: 0.1452 - val_mae: 0.4504\n",
            "Epoch 26/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7621 - mae: 1.2321 - val_loss: 0.1460 - val_mae: 0.4525\n",
            "Epoch 27/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7433 - mae: 1.2119 - val_loss: 0.1431 - val_mae: 0.4469\n",
            "Epoch 28/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7644 - mae: 1.2358 - val_loss: 0.1442 - val_mae: 0.4486\n",
            "Epoch 29/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.7446 - mae: 1.2131 - val_loss: 0.1433 - val_mae: 0.4456\n",
            "Epoch 30/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.7329 - mae: 1.1993 - val_loss: 0.1442 - val_mae: 0.4478\n",
            "Epoch 31/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.7365 - mae: 1.2033 - val_loss: 0.1443 - val_mae: 0.4479\n",
            "Epoch 32/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7398 - mae: 1.2069 - val_loss: 0.1448 - val_mae: 0.4485\n",
            "Epoch 33/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.7215 - mae: 1.1866 - val_loss: 0.1418 - val_mae: 0.4435\n",
            "Epoch 34/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.7480 - mae: 1.2173 - val_loss: 0.1420 - val_mae: 0.4444\n",
            "Epoch 35/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7312 - mae: 1.1975 - val_loss: 0.1419 - val_mae: 0.4436\n",
            "Epoch 36/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.7474 - mae: 1.2163 - val_loss: 0.1422 - val_mae: 0.4442\n",
            "Epoch 37/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.7332 - mae: 1.1994 - val_loss: 0.1413 - val_mae: 0.4420\n",
            "Epoch 38/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.7370 - mae: 1.2034 - val_loss: 0.1426 - val_mae: 0.4451\n",
            "Epoch 39/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.7148 - mae: 1.1788 - val_loss: 0.1398 - val_mae: 0.4407\n",
            "Epoch 40/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.7228 - mae: 1.1869 - val_loss: 0.1405 - val_mae: 0.4422\n",
            "Epoch 41/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.7319 - mae: 1.1969 - val_loss: 0.1418 - val_mae: 0.4447\n",
            "Epoch 42/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.7216 - mae: 1.1844 - val_loss: 0.1392 - val_mae: 0.4390\n",
            "Epoch 43/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.7159 - mae: 1.1795 - val_loss: 0.1401 - val_mae: 0.4407\n",
            "Epoch 44/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.7254 - mae: 1.1917 - val_loss: 0.1409 - val_mae: 0.4428\n",
            "Epoch 45/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.7097 - mae: 1.1716 - val_loss: 0.1407 - val_mae: 0.4415\n",
            "Epoch 46/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.7177 - mae: 1.1816 - val_loss: 0.1395 - val_mae: 0.4383\n",
            "Epoch 47/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7146 - mae: 1.1785 - val_loss: 0.1379 - val_mae: 0.4357\n",
            "Epoch 48/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7198 - mae: 1.1837 - val_loss: 0.1385 - val_mae: 0.4378\n",
            "Epoch 49/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7154 - mae: 1.1784 - val_loss: 0.1392 - val_mae: 0.4393\n",
            "Epoch 50/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7072 - mae: 1.1676 - val_loss: 0.1376 - val_mae: 0.4358\n",
            "Epoch 51/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.7240 - mae: 1.1892 - val_loss: 0.1399 - val_mae: 0.4396\n",
            "Epoch 52/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.7115 - mae: 1.1729 - val_loss: 0.1367 - val_mae: 0.4340\n",
            "Epoch 53/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7004 - mae: 1.1616 - val_loss: 0.1377 - val_mae: 0.4349\n",
            "Epoch 54/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.7160 - mae: 1.1778 - val_loss: 0.1379 - val_mae: 0.4348\n",
            "Epoch 55/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.6993 - mae: 1.1597 - val_loss: 0.1373 - val_mae: 0.4338\n",
            "Epoch 56/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.7125 - mae: 1.1745 - val_loss: 0.1361 - val_mae: 0.4322\n",
            "Epoch 57/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.7006 - mae: 1.1615 - val_loss: 0.1353 - val_mae: 0.4303\n",
            "Epoch 58/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.6855 - mae: 1.1439 - val_loss: 0.1346 - val_mae: 0.4285\n",
            "Epoch 59/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.6958 - mae: 1.1560 - val_loss: 0.1347 - val_mae: 0.4294\n",
            "Epoch 60/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7021 - mae: 1.1634 - val_loss: 0.1354 - val_mae: 0.4309\n",
            "Epoch 61/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.6937 - mae: 1.1533 - val_loss: 0.1349 - val_mae: 0.4293\n",
            "Epoch 62/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.6909 - mae: 1.1491 - val_loss: 0.1360 - val_mae: 0.4314\n",
            "Epoch 63/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.6826 - mae: 1.1394 - val_loss: 0.1346 - val_mae: 0.4285\n",
            "Epoch 64/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.6916 - mae: 1.1514 - val_loss: 0.1335 - val_mae: 0.4266\n",
            "Epoch 65/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.6929 - mae: 1.1530 - val_loss: 0.1325 - val_mae: 0.4248\n",
            "Epoch 66/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.6957 - mae: 1.1550 - val_loss: 0.1334 - val_mae: 0.4266\n",
            "Epoch 67/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6892 - mae: 1.1468 - val_loss: 0.1314 - val_mae: 0.4235\n",
            "Epoch 68/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6969 - mae: 1.1566 - val_loss: 0.1331 - val_mae: 0.4257\n",
            "Epoch 69/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.7008 - mae: 1.1602 - val_loss: 0.1334 - val_mae: 0.4265\n",
            "Epoch 70/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6885 - mae: 1.1469 - val_loss: 0.1318 - val_mae: 0.4236\n",
            "Epoch 71/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6729 - mae: 1.1292 - val_loss: 0.1316 - val_mae: 0.4221\n",
            "Epoch 72/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6874 - mae: 1.1454 - val_loss: 0.1323 - val_mae: 0.4227\n",
            "Epoch 73/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.6764 - mae: 1.1321 - val_loss: 0.1303 - val_mae: 0.4198\n",
            "Epoch 74/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6800 - mae: 1.1366 - val_loss: 0.1300 - val_mae: 0.4192\n",
            "Epoch 75/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6687 - mae: 1.1248 - val_loss: 0.1308 - val_mae: 0.4203\n",
            "Epoch 76/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6718 - mae: 1.1269 - val_loss: 0.1301 - val_mae: 0.4199\n",
            "Epoch 77/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6756 - mae: 1.1317 - val_loss: 0.1309 - val_mae: 0.4202\n",
            "Epoch 78/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6638 - mae: 1.1191 - val_loss: 0.1300 - val_mae: 0.4190\n",
            "Epoch 79/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6804 - mae: 1.1345 - val_loss: 0.1294 - val_mae: 0.4186\n",
            "Epoch 80/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6938 - mae: 1.1511 - val_loss: 0.1296 - val_mae: 0.4182\n",
            "Epoch 81/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6768 - mae: 1.1325 - val_loss: 0.1290 - val_mae: 0.4170\n",
            "Epoch 82/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6800 - mae: 1.1351 - val_loss: 0.1297 - val_mae: 0.4193\n",
            "Epoch 83/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6651 - mae: 1.1196 - val_loss: 0.1278 - val_mae: 0.4148\n",
            "Epoch 84/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6703 - mae: 1.1230 - val_loss: 0.1281 - val_mae: 0.4155\n",
            "Epoch 85/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6486 - mae: 1.0998 - val_loss: 0.1278 - val_mae: 0.4148\n",
            "Epoch 86/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6627 - mae: 1.1157 - val_loss: 0.1280 - val_mae: 0.4153\n",
            "Epoch 87/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6566 - mae: 1.1082 - val_loss: 0.1274 - val_mae: 0.4141\n",
            "Epoch 88/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6549 - mae: 1.1073 - val_loss: 0.1264 - val_mae: 0.4121\n",
            "Epoch 89/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.6633 - mae: 1.1156 - val_loss: 0.1266 - val_mae: 0.4119\n",
            "Epoch 90/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.6486 - mae: 1.0994 - val_loss: 0.1254 - val_mae: 0.4101\n",
            "Epoch 91/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.6610 - mae: 1.1115 - val_loss: 0.1255 - val_mae: 0.4102\n",
            "Epoch 92/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.6546 - mae: 1.1058 - val_loss: 0.1243 - val_mae: 0.4070\n",
            "Epoch 93/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.6517 - mae: 1.1004 - val_loss: 0.1254 - val_mae: 0.4082\n",
            "Epoch 94/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.6389 - mae: 1.0878 - val_loss: 0.1247 - val_mae: 0.4074\n",
            "Epoch 95/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6486 - mae: 1.1011 - val_loss: 0.1249 - val_mae: 0.4080\n",
            "Epoch 96/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6479 - mae: 1.0971 - val_loss: 0.1241 - val_mae: 0.4066\n",
            "Epoch 97/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6498 - mae: 1.0995 - val_loss: 0.1252 - val_mae: 0.4077\n",
            "Epoch 98/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6398 - mae: 1.0882 - val_loss: 0.1248 - val_mae: 0.4066\n",
            "Epoch 99/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6499 - mae: 1.1004 - val_loss: 0.1254 - val_mae: 0.4088\n",
            "Epoch 100/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6363 - mae: 1.0833 - val_loss: 0.1251 - val_mae: 0.4088\n",
            "Epoch 101/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6413 - mae: 1.0892 - val_loss: 0.1252 - val_mae: 0.4092\n",
            "Epoch 102/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6434 - mae: 1.0916 - val_loss: 0.1251 - val_mae: 0.4090\n",
            "Epoch 103/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6412 - mae: 1.0900 - val_loss: 0.1248 - val_mae: 0.4085\n",
            "Epoch 104/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6335 - mae: 1.0789 - val_loss: 0.1244 - val_mae: 0.4072\n",
            "Epoch 105/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6569 - mae: 1.1088 - val_loss: 0.1240 - val_mae: 0.4049\n",
            "Epoch 106/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6326 - mae: 1.0799 - val_loss: 0.1231 - val_mae: 0.4040\n",
            "Epoch 107/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6322 - mae: 1.0811 - val_loss: 0.1243 - val_mae: 0.4064\n",
            "Epoch 108/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6436 - mae: 1.0916 - val_loss: 0.1233 - val_mae: 0.4049\n",
            "Epoch 109/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.6366 - mae: 1.0826 - val_loss: 0.1234 - val_mae: 0.4045\n",
            "Epoch 110/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6253 - mae: 1.0700 - val_loss: 0.1219 - val_mae: 0.4020\n",
            "Epoch 111/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.6305 - mae: 1.0755 - val_loss: 0.1206 - val_mae: 0.4000\n",
            "Epoch 112/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6255 - mae: 1.0709 - val_loss: 0.1211 - val_mae: 0.4002\n",
            "Epoch 113/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6165 - mae: 1.0603 - val_loss: 0.1203 - val_mae: 0.3984\n",
            "Epoch 114/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6193 - mae: 1.0628 - val_loss: 0.1189 - val_mae: 0.3966\n",
            "Epoch 115/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6289 - mae: 1.0735 - val_loss: 0.1200 - val_mae: 0.3983\n",
            "Epoch 116/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.6121 - mae: 1.0553 - val_loss: 0.1206 - val_mae: 0.3991\n",
            "Epoch 117/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.6192 - mae: 1.0616 - val_loss: 0.1185 - val_mae: 0.3953\n",
            "Epoch 118/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.6217 - mae: 1.0655 - val_loss: 0.1200 - val_mae: 0.3978\n",
            "Epoch 119/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.6211 - mae: 1.0656 - val_loss: 0.1198 - val_mae: 0.3973\n",
            "Epoch 120/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.6181 - mae: 1.0612 - val_loss: 0.1202 - val_mae: 0.3976\n",
            "Epoch 121/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.6097 - mae: 1.0536 - val_loss: 0.1186 - val_mae: 0.3951\n",
            "Epoch 122/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.6158 - mae: 1.0587 - val_loss: 0.1183 - val_mae: 0.3937\n",
            "Epoch 123/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6124 - mae: 1.0539 - val_loss: 0.1179 - val_mae: 0.3936\n",
            "Epoch 124/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6057 - mae: 1.0452 - val_loss: 0.1166 - val_mae: 0.3915\n",
            "Epoch 125/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6135 - mae: 1.0546 - val_loss: 0.1176 - val_mae: 0.3929\n",
            "Epoch 126/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6003 - mae: 1.0410 - val_loss: 0.1174 - val_mae: 0.3930\n",
            "Epoch 127/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6130 - mae: 1.0548 - val_loss: 0.1188 - val_mae: 0.3949\n",
            "Epoch 128/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5951 - mae: 1.0330 - val_loss: 0.1175 - val_mae: 0.3916\n",
            "Epoch 129/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6070 - mae: 1.0473 - val_loss: 0.1173 - val_mae: 0.3919\n",
            "Epoch 130/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.6062 - mae: 1.0487 - val_loss: 0.1171 - val_mae: 0.3911\n",
            "Epoch 131/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5794 - mae: 1.0138 - val_loss: 0.1160 - val_mae: 0.3891\n",
            "Epoch 132/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6026 - mae: 1.0419 - val_loss: 0.1149 - val_mae: 0.3873\n",
            "Epoch 133/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6035 - mae: 1.0428 - val_loss: 0.1150 - val_mae: 0.3871\n",
            "Epoch 134/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.6021 - mae: 1.0419 - val_loss: 0.1146 - val_mae: 0.3861\n",
            "Epoch 135/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6075 - mae: 1.0480 - val_loss: 0.1154 - val_mae: 0.3882\n",
            "Epoch 136/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6028 - mae: 1.0420 - val_loss: 0.1165 - val_mae: 0.3900\n",
            "Epoch 137/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6029 - mae: 1.0427 - val_loss: 0.1170 - val_mae: 0.3910\n",
            "Epoch 138/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5935 - mae: 1.0309 - val_loss: 0.1153 - val_mae: 0.3873\n",
            "Epoch 139/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5998 - mae: 1.0389 - val_loss: 0.1157 - val_mae: 0.3878\n",
            "Epoch 140/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5924 - mae: 1.0300 - val_loss: 0.1157 - val_mae: 0.3876\n",
            "Epoch 141/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.6054 - mae: 1.0441 - val_loss: 0.1149 - val_mae: 0.3863\n",
            "Epoch 142/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5868 - mae: 1.0228 - val_loss: 0.1139 - val_mae: 0.3844\n",
            "Epoch 143/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.5853 - mae: 1.0206 - val_loss: 0.1131 - val_mae: 0.3830\n",
            "Epoch 144/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.5895 - mae: 1.0280 - val_loss: 0.1136 - val_mae: 0.3843\n",
            "Epoch 145/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.5836 - mae: 1.0175 - val_loss: 0.1135 - val_mae: 0.3835\n",
            "Epoch 146/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.5787 - mae: 1.0122 - val_loss: 0.1126 - val_mae: 0.3816\n",
            "Epoch 147/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.5888 - mae: 1.0244 - val_loss: 0.1129 - val_mae: 0.3826\n",
            "Epoch 148/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.5877 - mae: 1.0232 - val_loss: 0.1123 - val_mae: 0.3805\n",
            "Epoch 149/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.5924 - mae: 1.0268 - val_loss: 0.1121 - val_mae: 0.3801\n",
            "Epoch 150/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5835 - mae: 1.0183 - val_loss: 0.1112 - val_mae: 0.3789\n",
            "Epoch 151/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5818 - mae: 1.0184 - val_loss: 0.1116 - val_mae: 0.3793\n",
            "Epoch 152/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5866 - mae: 1.0212 - val_loss: 0.1120 - val_mae: 0.3806\n",
            "Epoch 153/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5734 - mae: 1.0068 - val_loss: 0.1120 - val_mae: 0.3805\n",
            "Epoch 154/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5675 - mae: 0.9985 - val_loss: 0.1115 - val_mae: 0.3799\n",
            "Epoch 155/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5722 - mae: 1.0055 - val_loss: 0.1116 - val_mae: 0.3797\n",
            "Epoch 156/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5750 - mae: 1.0081 - val_loss: 0.1107 - val_mae: 0.3777\n",
            "Epoch 157/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5785 - mae: 1.0114 - val_loss: 0.1100 - val_mae: 0.3762\n",
            "Epoch 158/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.5687 - mae: 1.0007 - val_loss: 0.1090 - val_mae: 0.3739\n",
            "Epoch 159/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.5683 - mae: 0.9990 - val_loss: 0.1095 - val_mae: 0.3754\n",
            "Epoch 160/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.5635 - mae: 0.9930 - val_loss: 0.1098 - val_mae: 0.3759\n",
            "Epoch 161/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5667 - mae: 0.9990 - val_loss: 0.1102 - val_mae: 0.3766\n",
            "Epoch 162/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5581 - mae: 0.9858 - val_loss: 0.1094 - val_mae: 0.3742\n",
            "Epoch 163/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5704 - mae: 1.0025 - val_loss: 0.1094 - val_mae: 0.3748\n",
            "Epoch 164/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.5540 - mae: 0.9822 - val_loss: 0.1091 - val_mae: 0.3740\n",
            "Epoch 165/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.5656 - mae: 0.9970 - val_loss: 0.1092 - val_mae: 0.3743\n",
            "Epoch 166/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.5517 - mae: 0.9801 - val_loss: 0.1082 - val_mae: 0.3728\n",
            "Epoch 167/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5527 - mae: 0.9820 - val_loss: 0.1084 - val_mae: 0.3730\n",
            "Epoch 168/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5523 - mae: 0.9801 - val_loss: 0.1076 - val_mae: 0.3716\n",
            "Epoch 169/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.5580 - mae: 0.9869 - val_loss: 0.1063 - val_mae: 0.3687\n",
            "Epoch 170/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.5503 - mae: 0.9767 - val_loss: 0.1069 - val_mae: 0.3690\n",
            "Epoch 171/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.5500 - mae: 0.9778 - val_loss: 0.1067 - val_mae: 0.3684\n",
            "Epoch 172/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.5467 - mae: 0.9736 - val_loss: 0.1067 - val_mae: 0.3690\n",
            "Epoch 173/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.5474 - mae: 0.9742 - val_loss: 0.1067 - val_mae: 0.3698\n",
            "Epoch 174/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.5394 - mae: 0.9663 - val_loss: 0.1064 - val_mae: 0.3680\n",
            "Epoch 175/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.5508 - mae: 0.9799 - val_loss: 0.1063 - val_mae: 0.3684\n",
            "Epoch 176/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.5537 - mae: 0.9819 - val_loss: 0.1071 - val_mae: 0.3699\n",
            "Epoch 177/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.5566 - mae: 0.9860 - val_loss: 0.1072 - val_mae: 0.3697\n",
            "Epoch 178/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5503 - mae: 0.9778 - val_loss: 0.1073 - val_mae: 0.3695\n",
            "Epoch 179/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5436 - mae: 0.9693 - val_loss: 0.1067 - val_mae: 0.3683\n",
            "Epoch 180/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.5502 - mae: 0.9780 - val_loss: 0.1059 - val_mae: 0.3675\n",
            "Epoch 181/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5252 - mae: 0.9476 - val_loss: 0.1051 - val_mae: 0.3656\n",
            "Epoch 182/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5408 - mae: 0.9664 - val_loss: 0.1054 - val_mae: 0.3650\n",
            "Epoch 183/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5471 - mae: 0.9727 - val_loss: 0.1049 - val_mae: 0.3636\n",
            "Epoch 184/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5251 - mae: 0.9460 - val_loss: 0.1052 - val_mae: 0.3647\n",
            "Epoch 185/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5266 - mae: 0.9485 - val_loss: 0.1047 - val_mae: 0.3642\n",
            "Epoch 186/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5465 - mae: 0.9714 - val_loss: 0.1045 - val_mae: 0.3637\n",
            "Epoch 187/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5283 - mae: 0.9515 - val_loss: 0.1052 - val_mae: 0.3647\n",
            "Epoch 188/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5445 - mae: 0.9682 - val_loss: 0.1053 - val_mae: 0.3655\n",
            "Epoch 189/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5361 - mae: 0.9590 - val_loss: 0.1053 - val_mae: 0.3648\n",
            "Epoch 190/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5388 - mae: 0.9615 - val_loss: 0.1038 - val_mae: 0.3621\n",
            "Epoch 191/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5358 - mae: 0.9576 - val_loss: 0.1030 - val_mae: 0.3605\n",
            "Epoch 192/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.5216 - mae: 0.9424 - val_loss: 0.1031 - val_mae: 0.3606\n",
            "Epoch 193/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.5350 - mae: 0.9579 - val_loss: 0.1029 - val_mae: 0.3602\n",
            "Epoch 194/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5228 - mae: 0.9427 - val_loss: 0.1027 - val_mae: 0.3598\n",
            "Epoch 195/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5188 - mae: 0.9399 - val_loss: 0.1034 - val_mae: 0.3612\n",
            "Epoch 196/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5290 - mae: 0.9513 - val_loss: 0.1034 - val_mae: 0.3618\n",
            "Epoch 197/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.5294 - mae: 0.9521 - val_loss: 0.1027 - val_mae: 0.3604\n",
            "Epoch 198/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.5198 - mae: 0.9413 - val_loss: 0.1030 - val_mae: 0.3606\n",
            "Epoch 199/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.5293 - mae: 0.9499 - val_loss: 0.1029 - val_mae: 0.3603\n",
            "Epoch 200/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.5281 - mae: 0.9494 - val_loss: 0.1023 - val_mae: 0.3587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:09:01,393] Trial 5 finished with value: 0.10227872431278229 and parameters: {'learning_rate': 5.60299594929503e-06, 'dropout_rate': 0.7066812161322902, 'batch_size': 64, 'epochs': 200}. Best is trial 0 with value: 0.039827607572078705.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "15/15 [==============================] - 3s 39ms/step - loss: 0.6373 - mae: 1.0953 - val_loss: 0.7957 - val_mae: 1.3311\n",
            "Epoch 2/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6317 - mae: 1.0868 - val_loss: 0.3813 - val_mae: 0.8123\n",
            "Epoch 3/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.6239 - mae: 1.0793 - val_loss: 0.2295 - val_mae: 0.5948\n",
            "Epoch 4/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.6373 - mae: 1.0950 - val_loss: 0.1814 - val_mae: 0.5300\n",
            "Epoch 5/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6378 - mae: 1.0951 - val_loss: 0.1727 - val_mae: 0.5156\n",
            "Epoch 6/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6446 - mae: 1.1032 - val_loss: 0.1705 - val_mae: 0.5029\n",
            "Epoch 7/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6457 - mae: 1.1066 - val_loss: 0.1718 - val_mae: 0.4960\n",
            "Epoch 8/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6374 - mae: 1.0946 - val_loss: 0.1745 - val_mae: 0.4962\n",
            "Epoch 9/200\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6382 - mae: 1.0977 - val_loss: 0.1772 - val_mae: 0.4988\n",
            "Epoch 10/200\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6368 - mae: 1.0949 - val_loss: 0.1816 - val_mae: 0.5071\n",
            "Epoch 11/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6463 - mae: 1.1059 - val_loss: 0.1867 - val_mae: 0.5177\n",
            "Epoch 12/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.6356 - mae: 1.0928 - val_loss: 0.1894 - val_mae: 0.5241\n",
            "Epoch 13/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6344 - mae: 1.0907 - val_loss: 0.1921 - val_mae: 0.5289\n",
            "Epoch 14/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.6250 - mae: 1.0805 - val_loss: 0.1942 - val_mae: 0.5321\n",
            "Epoch 15/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6243 - mae: 1.0787 - val_loss: 0.1945 - val_mae: 0.5321\n",
            "Epoch 16/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6219 - mae: 1.0789 - val_loss: 0.1957 - val_mae: 0.5336\n",
            "Epoch 17/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6372 - mae: 1.0929 - val_loss: 0.1965 - val_mae: 0.5346\n",
            "Epoch 18/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6245 - mae: 1.0808 - val_loss: 0.1986 - val_mae: 0.5375\n",
            "Epoch 19/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6261 - mae: 1.0817 - val_loss: 0.2016 - val_mae: 0.5420\n",
            "Epoch 20/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.6264 - mae: 1.0820 - val_loss: 0.2041 - val_mae: 0.5456\n",
            "Epoch 21/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6274 - mae: 1.0821 - val_loss: 0.2056 - val_mae: 0.5479\n",
            "Epoch 22/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6373 - mae: 1.0950 - val_loss: 0.2081 - val_mae: 0.5514\n",
            "Epoch 23/200\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6275 - mae: 1.0832 - val_loss: 0.2101 - val_mae: 0.5546\n",
            "Epoch 24/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.6164 - mae: 1.0709 - val_loss: 0.2114 - val_mae: 0.5571\n",
            "Epoch 25/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6190 - mae: 1.0738 - val_loss: 0.2117 - val_mae: 0.5579\n",
            "Epoch 26/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.6290 - mae: 1.0844 - val_loss: 0.2138 - val_mae: 0.5621\n",
            "Epoch 27/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.6214 - mae: 1.0753 - val_loss: 0.2145 - val_mae: 0.5634\n",
            "Epoch 28/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.6251 - mae: 1.0803 - val_loss: 0.2157 - val_mae: 0.5656\n",
            "Epoch 29/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6182 - mae: 1.0727 - val_loss: 0.2168 - val_mae: 0.5670\n",
            "Epoch 30/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6200 - mae: 1.0758 - val_loss: 0.2168 - val_mae: 0.5665\n",
            "Epoch 31/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.6237 - mae: 1.0767 - val_loss: 0.2184 - val_mae: 0.5693\n",
            "Epoch 32/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.6272 - mae: 1.0828 - val_loss: 0.2179 - val_mae: 0.5686\n",
            "Epoch 33/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6360 - mae: 1.0933 - val_loss: 0.2182 - val_mae: 0.5690\n",
            "Epoch 34/200\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6257 - mae: 1.0807 - val_loss: 0.2176 - val_mae: 0.5681\n",
            "Epoch 35/200\n",
            "15/15 [==============================] - 0s 28ms/step - loss: 0.6187 - mae: 1.0734 - val_loss: 0.2180 - val_mae: 0.5683\n",
            "Epoch 36/200\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.6268 - mae: 1.0820 - val_loss: 0.2182 - val_mae: 0.5687\n",
            "Epoch 37/200\n",
            "15/15 [==============================] - 0s 27ms/step - loss: 0.6178 - mae: 1.0727 - val_loss: 0.2179 - val_mae: 0.5679\n",
            "Epoch 38/200\n",
            "15/15 [==============================] - 0s 27ms/step - loss: 0.6198 - mae: 1.0744 - val_loss: 0.2189 - val_mae: 0.5697\n",
            "Epoch 39/200\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.6118 - mae: 1.0644 - val_loss: 0.2190 - val_mae: 0.5695\n",
            "Epoch 40/200\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.6273 - mae: 1.0820 - val_loss: 0.2187 - val_mae: 0.5688\n",
            "Epoch 41/200\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.6193 - mae: 1.0736 - val_loss: 0.2181 - val_mae: 0.5677\n",
            "Epoch 42/200\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.6192 - mae: 1.0724 - val_loss: 0.2177 - val_mae: 0.5671\n",
            "Epoch 43/200\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.6392 - mae: 1.0963 - val_loss: 0.2183 - val_mae: 0.5681\n",
            "Epoch 44/200\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.6241 - mae: 1.0788 - val_loss: 0.2190 - val_mae: 0.5692\n",
            "Epoch 45/200\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.6250 - mae: 1.0801 - val_loss: 0.2185 - val_mae: 0.5683\n",
            "Epoch 46/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.6337 - mae: 1.0897 - val_loss: 0.2186 - val_mae: 0.5686\n",
            "Epoch 47/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6164 - mae: 1.0699 - val_loss: 0.2181 - val_mae: 0.5687\n",
            "Epoch 48/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6251 - mae: 1.0792 - val_loss: 0.2174 - val_mae: 0.5671\n",
            "Epoch 49/200\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6128 - mae: 1.0663 - val_loss: 0.2169 - val_mae: 0.5668\n",
            "Epoch 50/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6255 - mae: 1.0798 - val_loss: 0.2164 - val_mae: 0.5660\n",
            "Epoch 51/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.6139 - mae: 1.0688 - val_loss: 0.2163 - val_mae: 0.5652\n",
            "Epoch 52/200\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.6157 - mae: 1.0696 - val_loss: 0.2154 - val_mae: 0.5631\n",
            "Epoch 53/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6155 - mae: 1.0669 - val_loss: 0.2154 - val_mae: 0.5628\n",
            "Epoch 54/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6095 - mae: 1.0617 - val_loss: 0.2163 - val_mae: 0.5643\n",
            "Epoch 55/200\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6176 - mae: 1.0704 - val_loss: 0.2160 - val_mae: 0.5637\n",
            "Epoch 56/200\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6182 - mae: 1.0706 - val_loss: 0.2155 - val_mae: 0.5630\n",
            "Epoch 57/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.6122 - mae: 1.0653 - val_loss: 0.2153 - val_mae: 0.5622\n",
            "Epoch 58/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6093 - mae: 1.0611 - val_loss: 0.2141 - val_mae: 0.5602\n",
            "Epoch 59/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6182 - mae: 1.0726 - val_loss: 0.2143 - val_mae: 0.5604\n",
            "Epoch 60/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6155 - mae: 1.0676 - val_loss: 0.2139 - val_mae: 0.5592\n",
            "Epoch 61/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.5972 - mae: 1.0489 - val_loss: 0.2148 - val_mae: 0.5607\n",
            "Epoch 62/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6145 - mae: 1.0678 - val_loss: 0.2153 - val_mae: 0.5620\n",
            "Epoch 63/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6242 - mae: 1.0784 - val_loss: 0.2149 - val_mae: 0.5613\n",
            "Epoch 64/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6187 - mae: 1.0717 - val_loss: 0.2149 - val_mae: 0.5617\n",
            "Epoch 65/200\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6223 - mae: 1.0755 - val_loss: 0.2150 - val_mae: 0.5617\n",
            "Epoch 66/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.6125 - mae: 1.0642 - val_loss: 0.2155 - val_mae: 0.5622\n",
            "Epoch 67/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6108 - mae: 1.0620 - val_loss: 0.2160 - val_mae: 0.5635\n",
            "Epoch 68/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5998 - mae: 1.0513 - val_loss: 0.2157 - val_mae: 0.5634\n",
            "Epoch 69/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6103 - mae: 1.0622 - val_loss: 0.2150 - val_mae: 0.5622\n",
            "Epoch 70/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6064 - mae: 1.0584 - val_loss: 0.2139 - val_mae: 0.5602\n",
            "Epoch 71/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6055 - mae: 1.0542 - val_loss: 0.2140 - val_mae: 0.5603\n",
            "Epoch 72/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.6078 - mae: 1.0588 - val_loss: 0.2131 - val_mae: 0.5585\n",
            "Epoch 73/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6167 - mae: 1.0703 - val_loss: 0.2138 - val_mae: 0.5600\n",
            "Epoch 74/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6012 - mae: 1.0537 - val_loss: 0.2133 - val_mae: 0.5586\n",
            "Epoch 75/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6081 - mae: 1.0591 - val_loss: 0.2131 - val_mae: 0.5586\n",
            "Epoch 76/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6048 - mae: 1.0555 - val_loss: 0.2128 - val_mae: 0.5580\n",
            "Epoch 77/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.6177 - mae: 1.0704 - val_loss: 0.2123 - val_mae: 0.5574\n",
            "Epoch 78/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6038 - mae: 1.0533 - val_loss: 0.2124 - val_mae: 0.5568\n",
            "Epoch 79/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.6151 - mae: 1.0674 - val_loss: 0.2131 - val_mae: 0.5579\n",
            "Epoch 80/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6165 - mae: 1.0691 - val_loss: 0.2123 - val_mae: 0.5566\n",
            "Epoch 81/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.6046 - mae: 1.0543 - val_loss: 0.2115 - val_mae: 0.5553\n",
            "Epoch 82/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5986 - mae: 1.0474 - val_loss: 0.2109 - val_mae: 0.5548\n",
            "Epoch 83/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.6177 - mae: 1.0719 - val_loss: 0.2111 - val_mae: 0.5552\n",
            "Epoch 84/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.5965 - mae: 1.0452 - val_loss: 0.2105 - val_mae: 0.5543\n",
            "Epoch 85/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6058 - mae: 1.0556 - val_loss: 0.2111 - val_mae: 0.5556\n",
            "Epoch 86/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6076 - mae: 1.0591 - val_loss: 0.2105 - val_mae: 0.5543\n",
            "Epoch 87/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6106 - mae: 1.0598 - val_loss: 0.2099 - val_mae: 0.5530\n",
            "Epoch 88/200\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.5945 - mae: 1.0433 - val_loss: 0.2096 - val_mae: 0.5527\n",
            "Epoch 89/200\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.5967 - mae: 1.0470 - val_loss: 0.2092 - val_mae: 0.5521\n",
            "Epoch 90/200\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.6132 - mae: 1.0652 - val_loss: 0.2091 - val_mae: 0.5517\n",
            "Epoch 91/200\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.6012 - mae: 1.0512 - val_loss: 0.2078 - val_mae: 0.5496\n",
            "Epoch 92/200\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.6037 - mae: 1.0540 - val_loss: 0.2083 - val_mae: 0.5505\n",
            "Epoch 93/200\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.6042 - mae: 1.0535 - val_loss: 0.2077 - val_mae: 0.5495\n",
            "Epoch 94/200\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.5985 - mae: 1.0478 - val_loss: 0.2082 - val_mae: 0.5505\n",
            "Epoch 95/200\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.5978 - mae: 1.0473 - val_loss: 0.2082 - val_mae: 0.5508\n",
            "Epoch 96/200\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.6023 - mae: 1.0511 - val_loss: 0.2073 - val_mae: 0.5495\n",
            "Epoch 97/200\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.5932 - mae: 1.0405 - val_loss: 0.2077 - val_mae: 0.5502\n",
            "Epoch 98/200\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.5983 - mae: 1.0452 - val_loss: 0.2078 - val_mae: 0.5503\n",
            "Epoch 99/200\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.5913 - mae: 1.0377 - val_loss: 0.2083 - val_mae: 0.5512\n",
            "Epoch 100/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5986 - mae: 1.0481 - val_loss: 0.2088 - val_mae: 0.5514\n",
            "Epoch 101/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6002 - mae: 1.0482 - val_loss: 0.2088 - val_mae: 0.5514\n",
            "Epoch 102/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.6061 - mae: 1.0554 - val_loss: 0.2090 - val_mae: 0.5520\n",
            "Epoch 103/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.5993 - mae: 1.0491 - val_loss: 0.2091 - val_mae: 0.5520\n",
            "Epoch 104/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.5944 - mae: 1.0406 - val_loss: 0.2089 - val_mae: 0.5515\n",
            "Epoch 105/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.5936 - mae: 1.0398 - val_loss: 0.2086 - val_mae: 0.5511\n",
            "Epoch 106/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.6027 - mae: 1.0520 - val_loss: 0.2083 - val_mae: 0.5505\n",
            "Epoch 107/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6108 - mae: 1.0640 - val_loss: 0.2075 - val_mae: 0.5491\n",
            "Epoch 108/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6045 - mae: 1.0545 - val_loss: 0.2080 - val_mae: 0.5498\n",
            "Epoch 109/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.5956 - mae: 1.0434 - val_loss: 0.2080 - val_mae: 0.5501\n",
            "Epoch 110/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.5957 - mae: 1.0447 - val_loss: 0.2072 - val_mae: 0.5489\n",
            "Epoch 111/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.5852 - mae: 1.0316 - val_loss: 0.2066 - val_mae: 0.5477\n",
            "Epoch 112/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.5893 - mae: 1.0351 - val_loss: 0.2068 - val_mae: 0.5480\n",
            "Epoch 113/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.5928 - mae: 1.0394 - val_loss: 0.2071 - val_mae: 0.5488\n",
            "Epoch 114/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5961 - mae: 1.0419 - val_loss: 0.2064 - val_mae: 0.5482\n",
            "Epoch 115/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.5912 - mae: 1.0395 - val_loss: 0.2061 - val_mae: 0.5475\n",
            "Epoch 116/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6004 - mae: 1.0485 - val_loss: 0.2055 - val_mae: 0.5464\n",
            "Epoch 117/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5919 - mae: 1.0395 - val_loss: 0.2061 - val_mae: 0.5473\n",
            "Epoch 118/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5908 - mae: 1.0360 - val_loss: 0.2063 - val_mae: 0.5474\n",
            "Epoch 119/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5961 - mae: 1.0438 - val_loss: 0.2066 - val_mae: 0.5476\n",
            "Epoch 120/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.5965 - mae: 1.0430 - val_loss: 0.2065 - val_mae: 0.5473\n",
            "Epoch 121/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.5885 - mae: 1.0355 - val_loss: 0.2066 - val_mae: 0.5478\n",
            "Epoch 122/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.6009 - mae: 1.0498 - val_loss: 0.2059 - val_mae: 0.5470\n",
            "Epoch 123/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5928 - mae: 1.0405 - val_loss: 0.2066 - val_mae: 0.5478\n",
            "Epoch 124/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5915 - mae: 1.0380 - val_loss: 0.2056 - val_mae: 0.5463\n",
            "Epoch 125/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.5890 - mae: 1.0343 - val_loss: 0.2053 - val_mae: 0.5461\n",
            "Epoch 126/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.5918 - mae: 1.0393 - val_loss: 0.2052 - val_mae: 0.5458\n",
            "Epoch 127/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5926 - mae: 1.0392 - val_loss: 0.2048 - val_mae: 0.5449\n",
            "Epoch 128/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5846 - mae: 1.0292 - val_loss: 0.2037 - val_mae: 0.5432\n",
            "Epoch 129/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.5942 - mae: 1.0418 - val_loss: 0.2043 - val_mae: 0.5440\n",
            "Epoch 130/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.5951 - mae: 1.0422 - val_loss: 0.2044 - val_mae: 0.5442\n",
            "Epoch 131/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.5842 - mae: 1.0268 - val_loss: 0.2043 - val_mae: 0.5438\n",
            "Epoch 132/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.5946 - mae: 1.0393 - val_loss: 0.2039 - val_mae: 0.5429\n",
            "Epoch 133/200\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.5920 - mae: 1.0389 - val_loss: 0.2041 - val_mae: 0.5432\n",
            "Epoch 134/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.5863 - mae: 1.0315 - val_loss: 0.2044 - val_mae: 0.5436\n",
            "Epoch 135/200\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.5786 - mae: 1.0231 - val_loss: 0.2036 - val_mae: 0.5423\n",
            "Epoch 136/200\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.5911 - mae: 1.0378 - val_loss: 0.2037 - val_mae: 0.5423\n",
            "Epoch 137/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.5856 - mae: 1.0323 - val_loss: 0.2042 - val_mae: 0.5432\n",
            "Epoch 138/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5784 - mae: 1.0223 - val_loss: 0.2036 - val_mae: 0.5425\n",
            "Epoch 139/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5792 - mae: 1.0238 - val_loss: 0.2033 - val_mae: 0.5422\n",
            "Epoch 140/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5865 - mae: 1.0332 - val_loss: 0.2036 - val_mae: 0.5422\n",
            "Epoch 141/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.5805 - mae: 1.0248 - val_loss: 0.2035 - val_mae: 0.5421\n",
            "Epoch 142/200\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.5744 - mae: 1.0177 - val_loss: 0.2031 - val_mae: 0.5413\n",
            "Epoch 143/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.5762 - mae: 1.0202 - val_loss: 0.2031 - val_mae: 0.5410\n",
            "Epoch 144/200\n",
            "15/15 [==============================] - 0s 28ms/step - loss: 0.5721 - mae: 1.0153 - val_loss: 0.2026 - val_mae: 0.5404\n",
            "Epoch 145/200\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.6003 - mae: 1.0482 - val_loss: 0.2027 - val_mae: 0.5411\n",
            "Epoch 146/200\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.5864 - mae: 1.0308 - val_loss: 0.2024 - val_mae: 0.5401\n",
            "Epoch 147/200\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.5983 - mae: 1.0469 - val_loss: 0.2022 - val_mae: 0.5399\n",
            "Epoch 148/200\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.5748 - mae: 1.0182 - val_loss: 0.2026 - val_mae: 0.5407\n",
            "Epoch 149/200\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.5713 - mae: 1.0131 - val_loss: 0.2019 - val_mae: 0.5398\n",
            "Epoch 150/200\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.5802 - mae: 1.0240 - val_loss: 0.2012 - val_mae: 0.5388\n",
            "Epoch 151/200\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.5687 - mae: 1.0090 - val_loss: 0.2019 - val_mae: 0.5406\n",
            "Epoch 152/200\n",
            "15/15 [==============================] - 0s 30ms/step - loss: 0.5772 - mae: 1.0208 - val_loss: 0.2021 - val_mae: 0.5410\n",
            "Epoch 153/200\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.5741 - mae: 1.0182 - val_loss: 0.2025 - val_mae: 0.5420\n",
            "Epoch 154/200\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.5783 - mae: 1.0215 - val_loss: 0.2028 - val_mae: 0.5423\n",
            "Epoch 155/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.5886 - mae: 1.0329 - val_loss: 0.2022 - val_mae: 0.5415\n",
            "Epoch 156/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.5729 - mae: 1.0161 - val_loss: 0.2024 - val_mae: 0.5415\n",
            "Epoch 157/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.5780 - mae: 1.0202 - val_loss: 0.2025 - val_mae: 0.5420\n",
            "Epoch 158/200\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.5846 - mae: 1.0294 - val_loss: 0.2019 - val_mae: 0.5414\n",
            "Epoch 159/200\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.5846 - mae: 1.0286 - val_loss: 0.2013 - val_mae: 0.5404\n",
            "Epoch 160/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.5767 - mae: 1.0187 - val_loss: 0.2012 - val_mae: 0.5399\n",
            "Epoch 161/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.5702 - mae: 1.0132 - val_loss: 0.2005 - val_mae: 0.5389\n",
            "Epoch 162/200\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.5808 - mae: 1.0249 - val_loss: 0.2007 - val_mae: 0.5390\n",
            "Epoch 163/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.5814 - mae: 1.0256 - val_loss: 0.2012 - val_mae: 0.5399\n",
            "Epoch 164/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.5678 - mae: 1.0065 - val_loss: 0.2007 - val_mae: 0.5394\n",
            "Epoch 165/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.5785 - mae: 1.0212 - val_loss: 0.2009 - val_mae: 0.5396\n",
            "Epoch 166/200\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.5729 - mae: 1.0155 - val_loss: 0.2010 - val_mae: 0.5397\n",
            "Epoch 167/200\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.5763 - mae: 1.0180 - val_loss: 0.2005 - val_mae: 0.5387\n",
            "Epoch 168/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5730 - mae: 1.0130 - val_loss: 0.1998 - val_mae: 0.5375\n",
            "Epoch 169/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5866 - mae: 1.0321 - val_loss: 0.1997 - val_mae: 0.5372\n",
            "Epoch 170/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5736 - mae: 1.0153 - val_loss: 0.1996 - val_mae: 0.5364\n",
            "Epoch 171/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5741 - mae: 1.0169 - val_loss: 0.1989 - val_mae: 0.5351\n",
            "Epoch 172/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5703 - mae: 1.0121 - val_loss: 0.1994 - val_mae: 0.5360\n",
            "Epoch 173/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5718 - mae: 1.0133 - val_loss: 0.1993 - val_mae: 0.5358\n",
            "Epoch 174/200\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.5691 - mae: 1.0116 - val_loss: 0.1990 - val_mae: 0.5348\n",
            "Epoch 175/200\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.5682 - mae: 1.0085 - val_loss: 0.1990 - val_mae: 0.5350\n",
            "Epoch 176/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5707 - mae: 1.0131 - val_loss: 0.1991 - val_mae: 0.5346\n",
            "Epoch 177/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.5797 - mae: 1.0219 - val_loss: 0.1995 - val_mae: 0.5358\n",
            "Epoch 178/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.5671 - mae: 1.0071 - val_loss: 0.1993 - val_mae: 0.5349\n",
            "Epoch 179/200\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.5728 - mae: 1.0151 - val_loss: 0.1998 - val_mae: 0.5359\n",
            "Epoch 180/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.5691 - mae: 1.0100 - val_loss: 0.1992 - val_mae: 0.5350\n",
            "Epoch 181/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.5735 - mae: 1.0142 - val_loss: 0.1987 - val_mae: 0.5342\n",
            "Epoch 182/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5659 - mae: 1.0058 - val_loss: 0.1986 - val_mae: 0.5343\n",
            "Epoch 183/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5528 - mae: 0.9894 - val_loss: 0.1977 - val_mae: 0.5326\n",
            "Epoch 184/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5747 - mae: 1.0152 - val_loss: 0.1970 - val_mae: 0.5313\n",
            "Epoch 185/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.5684 - mae: 1.0087 - val_loss: 0.1975 - val_mae: 0.5322\n",
            "Epoch 186/200\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.5651 - mae: 1.0054 - val_loss: 0.1979 - val_mae: 0.5329\n",
            "Epoch 187/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.5686 - mae: 1.0091 - val_loss: 0.1977 - val_mae: 0.5326\n",
            "Epoch 188/200\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.5476 - mae: 0.9862 - val_loss: 0.1982 - val_mae: 0.5334\n",
            "Epoch 189/200\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.5633 - mae: 1.0037 - val_loss: 0.1981 - val_mae: 0.5338\n",
            "Epoch 190/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5567 - mae: 0.9954 - val_loss: 0.1978 - val_mae: 0.5333\n",
            "Epoch 191/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5679 - mae: 1.0095 - val_loss: 0.1978 - val_mae: 0.5332\n",
            "Epoch 192/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.5713 - mae: 1.0126 - val_loss: 0.1976 - val_mae: 0.5334\n",
            "Epoch 193/200\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5732 - mae: 1.0150 - val_loss: 0.1972 - val_mae: 0.5330\n",
            "Epoch 194/200\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.5682 - mae: 1.0080 - val_loss: 0.1971 - val_mae: 0.5325\n",
            "Epoch 195/200\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.5700 - mae: 1.0105 - val_loss: 0.1969 - val_mae: 0.5320\n",
            "Epoch 196/200\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.5683 - mae: 1.0079 - val_loss: 0.1967 - val_mae: 0.5316\n",
            "Epoch 197/200\n",
            "15/15 [==============================] - 0s 28ms/step - loss: 0.5546 - mae: 0.9924 - val_loss: 0.1963 - val_mae: 0.5309\n",
            "Epoch 198/200\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.5551 - mae: 0.9922 - val_loss: 0.1966 - val_mae: 0.5316\n",
            "Epoch 199/200\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.5705 - mae: 1.0116 - val_loss: 0.1965 - val_mae: 0.5311\n",
            "Epoch 200/200\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.5571 - mae: 0.9943 - val_loss: 0.1966 - val_mae: 0.5311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:10:26,218] Trial 6 finished with value: 0.1966009885072708 and parameters: {'learning_rate': 4.392226423210589e-06, 'dropout_rate': 0.6067993354238659, 'batch_size': 256, 'epochs': 200}. Best is trial 0 with value: 0.039827607572078705.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "120/120 [==============================] - 3s 7ms/step - loss: 0.2315 - mae: 0.5694 - val_loss: 0.5476 - val_mae: 0.9873\n",
            "Epoch 2/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0624 - mae: 0.2770 - val_loss: 0.0451 - val_mae: 0.2372\n",
            "Epoch 3/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0425 - mae: 0.2314 - val_loss: 0.0401 - val_mae: 0.2276\n",
            "Epoch 4/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0376 - mae: 0.2166 - val_loss: 0.0456 - val_mae: 0.2325\n",
            "Epoch 5/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0354 - mae: 0.2084 - val_loss: 0.0383 - val_mae: 0.2184\n",
            "Epoch 6/200\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0346 - mae: 0.2052 - val_loss: 0.0444 - val_mae: 0.2317\n",
            "Epoch 7/200\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0342 - mae: 0.2037 - val_loss: 0.0342 - val_mae: 0.2064\n",
            "Epoch 8/200\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0337 - mae: 0.2014 - val_loss: 0.0781 - val_mae: 0.3008\n",
            "Epoch 9/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0338 - mae: 0.2017 - val_loss: 0.0720 - val_mae: 0.2864\n",
            "Epoch 10/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0332 - mae: 0.1992 - val_loss: 0.0318 - val_mae: 0.1994\n",
            "Epoch 11/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0332 - mae: 0.1993 - val_loss: 0.0391 - val_mae: 0.2136\n",
            "Epoch 12/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0332 - mae: 0.1991 - val_loss: 0.0464 - val_mae: 0.2302\n",
            "Epoch 13/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0330 - mae: 0.1982 - val_loss: 0.0418 - val_mae: 0.2215\n",
            "Epoch 14/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0329 - mae: 0.1976 - val_loss: 0.0535 - val_mae: 0.2438\n",
            "Epoch 15/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0329 - mae: 0.1975 - val_loss: 0.0444 - val_mae: 0.2308\n",
            "Epoch 16/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0326 - mae: 0.1968 - val_loss: 0.0495 - val_mae: 0.2395\n",
            "Epoch 17/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0325 - mae: 0.1957 - val_loss: 0.0751 - val_mae: 0.3027\n",
            "Epoch 18/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0327 - mae: 0.1968 - val_loss: 0.1273 - val_mae: 0.4142\n",
            "Epoch 19/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0325 - mae: 0.1964 - val_loss: 0.0706 - val_mae: 0.2878\n",
            "Epoch 20/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0325 - mae: 0.1961 - val_loss: 0.0942 - val_mae: 0.3229\n",
            "Epoch 21/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0323 - mae: 0.1948 - val_loss: 0.0569 - val_mae: 0.2683\n",
            "Epoch 22/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0322 - mae: 0.1949 - val_loss: 0.1121 - val_mae: 0.3627\n",
            "Epoch 23/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0321 - mae: 0.1941 - val_loss: 0.1439 - val_mae: 0.4514\n",
            "Epoch 24/200\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0322 - mae: 0.1947 - val_loss: 0.0884 - val_mae: 0.3205\n",
            "Epoch 25/200\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0324 - mae: 0.1954 - val_loss: 0.0418 - val_mae: 0.2314\n",
            "Epoch 26/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0325 - mae: 0.1958 - val_loss: 0.0759 - val_mae: 0.3021\n",
            "Epoch 27/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0324 - mae: 0.1952 - val_loss: 0.0637 - val_mae: 0.3006\n",
            "Epoch 28/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0329 - mae: 0.1976 - val_loss: 0.0353 - val_mae: 0.2121\n",
            "Epoch 29/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0324 - mae: 0.1954 - val_loss: 0.0405 - val_mae: 0.2403\n",
            "Epoch 30/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0327 - mae: 0.1968 - val_loss: 0.0477 - val_mae: 0.2613\n",
            "Epoch 31/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0332 - mae: 0.1984 - val_loss: 0.1424 - val_mae: 0.4430\n",
            "Epoch 32/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0328 - mae: 0.1974 - val_loss: 0.1335 - val_mae: 0.4248\n",
            "Epoch 33/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0325 - mae: 0.1960 - val_loss: 0.0817 - val_mae: 0.3140\n",
            "Epoch 34/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0326 - mae: 0.1964 - val_loss: 0.1604 - val_mae: 0.4678\n",
            "Epoch 35/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0337 - mae: 0.2005 - val_loss: 0.4967 - val_mae: 0.9412\n",
            "Epoch 36/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0332 - mae: 0.1983 - val_loss: 0.1033 - val_mae: 0.3684\n",
            "Epoch 37/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0330 - mae: 0.1983 - val_loss: 0.0611 - val_mae: 0.2908\n",
            "Epoch 38/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0327 - mae: 0.1966 - val_loss: 0.0973 - val_mae: 0.3498\n",
            "Epoch 39/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0323 - mae: 0.1948 - val_loss: 0.0354 - val_mae: 0.2184\n",
            "Epoch 40/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0327 - mae: 0.1965 - val_loss: 0.0425 - val_mae: 0.2250\n",
            "Epoch 41/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0324 - mae: 0.1957 - val_loss: 0.1134 - val_mae: 0.3689\n",
            "Epoch 42/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0327 - mae: 0.1964 - val_loss: 0.1485 - val_mae: 0.4248\n",
            "Epoch 43/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0328 - mae: 0.1970 - val_loss: 0.0986 - val_mae: 0.3566\n",
            "Epoch 44/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0326 - mae: 0.1961 - val_loss: 0.0464 - val_mae: 0.2503\n",
            "Epoch 45/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0325 - mae: 0.1957 - val_loss: 0.0809 - val_mae: 0.3142\n",
            "Epoch 46/200\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0322 - mae: 0.1946 - val_loss: 0.1293 - val_mae: 0.4143\n",
            "Epoch 47/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0330 - mae: 0.1974 - val_loss: 0.0599 - val_mae: 0.2805\n",
            "Epoch 48/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0337 - mae: 0.2007 - val_loss: 0.0841 - val_mae: 0.3275\n",
            "Epoch 49/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0358 - mae: 0.2091 - val_loss: 0.1731 - val_mae: 0.5012\n",
            "Epoch 50/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0395 - mae: 0.2252 - val_loss: 0.0587 - val_mae: 0.2712\n",
            "Epoch 51/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0442 - val_mae: 0.2443\n",
            "Epoch 52/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0408 - val_mae: 0.2317\n",
            "Epoch 53/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0400 - val_mae: 0.2296\n",
            "Epoch 54/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2286\n",
            "Epoch 55/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 56/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2277\n",
            "Epoch 57/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 58/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 59/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2264 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 60/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 61/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 62/200\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 63/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 64/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 65/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2273\n",
            "Epoch 66/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 67/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 68/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 69/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 70/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 71/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2284\n",
            "Epoch 72/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 73/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2286\n",
            "Epoch 74/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 75/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 76/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 77/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 78/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 79/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2277\n",
            "Epoch 80/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2274\n",
            "Epoch 81/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 82/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2285\n",
            "Epoch 83/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 84/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2274\n",
            "Epoch 85/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 86/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2273\n",
            "Epoch 87/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 88/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 89/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 90/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2284\n",
            "Epoch 91/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2272 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 92/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2277\n",
            "Epoch 93/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0400 - val_mae: 0.2279\n",
            "Epoch 94/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 95/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2269\n",
            "Epoch 96/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2263 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 97/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 98/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 99/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2279\n",
            "Epoch 100/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2278\n",
            "Epoch 101/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 102/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 103/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 104/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 105/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 106/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 107/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 108/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 109/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2275\n",
            "Epoch 110/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 111/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 112/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2270\n",
            "Epoch 113/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 114/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2277\n",
            "Epoch 115/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2273\n",
            "Epoch 116/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 117/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 118/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 119/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 120/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2278\n",
            "Epoch 121/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 122/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2277\n",
            "Epoch 123/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 124/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 125/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 126/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 127/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2271 - val_loss: 0.0398 - val_mae: 0.2274\n",
            "Epoch 128/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 129/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2275\n",
            "Epoch 130/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 131/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 132/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 133/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 134/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2273\n",
            "Epoch 135/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 136/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 137/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2270\n",
            "Epoch 138/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2264 - val_loss: 0.0399 - val_mae: 0.2284\n",
            "Epoch 139/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 140/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 141/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0400 - val_mae: 0.2282\n",
            "Epoch 142/200\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2265 - val_loss: 0.0398 - val_mae: 0.2289\n",
            "Epoch 143/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 144/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 145/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 146/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 147/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 148/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2278\n",
            "Epoch 149/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 150/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2277\n",
            "Epoch 151/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2277\n",
            "Epoch 152/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 153/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 154/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 155/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2272\n",
            "Epoch 156/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 157/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2278\n",
            "Epoch 158/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 159/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 160/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 161/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 162/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 163/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 164/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 165/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 166/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2272\n",
            "Epoch 167/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2265 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 168/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 169/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2271 - val_loss: 0.0398 - val_mae: 0.2273\n",
            "Epoch 170/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2265 - val_loss: 0.0398 - val_mae: 0.2279\n",
            "Epoch 171/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 172/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 173/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 174/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 175/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 176/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 177/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 178/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2277\n",
            "Epoch 179/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 180/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 181/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 182/200\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 183/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2275\n",
            "Epoch 184/200\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2272\n",
            "Epoch 185/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 186/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 187/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 188/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 189/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0400 - val_mae: 0.2283\n",
            "Epoch 190/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 191/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 192/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 193/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 194/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 195/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 196/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 197/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 198/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 199/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 200/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:13:50,590] Trial 7 finished with value: 0.03984434902667999 and parameters: {'learning_rate': 0.002010433586782222, 'dropout_rate': 0.3556357001260657, 'batch_size': 32, 'epochs': 200}. Best is trial 0 with value: 0.039827607572078705.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "30/30 [==============================] - 3s 20ms/step - loss: 0.6212 - mae: 1.0744 - val_loss: 3.7188 - val_mae: 4.3015\n",
            "Epoch 2/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.6132 - mae: 1.0702 - val_loss: 1.5655 - val_mae: 2.0930\n",
            "Epoch 3/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.6166 - mae: 1.0729 - val_loss: 0.6985 - val_mae: 1.2003\n",
            "Epoch 4/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.6233 - mae: 1.0790 - val_loss: 0.3607 - val_mae: 0.8024\n",
            "Epoch 5/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.6096 - mae: 1.0625 - val_loss: 0.2507 - val_mae: 0.6433\n",
            "Epoch 6/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.6242 - mae: 1.0811 - val_loss: 0.2327 - val_mae: 0.6195\n",
            "Epoch 7/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.6087 - mae: 1.0630 - val_loss: 0.2177 - val_mae: 0.6001\n",
            "Epoch 8/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.5988 - mae: 1.0505 - val_loss: 0.2088 - val_mae: 0.5800\n",
            "Epoch 9/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.6198 - mae: 1.0745 - val_loss: 0.2017 - val_mae: 0.5629\n",
            "Epoch 10/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.6010 - mae: 1.0539 - val_loss: 0.1957 - val_mae: 0.5486\n",
            "Epoch 11/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.6042 - mae: 1.0580 - val_loss: 0.1892 - val_mae: 0.5354\n",
            "Epoch 12/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.6026 - mae: 1.0547 - val_loss: 0.1855 - val_mae: 0.5282\n",
            "Epoch 13/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.6084 - mae: 1.0630 - val_loss: 0.1841 - val_mae: 0.5249\n",
            "Epoch 14/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.6087 - mae: 1.0607 - val_loss: 0.1827 - val_mae: 0.5221\n",
            "Epoch 15/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.5993 - mae: 1.0525 - val_loss: 0.1825 - val_mae: 0.5213\n",
            "Epoch 16/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.6011 - mae: 1.0531 - val_loss: 0.1817 - val_mae: 0.5199\n",
            "Epoch 17/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.6028 - mae: 1.0548 - val_loss: 0.1800 - val_mae: 0.5170\n",
            "Epoch 18/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.6028 - mae: 1.0536 - val_loss: 0.1795 - val_mae: 0.5161\n",
            "Epoch 19/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.5901 - mae: 1.0394 - val_loss: 0.1794 - val_mae: 0.5158\n",
            "Epoch 20/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.6102 - mae: 1.0644 - val_loss: 0.1803 - val_mae: 0.5176\n",
            "Epoch 21/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5906 - mae: 1.0403 - val_loss: 0.1789 - val_mae: 0.5150\n",
            "Epoch 22/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5947 - mae: 1.0449 - val_loss: 0.1801 - val_mae: 0.5171\n",
            "Epoch 23/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5900 - mae: 1.0394 - val_loss: 0.1781 - val_mae: 0.5136\n",
            "Epoch 24/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5939 - mae: 1.0438 - val_loss: 0.1777 - val_mae: 0.5127\n",
            "Epoch 25/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5961 - mae: 1.0465 - val_loss: 0.1773 - val_mae: 0.5117\n",
            "Epoch 26/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5833 - mae: 1.0323 - val_loss: 0.1764 - val_mae: 0.5098\n",
            "Epoch 27/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.6008 - mae: 1.0527 - val_loss: 0.1757 - val_mae: 0.5082\n",
            "Epoch 28/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.5894 - mae: 1.0386 - val_loss: 0.1768 - val_mae: 0.5104\n",
            "Epoch 29/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5971 - mae: 1.0491 - val_loss: 0.1774 - val_mae: 0.5117\n",
            "Epoch 30/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.5957 - mae: 1.0455 - val_loss: 0.1773 - val_mae: 0.5122\n",
            "Epoch 31/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5974 - mae: 1.0452 - val_loss: 0.1745 - val_mae: 0.5066\n",
            "Epoch 32/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.5943 - mae: 1.0435 - val_loss: 0.1753 - val_mae: 0.5082\n",
            "Epoch 33/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.5894 - mae: 1.0381 - val_loss: 0.1759 - val_mae: 0.5093\n",
            "Epoch 34/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5920 - mae: 1.0410 - val_loss: 0.1768 - val_mae: 0.5113\n",
            "Epoch 35/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.5827 - mae: 1.0313 - val_loss: 0.1776 - val_mae: 0.5127\n",
            "Epoch 36/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5867 - mae: 1.0350 - val_loss: 0.1759 - val_mae: 0.5095\n",
            "Epoch 37/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.5926 - mae: 1.0415 - val_loss: 0.1744 - val_mae: 0.5072\n",
            "Epoch 38/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.5804 - mae: 1.0282 - val_loss: 0.1744 - val_mae: 0.5067\n",
            "Epoch 39/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5949 - mae: 1.0448 - val_loss: 0.1730 - val_mae: 0.5043\n",
            "Epoch 40/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5822 - mae: 1.0282 - val_loss: 0.1721 - val_mae: 0.5024\n",
            "Epoch 41/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.5670 - mae: 1.0108 - val_loss: 0.1723 - val_mae: 0.5028\n",
            "Epoch 42/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.5825 - mae: 1.0290 - val_loss: 0.1718 - val_mae: 0.5020\n",
            "Epoch 43/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.5774 - mae: 1.0243 - val_loss: 0.1728 - val_mae: 0.5039\n",
            "Epoch 44/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.5737 - mae: 1.0202 - val_loss: 0.1734 - val_mae: 0.5051\n",
            "Epoch 45/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.5712 - mae: 1.0172 - val_loss: 0.1719 - val_mae: 0.5024\n",
            "Epoch 46/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.5713 - mae: 1.0149 - val_loss: 0.1716 - val_mae: 0.5021\n",
            "Epoch 47/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.5715 - mae: 1.0153 - val_loss: 0.1716 - val_mae: 0.5023\n",
            "Epoch 48/150\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.5731 - mae: 1.0173 - val_loss: 0.1700 - val_mae: 0.4992\n",
            "Epoch 49/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.5723 - mae: 1.0170 - val_loss: 0.1688 - val_mae: 0.4967\n",
            "Epoch 50/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5732 - mae: 1.0172 - val_loss: 0.1689 - val_mae: 0.4970\n",
            "Epoch 51/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.5669 - mae: 1.0106 - val_loss: 0.1695 - val_mae: 0.4976\n",
            "Epoch 52/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5724 - mae: 1.0171 - val_loss: 0.1689 - val_mae: 0.4967\n",
            "Epoch 53/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.5600 - mae: 1.0021 - val_loss: 0.1685 - val_mae: 0.4964\n",
            "Epoch 54/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5528 - mae: 0.9921 - val_loss: 0.1672 - val_mae: 0.4946\n",
            "Epoch 55/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5639 - mae: 1.0065 - val_loss: 0.1668 - val_mae: 0.4935\n",
            "Epoch 56/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5537 - mae: 0.9965 - val_loss: 0.1666 - val_mae: 0.4929\n",
            "Epoch 57/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5622 - mae: 1.0025 - val_loss: 0.1665 - val_mae: 0.4928\n",
            "Epoch 58/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.5612 - mae: 1.0033 - val_loss: 0.1673 - val_mae: 0.4944\n",
            "Epoch 59/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5631 - mae: 1.0068 - val_loss: 0.1678 - val_mae: 0.4952\n",
            "Epoch 60/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.5632 - mae: 1.0046 - val_loss: 0.1676 - val_mae: 0.4945\n",
            "Epoch 61/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.5623 - mae: 1.0056 - val_loss: 0.1671 - val_mae: 0.4939\n",
            "Epoch 62/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.5601 - mae: 1.0001 - val_loss: 0.1667 - val_mae: 0.4931\n",
            "Epoch 63/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.5583 - mae: 0.9996 - val_loss: 0.1663 - val_mae: 0.4915\n",
            "Epoch 64/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5494 - mae: 0.9880 - val_loss: 0.1644 - val_mae: 0.4882\n",
            "Epoch 65/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5570 - mae: 0.9981 - val_loss: 0.1629 - val_mae: 0.4860\n",
            "Epoch 66/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.5589 - mae: 0.9998 - val_loss: 0.1645 - val_mae: 0.4888\n",
            "Epoch 67/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5577 - mae: 0.9968 - val_loss: 0.1641 - val_mae: 0.4887\n",
            "Epoch 68/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5566 - mae: 0.9958 - val_loss: 0.1647 - val_mae: 0.4896\n",
            "Epoch 69/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.5512 - mae: 0.9917 - val_loss: 0.1655 - val_mae: 0.4914\n",
            "Epoch 70/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5422 - mae: 0.9802 - val_loss: 0.1654 - val_mae: 0.4913\n",
            "Epoch 71/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5455 - mae: 0.9847 - val_loss: 0.1635 - val_mae: 0.4879\n",
            "Epoch 72/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5432 - mae: 0.9804 - val_loss: 0.1639 - val_mae: 0.4889\n",
            "Epoch 73/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5472 - mae: 0.9835 - val_loss: 0.1633 - val_mae: 0.4877\n",
            "Epoch 74/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.5429 - mae: 0.9788 - val_loss: 0.1618 - val_mae: 0.4845\n",
            "Epoch 75/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5535 - mae: 0.9923 - val_loss: 0.1629 - val_mae: 0.4858\n",
            "Epoch 76/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5432 - mae: 0.9785 - val_loss: 0.1622 - val_mae: 0.4845\n",
            "Epoch 77/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.5414 - mae: 0.9764 - val_loss: 0.1608 - val_mae: 0.4819\n",
            "Epoch 78/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.5528 - mae: 0.9914 - val_loss: 0.1609 - val_mae: 0.4822\n",
            "Epoch 79/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.5378 - mae: 0.9715 - val_loss: 0.1605 - val_mae: 0.4820\n",
            "Epoch 80/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.5398 - mae: 0.9758 - val_loss: 0.1601 - val_mae: 0.4816\n",
            "Epoch 81/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.5292 - mae: 0.9629 - val_loss: 0.1603 - val_mae: 0.4827\n",
            "Epoch 82/150\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.5287 - mae: 0.9609 - val_loss: 0.1601 - val_mae: 0.4822\n",
            "Epoch 83/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.5320 - mae: 0.9663 - val_loss: 0.1590 - val_mae: 0.4801\n",
            "Epoch 84/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.5331 - mae: 0.9664 - val_loss: 0.1572 - val_mae: 0.4757\n",
            "Epoch 85/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.5397 - mae: 0.9754 - val_loss: 0.1576 - val_mae: 0.4764\n",
            "Epoch 86/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.5410 - mae: 0.9758 - val_loss: 0.1568 - val_mae: 0.4751\n",
            "Epoch 87/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5359 - mae: 0.9715 - val_loss: 0.1558 - val_mae: 0.4734\n",
            "Epoch 88/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.5327 - mae: 0.9659 - val_loss: 0.1565 - val_mae: 0.4745\n",
            "Epoch 89/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.5336 - mae: 0.9682 - val_loss: 0.1563 - val_mae: 0.4743\n",
            "Epoch 90/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5290 - mae: 0.9625 - val_loss: 0.1563 - val_mae: 0.4743\n",
            "Epoch 91/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5312 - mae: 0.9639 - val_loss: 0.1570 - val_mae: 0.4754\n",
            "Epoch 92/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5409 - mae: 0.9763 - val_loss: 0.1570 - val_mae: 0.4753\n",
            "Epoch 93/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.5296 - mae: 0.9606 - val_loss: 0.1566 - val_mae: 0.4751\n",
            "Epoch 94/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.5354 - mae: 0.9683 - val_loss: 0.1555 - val_mae: 0.4730\n",
            "Epoch 95/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5231 - mae: 0.9562 - val_loss: 0.1561 - val_mae: 0.4740\n",
            "Epoch 96/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5034 - mae: 0.9304 - val_loss: 0.1549 - val_mae: 0.4718\n",
            "Epoch 97/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5329 - mae: 0.9660 - val_loss: 0.1549 - val_mae: 0.4720\n",
            "Epoch 98/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5294 - mae: 0.9602 - val_loss: 0.1545 - val_mae: 0.4710\n",
            "Epoch 99/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5238 - mae: 0.9566 - val_loss: 0.1542 - val_mae: 0.4701\n",
            "Epoch 100/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5291 - mae: 0.9614 - val_loss: 0.1516 - val_mae: 0.4659\n",
            "Epoch 101/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.5162 - mae: 0.9466 - val_loss: 0.1518 - val_mae: 0.4661\n",
            "Epoch 102/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5222 - mae: 0.9505 - val_loss: 0.1511 - val_mae: 0.4653\n",
            "Epoch 103/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5139 - mae: 0.9425 - val_loss: 0.1519 - val_mae: 0.4665\n",
            "Epoch 104/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5122 - mae: 0.9410 - val_loss: 0.1524 - val_mae: 0.4674\n",
            "Epoch 105/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5181 - mae: 0.9453 - val_loss: 0.1527 - val_mae: 0.4685\n",
            "Epoch 106/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5160 - mae: 0.9443 - val_loss: 0.1516 - val_mae: 0.4665\n",
            "Epoch 107/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5100 - mae: 0.9372 - val_loss: 0.1524 - val_mae: 0.4678\n",
            "Epoch 108/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5163 - mae: 0.9446 - val_loss: 0.1525 - val_mae: 0.4675\n",
            "Epoch 109/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5154 - mae: 0.9445 - val_loss: 0.1520 - val_mae: 0.4664\n",
            "Epoch 110/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.5077 - mae: 0.9348 - val_loss: 0.1506 - val_mae: 0.4636\n",
            "Epoch 111/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.5035 - mae: 0.9297 - val_loss: 0.1519 - val_mae: 0.4659\n",
            "Epoch 112/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5118 - mae: 0.9393 - val_loss: 0.1509 - val_mae: 0.4641\n",
            "Epoch 113/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5071 - mae: 0.9333 - val_loss: 0.1514 - val_mae: 0.4651\n",
            "Epoch 114/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.5101 - mae: 0.9397 - val_loss: 0.1515 - val_mae: 0.4649\n",
            "Epoch 115/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.5044 - mae: 0.9300 - val_loss: 0.1507 - val_mae: 0.4640\n",
            "Epoch 116/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.4972 - mae: 0.9225 - val_loss: 0.1498 - val_mae: 0.4627\n",
            "Epoch 117/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.5035 - mae: 0.9297 - val_loss: 0.1499 - val_mae: 0.4627\n",
            "Epoch 118/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.5036 - mae: 0.9294 - val_loss: 0.1500 - val_mae: 0.4632\n",
            "Epoch 119/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.5084 - mae: 0.9355 - val_loss: 0.1494 - val_mae: 0.4616\n",
            "Epoch 120/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.5007 - mae: 0.9257 - val_loss: 0.1483 - val_mae: 0.4591\n",
            "Epoch 121/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.4998 - mae: 0.9229 - val_loss: 0.1481 - val_mae: 0.4592\n",
            "Epoch 122/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.4965 - mae: 0.9223 - val_loss: 0.1467 - val_mae: 0.4566\n",
            "Epoch 123/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.4947 - mae: 0.9180 - val_loss: 0.1467 - val_mae: 0.4568\n",
            "Epoch 124/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.4927 - mae: 0.9144 - val_loss: 0.1457 - val_mae: 0.4551\n",
            "Epoch 125/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.5063 - mae: 0.9314 - val_loss: 0.1464 - val_mae: 0.4563\n",
            "Epoch 126/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.4980 - mae: 0.9209 - val_loss: 0.1456 - val_mae: 0.4550\n",
            "Epoch 127/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.4946 - mae: 0.9164 - val_loss: 0.1453 - val_mae: 0.4544\n",
            "Epoch 128/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.4985 - mae: 0.9223 - val_loss: 0.1453 - val_mae: 0.4544\n",
            "Epoch 129/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.4956 - mae: 0.9184 - val_loss: 0.1456 - val_mae: 0.4550\n",
            "Epoch 130/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.4921 - mae: 0.9142 - val_loss: 0.1460 - val_mae: 0.4556\n",
            "Epoch 131/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.4978 - mae: 0.9214 - val_loss: 0.1461 - val_mae: 0.4558\n",
            "Epoch 132/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.4835 - mae: 0.9043 - val_loss: 0.1455 - val_mae: 0.4548\n",
            "Epoch 133/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.4836 - mae: 0.9024 - val_loss: 0.1456 - val_mae: 0.4551\n",
            "Epoch 134/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.4770 - mae: 0.8938 - val_loss: 0.1447 - val_mae: 0.4532\n",
            "Epoch 135/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.4791 - mae: 0.8981 - val_loss: 0.1439 - val_mae: 0.4519\n",
            "Epoch 136/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.4980 - mae: 0.9191 - val_loss: 0.1437 - val_mae: 0.4515\n",
            "Epoch 137/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.4867 - mae: 0.9076 - val_loss: 0.1447 - val_mae: 0.4532\n",
            "Epoch 138/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.4924 - mae: 0.9132 - val_loss: 0.1450 - val_mae: 0.4539\n",
            "Epoch 139/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.4756 - mae: 0.8928 - val_loss: 0.1441 - val_mae: 0.4520\n",
            "Epoch 140/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.4803 - mae: 0.8975 - val_loss: 0.1431 - val_mae: 0.4503\n",
            "Epoch 141/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.4848 - mae: 0.9032 - val_loss: 0.1432 - val_mae: 0.4504\n",
            "Epoch 142/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.4832 - mae: 0.9025 - val_loss: 0.1423 - val_mae: 0.4486\n",
            "Epoch 143/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.4801 - mae: 0.8980 - val_loss: 0.1424 - val_mae: 0.4484\n",
            "Epoch 144/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.4834 - mae: 0.9038 - val_loss: 0.1420 - val_mae: 0.4477\n",
            "Epoch 145/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.4725 - mae: 0.8892 - val_loss: 0.1421 - val_mae: 0.4479\n",
            "Epoch 146/150\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.4740 - mae: 0.8910 - val_loss: 0.1422 - val_mae: 0.4480\n",
            "Epoch 147/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.4833 - mae: 0.9016 - val_loss: 0.1423 - val_mae: 0.4482\n",
            "Epoch 148/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.4814 - mae: 0.9002 - val_loss: 0.1419 - val_mae: 0.4476\n",
            "Epoch 149/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.4788 - mae: 0.8947 - val_loss: 0.1413 - val_mae: 0.4466\n",
            "Epoch 150/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.4628 - mae: 0.8758 - val_loss: 0.1416 - val_mae: 0.4474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:15:15,011] Trial 8 finished with value: 0.14163735508918762 and parameters: {'learning_rate': 6.801977339799716e-06, 'dropout_rate': 0.5647435472799772, 'batch_size': 128, 'epochs': 150}. Best is trial 0 with value: 0.039827607572078705.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "60/60 [==============================] - 4s 15ms/step - loss: 0.4199 - mae: 0.8450 - val_loss: 1.2611 - val_mae: 1.7919\n",
            "Epoch 2/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.3992 - mae: 0.8162 - val_loss: 0.5157 - val_mae: 0.9713\n",
            "Epoch 3/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.3787 - mae: 0.7884 - val_loss: 0.3132 - val_mae: 0.6974\n",
            "Epoch 4/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.3585 - mae: 0.7606 - val_loss: 0.3070 - val_mae: 0.6975\n",
            "Epoch 5/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.3471 - mae: 0.7442 - val_loss: 0.2969 - val_mae: 0.6789\n",
            "Epoch 6/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.3323 - mae: 0.7220 - val_loss: 0.3131 - val_mae: 0.7074\n",
            "Epoch 7/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.3195 - mae: 0.7036 - val_loss: 0.2551 - val_mae: 0.6225\n",
            "Epoch 8/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.3086 - mae: 0.6870 - val_loss: 0.2357 - val_mae: 0.5938\n",
            "Epoch 9/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.3019 - mae: 0.6790 - val_loss: 0.2153 - val_mae: 0.5645\n",
            "Epoch 10/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.2902 - mae: 0.6612 - val_loss: 0.2169 - val_mae: 0.5678\n",
            "Epoch 11/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.2809 - mae: 0.6472 - val_loss: 0.1967 - val_mae: 0.5370\n",
            "Epoch 12/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.2733 - mae: 0.6359 - val_loss: 0.1953 - val_mae: 0.5382\n",
            "Epoch 13/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2665 - mae: 0.6257 - val_loss: 0.1823 - val_mae: 0.5157\n",
            "Epoch 14/150\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.2562 - mae: 0.6098 - val_loss: 0.1827 - val_mae: 0.5168\n",
            "Epoch 15/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2531 - mae: 0.6040 - val_loss: 0.1778 - val_mae: 0.5064\n",
            "Epoch 16/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2446 - mae: 0.5921 - val_loss: 0.1760 - val_mae: 0.5018\n",
            "Epoch 17/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.2387 - mae: 0.5827 - val_loss: 0.1770 - val_mae: 0.4950\n",
            "Epoch 18/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2332 - mae: 0.5734 - val_loss: 0.1659 - val_mae: 0.4846\n",
            "Epoch 19/150\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.2283 - mae: 0.5663 - val_loss: 0.1591 - val_mae: 0.4684\n",
            "Epoch 20/150\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.2215 - mae: 0.5562 - val_loss: 0.1571 - val_mae: 0.4625\n",
            "Epoch 21/150\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.2171 - mae: 0.5487 - val_loss: 0.1534 - val_mae: 0.4576\n",
            "Epoch 22/150\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.2098 - mae: 0.5372 - val_loss: 0.1465 - val_mae: 0.4508\n",
            "Epoch 23/150\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.2079 - mae: 0.5351 - val_loss: 0.1407 - val_mae: 0.4367\n",
            "Epoch 24/150\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.2018 - mae: 0.5246 - val_loss: 0.1419 - val_mae: 0.4338\n",
            "Epoch 25/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1965 - mae: 0.5164 - val_loss: 0.1374 - val_mae: 0.4290\n",
            "Epoch 26/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.1919 - mae: 0.5102 - val_loss: 0.1380 - val_mae: 0.4298\n",
            "Epoch 27/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.1902 - mae: 0.5062 - val_loss: 0.1306 - val_mae: 0.4145\n",
            "Epoch 28/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.1875 - mae: 0.5015 - val_loss: 0.1297 - val_mae: 0.4146\n",
            "Epoch 29/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1803 - mae: 0.4914 - val_loss: 0.1242 - val_mae: 0.4048\n",
            "Epoch 30/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1777 - mae: 0.4864 - val_loss: 0.1231 - val_mae: 0.4024\n",
            "Epoch 31/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.1744 - mae: 0.4816 - val_loss: 0.1225 - val_mae: 0.4002\n",
            "Epoch 32/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.1710 - mae: 0.4752 - val_loss: 0.1192 - val_mae: 0.3936\n",
            "Epoch 33/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.1684 - mae: 0.4709 - val_loss: 0.1142 - val_mae: 0.3860\n",
            "Epoch 34/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.1670 - mae: 0.4681 - val_loss: 0.1158 - val_mae: 0.3885\n",
            "Epoch 35/150\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.1622 - mae: 0.4618 - val_loss: 0.1157 - val_mae: 0.3901\n",
            "Epoch 36/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1611 - mae: 0.4589 - val_loss: 0.1102 - val_mae: 0.3791\n",
            "Epoch 37/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.1584 - mae: 0.4550 - val_loss: 0.1096 - val_mae: 0.3794\n",
            "Epoch 38/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.1568 - mae: 0.4515 - val_loss: 0.1070 - val_mae: 0.3703\n",
            "Epoch 39/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.1528 - mae: 0.4459 - val_loss: 0.1105 - val_mae: 0.3742\n",
            "Epoch 40/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.1518 - mae: 0.4435 - val_loss: 0.1065 - val_mae: 0.3682\n",
            "Epoch 41/150\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.1486 - mae: 0.4382 - val_loss: 0.1050 - val_mae: 0.3656\n",
            "Epoch 42/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1465 - mae: 0.4353 - val_loss: 0.1027 - val_mae: 0.3626\n",
            "Epoch 43/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.1438 - mae: 0.4294 - val_loss: 0.1012 - val_mae: 0.3568\n",
            "Epoch 44/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.1419 - mae: 0.4275 - val_loss: 0.1020 - val_mae: 0.3573\n",
            "Epoch 45/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1370 - mae: 0.4180 - val_loss: 0.1006 - val_mae: 0.3582\n",
            "Epoch 46/150\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.1342 - mae: 0.4147 - val_loss: 0.0975 - val_mae: 0.3505\n",
            "Epoch 47/150\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.1364 - mae: 0.4178 - val_loss: 0.0960 - val_mae: 0.3494\n",
            "Epoch 48/150\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.1323 - mae: 0.4114 - val_loss: 0.0955 - val_mae: 0.3493\n",
            "Epoch 49/150\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.1319 - mae: 0.4092 - val_loss: 0.0944 - val_mae: 0.3467\n",
            "Epoch 50/150\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.1313 - mae: 0.4077 - val_loss: 0.0939 - val_mae: 0.3433\n",
            "Epoch 51/150\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.1291 - mae: 0.4050 - val_loss: 0.0917 - val_mae: 0.3412\n",
            "Epoch 52/150\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.1258 - mae: 0.3988 - val_loss: 0.0901 - val_mae: 0.3394\n",
            "Epoch 53/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.1238 - mae: 0.3962 - val_loss: 0.0889 - val_mae: 0.3389\n",
            "Epoch 54/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.1216 - mae: 0.3916 - val_loss: 0.0872 - val_mae: 0.3391\n",
            "Epoch 55/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.1197 - mae: 0.3888 - val_loss: 0.0877 - val_mae: 0.3377\n",
            "Epoch 56/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.1201 - mae: 0.3884 - val_loss: 0.0860 - val_mae: 0.3339\n",
            "Epoch 57/150\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.1191 - mae: 0.3862 - val_loss: 0.0862 - val_mae: 0.3305\n",
            "Epoch 58/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1153 - mae: 0.3808 - val_loss: 0.0830 - val_mae: 0.3267\n",
            "Epoch 59/150\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.1156 - mae: 0.3802 - val_loss: 0.0815 - val_mae: 0.3237\n",
            "Epoch 60/150\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.1142 - mae: 0.3784 - val_loss: 0.0793 - val_mae: 0.3216\n",
            "Epoch 61/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1106 - mae: 0.3726 - val_loss: 0.0788 - val_mae: 0.3194\n",
            "Epoch 62/150\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.1111 - mae: 0.3727 - val_loss: 0.0781 - val_mae: 0.3201\n",
            "Epoch 63/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.1093 - mae: 0.3697 - val_loss: 0.0783 - val_mae: 0.3186\n",
            "Epoch 64/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.1071 - mae: 0.3663 - val_loss: 0.0765 - val_mae: 0.3145\n",
            "Epoch 65/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.1090 - mae: 0.3684 - val_loss: 0.0759 - val_mae: 0.3144\n",
            "Epoch 66/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1051 - mae: 0.3613 - val_loss: 0.0772 - val_mae: 0.3155\n",
            "Epoch 67/150\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.1058 - mae: 0.3620 - val_loss: 0.0763 - val_mae: 0.3148\n",
            "Epoch 68/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.1032 - mae: 0.3579 - val_loss: 0.0729 - val_mae: 0.3091\n",
            "Epoch 69/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.1037 - mae: 0.3580 - val_loss: 0.0730 - val_mae: 0.3073\n",
            "Epoch 70/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0980 - mae: 0.3504 - val_loss: 0.0732 - val_mae: 0.3069\n",
            "Epoch 71/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0994 - mae: 0.3508 - val_loss: 0.0710 - val_mae: 0.3052\n",
            "Epoch 72/150\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.1017 - mae: 0.3533 - val_loss: 0.0702 - val_mae: 0.3024\n",
            "Epoch 73/150\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0973 - mae: 0.3480 - val_loss: 0.0699 - val_mae: 0.3025\n",
            "Epoch 74/150\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0966 - mae: 0.3450 - val_loss: 0.0714 - val_mae: 0.3035\n",
            "Epoch 75/150\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0949 - mae: 0.3433 - val_loss: 0.0676 - val_mae: 0.2974\n",
            "Epoch 76/150\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0950 - mae: 0.3419 - val_loss: 0.0662 - val_mae: 0.2954\n",
            "Epoch 77/150\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0931 - mae: 0.3382 - val_loss: 0.0650 - val_mae: 0.2933\n",
            "Epoch 78/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0939 - mae: 0.3396 - val_loss: 0.0684 - val_mae: 0.2966\n",
            "Epoch 79/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0938 - mae: 0.3384 - val_loss: 0.0673 - val_mae: 0.2950\n",
            "Epoch 80/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.0891 - mae: 0.3314 - val_loss: 0.0651 - val_mae: 0.2907\n",
            "Epoch 81/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.0910 - mae: 0.3329 - val_loss: 0.0620 - val_mae: 0.2863\n",
            "Epoch 82/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.0922 - mae: 0.3353 - val_loss: 0.0634 - val_mae: 0.2874\n",
            "Epoch 83/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.0890 - mae: 0.3287 - val_loss: 0.0639 - val_mae: 0.2881\n",
            "Epoch 84/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.0869 - mae: 0.3251 - val_loss: 0.0616 - val_mae: 0.2836\n",
            "Epoch 85/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.0864 - mae: 0.3244 - val_loss: 0.0607 - val_mae: 0.2823\n",
            "Epoch 86/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0870 - mae: 0.3244 - val_loss: 0.0598 - val_mae: 0.2811\n",
            "Epoch 87/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.0853 - mae: 0.3216 - val_loss: 0.0588 - val_mae: 0.2774\n",
            "Epoch 88/150\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0848 - mae: 0.3197 - val_loss: 0.0582 - val_mae: 0.2780\n",
            "Epoch 89/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0821 - mae: 0.3154 - val_loss: 0.0576 - val_mae: 0.2764\n",
            "Epoch 90/150\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0836 - mae: 0.3181 - val_loss: 0.0565 - val_mae: 0.2727\n",
            "Epoch 91/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0811 - mae: 0.3126 - val_loss: 0.0575 - val_mae: 0.2735\n",
            "Epoch 92/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0825 - mae: 0.3138 - val_loss: 0.0575 - val_mae: 0.2731\n",
            "Epoch 93/150\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0811 - mae: 0.3124 - val_loss: 0.0560 - val_mae: 0.2704\n",
            "Epoch 94/150\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0792 - mae: 0.3107 - val_loss: 0.0570 - val_mae: 0.2717\n",
            "Epoch 95/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0780 - mae: 0.3063 - val_loss: 0.0546 - val_mae: 0.2667\n",
            "Epoch 96/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0796 - mae: 0.3083 - val_loss: 0.0527 - val_mae: 0.2630\n",
            "Epoch 97/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.0791 - mae: 0.3065 - val_loss: 0.0539 - val_mae: 0.2648\n",
            "Epoch 98/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0777 - mae: 0.3040 - val_loss: 0.0534 - val_mae: 0.2633\n",
            "Epoch 99/150\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0747 - mae: 0.2991 - val_loss: 0.0517 - val_mae: 0.2599\n",
            "Epoch 100/150\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0766 - mae: 0.3009 - val_loss: 0.0515 - val_mae: 0.2588\n",
            "Epoch 101/150\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.0734 - mae: 0.2956 - val_loss: 0.0515 - val_mae: 0.2584\n",
            "Epoch 102/150\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0757 - mae: 0.2987 - val_loss: 0.0504 - val_mae: 0.2564\n",
            "Epoch 103/150\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0749 - mae: 0.2981 - val_loss: 0.0476 - val_mae: 0.2516\n",
            "Epoch 104/150\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0733 - mae: 0.2957 - val_loss: 0.0502 - val_mae: 0.2553\n",
            "Epoch 105/150\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0732 - mae: 0.2948 - val_loss: 0.0513 - val_mae: 0.2567\n",
            "Epoch 106/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.0711 - mae: 0.2902 - val_loss: 0.0477 - val_mae: 0.2491\n",
            "Epoch 107/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.0718 - mae: 0.2908 - val_loss: 0.0489 - val_mae: 0.2512\n",
            "Epoch 108/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.0702 - mae: 0.2884 - val_loss: 0.0486 - val_mae: 0.2501\n",
            "Epoch 109/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0699 - mae: 0.2899 - val_loss: 0.0461 - val_mae: 0.2452\n",
            "Epoch 110/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0708 - mae: 0.2909 - val_loss: 0.0482 - val_mae: 0.2491\n",
            "Epoch 111/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0731 - mae: 0.2928 - val_loss: 0.0438 - val_mae: 0.2418\n",
            "Epoch 112/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0717 - mae: 0.2913 - val_loss: 0.0464 - val_mae: 0.2443\n",
            "Epoch 113/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0696 - mae: 0.2873 - val_loss: 0.0464 - val_mae: 0.2439\n",
            "Epoch 114/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0667 - mae: 0.2816 - val_loss: 0.0460 - val_mae: 0.2430\n",
            "Epoch 115/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.0687 - mae: 0.2849 - val_loss: 0.0457 - val_mae: 0.2425\n",
            "Epoch 116/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0674 - mae: 0.2820 - val_loss: 0.0448 - val_mae: 0.2402\n",
            "Epoch 117/150\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0668 - mae: 0.2805 - val_loss: 0.0434 - val_mae: 0.2366\n",
            "Epoch 118/150\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0675 - mae: 0.2809 - val_loss: 0.0426 - val_mae: 0.2348\n",
            "Epoch 119/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0655 - mae: 0.2779 - val_loss: 0.0431 - val_mae: 0.2351\n",
            "Epoch 120/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0640 - mae: 0.2741 - val_loss: 0.0413 - val_mae: 0.2318\n",
            "Epoch 121/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0641 - mae: 0.2749 - val_loss: 0.0402 - val_mae: 0.2321\n",
            "Epoch 122/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0644 - mae: 0.2744 - val_loss: 0.0401 - val_mae: 0.2296\n",
            "Epoch 123/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.0633 - mae: 0.2728 - val_loss: 0.0397 - val_mae: 0.2283\n",
            "Epoch 124/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.0620 - mae: 0.2694 - val_loss: 0.0401 - val_mae: 0.2280\n",
            "Epoch 125/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0633 - mae: 0.2714 - val_loss: 0.0418 - val_mae: 0.2315\n",
            "Epoch 126/150\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0615 - mae: 0.2671 - val_loss: 0.0397 - val_mae: 0.2263\n",
            "Epoch 127/150\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0622 - mae: 0.2687 - val_loss: 0.0394 - val_mae: 0.2259\n",
            "Epoch 128/150\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0615 - mae: 0.2669 - val_loss: 0.0390 - val_mae: 0.2245\n",
            "Epoch 129/150\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0611 - mae: 0.2673 - val_loss: 0.0382 - val_mae: 0.2233\n",
            "Epoch 130/150\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0594 - mae: 0.2635 - val_loss: 0.0379 - val_mae: 0.2227\n",
            "Epoch 131/150\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0595 - mae: 0.2622 - val_loss: 0.0397 - val_mae: 0.2251\n",
            "Epoch 132/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0600 - mae: 0.2635 - val_loss: 0.0379 - val_mae: 0.2211\n",
            "Epoch 133/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0600 - mae: 0.2622 - val_loss: 0.0389 - val_mae: 0.2231\n",
            "Epoch 134/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0588 - mae: 0.2610 - val_loss: 0.0368 - val_mae: 0.2198\n",
            "Epoch 135/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0595 - mae: 0.2611 - val_loss: 0.0391 - val_mae: 0.2231\n",
            "Epoch 136/150\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0593 - mae: 0.2604 - val_loss: 0.0390 - val_mae: 0.2225\n",
            "Epoch 137/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0571 - mae: 0.2573 - val_loss: 0.0379 - val_mae: 0.2195\n",
            "Epoch 138/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0578 - mae: 0.2580 - val_loss: 0.0365 - val_mae: 0.2169\n",
            "Epoch 139/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0565 - mae: 0.2551 - val_loss: 0.0360 - val_mae: 0.2169\n",
            "Epoch 140/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.0577 - mae: 0.2572 - val_loss: 0.0369 - val_mae: 0.2168\n",
            "Epoch 141/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0579 - mae: 0.2571 - val_loss: 0.0358 - val_mae: 0.2155\n",
            "Epoch 142/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.0568 - mae: 0.2544 - val_loss: 0.0362 - val_mae: 0.2188\n",
            "Epoch 143/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0578 - mae: 0.2565 - val_loss: 0.0391 - val_mae: 0.2224\n",
            "Epoch 144/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0583 - mae: 0.2567 - val_loss: 0.0352 - val_mae: 0.2141\n",
            "Epoch 145/150\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0563 - mae: 0.2531 - val_loss: 0.0355 - val_mae: 0.2136\n",
            "Epoch 146/150\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0571 - mae: 0.2547 - val_loss: 0.0384 - val_mae: 0.2199\n",
            "Epoch 147/150\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0556 - mae: 0.2522 - val_loss: 0.0359 - val_mae: 0.2138\n",
            "Epoch 148/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0579 - mae: 0.2555 - val_loss: 0.0354 - val_mae: 0.2131\n",
            "Epoch 149/150\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.0560 - mae: 0.2520 - val_loss: 0.0350 - val_mae: 0.2138\n",
            "Epoch 150/150\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0550 - mae: 0.2502 - val_loss: 0.0357 - val_mae: 0.2131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:16:38,720] Trial 9 finished with value: 0.035705674439668655 and parameters: {'learning_rate': 3.0124937718760044e-05, 'dropout_rate': 0.11027127446423811, 'batch_size': 64, 'epochs': 150}. Best is trial 9 with value: 0.035705674439668655.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "120/120 [==============================] - 4s 10ms/step - loss: 0.0475 - mae: 0.2397 - val_loss: 0.0479 - val_mae: 0.2332\n",
            "Epoch 2/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0371 - mae: 0.2120 - val_loss: 0.0404 - val_mae: 0.2368\n",
            "Epoch 3/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0358 - mae: 0.2077 - val_loss: 0.7035 - val_mae: 1.1894\n",
            "Epoch 4/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0352 - mae: 0.2052 - val_loss: 0.0365 - val_mae: 0.2162\n",
            "Epoch 5/150\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0351 - mae: 0.2041 - val_loss: 0.1166 - val_mae: 0.3928\n",
            "Epoch 6/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0344 - mae: 0.2018 - val_loss: 0.1480 - val_mae: 0.4642\n",
            "Epoch 7/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0346 - mae: 0.2022 - val_loss: 0.2201 - val_mae: 0.5529\n",
            "Epoch 8/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0346 - mae: 0.2023 - val_loss: 0.0667 - val_mae: 0.2783\n",
            "Epoch 9/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0358 - mae: 0.2070 - val_loss: 0.2365 - val_mae: 0.6048\n",
            "Epoch 10/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0350 - mae: 0.2039 - val_loss: 0.1646 - val_mae: 0.4941\n",
            "Epoch 11/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0346 - mae: 0.2026 - val_loss: 0.7361 - val_mae: 1.2276\n",
            "Epoch 12/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0346 - mae: 0.2030 - val_loss: 0.2165 - val_mae: 0.5797\n",
            "Epoch 13/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0345 - mae: 0.2018 - val_loss: 0.6936 - val_mae: 1.1825\n",
            "Epoch 14/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0344 - mae: 0.2019 - val_loss: 0.4567 - val_mae: 0.8470\n",
            "Epoch 15/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0339 - mae: 0.1998 - val_loss: 0.0755 - val_mae: 0.3185\n",
            "Epoch 16/150\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0347 - mae: 0.2028 - val_loss: 0.2859 - val_mae: 0.6662\n",
            "Epoch 17/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0344 - mae: 0.2019 - val_loss: 0.1542 - val_mae: 0.4463\n",
            "Epoch 18/150\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0343 - mae: 0.2014 - val_loss: 0.3260 - val_mae: 0.6957\n",
            "Epoch 19/150\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0351 - mae: 0.2044 - val_loss: 0.2316 - val_mae: 0.5901\n",
            "Epoch 20/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0345 - mae: 0.2017 - val_loss: 0.3728 - val_mae: 0.7798\n",
            "Epoch 21/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0340 - mae: 0.1999 - val_loss: 0.0735 - val_mae: 0.2975\n",
            "Epoch 22/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0337 - mae: 0.1988 - val_loss: 0.9839 - val_mae: 1.5086\n",
            "Epoch 23/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0340 - mae: 0.1998 - val_loss: 0.0765 - val_mae: 0.2964\n",
            "Epoch 24/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0338 - mae: 0.1992 - val_loss: 0.0444 - val_mae: 0.2275\n",
            "Epoch 25/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0339 - mae: 0.1998 - val_loss: 0.0514 - val_mae: 0.2489\n",
            "Epoch 26/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0338 - mae: 0.1995 - val_loss: 0.0386 - val_mae: 0.2157\n",
            "Epoch 27/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0336 - mae: 0.1986 - val_loss: 0.0675 - val_mae: 0.2891\n",
            "Epoch 28/150\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0336 - mae: 0.1983 - val_loss: 0.0557 - val_mae: 0.2554\n",
            "Epoch 29/150\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0343 - mae: 0.2016 - val_loss: 0.1620 - val_mae: 0.4789\n",
            "Epoch 30/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0341 - mae: 0.2007 - val_loss: 0.1260 - val_mae: 0.3961\n",
            "Epoch 31/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0341 - mae: 0.2005 - val_loss: 0.0699 - val_mae: 0.3185\n",
            "Epoch 32/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0339 - mae: 0.1997 - val_loss: 0.1635 - val_mae: 0.4625\n",
            "Epoch 33/150\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0337 - mae: 0.1991 - val_loss: 0.0681 - val_mae: 0.2745\n",
            "Epoch 34/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0337 - mae: 0.1986 - val_loss: 0.0393 - val_mae: 0.2171\n",
            "Epoch 35/150\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0335 - mae: 0.1985 - val_loss: 0.0529 - val_mae: 0.2489\n",
            "Epoch 36/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0335 - mae: 0.1982 - val_loss: 0.0544 - val_mae: 0.2438\n",
            "Epoch 37/150\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0343 - mae: 0.2010 - val_loss: 0.0626 - val_mae: 0.2764\n",
            "Epoch 38/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0338 - mae: 0.1994 - val_loss: 0.1460 - val_mae: 0.4470\n",
            "Epoch 39/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0337 - mae: 0.1995 - val_loss: 0.1031 - val_mae: 0.3538\n",
            "Epoch 40/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0337 - mae: 0.1992 - val_loss: 0.0457 - val_mae: 0.2346\n",
            "Epoch 41/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0337 - mae: 0.1989 - val_loss: 0.0530 - val_mae: 0.2454\n",
            "Epoch 42/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0337 - mae: 0.1989 - val_loss: 0.0553 - val_mae: 0.2642\n",
            "Epoch 43/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0338 - mae: 0.1991 - val_loss: 0.0884 - val_mae: 0.3210\n",
            "Epoch 44/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0348 - mae: 0.2030 - val_loss: 0.2570 - val_mae: 0.6161\n",
            "Epoch 45/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0361 - mae: 0.2083 - val_loss: 1.0025 - val_mae: 1.5183\n",
            "Epoch 46/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0378 - mae: 0.2155 - val_loss: 0.9734 - val_mae: 1.5198\n",
            "Epoch 47/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0401 - mae: 0.2251 - val_loss: 0.0713 - val_mae: 0.3100\n",
            "Epoch 48/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0412 - mae: 0.2303 - val_loss: 0.3608 - val_mae: 0.7628\n",
            "Epoch 49/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0414 - mae: 0.2304 - val_loss: 0.2567 - val_mae: 0.6153\n",
            "Epoch 50/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0413 - mae: 0.2301 - val_loss: 0.0813 - val_mae: 0.3086\n",
            "Epoch 51/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0414 - mae: 0.2306 - val_loss: 0.0476 - val_mae: 0.2525\n",
            "Epoch 52/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0412 - mae: 0.2301 - val_loss: 0.0572 - val_mae: 0.2542\n",
            "Epoch 53/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0414 - mae: 0.2306 - val_loss: 0.0415 - val_mae: 0.2353\n",
            "Epoch 54/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0412 - mae: 0.2301 - val_loss: 0.0407 - val_mae: 0.2271\n",
            "Epoch 55/150\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0412 - mae: 0.2301 - val_loss: 0.0403 - val_mae: 0.2295\n",
            "Epoch 56/150\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0412 - mae: 0.2297 - val_loss: 0.0416 - val_mae: 0.2301\n",
            "Epoch 57/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0407 - mae: 0.2278 - val_loss: 0.0433 - val_mae: 0.2337\n",
            "Epoch 58/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0412 - mae: 0.2303 - val_loss: 0.0442 - val_mae: 0.2419\n",
            "Epoch 59/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0412 - mae: 0.2301 - val_loss: 0.0415 - val_mae: 0.2301\n",
            "Epoch 60/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0413 - mae: 0.2307 - val_loss: 0.0470 - val_mae: 0.2266\n",
            "Epoch 61/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0412 - mae: 0.2301 - val_loss: 0.0405 - val_mae: 0.2276\n",
            "Epoch 62/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0412 - mae: 0.2303 - val_loss: 0.0419 - val_mae: 0.2294\n",
            "Epoch 63/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0412 - mae: 0.2301 - val_loss: 0.0407 - val_mae: 0.2323\n",
            "Epoch 64/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0412 - mae: 0.2304 - val_loss: 0.0406 - val_mae: 0.2270\n",
            "Epoch 65/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0413 - mae: 0.2307 - val_loss: 0.0419 - val_mae: 0.2329\n",
            "Epoch 66/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0412 - mae: 0.2303 - val_loss: 0.0420 - val_mae: 0.2302\n",
            "Epoch 67/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0413 - mae: 0.2304 - val_loss: 0.0409 - val_mae: 0.2356\n",
            "Epoch 68/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0412 - mae: 0.2301 - val_loss: 0.0432 - val_mae: 0.2354\n",
            "Epoch 69/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0413 - mae: 0.2305 - val_loss: 0.0407 - val_mae: 0.2208\n",
            "Epoch 70/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0411 - mae: 0.2300 - val_loss: 0.0414 - val_mae: 0.2257\n",
            "Epoch 71/150\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0412 - mae: 0.2303 - val_loss: 0.0415 - val_mae: 0.2297\n",
            "Epoch 72/150\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0412 - mae: 0.2300 - val_loss: 0.0409 - val_mae: 0.2306\n",
            "Epoch 73/150\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0412 - mae: 0.2302 - val_loss: 0.0417 - val_mae: 0.2322\n",
            "Epoch 74/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0415 - mae: 0.2312 - val_loss: 0.0411 - val_mae: 0.2345\n",
            "Epoch 75/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0412 - mae: 0.2302 - val_loss: 0.0418 - val_mae: 0.2390\n",
            "Epoch 76/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0412 - mae: 0.2302 - val_loss: 0.0408 - val_mae: 0.2234\n",
            "Epoch 77/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0411 - mae: 0.2299 - val_loss: 0.0406 - val_mae: 0.2315\n",
            "Epoch 78/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0411 - mae: 0.2302 - val_loss: 0.0409 - val_mae: 0.2325\n",
            "Epoch 79/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0410 - mae: 0.2297 - val_loss: 0.0403 - val_mae: 0.2297\n",
            "Epoch 80/150\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0411 - mae: 0.2298 - val_loss: 0.0418 - val_mae: 0.2408\n",
            "Epoch 81/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0411 - mae: 0.2301 - val_loss: 0.0416 - val_mae: 0.2340\n",
            "Epoch 82/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0411 - mae: 0.2300 - val_loss: 0.0443 - val_mae: 0.2424\n",
            "Epoch 83/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0413 - mae: 0.2302 - val_loss: 0.0404 - val_mae: 0.2287\n",
            "Epoch 84/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0410 - mae: 0.2299 - val_loss: 0.0405 - val_mae: 0.2300\n",
            "Epoch 85/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0408 - mae: 0.2294 - val_loss: 0.0411 - val_mae: 0.2322\n",
            "Epoch 86/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2297 - val_loss: 0.0404 - val_mae: 0.2271\n",
            "Epoch 87/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0410 - mae: 0.2296 - val_loss: 0.0403 - val_mae: 0.2281\n",
            "Epoch 88/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0407 - mae: 0.2290 - val_loss: 0.0420 - val_mae: 0.2368\n",
            "Epoch 89/150\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0408 - mae: 0.2293 - val_loss: 0.0403 - val_mae: 0.2300\n",
            "Epoch 90/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0409 - mae: 0.2296 - val_loss: 0.0406 - val_mae: 0.2300\n",
            "Epoch 91/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0407 - mae: 0.2291 - val_loss: 0.0412 - val_mae: 0.2343\n",
            "Epoch 92/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0407 - mae: 0.2285 - val_loss: 0.6463 - val_mae: 1.1486\n",
            "Epoch 93/150\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0408 - mae: 0.2290 - val_loss: 0.0424 - val_mae: 0.2408\n",
            "Epoch 94/150\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0410 - mae: 0.2299 - val_loss: 0.0407 - val_mae: 0.2317\n",
            "Epoch 95/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0409 - mae: 0.2294 - val_loss: 0.0414 - val_mae: 0.2298\n",
            "Epoch 96/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2298 - val_loss: 0.0405 - val_mae: 0.2233\n",
            "Epoch 97/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2292 - val_loss: 0.0402 - val_mae: 0.2296\n",
            "Epoch 98/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2297 - val_loss: 0.0407 - val_mae: 0.2285\n",
            "Epoch 99/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0408 - mae: 0.2292 - val_loss: 0.0421 - val_mae: 0.2363\n",
            "Epoch 100/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2294 - val_loss: 0.0407 - val_mae: 0.2295\n",
            "Epoch 101/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0410 - mae: 0.2299 - val_loss: 0.0409 - val_mae: 0.2314\n",
            "Epoch 102/150\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0409 - mae: 0.2296 - val_loss: 0.0409 - val_mae: 0.2291\n",
            "Epoch 103/150\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0409 - mae: 0.2297 - val_loss: 0.0408 - val_mae: 0.2234\n",
            "Epoch 104/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0408 - mae: 0.2292 - val_loss: 0.0410 - val_mae: 0.2273\n",
            "Epoch 105/150\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0408 - mae: 0.2293 - val_loss: 0.0411 - val_mae: 0.2268\n",
            "Epoch 106/150\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0408 - mae: 0.2293 - val_loss: 0.0408 - val_mae: 0.2236\n",
            "Epoch 107/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0410 - mae: 0.2299 - val_loss: 0.0405 - val_mae: 0.2279\n",
            "Epoch 108/150\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0408 - mae: 0.2292 - val_loss: 0.0410 - val_mae: 0.2319\n",
            "Epoch 109/150\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0408 - mae: 0.2292 - val_loss: 0.0408 - val_mae: 0.2273\n",
            "Epoch 110/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0409 - mae: 0.2292 - val_loss: 0.0402 - val_mae: 0.2294\n",
            "Epoch 111/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0408 - mae: 0.2292 - val_loss: 0.0404 - val_mae: 0.2326\n",
            "Epoch 112/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2296 - val_loss: 0.0415 - val_mae: 0.2372\n",
            "Epoch 113/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2297 - val_loss: 0.0412 - val_mae: 0.2365\n",
            "Epoch 114/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0408 - mae: 0.2293 - val_loss: 0.0411 - val_mae: 0.2365\n",
            "Epoch 115/150\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0410 - mae: 0.2297 - val_loss: 0.0435 - val_mae: 0.2359\n",
            "Epoch 116/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0408 - mae: 0.2290 - val_loss: 0.0429 - val_mae: 0.2461\n",
            "Epoch 117/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0408 - mae: 0.2294 - val_loss: 0.0414 - val_mae: 0.2287\n",
            "Epoch 118/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0408 - mae: 0.2290 - val_loss: 0.0415 - val_mae: 0.2343\n",
            "Epoch 119/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0408 - mae: 0.2295 - val_loss: 0.0420 - val_mae: 0.2275\n",
            "Epoch 120/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2295 - val_loss: 0.0411 - val_mae: 0.2354\n",
            "Epoch 121/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0410 - mae: 0.2297 - val_loss: 0.0410 - val_mae: 0.2293\n",
            "Epoch 122/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0407 - mae: 0.2291 - val_loss: 0.0421 - val_mae: 0.2405\n",
            "Epoch 123/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2295 - val_loss: 0.0405 - val_mae: 0.2234\n",
            "Epoch 124/150\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0408 - mae: 0.2293 - val_loss: 0.0432 - val_mae: 0.2319\n",
            "Epoch 125/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0408 - mae: 0.2293 - val_loss: 0.0410 - val_mae: 0.2326\n",
            "Epoch 126/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0409 - mae: 0.2294 - val_loss: 0.0410 - val_mae: 0.2347\n",
            "Epoch 127/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0408 - mae: 0.2294 - val_loss: 0.0402 - val_mae: 0.2329\n",
            "Epoch 128/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0408 - mae: 0.2294 - val_loss: 0.0414 - val_mae: 0.2288\n",
            "Epoch 129/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2295 - val_loss: 0.0404 - val_mae: 0.2298\n",
            "Epoch 130/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0408 - mae: 0.2292 - val_loss: 0.0411 - val_mae: 0.2304\n",
            "Epoch 131/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2295 - val_loss: 0.0408 - val_mae: 0.2283\n",
            "Epoch 132/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2296 - val_loss: 0.0404 - val_mae: 0.2304\n",
            "Epoch 133/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2296 - val_loss: 0.0408 - val_mae: 0.2310\n",
            "Epoch 134/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2296 - val_loss: 0.0403 - val_mae: 0.2290\n",
            "Epoch 135/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2297 - val_loss: 0.0408 - val_mae: 0.2242\n",
            "Epoch 136/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0408 - mae: 0.2293 - val_loss: 0.0403 - val_mae: 0.2268\n",
            "Epoch 137/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2295 - val_loss: 0.0403 - val_mae: 0.2287\n",
            "Epoch 138/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0408 - mae: 0.2293 - val_loss: 0.0406 - val_mae: 0.2299\n",
            "Epoch 139/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2296 - val_loss: 0.0405 - val_mae: 0.2301\n",
            "Epoch 140/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2295 - val_loss: 0.0404 - val_mae: 0.2301\n",
            "Epoch 141/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2295 - val_loss: 0.0401 - val_mae: 0.2299\n",
            "Epoch 142/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0408 - mae: 0.2294 - val_loss: 0.0405 - val_mae: 0.2316\n",
            "Epoch 143/150\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0410 - mae: 0.2299 - val_loss: 0.0407 - val_mae: 0.2289\n",
            "Epoch 144/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0409 - mae: 0.2297 - val_loss: 0.0422 - val_mae: 0.2355\n",
            "Epoch 145/150\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0409 - mae: 0.2296 - val_loss: 0.0407 - val_mae: 0.2231\n",
            "Epoch 146/150\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0408 - mae: 0.2292 - val_loss: 0.0411 - val_mae: 0.2336\n",
            "Epoch 147/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0410 - mae: 0.2298 - val_loss: 0.0407 - val_mae: 0.2316\n",
            "Epoch 148/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0410 - mae: 0.2298 - val_loss: 0.0407 - val_mae: 0.2259\n",
            "Epoch 149/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2294 - val_loss: 0.0415 - val_mae: 0.2296\n",
            "Epoch 150/150\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0408 - mae: 0.2293 - val_loss: 0.0412 - val_mae: 0.2220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:18:39,548] Trial 10 finished with value: 0.041162628680467606 and parameters: {'learning_rate': 0.06254465646016179, 'dropout_rate': 0.11256689092814054, 'batch_size': 32, 'epochs': 150}. Best is trial 9 with value: 0.035705674439668655.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "30/30 [==============================] - 3s 23ms/step - loss: 0.4693 - mae: 0.8976 - val_loss: 2.3776 - val_mae: 3.0155\n",
            "Epoch 2/50\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.4469 - mae: 0.8679 - val_loss: 1.6658 - val_mae: 2.2621\n",
            "Epoch 3/50\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.4303 - mae: 0.8450 - val_loss: 1.2530 - val_mae: 1.8293\n",
            "Epoch 4/50\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.4159 - mae: 0.8251 - val_loss: 0.7530 - val_mae: 1.2937\n",
            "Epoch 5/50\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.3924 - mae: 0.7924 - val_loss: 0.5045 - val_mae: 0.9728\n",
            "Epoch 6/50\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.3750 - mae: 0.7682 - val_loss: 0.4076 - val_mae: 0.8548\n",
            "Epoch 7/50\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.3631 - mae: 0.7511 - val_loss: 0.3153 - val_mae: 0.7153\n",
            "Epoch 8/50\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.3499 - mae: 0.7316 - val_loss: 0.3120 - val_mae: 0.7111\n",
            "Epoch 9/50\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.3328 - mae: 0.7081 - val_loss: 0.2315 - val_mae: 0.5957\n",
            "Epoch 10/50\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.3246 - mae: 0.6952 - val_loss: 0.1777 - val_mae: 0.5019\n",
            "Epoch 11/50\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.3167 - mae: 0.6839 - val_loss: 0.1964 - val_mae: 0.5393\n",
            "Epoch 12/50\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.3015 - mae: 0.6606 - val_loss: 0.2231 - val_mae: 0.5882\n",
            "Epoch 13/50\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.2899 - mae: 0.6424 - val_loss: 0.2149 - val_mae: 0.5642\n",
            "Epoch 14/50\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.2808 - mae: 0.6282 - val_loss: 0.1885 - val_mae: 0.5256\n",
            "Epoch 15/50\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.2669 - mae: 0.6089 - val_loss: 0.1775 - val_mae: 0.5171\n",
            "Epoch 16/50\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.2607 - mae: 0.5975 - val_loss: 0.1609 - val_mae: 0.4852\n",
            "Epoch 17/50\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.2474 - mae: 0.5777 - val_loss: 0.1468 - val_mae: 0.4648\n",
            "Epoch 18/50\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.2434 - mae: 0.5708 - val_loss: 0.1441 - val_mae: 0.4597\n",
            "Epoch 19/50\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.2338 - mae: 0.5567 - val_loss: 0.1476 - val_mae: 0.4617\n",
            "Epoch 20/50\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.2283 - mae: 0.5464 - val_loss: 0.1233 - val_mae: 0.4188\n",
            "Epoch 21/50\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.2176 - mae: 0.5305 - val_loss: 0.1205 - val_mae: 0.4116\n",
            "Epoch 22/50\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.2115 - mae: 0.5193 - val_loss: 0.1055 - val_mae: 0.3827\n",
            "Epoch 23/50\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.2074 - mae: 0.5129 - val_loss: 0.1020 - val_mae: 0.3711\n",
            "Epoch 24/50\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.2016 - mae: 0.5021 - val_loss: 0.0965 - val_mae: 0.3614\n",
            "Epoch 25/50\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.1970 - mae: 0.4947 - val_loss: 0.0974 - val_mae: 0.3591\n",
            "Epoch 26/50\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1893 - mae: 0.4815 - val_loss: 0.0925 - val_mae: 0.3503\n",
            "Epoch 27/50\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1821 - mae: 0.4713 - val_loss: 0.0887 - val_mae: 0.3420\n",
            "Epoch 28/50\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.1782 - mae: 0.4649 - val_loss: 0.0883 - val_mae: 0.3352\n",
            "Epoch 29/50\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1759 - mae: 0.4587 - val_loss: 0.0856 - val_mae: 0.3311\n",
            "Epoch 30/50\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1676 - mae: 0.4477 - val_loss: 0.0835 - val_mae: 0.3260\n",
            "Epoch 31/50\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.1649 - mae: 0.4420 - val_loss: 0.0826 - val_mae: 0.3236\n",
            "Epoch 32/50\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.1611 - mae: 0.4350 - val_loss: 0.0786 - val_mae: 0.3168\n",
            "Epoch 33/50\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.1576 - mae: 0.4292 - val_loss: 0.0820 - val_mae: 0.3244\n",
            "Epoch 34/50\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1553 - mae: 0.4239 - val_loss: 0.0849 - val_mae: 0.3301\n",
            "Epoch 35/50\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1484 - mae: 0.4134 - val_loss: 0.0776 - val_mae: 0.3152\n",
            "Epoch 36/50\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1528 - mae: 0.4185 - val_loss: 0.0754 - val_mae: 0.3071\n",
            "Epoch 37/50\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1446 - mae: 0.4058 - val_loss: 0.0736 - val_mae: 0.3038\n",
            "Epoch 38/50\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1393 - mae: 0.3982 - val_loss: 0.0726 - val_mae: 0.3041\n",
            "Epoch 39/50\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.1365 - mae: 0.3939 - val_loss: 0.0710 - val_mae: 0.2994\n",
            "Epoch 40/50\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1320 - mae: 0.3866 - val_loss: 0.0695 - val_mae: 0.2952\n",
            "Epoch 41/50\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1326 - mae: 0.3864 - val_loss: 0.0684 - val_mae: 0.2945\n",
            "Epoch 42/50\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.1294 - mae: 0.3819 - val_loss: 0.0668 - val_mae: 0.2907\n",
            "Epoch 43/50\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.1236 - mae: 0.3731 - val_loss: 0.0652 - val_mae: 0.2872\n",
            "Epoch 44/50\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.1197 - mae: 0.3665 - val_loss: 0.0639 - val_mae: 0.2836\n",
            "Epoch 45/50\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1223 - mae: 0.3697 - val_loss: 0.0628 - val_mae: 0.2814\n",
            "Epoch 46/50\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.1166 - mae: 0.3604 - val_loss: 0.0618 - val_mae: 0.2797\n",
            "Epoch 47/50\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.1165 - mae: 0.3591 - val_loss: 0.0611 - val_mae: 0.2783\n",
            "Epoch 48/50\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1108 - mae: 0.3520 - val_loss: 0.0601 - val_mae: 0.2760\n",
            "Epoch 49/50\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1117 - mae: 0.3518 - val_loss: 0.0593 - val_mae: 0.2737\n",
            "Epoch 50/50\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.1100 - mae: 0.3486 - val_loss: 0.0597 - val_mae: 0.2758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:19:01,422] Trial 11 finished with value: 0.059674084186553955 and parameters: {'learning_rate': 0.00011308952969778497, 'dropout_rate': 0.34754981727282236, 'batch_size': 128, 'epochs': 50}. Best is trial 9 with value: 0.035705674439668655.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "60/60 [==============================] - 4s 13ms/step - loss: 0.0634 - mae: 0.2729 - val_loss: 0.1520 - val_mae: 0.4706\n",
            "Epoch 2/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0385 - mae: 0.2202 - val_loss: 0.0408 - val_mae: 0.2304\n",
            "Epoch 3/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0355 - mae: 0.2082 - val_loss: 0.3482 - val_mae: 0.6502\n",
            "Epoch 4/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0344 - mae: 0.2035 - val_loss: 0.0908 - val_mae: 0.3149\n",
            "Epoch 5/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0343 - mae: 0.2025 - val_loss: 0.0390 - val_mae: 0.2269\n",
            "Epoch 6/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0337 - mae: 0.2005 - val_loss: 0.0801 - val_mae: 0.3205\n",
            "Epoch 7/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0336 - mae: 0.1999 - val_loss: 0.0662 - val_mae: 0.2719\n",
            "Epoch 8/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0336 - mae: 0.2000 - val_loss: 0.0679 - val_mae: 0.2717\n",
            "Epoch 9/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0329 - mae: 0.1968 - val_loss: 0.0400 - val_mae: 0.2405\n",
            "Epoch 10/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0329 - mae: 0.1969 - val_loss: 0.0584 - val_mae: 0.2697\n",
            "Epoch 11/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0329 - mae: 0.1969 - val_loss: 0.1936 - val_mae: 0.5317\n",
            "Epoch 12/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0331 - mae: 0.1976 - val_loss: 0.1924 - val_mae: 0.5137\n",
            "Epoch 13/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0328 - mae: 0.1965 - val_loss: 0.1535 - val_mae: 0.4591\n",
            "Epoch 14/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0330 - mae: 0.1974 - val_loss: 0.0731 - val_mae: 0.2942\n",
            "Epoch 15/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0327 - mae: 0.1961 - val_loss: 0.1561 - val_mae: 0.4263\n",
            "Epoch 16/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0329 - mae: 0.1968 - val_loss: 0.5603 - val_mae: 0.9672\n",
            "Epoch 17/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0325 - mae: 0.1947 - val_loss: 0.1983 - val_mae: 0.4953\n",
            "Epoch 18/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0325 - mae: 0.1949 - val_loss: 0.2672 - val_mae: 0.5855\n",
            "Epoch 19/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0326 - mae: 0.1955 - val_loss: 0.1809 - val_mae: 0.4814\n",
            "Epoch 20/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0325 - mae: 0.1954 - val_loss: 0.1403 - val_mae: 0.4085\n",
            "Epoch 21/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0326 - mae: 0.1956 - val_loss: 0.2387 - val_mae: 0.5466\n",
            "Epoch 22/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0332 - mae: 0.1975 - val_loss: 0.0710 - val_mae: 0.2778\n",
            "Epoch 23/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0343 - mae: 0.2027 - val_loss: 0.2278 - val_mae: 0.5274\n",
            "Epoch 24/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0336 - mae: 0.1999 - val_loss: 0.4855 - val_mae: 0.8305\n",
            "Epoch 25/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0342 - mae: 0.2023 - val_loss: 0.6578 - val_mae: 1.1095\n",
            "Epoch 26/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0340 - mae: 0.2012 - val_loss: 0.0900 - val_mae: 0.3181\n",
            "Epoch 27/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0359 - mae: 0.2090 - val_loss: 0.2609 - val_mae: 0.5812\n",
            "Epoch 28/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0360 - mae: 0.2097 - val_loss: 0.1567 - val_mae: 0.4437\n",
            "Epoch 29/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0356 - mae: 0.2084 - val_loss: 0.5894 - val_mae: 0.9563\n",
            "Epoch 30/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0379 - mae: 0.2175 - val_loss: 0.0586 - val_mae: 0.2731\n",
            "Epoch 31/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0613 - val_mae: 0.2711\n",
            "Epoch 32/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0894 - val_mae: 0.3448\n",
            "Epoch 33/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0431 - val_mae: 0.2422\n",
            "Epoch 34/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0411 - val_mae: 0.2341\n",
            "Epoch 35/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0444 - val_mae: 0.2396\n",
            "Epoch 36/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0400 - mae: 0.2275 - val_loss: 0.0420 - val_mae: 0.2393\n",
            "Epoch 37/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0421 - val_mae: 0.2419\n",
            "Epoch 38/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0401 - val_mae: 0.2281\n",
            "Epoch 39/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2276 - val_loss: 0.0404 - val_mae: 0.2291\n",
            "Epoch 40/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0410 - val_mae: 0.2288\n",
            "Epoch 41/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0413 - val_mae: 0.2302\n",
            "Epoch 42/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0404 - val_mae: 0.2329\n",
            "Epoch 43/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0427 - val_mae: 0.2331\n",
            "Epoch 44/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0419 - val_mae: 0.2308\n",
            "Epoch 45/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0408 - val_mae: 0.2287\n",
            "Epoch 46/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0399 - mae: 0.2271 - val_loss: 0.0410 - val_mae: 0.2326\n",
            "Epoch 47/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0401 - val_mae: 0.2292\n",
            "Epoch 48/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0400 - val_mae: 0.2253\n",
            "Epoch 49/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0403 - val_mae: 0.2300\n",
            "Epoch 50/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0406 - val_mae: 0.2303\n",
            "Epoch 51/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0405 - val_mae: 0.2275\n",
            "Epoch 52/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0401 - val_mae: 0.2280\n",
            "Epoch 53/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0398 - mae: 0.2270 - val_loss: 0.0406 - val_mae: 0.2341\n",
            "Epoch 54/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0400 - mae: 0.2277 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 55/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0404 - val_mae: 0.2306\n",
            "Epoch 56/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0403 - val_mae: 0.2301\n",
            "Epoch 57/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0399 - mae: 0.2276 - val_loss: 0.0416 - val_mae: 0.2280\n",
            "Epoch 58/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0405 - val_mae: 0.2282\n",
            "Epoch 59/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0401 - val_mae: 0.2271\n",
            "Epoch 60/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2271 - val_loss: 0.0403 - val_mae: 0.2284\n",
            "Epoch 61/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0398 - mae: 0.2271 - val_loss: 0.0408 - val_mae: 0.2291\n",
            "Epoch 62/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0403 - val_mae: 0.2275\n",
            "Epoch 63/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0398 - mae: 0.2271 - val_loss: 0.0402 - val_mae: 0.2283\n",
            "Epoch 64/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0404 - val_mae: 0.2313\n",
            "Epoch 65/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0413 - val_mae: 0.2308\n",
            "Epoch 66/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0406 - val_mae: 0.2283\n",
            "Epoch 67/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0401 - val_mae: 0.2309\n",
            "Epoch 68/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0415 - val_mae: 0.2301\n",
            "Epoch 69/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0403 - val_mae: 0.2296\n",
            "Epoch 70/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0399 - mae: 0.2276 - val_loss: 0.0402 - val_mae: 0.2256\n",
            "Epoch 71/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0399 - mae: 0.2270 - val_loss: 0.0404 - val_mae: 0.2315\n",
            "Epoch 72/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0399 - mae: 0.2276 - val_loss: 0.0408 - val_mae: 0.2278\n",
            "Epoch 73/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0399 - mae: 0.2276 - val_loss: 0.0402 - val_mae: 0.2285\n",
            "Epoch 74/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0411 - val_mae: 0.2312\n",
            "Epoch 75/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0408 - val_mae: 0.2289\n",
            "Epoch 76/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0402 - val_mae: 0.2268\n",
            "Epoch 77/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0399 - mae: 0.2272 - val_loss: 0.0408 - val_mae: 0.2316\n",
            "Epoch 78/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0405 - val_mae: 0.2274\n",
            "Epoch 79/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2272 - val_loss: 0.0403 - val_mae: 0.2284\n",
            "Epoch 80/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0400 - val_mae: 0.2297\n",
            "Epoch 81/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0408 - val_mae: 0.2290\n",
            "Epoch 82/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0405 - val_mae: 0.2276\n",
            "Epoch 83/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0404 - val_mae: 0.2297\n",
            "Epoch 84/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0403 - val_mae: 0.2302\n",
            "Epoch 85/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.0399 - mae: 0.2276 - val_loss: 0.0405 - val_mae: 0.2266\n",
            "Epoch 86/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0403 - val_mae: 0.2322\n",
            "Epoch 87/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0401 - val_mae: 0.2308\n",
            "Epoch 88/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0406 - val_mae: 0.2291\n",
            "Epoch 89/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0402 - val_mae: 0.2294\n",
            "Epoch 90/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0403 - val_mae: 0.2282\n",
            "Epoch 91/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0424 - val_mae: 0.2270\n",
            "Epoch 92/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0408 - val_mae: 0.2258\n",
            "Epoch 93/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0408 - val_mae: 0.2281\n",
            "Epoch 94/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0400 - mae: 0.2276 - val_loss: 0.0410 - val_mae: 0.2289\n",
            "Epoch 95/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0403 - val_mae: 0.2264\n",
            "Epoch 96/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0409 - val_mae: 0.2290\n",
            "Epoch 97/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0402 - val_mae: 0.2286\n",
            "Epoch 98/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0404 - val_mae: 0.2284\n",
            "Epoch 99/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0407 - val_mae: 0.2295\n",
            "Epoch 100/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0402 - val_mae: 0.2294\n",
            "Epoch 101/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0398 - mae: 0.2271 - val_loss: 0.0421 - val_mae: 0.2333\n",
            "Epoch 102/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0435 - val_mae: 0.2354\n",
            "Epoch 103/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0400 - mae: 0.2276 - val_loss: 0.0403 - val_mae: 0.2295\n",
            "Epoch 104/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2272 - val_loss: 0.0401 - val_mae: 0.2300\n",
            "Epoch 105/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0398 - mae: 0.2272 - val_loss: 0.0402 - val_mae: 0.2265\n",
            "Epoch 106/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0411 - val_mae: 0.2268\n",
            "Epoch 107/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0399 - mae: 0.2272 - val_loss: 0.0402 - val_mae: 0.2273\n",
            "Epoch 108/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0416 - val_mae: 0.2318\n",
            "Epoch 109/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0403 - val_mae: 0.2308\n",
            "Epoch 110/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0406 - val_mae: 0.2284\n",
            "Epoch 111/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0402 - val_mae: 0.2292\n",
            "Epoch 112/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0410 - val_mae: 0.2277\n",
            "Epoch 113/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0399 - mae: 0.2272 - val_loss: 0.0412 - val_mae: 0.2276\n",
            "Epoch 114/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0404 - val_mae: 0.2310\n",
            "Epoch 115/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0403 - val_mae: 0.2285\n",
            "Epoch 116/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0406 - val_mae: 0.2331\n",
            "Epoch 117/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0403 - val_mae: 0.2282\n",
            "Epoch 118/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0404 - val_mae: 0.2308\n",
            "Epoch 119/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0407 - val_mae: 0.2314\n",
            "Epoch 120/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0407 - val_mae: 0.2286\n",
            "Epoch 121/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0399 - mae: 0.2272 - val_loss: 0.0419 - val_mae: 0.2340\n",
            "Epoch 122/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0399 - mae: 0.2276 - val_loss: 0.0407 - val_mae: 0.2278\n",
            "Epoch 123/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0430 - val_mae: 0.2343\n",
            "Epoch 124/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0399 - mae: 0.2276 - val_loss: 0.0407 - val_mae: 0.2248\n",
            "Epoch 125/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0399 - mae: 0.2271 - val_loss: 0.0404 - val_mae: 0.2292\n",
            "Epoch 126/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0399 - mae: 0.2272 - val_loss: 0.0414 - val_mae: 0.2341\n",
            "Epoch 127/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0403 - val_mae: 0.2294\n",
            "Epoch 128/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0398 - mae: 0.2271 - val_loss: 0.0404 - val_mae: 0.2286\n",
            "Epoch 129/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0410 - val_mae: 0.2276\n",
            "Epoch 130/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0406 - val_mae: 0.2310\n",
            "Epoch 131/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0407 - val_mae: 0.2297\n",
            "Epoch 132/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0408 - val_mae: 0.2333\n",
            "Epoch 133/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0433 - val_mae: 0.2312\n",
            "Epoch 134/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0403 - val_mae: 0.2276\n",
            "Epoch 135/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0403 - val_mae: 0.2287\n",
            "Epoch 136/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0405 - val_mae: 0.2288\n",
            "Epoch 137/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0408 - val_mae: 0.2325\n",
            "Epoch 138/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2276 - val_loss: 0.0411 - val_mae: 0.2287\n",
            "Epoch 139/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0399 - mae: 0.2272 - val_loss: 0.0405 - val_mae: 0.2295\n",
            "Epoch 140/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0408 - val_mae: 0.2294\n",
            "Epoch 141/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0412 - val_mae: 0.2277\n",
            "Epoch 142/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0418 - val_mae: 0.2290\n",
            "Epoch 143/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0399 - mae: 0.2271 - val_loss: 0.0483 - val_mae: 0.2416\n",
            "Epoch 144/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0400 - mae: 0.2277 - val_loss: 0.0407 - val_mae: 0.2299\n",
            "Epoch 145/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0404 - val_mae: 0.2300\n",
            "Epoch 146/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2276 - val_loss: 0.0408 - val_mae: 0.2279\n",
            "Epoch 147/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0409 - val_mae: 0.2330\n",
            "Epoch 148/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0404 - val_mae: 0.2305\n",
            "Epoch 149/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0414 - val_mae: 0.2301\n",
            "Epoch 150/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0437 - val_mae: 0.2356\n",
            "Epoch 151/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0399 - mae: 0.2276 - val_loss: 0.0404 - val_mae: 0.2268\n",
            "Epoch 152/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0399 - mae: 0.2272 - val_loss: 0.0404 - val_mae: 0.2294\n",
            "Epoch 153/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0404 - val_mae: 0.2291\n",
            "Epoch 154/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0408 - val_mae: 0.2309\n",
            "Epoch 155/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0401 - val_mae: 0.2258\n",
            "Epoch 156/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0417 - val_mae: 0.2294\n",
            "Epoch 157/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0404 - val_mae: 0.2294\n",
            "Epoch 158/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0398 - mae: 0.2272 - val_loss: 0.0404 - val_mae: 0.2276\n",
            "Epoch 159/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0399 - mae: 0.2276 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 160/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0403 - val_mae: 0.2299\n",
            "Epoch 161/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0407 - val_mae: 0.2290\n",
            "Epoch 162/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0405 - val_mae: 0.2269\n",
            "Epoch 163/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0403 - val_mae: 0.2309\n",
            "Epoch 164/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0408 - val_mae: 0.2274\n",
            "Epoch 165/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0400 - mae: 0.2274 - val_loss: 0.0400 - val_mae: 0.2284\n",
            "Epoch 166/200\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0402 - val_mae: 0.2294\n",
            "Epoch 167/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0400 - val_mae: 0.2285\n",
            "Epoch 168/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0403 - val_mae: 0.2317\n",
            "Epoch 169/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0398 - mae: 0.2273 - val_loss: 0.0400 - val_mae: 0.2296\n",
            "Epoch 170/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0400 - val_mae: 0.2277\n",
            "Epoch 171/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0404 - val_mae: 0.2282\n",
            "Epoch 172/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0401 - val_mae: 0.2298\n",
            "Epoch 173/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0404 - val_mae: 0.2301\n",
            "Epoch 174/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0403 - val_mae: 0.2325\n",
            "Epoch 175/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0401 - val_mae: 0.2283\n",
            "Epoch 176/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0400 - val_mae: 0.2281\n",
            "Epoch 177/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0407 - val_mae: 0.2294\n",
            "Epoch 178/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0403 - val_mae: 0.2277\n",
            "Epoch 179/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.0398 - mae: 0.2272 - val_loss: 0.0407 - val_mae: 0.2284\n",
            "Epoch 180/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0398 - mae: 0.2273 - val_loss: 0.0403 - val_mae: 0.2278\n",
            "Epoch 181/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0399 - mae: 0.2273 - val_loss: 0.0400 - val_mae: 0.2296\n",
            "Epoch 182/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0404 - val_mae: 0.2289\n",
            "Epoch 183/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0400 - val_mae: 0.2270\n",
            "Epoch 184/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0398 - mae: 0.2272 - val_loss: 0.0400 - val_mae: 0.2305\n",
            "Epoch 185/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0401 - val_mae: 0.2307\n",
            "Epoch 186/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0400 - val_mae: 0.2281\n",
            "Epoch 187/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 188/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0398 - mae: 0.2271 - val_loss: 0.0403 - val_mae: 0.2311\n",
            "Epoch 189/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0405 - val_mae: 0.2304\n",
            "Epoch 190/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0404 - val_mae: 0.2285\n",
            "Epoch 191/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0400 - val_mae: 0.2293\n",
            "Epoch 192/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0398 - mae: 0.2274 - val_loss: 0.0402 - val_mae: 0.2274\n",
            "Epoch 193/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0398 - mae: 0.2271 - val_loss: 0.0402 - val_mae: 0.2315\n",
            "Epoch 194/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2275 - val_loss: 0.0401 - val_mae: 0.2287\n",
            "Epoch 195/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0402 - val_mae: 0.2290\n",
            "Epoch 196/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0403 - val_mae: 0.2312\n",
            "Epoch 197/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0404 - val_mae: 0.2285\n",
            "Epoch 198/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0399 - mae: 0.2272 - val_loss: 0.0403 - val_mae: 0.2271\n",
            "Epoch 199/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0398 - mae: 0.2271 - val_loss: 0.0415 - val_mae: 0.2285\n",
            "Epoch 200/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0398 - mae: 0.2271 - val_loss: 0.0402 - val_mae: 0.2299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:20:56,364] Trial 12 finished with value: 0.04016079008579254 and parameters: {'learning_rate': 0.027019877492637887, 'dropout_rate': 0.29643715406422977, 'batch_size': 64, 'epochs': 200}. Best is trial 9 with value: 0.035705674439668655.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "30/30 [==============================] - 5s 23ms/step - loss: 0.4096 - mae: 0.8104 - val_loss: 6.5451 - val_mae: 7.1848\n",
            "Epoch 2/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.2736 - mae: 0.6160 - val_loss: 4.3322 - val_mae: 4.9760\n",
            "Epoch 3/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1945 - mae: 0.4880 - val_loss: 2.6648 - val_mae: 3.2102\n",
            "Epoch 4/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1499 - mae: 0.4131 - val_loss: 0.9935 - val_mae: 1.4769\n",
            "Epoch 5/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1192 - mae: 0.3629 - val_loss: 0.6006 - val_mae: 1.0482\n",
            "Epoch 6/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0958 - mae: 0.3261 - val_loss: 0.1903 - val_mae: 0.5116\n",
            "Epoch 7/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0810 - mae: 0.3011 - val_loss: 0.0766 - val_mae: 0.3227\n",
            "Epoch 8/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0708 - mae: 0.2838 - val_loss: 0.0897 - val_mae: 0.3572\n",
            "Epoch 9/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0638 - mae: 0.2705 - val_loss: 0.0462 - val_mae: 0.2389\n",
            "Epoch 10/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0582 - mae: 0.2605 - val_loss: 0.0487 - val_mae: 0.2571\n",
            "Epoch 11/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0527 - mae: 0.2519 - val_loss: 0.0432 - val_mae: 0.2360\n",
            "Epoch 12/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0500 - mae: 0.2474 - val_loss: 0.0406 - val_mae: 0.2326\n",
            "Epoch 13/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0469 - mae: 0.2408 - val_loss: 0.0402 - val_mae: 0.2308\n",
            "Epoch 14/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0453 - mae: 0.2382 - val_loss: 0.0400 - val_mae: 0.2294\n",
            "Epoch 15/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0435 - mae: 0.2350 - val_loss: 0.0400 - val_mae: 0.2292\n",
            "Epoch 16/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0424 - mae: 0.2325 - val_loss: 0.0399 - val_mae: 0.2291\n",
            "Epoch 17/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0412 - mae: 0.2303 - val_loss: 0.0399 - val_mae: 0.2289\n",
            "Epoch 18/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0408 - mae: 0.2295 - val_loss: 0.0399 - val_mae: 0.2284\n",
            "Epoch 19/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0403 - mae: 0.2287 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 20/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0401 - mae: 0.2280 - val_loss: 0.0399 - val_mae: 0.2286\n",
            "Epoch 21/150\n",
            "30/30 [==============================] - 1s 22ms/step - loss: 0.0400 - mae: 0.2280 - val_loss: 0.0398 - val_mae: 0.2285\n",
            "Epoch 22/150\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.0399 - mae: 0.2278 - val_loss: 0.0399 - val_mae: 0.2284\n",
            "Epoch 23/150\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0398 - mae: 0.2276 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 24/150\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0398 - mae: 0.2274 - val_loss: 0.0398 - val_mae: 0.2291\n",
            "Epoch 25/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0397 - mae: 0.2275 - val_loss: 0.0398 - val_mae: 0.2289\n",
            "Epoch 26/150\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.0397 - mae: 0.2273 - val_loss: 0.0398 - val_mae: 0.2288\n",
            "Epoch 27/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 28/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 29/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0397 - mae: 0.2273 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 30/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 31/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 32/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 33/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2286\n",
            "Epoch 34/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 35/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2285\n",
            "Epoch 36/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 37/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2285\n",
            "Epoch 38/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 39/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 40/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 41/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 42/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0396 - mae: 0.2265 - val_loss: 0.0399 - val_mae: 0.2284\n",
            "Epoch 43/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2279\n",
            "Epoch 44/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0395 - mae: 0.2263 - val_loss: 0.0399 - val_mae: 0.2284\n",
            "Epoch 45/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 46/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0396 - mae: 0.2263 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 47/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0395 - mae: 0.2262 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 48/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0395 - mae: 0.2265 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 49/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0394 - mae: 0.2260 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 50/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0395 - mae: 0.2263 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 51/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0394 - mae: 0.2260 - val_loss: 0.0398 - val_mae: 0.2277\n",
            "Epoch 52/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0394 - mae: 0.2258 - val_loss: 0.0398 - val_mae: 0.2277\n",
            "Epoch 53/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0394 - mae: 0.2260 - val_loss: 0.0398 - val_mae: 0.2279\n",
            "Epoch 54/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0394 - mae: 0.2258 - val_loss: 0.0398 - val_mae: 0.2279\n",
            "Epoch 55/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0394 - mae: 0.2260 - val_loss: 0.0398 - val_mae: 0.2277\n",
            "Epoch 56/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0395 - mae: 0.2261 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 57/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0394 - mae: 0.2257 - val_loss: 0.0398 - val_mae: 0.2275\n",
            "Epoch 58/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0394 - mae: 0.2256 - val_loss: 0.0398 - val_mae: 0.2279\n",
            "Epoch 59/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0393 - mae: 0.2253 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 60/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0393 - mae: 0.2254 - val_loss: 0.0398 - val_mae: 0.2275\n",
            "Epoch 61/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0392 - mae: 0.2249 - val_loss: 0.0398 - val_mae: 0.2274\n",
            "Epoch 62/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0391 - mae: 0.2247 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 63/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0392 - mae: 0.2248 - val_loss: 0.0398 - val_mae: 0.2275\n",
            "Epoch 64/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0391 - mae: 0.2246 - val_loss: 0.0398 - val_mae: 0.2275\n",
            "Epoch 65/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0391 - mae: 0.2246 - val_loss: 0.0398 - val_mae: 0.2275\n",
            "Epoch 66/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0391 - mae: 0.2245 - val_loss: 0.0398 - val_mae: 0.2274\n",
            "Epoch 67/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0391 - mae: 0.2247 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 68/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0391 - mae: 0.2244 - val_loss: 0.0398 - val_mae: 0.2273\n",
            "Epoch 69/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0391 - mae: 0.2243 - val_loss: 0.0399 - val_mae: 0.2273\n",
            "Epoch 70/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0388 - mae: 0.2235 - val_loss: 0.0399 - val_mae: 0.2271\n",
            "Epoch 71/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0388 - mae: 0.2231 - val_loss: 0.0399 - val_mae: 0.2271\n",
            "Epoch 72/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0387 - mae: 0.2229 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 73/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0390 - mae: 0.2242 - val_loss: 0.0398 - val_mae: 0.2273\n",
            "Epoch 74/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0389 - mae: 0.2239 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 75/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0386 - mae: 0.2224 - val_loss: 0.0399 - val_mae: 0.2272\n",
            "Epoch 76/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0386 - mae: 0.2223 - val_loss: 0.0399 - val_mae: 0.2269\n",
            "Epoch 77/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0385 - mae: 0.2221 - val_loss: 0.0399 - val_mae: 0.2272\n",
            "Epoch 78/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0383 - mae: 0.2211 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 79/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0383 - mae: 0.2215 - val_loss: 0.0399 - val_mae: 0.2270\n",
            "Epoch 80/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0382 - mae: 0.2213 - val_loss: 0.0399 - val_mae: 0.2267\n",
            "Epoch 81/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0381 - mae: 0.2204 - val_loss: 0.0399 - val_mae: 0.2267\n",
            "Epoch 82/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0383 - mae: 0.2212 - val_loss: 0.0399 - val_mae: 0.2270\n",
            "Epoch 83/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0381 - mae: 0.2205 - val_loss: 0.0399 - val_mae: 0.2272\n",
            "Epoch 84/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0379 - mae: 0.2198 - val_loss: 0.0399 - val_mae: 0.2268\n",
            "Epoch 85/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0379 - mae: 0.2196 - val_loss: 0.0400 - val_mae: 0.2271\n",
            "Epoch 86/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0377 - mae: 0.2190 - val_loss: 0.0399 - val_mae: 0.2271\n",
            "Epoch 87/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0379 - mae: 0.2199 - val_loss: 0.0399 - val_mae: 0.2272\n",
            "Epoch 88/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0376 - mae: 0.2188 - val_loss: 0.0400 - val_mae: 0.2267\n",
            "Epoch 89/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0378 - mae: 0.2191 - val_loss: 0.0400 - val_mae: 0.2273\n",
            "Epoch 90/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0376 - mae: 0.2185 - val_loss: 0.0709 - val_mae: 0.2870\n",
            "Epoch 91/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0377 - mae: 0.2191 - val_loss: 0.1156 - val_mae: 0.3709\n",
            "Epoch 92/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0377 - mae: 0.2189 - val_loss: 0.0695 - val_mae: 0.2787\n",
            "Epoch 93/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0375 - mae: 0.2180 - val_loss: 0.0881 - val_mae: 0.3178\n",
            "Epoch 94/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0374 - mae: 0.2178 - val_loss: 0.1360 - val_mae: 0.4108\n",
            "Epoch 95/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0374 - mae: 0.2179 - val_loss: 0.0724 - val_mae: 0.2895\n",
            "Epoch 96/150\n",
            "30/30 [==============================] - 0s 17ms/step - loss: 0.0374 - mae: 0.2175 - val_loss: 0.3188 - val_mae: 0.6782\n",
            "Epoch 97/150\n",
            "30/30 [==============================] - 0s 17ms/step - loss: 0.0373 - mae: 0.2176 - val_loss: 0.0881 - val_mae: 0.3222\n",
            "Epoch 98/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0372 - mae: 0.2172 - val_loss: 0.2322 - val_mae: 0.5539\n",
            "Epoch 99/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0371 - mae: 0.2166 - val_loss: 0.3826 - val_mae: 0.7619\n",
            "Epoch 100/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0370 - mae: 0.2162 - val_loss: 0.2732 - val_mae: 0.6121\n",
            "Epoch 101/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0368 - mae: 0.2157 - val_loss: 0.6399 - val_mae: 1.0780\n",
            "Epoch 102/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0367 - mae: 0.2149 - val_loss: 0.0374 - val_mae: 0.2198\n",
            "Epoch 103/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0366 - mae: 0.2149 - val_loss: 0.0446 - val_mae: 0.2282\n",
            "Epoch 104/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0367 - mae: 0.2147 - val_loss: 0.2998 - val_mae: 0.6524\n",
            "Epoch 105/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0366 - mae: 0.2144 - val_loss: 0.2814 - val_mae: 0.6271\n",
            "Epoch 106/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0366 - mae: 0.2143 - val_loss: 0.2246 - val_mae: 0.5532\n",
            "Epoch 107/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0362 - mae: 0.2127 - val_loss: 0.0840 - val_mae: 0.3113\n",
            "Epoch 108/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0362 - mae: 0.2126 - val_loss: 0.2079 - val_mae: 0.5224\n",
            "Epoch 109/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0360 - mae: 0.2123 - val_loss: 0.0659 - val_mae: 0.2760\n",
            "Epoch 110/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0359 - mae: 0.2114 - val_loss: 0.1867 - val_mae: 0.4973\n",
            "Epoch 111/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0357 - mae: 0.2111 - val_loss: 0.1419 - val_mae: 0.4140\n",
            "Epoch 112/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0353 - mae: 0.2092 - val_loss: 0.1101 - val_mae: 0.3624\n",
            "Epoch 113/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0350 - mae: 0.2081 - val_loss: 0.3004 - val_mae: 0.6552\n",
            "Epoch 114/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0352 - mae: 0.2085 - val_loss: 0.2435 - val_mae: 0.5732\n",
            "Epoch 115/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0350 - mae: 0.2078 - val_loss: 0.1759 - val_mae: 0.4738\n",
            "Epoch 116/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0349 - mae: 0.2069 - val_loss: 0.2682 - val_mae: 0.6203\n",
            "Epoch 117/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0349 - mae: 0.2069 - val_loss: 0.1209 - val_mae: 0.3812\n",
            "Epoch 118/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0349 - mae: 0.2069 - val_loss: 0.1569 - val_mae: 0.4377\n",
            "Epoch 119/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0347 - mae: 0.2063 - val_loss: 0.0621 - val_mae: 0.2620\n",
            "Epoch 120/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0344 - mae: 0.2055 - val_loss: 0.0405 - val_mae: 0.2245\n",
            "Epoch 121/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0345 - mae: 0.2051 - val_loss: 0.0385 - val_mae: 0.2213\n",
            "Epoch 122/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0342 - mae: 0.2043 - val_loss: 0.0484 - val_mae: 0.2365\n",
            "Epoch 123/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0345 - mae: 0.2052 - val_loss: 0.1011 - val_mae: 0.3436\n",
            "Epoch 124/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0342 - mae: 0.2044 - val_loss: 0.1069 - val_mae: 0.3550\n",
            "Epoch 125/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0342 - mae: 0.2042 - val_loss: 0.0768 - val_mae: 0.2921\n",
            "Epoch 126/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0341 - mae: 0.2036 - val_loss: 0.2124 - val_mae: 0.5265\n",
            "Epoch 127/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0345 - mae: 0.2049 - val_loss: 0.0572 - val_mae: 0.2520\n",
            "Epoch 128/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0339 - mae: 0.2032 - val_loss: 0.0455 - val_mae: 0.2323\n",
            "Epoch 129/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0342 - mae: 0.2042 - val_loss: 0.0679 - val_mae: 0.2772\n",
            "Epoch 130/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0341 - mae: 0.2036 - val_loss: 0.0434 - val_mae: 0.2295\n",
            "Epoch 131/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0340 - mae: 0.2034 - val_loss: 0.0410 - val_mae: 0.2242\n",
            "Epoch 132/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0340 - mae: 0.2035 - val_loss: 0.0403 - val_mae: 0.2242\n",
            "Epoch 133/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0340 - mae: 0.2030 - val_loss: 0.0696 - val_mae: 0.2774\n",
            "Epoch 134/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0341 - mae: 0.2033 - val_loss: 0.1606 - val_mae: 0.4478\n",
            "Epoch 135/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0343 - mae: 0.2042 - val_loss: 0.0363 - val_mae: 0.2161\n",
            "Epoch 136/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0340 - mae: 0.2032 - val_loss: 0.0557 - val_mae: 0.2486\n",
            "Epoch 137/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0341 - mae: 0.2037 - val_loss: 0.2406 - val_mae: 0.5698\n",
            "Epoch 138/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0343 - mae: 0.2044 - val_loss: 0.1051 - val_mae: 0.3485\n",
            "Epoch 139/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0340 - mae: 0.2032 - val_loss: 0.0956 - val_mae: 0.3295\n",
            "Epoch 140/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0339 - mae: 0.2029 - val_loss: 0.0693 - val_mae: 0.2752\n",
            "Epoch 141/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0340 - mae: 0.2032 - val_loss: 0.0518 - val_mae: 0.2426\n",
            "Epoch 142/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0341 - mae: 0.2037 - val_loss: 0.1202 - val_mae: 0.3789\n",
            "Epoch 143/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0341 - mae: 0.2034 - val_loss: 0.0657 - val_mae: 0.2721\n",
            "Epoch 144/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0339 - mae: 0.2030 - val_loss: 0.1192 - val_mae: 0.3788\n",
            "Epoch 145/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0336 - mae: 0.2017 - val_loss: 0.1352 - val_mae: 0.3985\n",
            "Epoch 146/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0338 - mae: 0.2024 - val_loss: 0.0776 - val_mae: 0.2974\n",
            "Epoch 147/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0340 - mae: 0.2029 - val_loss: 0.0565 - val_mae: 0.2532\n",
            "Epoch 148/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0336 - mae: 0.2017 - val_loss: 0.0607 - val_mae: 0.2605\n",
            "Epoch 149/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0336 - mae: 0.2014 - val_loss: 0.1838 - val_mae: 0.4824\n",
            "Epoch 150/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0339 - mae: 0.2026 - val_loss: 0.0877 - val_mae: 0.3148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:22:01,956] Trial 13 finished with value: 0.08767053484916687 and parameters: {'learning_rate': 0.001362884341671699, 'dropout_rate': 0.45722170198669065, 'batch_size': 128, 'epochs': 150}. Best is trial 9 with value: 0.035705674439668655.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "30/30 [==============================] - 3s 25ms/step - loss: 0.4347 - mae: 0.8599 - val_loss: 2.9500 - val_mae: 3.5680\n",
            "Epoch 2/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.3829 - mae: 0.7906 - val_loss: 3.4779 - val_mae: 4.1148\n",
            "Epoch 3/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.3430 - mae: 0.7345 - val_loss: 2.5348 - val_mae: 3.1642\n",
            "Epoch 4/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.2984 - mae: 0.6714 - val_loss: 1.6185 - val_mae: 2.1744\n",
            "Epoch 5/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.2600 - mae: 0.6145 - val_loss: 1.1115 - val_mae: 1.6198\n",
            "Epoch 6/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.2311 - mae: 0.5704 - val_loss: 0.8730 - val_mae: 1.3973\n",
            "Epoch 7/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.2082 - mae: 0.5354 - val_loss: 0.6901 - val_mae: 1.1861\n",
            "Epoch 8/150\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.1845 - mae: 0.4963 - val_loss: 0.6328 - val_mae: 1.1345\n",
            "Epoch 9/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.1669 - mae: 0.4671 - val_loss: 0.5484 - val_mae: 1.0100\n",
            "Epoch 10/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.1517 - mae: 0.4403 - val_loss: 0.2725 - val_mae: 0.6595\n",
            "Epoch 11/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1387 - mae: 0.4170 - val_loss: 0.2022 - val_mae: 0.5662\n",
            "Epoch 12/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1288 - mae: 0.3983 - val_loss: 0.1924 - val_mae: 0.5607\n",
            "Epoch 13/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.1180 - mae: 0.3792 - val_loss: 0.0901 - val_mae: 0.3671\n",
            "Epoch 14/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1081 - mae: 0.3611 - val_loss: 0.0797 - val_mae: 0.3256\n",
            "Epoch 15/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1015 - mae: 0.3484 - val_loss: 0.0683 - val_mae: 0.3074\n",
            "Epoch 16/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0950 - mae: 0.3363 - val_loss: 0.0890 - val_mae: 0.3505\n",
            "Epoch 17/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0882 - mae: 0.3222 - val_loss: 0.0688 - val_mae: 0.3009\n",
            "Epoch 18/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0829 - mae: 0.3115 - val_loss: 0.0488 - val_mae: 0.2576\n",
            "Epoch 19/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0790 - mae: 0.3048 - val_loss: 0.0531 - val_mae: 0.2713\n",
            "Epoch 20/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0759 - mae: 0.2965 - val_loss: 0.0542 - val_mae: 0.2783\n",
            "Epoch 21/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0710 - mae: 0.2874 - val_loss: 0.0435 - val_mae: 0.2429\n",
            "Epoch 22/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0690 - mae: 0.2820 - val_loss: 0.0463 - val_mae: 0.2549\n",
            "Epoch 23/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0662 - mae: 0.2755 - val_loss: 0.0411 - val_mae: 0.2331\n",
            "Epoch 24/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0640 - mae: 0.2704 - val_loss: 0.0410 - val_mae: 0.2346\n",
            "Epoch 25/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0621 - mae: 0.2658 - val_loss: 0.0424 - val_mae: 0.2335\n",
            "Epoch 26/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0603 - mae: 0.2610 - val_loss: 0.0439 - val_mae: 0.2308\n",
            "Epoch 27/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0565 - mae: 0.2534 - val_loss: 0.0424 - val_mae: 0.2299\n",
            "Epoch 28/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0583 - mae: 0.2552 - val_loss: 0.0409 - val_mae: 0.2306\n",
            "Epoch 29/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0556 - mae: 0.2495 - val_loss: 0.0389 - val_mae: 0.2282\n",
            "Epoch 30/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0532 - mae: 0.2451 - val_loss: 0.0417 - val_mae: 0.2331\n",
            "Epoch 31/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0534 - mae: 0.2437 - val_loss: 0.0368 - val_mae: 0.2207\n",
            "Epoch 32/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0528 - mae: 0.2423 - val_loss: 0.0382 - val_mae: 0.2227\n",
            "Epoch 33/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0505 - mae: 0.2385 - val_loss: 0.0369 - val_mae: 0.2183\n",
            "Epoch 34/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0503 - mae: 0.2362 - val_loss: 0.0366 - val_mae: 0.2189\n",
            "Epoch 35/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0480 - mae: 0.2330 - val_loss: 0.0375 - val_mae: 0.2226\n",
            "Epoch 36/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0472 - mae: 0.2313 - val_loss: 0.0386 - val_mae: 0.2241\n",
            "Epoch 37/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0471 - mae: 0.2307 - val_loss: 0.0681 - val_mae: 0.2914\n",
            "Epoch 38/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0470 - mae: 0.2298 - val_loss: 0.0657 - val_mae: 0.2881\n",
            "Epoch 39/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0457 - mae: 0.2280 - val_loss: 0.0699 - val_mae: 0.2979\n",
            "Epoch 40/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0458 - mae: 0.2279 - val_loss: 0.0369 - val_mae: 0.2146\n",
            "Epoch 41/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0442 - mae: 0.2250 - val_loss: 0.0354 - val_mae: 0.2136\n",
            "Epoch 42/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0443 - mae: 0.2241 - val_loss: 0.0359 - val_mae: 0.2152\n",
            "Epoch 43/150\n",
            "30/30 [==============================] - 0s 17ms/step - loss: 0.0436 - mae: 0.2230 - val_loss: 0.0438 - val_mae: 0.2292\n",
            "Epoch 44/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0432 - mae: 0.2228 - val_loss: 0.0412 - val_mae: 0.2232\n",
            "Epoch 45/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0430 - mae: 0.2222 - val_loss: 0.0412 - val_mae: 0.2226\n",
            "Epoch 46/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0425 - mae: 0.2209 - val_loss: 0.0341 - val_mae: 0.2078\n",
            "Epoch 47/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0420 - mae: 0.2199 - val_loss: 0.0396 - val_mae: 0.2174\n",
            "Epoch 48/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0413 - mae: 0.2187 - val_loss: 0.0391 - val_mae: 0.2180\n",
            "Epoch 49/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0409 - mae: 0.2180 - val_loss: 0.0376 - val_mae: 0.2171\n",
            "Epoch 50/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0406 - mae: 0.2177 - val_loss: 0.0450 - val_mae: 0.2311\n",
            "Epoch 51/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0406 - mae: 0.2173 - val_loss: 0.0335 - val_mae: 0.2059\n",
            "Epoch 52/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0398 - mae: 0.2157 - val_loss: 0.0444 - val_mae: 0.2308\n",
            "Epoch 53/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0397 - mae: 0.2151 - val_loss: 0.0372 - val_mae: 0.2125\n",
            "Epoch 54/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0389 - mae: 0.2135 - val_loss: 0.0451 - val_mae: 0.2310\n",
            "Epoch 55/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0391 - mae: 0.2140 - val_loss: 0.0458 - val_mae: 0.2334\n",
            "Epoch 56/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0386 - mae: 0.2125 - val_loss: 0.0441 - val_mae: 0.2286\n",
            "Epoch 57/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0380 - mae: 0.2116 - val_loss: 0.0355 - val_mae: 0.2103\n",
            "Epoch 58/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0378 - mae: 0.2109 - val_loss: 0.0323 - val_mae: 0.2004\n",
            "Epoch 59/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0374 - mae: 0.2096 - val_loss: 0.0331 - val_mae: 0.2000\n",
            "Epoch 60/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0374 - mae: 0.2094 - val_loss: 0.0411 - val_mae: 0.2210\n",
            "Epoch 61/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0375 - mae: 0.2103 - val_loss: 0.0377 - val_mae: 0.2164\n",
            "Epoch 62/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0369 - mae: 0.2090 - val_loss: 0.0384 - val_mae: 0.2177\n",
            "Epoch 63/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0370 - mae: 0.2090 - val_loss: 0.0511 - val_mae: 0.2502\n",
            "Epoch 64/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0368 - mae: 0.2083 - val_loss: 0.0373 - val_mae: 0.2157\n",
            "Epoch 65/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0358 - mae: 0.2063 - val_loss: 0.0334 - val_mae: 0.2033\n",
            "Epoch 66/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0359 - mae: 0.2065 - val_loss: 0.0372 - val_mae: 0.2139\n",
            "Epoch 67/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0355 - mae: 0.2058 - val_loss: 0.0320 - val_mae: 0.1981\n",
            "Epoch 68/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0356 - mae: 0.2053 - val_loss: 0.0320 - val_mae: 0.1989\n",
            "Epoch 69/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0354 - mae: 0.2054 - val_loss: 0.0378 - val_mae: 0.2134\n",
            "Epoch 70/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0352 - mae: 0.2042 - val_loss: 0.0328 - val_mae: 0.2005\n",
            "Epoch 71/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0348 - mae: 0.2038 - val_loss: 0.0324 - val_mae: 0.2007\n",
            "Epoch 72/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0352 - mae: 0.2044 - val_loss: 0.0320 - val_mae: 0.1993\n",
            "Epoch 73/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0348 - mae: 0.2032 - val_loss: 0.0421 - val_mae: 0.2225\n",
            "Epoch 74/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0345 - mae: 0.2026 - val_loss: 0.0414 - val_mae: 0.2209\n",
            "Epoch 75/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0346 - mae: 0.2033 - val_loss: 0.0328 - val_mae: 0.2012\n",
            "Epoch 76/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0346 - mae: 0.2024 - val_loss: 0.0436 - val_mae: 0.2297\n",
            "Epoch 77/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0340 - mae: 0.2011 - val_loss: 0.0326 - val_mae: 0.2011\n",
            "Epoch 78/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0340 - mae: 0.2012 - val_loss: 0.0335 - val_mae: 0.2036\n",
            "Epoch 79/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0340 - mae: 0.2006 - val_loss: 0.0488 - val_mae: 0.2342\n",
            "Epoch 80/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0338 - mae: 0.2001 - val_loss: 0.0464 - val_mae: 0.2307\n",
            "Epoch 81/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0337 - mae: 0.1999 - val_loss: 0.0355 - val_mae: 0.2082\n",
            "Epoch 82/150\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0339 - mae: 0.2008 - val_loss: 0.0390 - val_mae: 0.2161\n",
            "Epoch 83/150\n",
            "30/30 [==============================] - 0s 17ms/step - loss: 0.0334 - mae: 0.1993 - val_loss: 0.0447 - val_mae: 0.2242\n",
            "Epoch 84/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0333 - mae: 0.1988 - val_loss: 0.0386 - val_mae: 0.2111\n",
            "Epoch 85/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0332 - mae: 0.1985 - val_loss: 0.0519 - val_mae: 0.2406\n",
            "Epoch 86/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0331 - mae: 0.1981 - val_loss: 0.0815 - val_mae: 0.2979\n",
            "Epoch 87/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0331 - mae: 0.1983 - val_loss: 0.0423 - val_mae: 0.2191\n",
            "Epoch 88/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0328 - mae: 0.1972 - val_loss: 0.0353 - val_mae: 0.2052\n",
            "Epoch 89/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0331 - mae: 0.1981 - val_loss: 0.0423 - val_mae: 0.2205\n",
            "Epoch 90/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0327 - mae: 0.1975 - val_loss: 0.1015 - val_mae: 0.3362\n",
            "Epoch 91/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0326 - mae: 0.1966 - val_loss: 0.0868 - val_mae: 0.3105\n",
            "Epoch 92/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0327 - mae: 0.1966 - val_loss: 0.1027 - val_mae: 0.3398\n",
            "Epoch 93/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0325 - mae: 0.1959 - val_loss: 0.0939 - val_mae: 0.3236\n",
            "Epoch 94/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0323 - mae: 0.1956 - val_loss: 0.0477 - val_mae: 0.2313\n",
            "Epoch 95/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0323 - mae: 0.1954 - val_loss: 0.0582 - val_mae: 0.2540\n",
            "Epoch 96/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0322 - mae: 0.1950 - val_loss: 0.0517 - val_mae: 0.2396\n",
            "Epoch 97/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0320 - mae: 0.1945 - val_loss: 0.0988 - val_mae: 0.3309\n",
            "Epoch 98/150\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.0323 - mae: 0.1951 - val_loss: 0.0373 - val_mae: 0.2087\n",
            "Epoch 99/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0322 - mae: 0.1946 - val_loss: 0.0428 - val_mae: 0.2228\n",
            "Epoch 100/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0321 - mae: 0.1943 - val_loss: 0.0409 - val_mae: 0.2186\n",
            "Epoch 101/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0318 - mae: 0.1937 - val_loss: 0.0595 - val_mae: 0.2572\n",
            "Epoch 102/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0319 - mae: 0.1936 - val_loss: 0.0495 - val_mae: 0.2365\n",
            "Epoch 103/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0319 - mae: 0.1938 - val_loss: 0.0374 - val_mae: 0.2115\n",
            "Epoch 104/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0317 - mae: 0.1932 - val_loss: 0.0492 - val_mae: 0.2359\n",
            "Epoch 105/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0316 - mae: 0.1931 - val_loss: 0.0354 - val_mae: 0.2047\n",
            "Epoch 106/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0317 - mae: 0.1929 - val_loss: 0.0460 - val_mae: 0.2277\n",
            "Epoch 107/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0317 - mae: 0.1929 - val_loss: 0.0405 - val_mae: 0.2190\n",
            "Epoch 108/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0315 - mae: 0.1926 - val_loss: 0.0356 - val_mae: 0.2063\n",
            "Epoch 109/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0316 - mae: 0.1928 - val_loss: 0.0381 - val_mae: 0.2136\n",
            "Epoch 110/150\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1922 - val_loss: 0.0327 - val_mae: 0.2009\n",
            "Epoch 111/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0315 - mae: 0.1917 - val_loss: 0.0336 - val_mae: 0.2013\n",
            "Epoch 112/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0314 - mae: 0.1920 - val_loss: 0.0535 - val_mae: 0.2529\n",
            "Epoch 113/150\n",
            "30/30 [==============================] - 0s 17ms/step - loss: 0.0314 - mae: 0.1921 - val_loss: 0.0363 - val_mae: 0.2111\n",
            "Epoch 114/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0313 - mae: 0.1912 - val_loss: 0.0340 - val_mae: 0.2009\n",
            "Epoch 115/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0313 - mae: 0.1912 - val_loss: 0.0367 - val_mae: 0.2126\n",
            "Epoch 116/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0310 - mae: 0.1904 - val_loss: 0.0341 - val_mae: 0.2018\n",
            "Epoch 117/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0313 - mae: 0.1913 - val_loss: 0.0318 - val_mae: 0.1973\n",
            "Epoch 118/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0310 - mae: 0.1900 - val_loss: 0.0329 - val_mae: 0.2042\n",
            "Epoch 119/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0311 - mae: 0.1906 - val_loss: 0.0378 - val_mae: 0.2167\n",
            "Epoch 120/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0314 - mae: 0.1916 - val_loss: 0.0360 - val_mae: 0.2123\n",
            "Epoch 121/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0310 - mae: 0.1905 - val_loss: 0.0318 - val_mae: 0.1992\n",
            "Epoch 122/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0312 - mae: 0.1912 - val_loss: 0.0331 - val_mae: 0.2027\n",
            "Epoch 123/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0312 - mae: 0.1908 - val_loss: 0.0424 - val_mae: 0.2276\n",
            "Epoch 124/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0311 - mae: 0.1903 - val_loss: 0.0310 - val_mae: 0.1939\n",
            "Epoch 125/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0309 - mae: 0.1899 - val_loss: 0.0304 - val_mae: 0.1913\n",
            "Epoch 126/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0309 - mae: 0.1900 - val_loss: 0.0305 - val_mae: 0.1932\n",
            "Epoch 127/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0309 - mae: 0.1897 - val_loss: 0.0387 - val_mae: 0.2126\n",
            "Epoch 128/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0309 - mae: 0.1897 - val_loss: 0.0379 - val_mae: 0.2080\n",
            "Epoch 129/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0310 - mae: 0.1900 - val_loss: 0.0339 - val_mae: 0.1984\n",
            "Epoch 130/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0308 - mae: 0.1894 - val_loss: 0.0487 - val_mae: 0.2361\n",
            "Epoch 131/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0306 - mae: 0.1889 - val_loss: 0.0327 - val_mae: 0.1955\n",
            "Epoch 132/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0309 - mae: 0.1898 - val_loss: 0.0308 - val_mae: 0.1960\n",
            "Epoch 133/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0307 - mae: 0.1891 - val_loss: 0.0644 - val_mae: 0.2699\n",
            "Epoch 134/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0306 - mae: 0.1886 - val_loss: 0.0772 - val_mae: 0.2953\n",
            "Epoch 135/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0306 - mae: 0.1889 - val_loss: 0.0409 - val_mae: 0.2172\n",
            "Epoch 136/150\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0308 - mae: 0.1891 - val_loss: 0.0401 - val_mae: 0.2139\n",
            "Epoch 137/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0307 - mae: 0.1887 - val_loss: 0.0330 - val_mae: 0.2062\n",
            "Epoch 138/150\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.0307 - mae: 0.1887 - val_loss: 0.0368 - val_mae: 0.2197\n",
            "Epoch 139/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0304 - mae: 0.1876 - val_loss: 0.0390 - val_mae: 0.2172\n",
            "Epoch 140/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0306 - mae: 0.1886 - val_loss: 0.0517 - val_mae: 0.2442\n",
            "Epoch 141/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0304 - mae: 0.1880 - val_loss: 0.0572 - val_mae: 0.2535\n",
            "Epoch 142/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0305 - mae: 0.1882 - val_loss: 0.0312 - val_mae: 0.2003\n",
            "Epoch 143/150\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0304 - mae: 0.1880 - val_loss: 0.0325 - val_mae: 0.2016\n",
            "Epoch 144/150\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0306 - mae: 0.1884 - val_loss: 0.0407 - val_mae: 0.2180\n",
            "Epoch 145/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0304 - mae: 0.1879 - val_loss: 0.0355 - val_mae: 0.2034\n",
            "Epoch 146/150\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0304 - mae: 0.1877 - val_loss: 0.0316 - val_mae: 0.1940\n",
            "Epoch 147/150\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0303 - mae: 0.1870 - val_loss: 0.0364 - val_mae: 0.2088\n",
            "Epoch 148/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0304 - mae: 0.1876 - val_loss: 0.0365 - val_mae: 0.2047\n",
            "Epoch 149/150\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0301 - mae: 0.1866 - val_loss: 0.0412 - val_mae: 0.2224\n",
            "Epoch 150/150\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0302 - mae: 0.1872 - val_loss: 0.0419 - val_mae: 0.2244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:23:06,244] Trial 14 finished with value: 0.041910868138074875 and parameters: {'learning_rate': 0.00035702833881481687, 'dropout_rate': 0.23409441653832422, 'batch_size': 128, 'epochs': 150}. Best is trial 9 with value: 0.035705674439668655.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "60/60 [==============================] - 4s 15ms/step - loss: 0.0960 - mae: 0.3282 - val_loss: 1.4635 - val_mae: 2.0422\n",
            "Epoch 2/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0415 - mae: 0.2314 - val_loss: 0.0588 - val_mae: 0.2596\n",
            "Epoch 3/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0401 - mae: 0.2279 - val_loss: 0.0405 - val_mae: 0.2290\n",
            "Epoch 4/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 5/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 6/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0395 - mae: 0.2260 - val_loss: 0.0395 - val_mae: 0.2264\n",
            "Epoch 7/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0389 - mae: 0.2230 - val_loss: 0.0400 - val_mae: 0.2276\n",
            "Epoch 8/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0376 - mae: 0.2181 - val_loss: 0.1081 - val_mae: 0.3594\n",
            "Epoch 9/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0367 - mae: 0.2143 - val_loss: 0.1967 - val_mae: 0.5119\n",
            "Epoch 10/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0363 - mae: 0.2127 - val_loss: 0.0584 - val_mae: 0.2592\n",
            "Epoch 11/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0355 - mae: 0.2091 - val_loss: 0.0419 - val_mae: 0.2242\n",
            "Epoch 12/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0351 - mae: 0.2073 - val_loss: 0.0860 - val_mae: 0.3129\n",
            "Epoch 13/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0346 - mae: 0.2054 - val_loss: 0.0983 - val_mae: 0.3276\n",
            "Epoch 14/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0348 - mae: 0.2061 - val_loss: 0.0459 - val_mae: 0.2299\n",
            "Epoch 15/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0345 - mae: 0.2048 - val_loss: 0.0524 - val_mae: 0.2497\n",
            "Epoch 16/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0346 - mae: 0.2048 - val_loss: 0.1846 - val_mae: 0.5269\n",
            "Epoch 17/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0342 - mae: 0.2037 - val_loss: 0.1125 - val_mae: 0.3864\n",
            "Epoch 18/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0339 - mae: 0.2020 - val_loss: 0.2973 - val_mae: 0.7014\n",
            "Epoch 19/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0337 - mae: 0.2011 - val_loss: 0.1114 - val_mae: 0.3826\n",
            "Epoch 20/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.0336 - mae: 0.2008 - val_loss: 0.0353 - val_mae: 0.2081\n",
            "Epoch 21/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0339 - mae: 0.2023 - val_loss: 0.0460 - val_mae: 0.2338\n",
            "Epoch 22/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0339 - mae: 0.2017 - val_loss: 0.1016 - val_mae: 0.3343\n",
            "Epoch 23/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0338 - mae: 0.2017 - val_loss: 0.1193 - val_mae: 0.3692\n",
            "Epoch 24/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0341 - mae: 0.2024 - val_loss: 0.0613 - val_mae: 0.2594\n",
            "Epoch 25/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0338 - mae: 0.2012 - val_loss: 0.0578 - val_mae: 0.2537\n",
            "Epoch 26/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0338 - mae: 0.2013 - val_loss: 0.2073 - val_mae: 0.5482\n",
            "Epoch 27/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0349 - mae: 0.2059 - val_loss: 0.1131 - val_mae: 0.3805\n",
            "Epoch 28/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0345 - mae: 0.2040 - val_loss: 0.0433 - val_mae: 0.2258\n",
            "Epoch 29/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0346 - mae: 0.2041 - val_loss: 0.0569 - val_mae: 0.2666\n",
            "Epoch 30/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0343 - mae: 0.2033 - val_loss: 0.1377 - val_mae: 0.4348\n",
            "Epoch 31/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0342 - mae: 0.2028 - val_loss: 0.0937 - val_mae: 0.3530\n",
            "Epoch 32/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0343 - mae: 0.2034 - val_loss: 0.0750 - val_mae: 0.2812\n",
            "Epoch 33/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0341 - mae: 0.2023 - val_loss: 0.1028 - val_mae: 0.3674\n",
            "Epoch 34/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0341 - mae: 0.2026 - val_loss: 0.0739 - val_mae: 0.3018\n",
            "Epoch 35/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0349 - mae: 0.2058 - val_loss: 0.0705 - val_mae: 0.2938\n",
            "Epoch 36/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0348 - mae: 0.2052 - val_loss: 0.1513 - val_mae: 0.4641\n",
            "Epoch 37/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0354 - mae: 0.2077 - val_loss: 0.0872 - val_mae: 0.3151\n",
            "Epoch 38/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0356 - mae: 0.2087 - val_loss: 0.0459 - val_mae: 0.2413\n",
            "Epoch 39/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0358 - mae: 0.2096 - val_loss: 0.2265 - val_mae: 0.5931\n",
            "Epoch 40/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0362 - mae: 0.2115 - val_loss: 0.1300 - val_mae: 0.4204\n",
            "Epoch 41/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0369 - mae: 0.2142 - val_loss: 0.0366 - val_mae: 0.2184\n",
            "Epoch 42/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0382 - mae: 0.2199 - val_loss: 0.0419 - val_mae: 0.2419\n",
            "Epoch 43/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0413 - val_mae: 0.2385\n",
            "Epoch 44/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0403 - val_mae: 0.2296\n",
            "Epoch 45/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0409 - val_mae: 0.2225\n",
            "Epoch 46/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0429 - val_mae: 0.2219\n",
            "Epoch 47/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2273\n",
            "Epoch 48/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0404 - val_mae: 0.2283\n",
            "Epoch 49/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0401 - val_mae: 0.2278\n",
            "Epoch 50/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0402 - val_mae: 0.2283\n",
            "Epoch 51/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0400 - val_mae: 0.2302\n",
            "Epoch 52/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0401 - val_mae: 0.2282\n",
            "Epoch 53/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2270\n",
            "Epoch 54/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0400 - val_mae: 0.2268\n",
            "Epoch 55/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0400 - val_mae: 0.2299\n",
            "Epoch 56/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0400 - val_mae: 0.2292\n",
            "Epoch 57/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0400 - val_mae: 0.2288\n",
            "Epoch 58/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 59/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2285\n",
            "Epoch 60/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0398 - mae: 0.2272 - val_loss: 0.0399 - val_mae: 0.2286\n",
            "Epoch 61/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0401 - val_mae: 0.2253\n",
            "Epoch 62/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0400 - val_mae: 0.2293\n",
            "Epoch 63/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0397 - mae: 0.2273 - val_loss: 0.0400 - val_mae: 0.2268\n",
            "Epoch 64/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0400 - val_mae: 0.2288\n",
            "Epoch 65/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0400 - val_mae: 0.2290\n",
            "Epoch 66/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 67/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0401 - val_mae: 0.2287\n",
            "Epoch 68/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 69/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 70/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2288\n",
            "Epoch 71/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2288\n",
            "Epoch 72/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0400 - val_mae: 0.2281\n",
            "Epoch 73/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2292\n",
            "Epoch 74/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 75/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0401 - val_mae: 0.2290\n",
            "Epoch 76/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 77/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0400 - val_mae: 0.2288\n",
            "Epoch 78/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 79/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0400 - val_mae: 0.2302\n",
            "Epoch 80/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 81/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0400 - val_mae: 0.2277\n",
            "Epoch 82/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0402 - val_mae: 0.2295\n",
            "Epoch 83/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0400 - val_mae: 0.2306\n",
            "Epoch 84/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0399 - val_mae: 0.2292\n",
            "Epoch 85/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0401 - val_mae: 0.2292\n",
            "Epoch 86/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2288\n",
            "Epoch 87/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0401 - val_mae: 0.2300\n",
            "Epoch 88/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 89/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 90/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0400 - val_mae: 0.2301\n",
            "Epoch 91/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0400 - val_mae: 0.2288\n",
            "Epoch 92/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 93/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0399 - val_mae: 0.2272\n",
            "Epoch 94/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0400 - val_mae: 0.2292\n",
            "Epoch 95/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0400 - val_mae: 0.2283\n",
            "Epoch 96/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2284\n",
            "Epoch 97/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0400 - val_mae: 0.2294\n",
            "Epoch 98/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 99/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0398 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2285\n",
            "Epoch 100/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 101/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 102/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 103/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0400 - val_mae: 0.2300\n",
            "Epoch 104/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0399 - val_mae: 0.2270\n",
            "Epoch 105/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 106/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 107/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2297\n",
            "Epoch 108/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0400 - val_mae: 0.2311\n",
            "Epoch 109/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0399 - val_mae: 0.2285\n",
            "Epoch 110/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 111/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 112/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0401 - val_mae: 0.2270\n",
            "Epoch 113/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 114/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0398 - mae: 0.2273 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 115/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0400 - val_mae: 0.2272\n",
            "Epoch 116/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2293\n",
            "Epoch 117/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0401 - val_mae: 0.2278\n",
            "Epoch 118/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0400 - val_mae: 0.2266\n",
            "Epoch 119/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2272\n",
            "Epoch 120/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2284\n",
            "Epoch 121/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0400 - val_mae: 0.2286\n",
            "Epoch 122/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0400 - val_mae: 0.2292\n",
            "Epoch 123/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2285\n",
            "Epoch 124/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0400 - val_mae: 0.2272\n",
            "Epoch 125/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2267 - val_loss: 0.0400 - val_mae: 0.2309\n",
            "Epoch 126/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2273 - val_loss: 0.0400 - val_mae: 0.2248\n",
            "Epoch 127/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2288\n",
            "Epoch 128/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0400 - val_mae: 0.2294\n",
            "Epoch 129/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0399 - val_mae: 0.2271\n",
            "Epoch 130/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 131/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 132/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0400 - val_mae: 0.2279\n",
            "Epoch 133/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0400 - val_mae: 0.2279\n",
            "Epoch 134/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2291\n",
            "Epoch 135/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2271\n",
            "Epoch 136/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0401 - val_mae: 0.2279\n",
            "Epoch 137/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0400 - val_mae: 0.2268\n",
            "Epoch 138/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2278\n",
            "Epoch 139/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0400 - val_mae: 0.2274\n",
            "Epoch 140/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0400 - val_mae: 0.2269\n",
            "Epoch 141/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 142/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 143/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0400 - val_mae: 0.2273\n",
            "Epoch 144/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 145/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0400 - val_mae: 0.2264\n",
            "Epoch 146/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 147/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 148/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 149/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 150/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0400 - val_mae: 0.2287\n",
            "Epoch 151/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0400 - val_mae: 0.2273\n",
            "Epoch 152/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 153/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 154/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2291\n",
            "Epoch 155/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0398 - val_mae: 0.2275\n",
            "Epoch 156/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0400 - val_mae: 0.2273\n",
            "Epoch 157/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0400 - val_mae: 0.2260\n",
            "Epoch 158/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0400 - val_mae: 0.2291\n",
            "Epoch 159/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0397 - mae: 0.2273 - val_loss: 0.0399 - val_mae: 0.2262\n",
            "Epoch 160/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0400 - val_mae: 0.2272\n",
            "Epoch 161/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0399 - val_mae: 0.2273\n",
            "Epoch 162/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0398 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2273\n",
            "Epoch 163/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2284\n",
            "Epoch 164/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2295\n",
            "Epoch 165/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2292\n",
            "Epoch 166/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2299\n",
            "Epoch 167/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 168/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2263\n",
            "Epoch 169/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2286\n",
            "Epoch 170/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0399 - val_mae: 0.2270\n",
            "Epoch 171/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 172/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2289\n",
            "Epoch 173/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2271\n",
            "Epoch 174/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0400 - val_mae: 0.2286\n",
            "Epoch 175/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2268\n",
            "Epoch 176/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0402 - val_mae: 0.2287\n",
            "Epoch 177/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 178/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2263\n",
            "Epoch 179/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 180/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 181/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2286\n",
            "Epoch 182/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2285\n",
            "Epoch 183/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 184/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 185/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 186/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0400 - val_mae: 0.2250\n",
            "Epoch 187/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 188/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 189/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2303\n",
            "Epoch 190/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2285\n",
            "Epoch 191/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2289\n",
            "Epoch 192/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 193/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2291\n",
            "Epoch 194/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2271\n",
            "Epoch 195/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0400 - val_mae: 0.2285\n",
            "Epoch 196/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 197/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2263\n",
            "Epoch 198/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0401 - val_mae: 0.2275\n",
            "Epoch 199/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0401 - val_mae: 0.2266\n",
            "Epoch 200/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:25:31,296] Trial 15 finished with value: 0.03988869860768318 and parameters: {'learning_rate': 0.013073258905112456, 'dropout_rate': 0.47890844455020587, 'batch_size': 64, 'epochs': 200}. Best is trial 9 with value: 0.035705674439668655.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 3s 8ms/step - loss: 0.4489 - mae: 0.8878 - val_loss: 0.5969 - val_mae: 1.0922\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4288 - mae: 0.8623 - val_loss: 0.3178 - val_mae: 0.7264\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4109 - mae: 0.8399 - val_loss: 0.2217 - val_mae: 0.5767\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3935 - mae: 0.8170 - val_loss: 0.2384 - val_mae: 0.6032\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3734 - mae: 0.7895 - val_loss: 0.2111 - val_mae: 0.5561\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3585 - mae: 0.7698 - val_loss: 0.1853 - val_mae: 0.5167\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3425 - mae: 0.7479 - val_loss: 0.1858 - val_mae: 0.5238\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 0.3261 - mae: 0.7254 - val_loss: 0.1953 - val_mae: 0.5336\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3150 - mae: 0.7104 - val_loss: 0.1877 - val_mae: 0.5221\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.3025 - mae: 0.6929 - val_loss: 0.1560 - val_mae: 0.4731\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2908 - mae: 0.6760 - val_loss: 0.1540 - val_mae: 0.4729\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2773 - mae: 0.6563 - val_loss: 0.1465 - val_mae: 0.4591\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2644 - mae: 0.6372 - val_loss: 0.1346 - val_mae: 0.4390\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2539 - mae: 0.6215 - val_loss: 0.1287 - val_mae: 0.4301\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2430 - mae: 0.6064 - val_loss: 0.1228 - val_mae: 0.4134\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2356 - mae: 0.5941 - val_loss: 0.1182 - val_mae: 0.4102\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2250 - mae: 0.5784 - val_loss: 0.1111 - val_mae: 0.3950\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2164 - mae: 0.5652 - val_loss: 0.1117 - val_mae: 0.3964\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2075 - mae: 0.5515 - val_loss: 0.1096 - val_mae: 0.3944\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1976 - mae: 0.5348 - val_loss: 0.1047 - val_mae: 0.3842\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1912 - mae: 0.5248 - val_loss: 0.0985 - val_mae: 0.3734\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1840 - mae: 0.5127 - val_loss: 0.0969 - val_mae: 0.3656\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1755 - mae: 0.4996 - val_loss: 0.1005 - val_mae: 0.3718\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1691 - mae: 0.4889 - val_loss: 0.0926 - val_mae: 0.3568\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1630 - mae: 0.4782 - val_loss: 0.0911 - val_mae: 0.3531\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1595 - mae: 0.4714 - val_loss: 0.0883 - val_mae: 0.3480\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1536 - mae: 0.4612 - val_loss: 0.0837 - val_mae: 0.3368\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1480 - mae: 0.4511 - val_loss: 0.0803 - val_mae: 0.3289\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1423 - mae: 0.4420 - val_loss: 0.0778 - val_mae: 0.3250\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1374 - mae: 0.4324 - val_loss: 0.0768 - val_mae: 0.3214\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1336 - mae: 0.4250 - val_loss: 0.0737 - val_mae: 0.3159\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1283 - mae: 0.4164 - val_loss: 0.0729 - val_mae: 0.3119\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1256 - mae: 0.4104 - val_loss: 0.0709 - val_mae: 0.3077\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1217 - mae: 0.4031 - val_loss: 0.0707 - val_mae: 0.3072\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1181 - mae: 0.3958 - val_loss: 0.0685 - val_mae: 0.3011\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1148 - mae: 0.3902 - val_loss: 0.0663 - val_mae: 0.2949\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1113 - mae: 0.3827 - val_loss: 0.0653 - val_mae: 0.2929\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1082 - mae: 0.3763 - val_loss: 0.0637 - val_mae: 0.2890\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1057 - mae: 0.3713 - val_loss: 0.0616 - val_mae: 0.2839\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1026 - mae: 0.3658 - val_loss: 0.0602 - val_mae: 0.2804\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0999 - mae: 0.3600 - val_loss: 0.0587 - val_mae: 0.2771\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0993 - mae: 0.3579 - val_loss: 0.0575 - val_mae: 0.2732\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0950 - mae: 0.3499 - val_loss: 0.0564 - val_mae: 0.2708\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0920 - mae: 0.3442 - val_loss: 0.0542 - val_mae: 0.2649\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0897 - mae: 0.3389 - val_loss: 0.0532 - val_mae: 0.2633\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0876 - mae: 0.3343 - val_loss: 0.0525 - val_mae: 0.2616\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0865 - mae: 0.3317 - val_loss: 0.0518 - val_mae: 0.2603\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0840 - mae: 0.3266 - val_loss: 0.0508 - val_mae: 0.2578\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0828 - mae: 0.3239 - val_loss: 0.0503 - val_mae: 0.2564\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0800 - mae: 0.3182 - val_loss: 0.0492 - val_mae: 0.2537\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0770 - mae: 0.3114 - val_loss: 0.0484 - val_mae: 0.2523\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0761 - mae: 0.3100 - val_loss: 0.0470 - val_mae: 0.2479\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0753 - mae: 0.3075 - val_loss: 0.0465 - val_mae: 0.2465\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0740 - mae: 0.3051 - val_loss: 0.0459 - val_mae: 0.2453\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0726 - mae: 0.3013 - val_loss: 0.0449 - val_mae: 0.2425\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0705 - mae: 0.2974 - val_loss: 0.0445 - val_mae: 0.2421\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0699 - mae: 0.2949 - val_loss: 0.0437 - val_mae: 0.2397\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0684 - mae: 0.2919 - val_loss: 0.0430 - val_mae: 0.2370\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0658 - mae: 0.2869 - val_loss: 0.0418 - val_mae: 0.2334\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0662 - mae: 0.2866 - val_loss: 0.0418 - val_mae: 0.2332\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0650 - mae: 0.2838 - val_loss: 0.0415 - val_mae: 0.2332\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0624 - mae: 0.2789 - val_loss: 0.0409 - val_mae: 0.2316\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0627 - mae: 0.2790 - val_loss: 0.0409 - val_mae: 0.2318\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0622 - mae: 0.2767 - val_loss: 0.0404 - val_mae: 0.2294\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0597 - mae: 0.2719 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0591 - mae: 0.2698 - val_loss: 0.0386 - val_mae: 0.2230\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0580 - mae: 0.2672 - val_loss: 0.0385 - val_mae: 0.2232\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0583 - mae: 0.2665 - val_loss: 0.0386 - val_mae: 0.2235\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0569 - mae: 0.2635 - val_loss: 0.0377 - val_mae: 0.2201\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0554 - mae: 0.2605 - val_loss: 0.0374 - val_mae: 0.2194\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0551 - mae: 0.2589 - val_loss: 0.0378 - val_mae: 0.2205\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0543 - mae: 0.2576 - val_loss: 0.0373 - val_mae: 0.2188\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0538 - mae: 0.2560 - val_loss: 0.0367 - val_mae: 0.2168\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0526 - mae: 0.2529 - val_loss: 0.0361 - val_mae: 0.2146\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0523 - mae: 0.2517 - val_loss: 0.0364 - val_mae: 0.2161\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0530 - mae: 0.2528 - val_loss: 0.0356 - val_mae: 0.2128\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0520 - mae: 0.2503 - val_loss: 0.0365 - val_mae: 0.2151\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0514 - mae: 0.2483 - val_loss: 0.0355 - val_mae: 0.2132\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0508 - mae: 0.2470 - val_loss: 0.0345 - val_mae: 0.2090\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0506 - mae: 0.2460 - val_loss: 0.0355 - val_mae: 0.2103\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0496 - mae: 0.2439 - val_loss: 0.0355 - val_mae: 0.2097\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0490 - mae: 0.2426 - val_loss: 0.0340 - val_mae: 0.2065\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0484 - mae: 0.2412 - val_loss: 0.0352 - val_mae: 0.2096\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0480 - mae: 0.2396 - val_loss: 0.0338 - val_mae: 0.2059\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0478 - mae: 0.2395 - val_loss: 0.0345 - val_mae: 0.2076\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0473 - mae: 0.2372 - val_loss: 0.0387 - val_mae: 0.2177\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0467 - mae: 0.2359 - val_loss: 0.0333 - val_mae: 0.2034\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0467 - mae: 0.2359 - val_loss: 0.0329 - val_mae: 0.2019\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0456 - mae: 0.2334 - val_loss: 0.0343 - val_mae: 0.2054\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0459 - mae: 0.2331 - val_loss: 0.0363 - val_mae: 0.2105\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0461 - mae: 0.2337 - val_loss: 0.0333 - val_mae: 0.2022\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0452 - mae: 0.2322 - val_loss: 0.0345 - val_mae: 0.2064\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0451 - mae: 0.2317 - val_loss: 0.0336 - val_mae: 0.2040\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0448 - mae: 0.2303 - val_loss: 0.0379 - val_mae: 0.2147\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0442 - mae: 0.2288 - val_loss: 0.0350 - val_mae: 0.2067\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0444 - mae: 0.2292 - val_loss: 0.0325 - val_mae: 0.2006\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0445 - mae: 0.2291 - val_loss: 0.0324 - val_mae: 0.2000\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0439 - mae: 0.2279 - val_loss: 0.0337 - val_mae: 0.2035\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0435 - mae: 0.2265 - val_loss: 0.0331 - val_mae: 0.2016\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0430 - mae: 0.2258 - val_loss: 0.0330 - val_mae: 0.2018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:27:56,134] Trial 16 finished with value: 0.03297249600291252 and parameters: {'learning_rate': 3.7483984438544415e-05, 'dropout_rate': 0.20758664540589944, 'batch_size': 32, 'epochs': 100}. Best is trial 16 with value: 0.03297249600291252.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 3s 9ms/step - loss: 0.4310 - mae: 0.8639 - val_loss: 0.6142 - val_mae: 1.1137\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4033 - mae: 0.8281 - val_loss: 0.2179 - val_mae: 0.5813\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3816 - mae: 0.7998 - val_loss: 0.2362 - val_mae: 0.6106\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3581 - mae: 0.7684 - val_loss: 0.2006 - val_mae: 0.5558\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.3371 - mae: 0.7392 - val_loss: 0.1946 - val_mae: 0.5427\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.3149 - mae: 0.7092 - val_loss: 0.1917 - val_mae: 0.5352\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2967 - mae: 0.6846 - val_loss: 0.1807 - val_mae: 0.5184\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.2813 - mae: 0.6611 - val_loss: 0.1704 - val_mae: 0.4976\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.2638 - mae: 0.6366 - val_loss: 0.1608 - val_mae: 0.4838\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 0.2496 - mae: 0.6159 - val_loss: 0.1491 - val_mae: 0.4656\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.2381 - mae: 0.5990 - val_loss: 0.1385 - val_mae: 0.4472\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2252 - mae: 0.5780 - val_loss: 0.1355 - val_mae: 0.4389\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2160 - mae: 0.5641 - val_loss: 0.1221 - val_mae: 0.4090\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2053 - mae: 0.5480 - val_loss: 0.1193 - val_mae: 0.4028\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1963 - mae: 0.5345 - val_loss: 0.1178 - val_mae: 0.3978\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1867 - mae: 0.5181 - val_loss: 0.1108 - val_mae: 0.3876\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1780 - mae: 0.5045 - val_loss: 0.1045 - val_mae: 0.3800\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1699 - mae: 0.4921 - val_loss: 0.1016 - val_mae: 0.3748\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1652 - mae: 0.4826 - val_loss: 0.0983 - val_mae: 0.3613\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1570 - mae: 0.4698 - val_loss: 0.0920 - val_mae: 0.3549\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1504 - mae: 0.4591 - val_loss: 0.0916 - val_mae: 0.3568\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1454 - mae: 0.4499 - val_loss: 0.0861 - val_mae: 0.3351\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1384 - mae: 0.4385 - val_loss: 0.0868 - val_mae: 0.3348\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1350 - mae: 0.4316 - val_loss: 0.0827 - val_mae: 0.3319\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1295 - mae: 0.4217 - val_loss: 0.0792 - val_mae: 0.3209\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1254 - mae: 0.4147 - val_loss: 0.0768 - val_mae: 0.3183\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1213 - mae: 0.4072 - val_loss: 0.0740 - val_mae: 0.3147\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1168 - mae: 0.3985 - val_loss: 0.0700 - val_mae: 0.3020\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1128 - mae: 0.3903 - val_loss: 0.0696 - val_mae: 0.3015\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1082 - mae: 0.3832 - val_loss: 0.0657 - val_mae: 0.2946\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1062 - mae: 0.3772 - val_loss: 0.0654 - val_mae: 0.2943\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1031 - mae: 0.3712 - val_loss: 0.0633 - val_mae: 0.2877\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0986 - mae: 0.3623 - val_loss: 0.0609 - val_mae: 0.2830\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0958 - mae: 0.3571 - val_loss: 0.0599 - val_mae: 0.2803\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0942 - mae: 0.3533 - val_loss: 0.0580 - val_mae: 0.2735\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0919 - mae: 0.3482 - val_loss: 0.0569 - val_mae: 0.2709\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0896 - mae: 0.3437 - val_loss: 0.0539 - val_mae: 0.2654\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0860 - mae: 0.3359 - val_loss: 0.0528 - val_mae: 0.2637\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0844 - mae: 0.3328 - val_loss: 0.0514 - val_mae: 0.2599\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0813 - mae: 0.3262 - val_loss: 0.0504 - val_mae: 0.2570\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0800 - mae: 0.3227 - val_loss: 0.0494 - val_mae: 0.2538\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0775 - mae: 0.3181 - val_loss: 0.0485 - val_mae: 0.2524\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0761 - mae: 0.3143 - val_loss: 0.0474 - val_mae: 0.2487\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0744 - mae: 0.3104 - val_loss: 0.0461 - val_mae: 0.2465\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0727 - mae: 0.3065 - val_loss: 0.0458 - val_mae: 0.2447\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0713 - mae: 0.3034 - val_loss: 0.0447 - val_mae: 0.2420\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0699 - mae: 0.2999 - val_loss: 0.0441 - val_mae: 0.2403\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0678 - mae: 0.2959 - val_loss: 0.0436 - val_mae: 0.2391\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0671 - mae: 0.2932 - val_loss: 0.0429 - val_mae: 0.2378\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0664 - mae: 0.2915 - val_loss: 0.0423 - val_mae: 0.2359\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0645 - mae: 0.2874 - val_loss: 0.0419 - val_mae: 0.2343\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0633 - mae: 0.2847 - val_loss: 0.0414 - val_mae: 0.2331\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0626 - mae: 0.2826 - val_loss: 0.0411 - val_mae: 0.2327\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0622 - mae: 0.2811 - val_loss: 0.0407 - val_mae: 0.2314\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0611 - mae: 0.2787 - val_loss: 0.0406 - val_mae: 0.2313\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0598 - mae: 0.2754 - val_loss: 0.0401 - val_mae: 0.2300\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0588 - mae: 0.2729 - val_loss: 0.0397 - val_mae: 0.2289\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0590 - mae: 0.2730 - val_loss: 0.0395 - val_mae: 0.2281\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0572 - mae: 0.2693 - val_loss: 0.0396 - val_mae: 0.2278\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0567 - mae: 0.2680 - val_loss: 0.0394 - val_mae: 0.2270\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0558 - mae: 0.2653 - val_loss: 0.0394 - val_mae: 0.2266\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0561 - mae: 0.2660 - val_loss: 0.0392 - val_mae: 0.2266\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0550 - mae: 0.2633 - val_loss: 0.0391 - val_mae: 0.2261\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0547 - mae: 0.2624 - val_loss: 0.0389 - val_mae: 0.2250\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0539 - mae: 0.2605 - val_loss: 0.0387 - val_mae: 0.2246\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0525 - mae: 0.2576 - val_loss: 0.0386 - val_mae: 0.2239\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0530 - mae: 0.2580 - val_loss: 0.0387 - val_mae: 0.2236\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0521 - mae: 0.2555 - val_loss: 0.0384 - val_mae: 0.2225\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0512 - mae: 0.2538 - val_loss: 0.0386 - val_mae: 0.2229\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0510 - mae: 0.2524 - val_loss: 0.0382 - val_mae: 0.2223\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0504 - mae: 0.2512 - val_loss: 0.0379 - val_mae: 0.2211\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0494 - mae: 0.2488 - val_loss: 0.0379 - val_mae: 0.2218\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0493 - mae: 0.2482 - val_loss: 0.0376 - val_mae: 0.2201\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0489 - mae: 0.2472 - val_loss: 0.0375 - val_mae: 0.2193\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0485 - mae: 0.2462 - val_loss: 0.0384 - val_mae: 0.2221\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0480 - mae: 0.2451 - val_loss: 0.0371 - val_mae: 0.2188\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0476 - mae: 0.2438 - val_loss: 0.0371 - val_mae: 0.2188\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0476 - mae: 0.2430 - val_loss: 0.0370 - val_mae: 0.2173\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0468 - mae: 0.2412 - val_loss: 0.0367 - val_mae: 0.2164\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0464 - mae: 0.2398 - val_loss: 0.0361 - val_mae: 0.2148\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0459 - mae: 0.2388 - val_loss: 0.0382 - val_mae: 0.2200\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0453 - mae: 0.2369 - val_loss: 0.0361 - val_mae: 0.2146\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0446 - mae: 0.2353 - val_loss: 0.0382 - val_mae: 0.2201\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0444 - mae: 0.2340 - val_loss: 0.0392 - val_mae: 0.2217\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0444 - mae: 0.2341 - val_loss: 0.0365 - val_mae: 0.2145\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0437 - mae: 0.2319 - val_loss: 0.0374 - val_mae: 0.2160\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0435 - mae: 0.2312 - val_loss: 0.0388 - val_mae: 0.2211\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0436 - mae: 0.2315 - val_loss: 0.0369 - val_mae: 0.2147\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0434 - mae: 0.2308 - val_loss: 0.0390 - val_mae: 0.2201\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0429 - mae: 0.2299 - val_loss: 0.0382 - val_mae: 0.2170\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0424 - mae: 0.2280 - val_loss: 0.0347 - val_mae: 0.2086\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0417 - mae: 0.2260 - val_loss: 0.0373 - val_mae: 0.2152\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0419 - mae: 0.2266 - val_loss: 0.0351 - val_mae: 0.2105\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0418 - mae: 0.2260 - val_loss: 0.0352 - val_mae: 0.2081\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0416 - mae: 0.2253 - val_loss: 0.0352 - val_mae: 0.2099\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0415 - mae: 0.2249 - val_loss: 0.0342 - val_mae: 0.2071\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0404 - mae: 0.2220 - val_loss: 0.0369 - val_mae: 0.2152\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0406 - mae: 0.2220 - val_loss: 0.0395 - val_mae: 0.2214\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0406 - mae: 0.2219 - val_loss: 0.0358 - val_mae: 0.2122\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0404 - mae: 0.2214 - val_loss: 0.0383 - val_mae: 0.2177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:30:20,815] Trial 17 finished with value: 0.03830673173069954 and parameters: {'learning_rate': 4.26021107264253e-05, 'dropout_rate': 0.18093048747323115, 'batch_size': 32, 'epochs': 100}. Best is trial 16 with value: 0.03297249600291252.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 4s 9ms/step - loss: 0.4760 - mae: 0.9213 - val_loss: 0.2155 - val_mae: 0.5608\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4756 - mae: 0.9199 - val_loss: 0.1854 - val_mae: 0.5324\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4758 - mae: 0.9208 - val_loss: 0.1948 - val_mae: 0.5434\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4766 - mae: 0.9224 - val_loss: 0.2106 - val_mae: 0.5671\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4757 - mae: 0.9201 - val_loss: 0.2183 - val_mae: 0.5796\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4749 - mae: 0.9191 - val_loss: 0.2208 - val_mae: 0.5837\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4790 - mae: 0.9256 - val_loss: 0.2224 - val_mae: 0.5857\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4709 - mae: 0.9143 - val_loss: 0.2209 - val_mae: 0.5830\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4737 - mae: 0.9178 - val_loss: 0.2200 - val_mae: 0.5818\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4688 - mae: 0.9128 - val_loss: 0.2191 - val_mae: 0.5793\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4727 - mae: 0.9164 - val_loss: 0.2178 - val_mae: 0.5774\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4685 - mae: 0.9120 - val_loss: 0.2170 - val_mae: 0.5763\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4695 - mae: 0.9121 - val_loss: 0.2168 - val_mae: 0.5766\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4662 - mae: 0.9083 - val_loss: 0.2153 - val_mae: 0.5744\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4685 - mae: 0.9124 - val_loss: 0.2194 - val_mae: 0.5809\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4702 - mae: 0.9137 - val_loss: 0.2179 - val_mae: 0.5784\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4731 - mae: 0.9179 - val_loss: 0.2185 - val_mae: 0.5789\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4673 - mae: 0.9096 - val_loss: 0.2162 - val_mae: 0.5751\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4690 - mae: 0.9120 - val_loss: 0.2155 - val_mae: 0.5741\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4605 - mae: 0.9013 - val_loss: 0.2153 - val_mae: 0.5738\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4665 - mae: 0.9091 - val_loss: 0.2168 - val_mae: 0.5766\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4674 - mae: 0.9107 - val_loss: 0.2131 - val_mae: 0.5711\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4634 - mae: 0.9055 - val_loss: 0.2130 - val_mae: 0.5703\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4663 - mae: 0.9084 - val_loss: 0.2124 - val_mae: 0.5693\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4657 - mae: 0.9079 - val_loss: 0.2114 - val_mae: 0.5681\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4600 - mae: 0.9004 - val_loss: 0.2116 - val_mae: 0.5689\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4643 - mae: 0.9066 - val_loss: 0.2133 - val_mae: 0.5707\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4657 - mae: 0.9082 - val_loss: 0.2106 - val_mae: 0.5663\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4587 - mae: 0.8994 - val_loss: 0.2104 - val_mae: 0.5654\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4641 - mae: 0.9058 - val_loss: 0.2109 - val_mae: 0.5664\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4580 - mae: 0.8983 - val_loss: 0.2108 - val_mae: 0.5669\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4602 - mae: 0.9002 - val_loss: 0.2081 - val_mae: 0.5624\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4580 - mae: 0.8981 - val_loss: 0.2103 - val_mae: 0.5671\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4602 - mae: 0.9002 - val_loss: 0.2108 - val_mae: 0.5674\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4559 - mae: 0.8958 - val_loss: 0.2094 - val_mae: 0.5652\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4573 - mae: 0.8975 - val_loss: 0.2113 - val_mae: 0.5679\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4572 - mae: 0.8974 - val_loss: 0.2074 - val_mae: 0.5621\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4593 - mae: 0.8994 - val_loss: 0.2107 - val_mae: 0.5673\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4552 - mae: 0.8943 - val_loss: 0.2077 - val_mae: 0.5630\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4546 - mae: 0.8945 - val_loss: 0.2060 - val_mae: 0.5594\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4558 - mae: 0.8954 - val_loss: 0.2052 - val_mae: 0.5582\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4527 - mae: 0.8912 - val_loss: 0.2045 - val_mae: 0.5570\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4560 - mae: 0.8956 - val_loss: 0.2070 - val_mae: 0.5611\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4540 - mae: 0.8923 - val_loss: 0.2057 - val_mae: 0.5592\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4515 - mae: 0.8908 - val_loss: 0.2070 - val_mae: 0.5614\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4495 - mae: 0.8869 - val_loss: 0.2067 - val_mae: 0.5609\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4562 - mae: 0.8958 - val_loss: 0.2087 - val_mae: 0.5647\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.4510 - mae: 0.8900 - val_loss: 0.2066 - val_mae: 0.5603\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4488 - mae: 0.8862 - val_loss: 0.2060 - val_mae: 0.5600\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4505 - mae: 0.8883 - val_loss: 0.2066 - val_mae: 0.5609\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4523 - mae: 0.8911 - val_loss: 0.2036 - val_mae: 0.5557\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4496 - mae: 0.8881 - val_loss: 0.2039 - val_mae: 0.5570\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4496 - mae: 0.8882 - val_loss: 0.2037 - val_mae: 0.5561\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4473 - mae: 0.8848 - val_loss: 0.2068 - val_mae: 0.5609\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4413 - mae: 0.8766 - val_loss: 0.2024 - val_mae: 0.5538\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4467 - mae: 0.8833 - val_loss: 0.2026 - val_mae: 0.5549\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4418 - mae: 0.8773 - val_loss: 0.2038 - val_mae: 0.5567\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4466 - mae: 0.8835 - val_loss: 0.2000 - val_mae: 0.5506\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4450 - mae: 0.8822 - val_loss: 0.2024 - val_mae: 0.5542\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4505 - mae: 0.8885 - val_loss: 0.2008 - val_mae: 0.5517\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4474 - mae: 0.8838 - val_loss: 0.2004 - val_mae: 0.5510\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4439 - mae: 0.8801 - val_loss: 0.1997 - val_mae: 0.5499\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4435 - mae: 0.8802 - val_loss: 0.2003 - val_mae: 0.5510\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4447 - mae: 0.8814 - val_loss: 0.1993 - val_mae: 0.5487\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4400 - mae: 0.8754 - val_loss: 0.2016 - val_mae: 0.5522\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4395 - mae: 0.8753 - val_loss: 0.2000 - val_mae: 0.5507\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4423 - mae: 0.8782 - val_loss: 0.1966 - val_mae: 0.5453\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4417 - mae: 0.8773 - val_loss: 0.1952 - val_mae: 0.5427\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4395 - mae: 0.8751 - val_loss: 0.1959 - val_mae: 0.5435\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4406 - mae: 0.8764 - val_loss: 0.1969 - val_mae: 0.5445\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4385 - mae: 0.8734 - val_loss: 0.1946 - val_mae: 0.5405\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4366 - mae: 0.8714 - val_loss: 0.1968 - val_mae: 0.5440\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4361 - mae: 0.8698 - val_loss: 0.1965 - val_mae: 0.5437\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4356 - mae: 0.8703 - val_loss: 0.1971 - val_mae: 0.5451\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4354 - mae: 0.8698 - val_loss: 0.1951 - val_mae: 0.5423\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4358 - mae: 0.8699 - val_loss: 0.1950 - val_mae: 0.5427\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4340 - mae: 0.8673 - val_loss: 0.1951 - val_mae: 0.5419\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4345 - mae: 0.8671 - val_loss: 0.1943 - val_mae: 0.5401\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4350 - mae: 0.8682 - val_loss: 0.1920 - val_mae: 0.5370\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4330 - mae: 0.8656 - val_loss: 0.1919 - val_mae: 0.5367\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4341 - mae: 0.8680 - val_loss: 0.1942 - val_mae: 0.5407\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4324 - mae: 0.8659 - val_loss: 0.1942 - val_mae: 0.5405\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4350 - mae: 0.8690 - val_loss: 0.1946 - val_mae: 0.5419\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4303 - mae: 0.8635 - val_loss: 0.1968 - val_mae: 0.5451\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4298 - mae: 0.8622 - val_loss: 0.1960 - val_mae: 0.5429\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4328 - mae: 0.8660 - val_loss: 0.1937 - val_mae: 0.5402\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4307 - mae: 0.8631 - val_loss: 0.1958 - val_mae: 0.5434\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4282 - mae: 0.8595 - val_loss: 0.1927 - val_mae: 0.5385\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4279 - mae: 0.8588 - val_loss: 0.1913 - val_mae: 0.5361\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4330 - mae: 0.8668 - val_loss: 0.1962 - val_mae: 0.5438\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4301 - mae: 0.8632 - val_loss: 0.1953 - val_mae: 0.5421\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4297 - mae: 0.8616 - val_loss: 0.1924 - val_mae: 0.5376\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4293 - mae: 0.8622 - val_loss: 0.1934 - val_mae: 0.5398\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4290 - mae: 0.8615 - val_loss: 0.1937 - val_mae: 0.5397\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4294 - mae: 0.8610 - val_loss: 0.1939 - val_mae: 0.5403\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4249 - mae: 0.8554 - val_loss: 0.1934 - val_mae: 0.5393\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4264 - mae: 0.8573 - val_loss: 0.1926 - val_mae: 0.5376\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4260 - mae: 0.8565 - val_loss: 0.1910 - val_mae: 0.5350\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4229 - mae: 0.8532 - val_loss: 0.1926 - val_mae: 0.5382\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4216 - mae: 0.8510 - val_loss: 0.1891 - val_mae: 0.5330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:32:46,045] Trial 18 finished with value: 0.1891009360551834 and parameters: {'learning_rate': 1.15975732163595e-06, 'dropout_rate': 0.26560906433560494, 'batch_size': 32, 'epochs': 100}. Best is trial 16 with value: 0.03297249600291252.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 5s 12ms/step - loss: 0.3206 - mae: 0.7161 - val_loss: 0.4368 - val_mae: 0.8911\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1631 - mae: 0.4804 - val_loss: 0.1372 - val_mae: 0.4445\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1038 - mae: 0.3696 - val_loss: 0.0729 - val_mae: 0.3072\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0746 - mae: 0.3051 - val_loss: 0.0504 - val_mae: 0.2642\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0630 - mae: 0.2773 - val_loss: 0.0433 - val_mae: 0.2344\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0571 - mae: 0.2637 - val_loss: 0.0460 - val_mae: 0.2391\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0521 - mae: 0.2514 - val_loss: 0.0575 - val_mae: 0.2710\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0488 - mae: 0.2420 - val_loss: 0.0407 - val_mae: 0.2260\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0445 - mae: 0.2304 - val_loss: 0.0415 - val_mae: 0.2329\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0419 - mae: 0.2224 - val_loss: 0.0568 - val_mae: 0.2763\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0400 - mae: 0.2171 - val_loss: 0.0486 - val_mae: 0.2392\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0381 - mae: 0.2114 - val_loss: 0.0385 - val_mae: 0.2166\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0368 - mae: 0.2070 - val_loss: 0.0340 - val_mae: 0.2101\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0360 - mae: 0.2046 - val_loss: 0.0912 - val_mae: 0.3515\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0348 - mae: 0.2014 - val_loss: 0.0349 - val_mae: 0.2151\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0337 - mae: 0.1976 - val_loss: 0.0657 - val_mae: 0.2881\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0332 - mae: 0.1955 - val_loss: 0.0419 - val_mae: 0.2263\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0326 - mae: 0.1940 - val_loss: 0.0335 - val_mae: 0.2112\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0322 - mae: 0.1923 - val_loss: 0.0526 - val_mae: 0.2514\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0318 - mae: 0.1912 - val_loss: 0.0358 - val_mae: 0.2052\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0315 - mae: 0.1899 - val_loss: 0.0369 - val_mae: 0.2133\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0312 - mae: 0.1894 - val_loss: 0.0431 - val_mae: 0.2177\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0310 - mae: 0.1882 - val_loss: 0.0429 - val_mae: 0.2382\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0308 - mae: 0.1874 - val_loss: 0.0337 - val_mae: 0.2073\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0306 - mae: 0.1871 - val_loss: 0.0605 - val_mae: 0.2686\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0305 - mae: 0.1865 - val_loss: 0.0377 - val_mae: 0.2076\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0302 - mae: 0.1851 - val_loss: 0.0331 - val_mae: 0.1960\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0300 - mae: 0.1847 - val_loss: 0.0415 - val_mae: 0.2310\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0300 - mae: 0.1847 - val_loss: 0.0512 - val_mae: 0.2574\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0297 - mae: 0.1836 - val_loss: 0.0354 - val_mae: 0.2167\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0298 - mae: 0.1836 - val_loss: 0.0350 - val_mae: 0.2045\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0297 - mae: 0.1836 - val_loss: 0.0373 - val_mae: 0.2170\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0298 - mae: 0.1835 - val_loss: 0.0342 - val_mae: 0.2035\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0294 - mae: 0.1823 - val_loss: 0.0396 - val_mae: 0.2161\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0292 - mae: 0.1818 - val_loss: 0.0409 - val_mae: 0.2176\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0292 - mae: 0.1812 - val_loss: 0.0459 - val_mae: 0.2293\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0292 - mae: 0.1813 - val_loss: 0.0420 - val_mae: 0.2172\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0291 - mae: 0.1808 - val_loss: 0.0530 - val_mae: 0.2408\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0291 - mae: 0.1811 - val_loss: 0.0951 - val_mae: 0.3696\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0290 - mae: 0.1804 - val_loss: 0.0374 - val_mae: 0.2235\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0289 - mae: 0.1803 - val_loss: 0.0326 - val_mae: 0.1978\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0287 - mae: 0.1788 - val_loss: 0.0442 - val_mae: 0.2231\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0287 - mae: 0.1788 - val_loss: 0.0344 - val_mae: 0.1972\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0286 - mae: 0.1787 - val_loss: 0.0347 - val_mae: 0.1992\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0286 - mae: 0.1789 - val_loss: 0.0328 - val_mae: 0.2017\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0285 - mae: 0.1782 - val_loss: 0.0354 - val_mae: 0.2168\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0285 - mae: 0.1778 - val_loss: 0.0431 - val_mae: 0.2179\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0284 - mae: 0.1780 - val_loss: 0.0767 - val_mae: 0.2887\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0285 - mae: 0.1783 - val_loss: 0.0663 - val_mae: 0.2811\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0286 - mae: 0.1782 - val_loss: 0.0334 - val_mae: 0.1999\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0284 - mae: 0.1779 - val_loss: 0.0338 - val_mae: 0.2020\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0284 - mae: 0.1777 - val_loss: 0.0360 - val_mae: 0.1993\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0284 - mae: 0.1776 - val_loss: 0.0392 - val_mae: 0.2099\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0284 - mae: 0.1778 - val_loss: 0.0421 - val_mae: 0.2123\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0283 - mae: 0.1771 - val_loss: 0.0368 - val_mae: 0.2031\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0284 - mae: 0.1778 - val_loss: 0.0496 - val_mae: 0.2380\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0283 - mae: 0.1772 - val_loss: 0.0338 - val_mae: 0.2105\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0282 - mae: 0.1769 - val_loss: 0.0335 - val_mae: 0.1960\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0281 - mae: 0.1765 - val_loss: 0.0387 - val_mae: 0.2116\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0284 - mae: 0.1773 - val_loss: 0.0532 - val_mae: 0.2410\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0283 - mae: 0.1773 - val_loss: 0.0351 - val_mae: 0.1927\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0281 - mae: 0.1765 - val_loss: 0.0422 - val_mae: 0.2402\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0283 - mae: 0.1776 - val_loss: 0.0360 - val_mae: 0.2014\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0281 - mae: 0.1764 - val_loss: 0.0340 - val_mae: 0.2026\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0283 - mae: 0.1771 - val_loss: 0.0466 - val_mae: 0.2402\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0285 - mae: 0.1778 - val_loss: 0.0369 - val_mae: 0.2127\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0283 - mae: 0.1770 - val_loss: 0.0348 - val_mae: 0.2055\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0282 - mae: 0.1766 - val_loss: 0.0535 - val_mae: 0.2536\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0282 - mae: 0.1771 - val_loss: 0.0434 - val_mae: 0.2291\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0282 - mae: 0.1768 - val_loss: 0.0317 - val_mae: 0.1941\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0284 - mae: 0.1773 - val_loss: 0.0350 - val_mae: 0.2006\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0282 - mae: 0.1765 - val_loss: 0.0465 - val_mae: 0.2403\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0284 - mae: 0.1772 - val_loss: 0.0463 - val_mae: 0.2353\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0281 - mae: 0.1760 - val_loss: 0.0332 - val_mae: 0.1968\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0285 - mae: 0.1782 - val_loss: 0.0568 - val_mae: 0.2779\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0285 - mae: 0.1778 - val_loss: 0.0515 - val_mae: 0.2671\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0286 - mae: 0.1787 - val_loss: 0.0390 - val_mae: 0.2063\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0284 - mae: 0.1776 - val_loss: 0.0356 - val_mae: 0.1981\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0280 - mae: 0.1760 - val_loss: 0.0333 - val_mae: 0.1987\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0287 - mae: 0.1789 - val_loss: 0.0341 - val_mae: 0.2038\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0288 - mae: 0.1789 - val_loss: 0.0512 - val_mae: 0.2456\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0285 - mae: 0.1777 - val_loss: 0.0537 - val_mae: 0.2546\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0283 - mae: 0.1776 - val_loss: 0.0368 - val_mae: 0.1995\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0283 - mae: 0.1768 - val_loss: 0.0426 - val_mae: 0.2109\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0284 - mae: 0.1769 - val_loss: 0.0419 - val_mae: 0.2212\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0286 - mae: 0.1781 - val_loss: 0.0301 - val_mae: 0.1860\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0282 - mae: 0.1764 - val_loss: 0.0352 - val_mae: 0.1944\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0280 - mae: 0.1755 - val_loss: 0.0542 - val_mae: 0.2542\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0283 - mae: 0.1765 - val_loss: 0.0293 - val_mae: 0.1872\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0280 - mae: 0.1754 - val_loss: 0.0401 - val_mae: 0.2030\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0279 - mae: 0.1754 - val_loss: 0.0585 - val_mae: 0.2556\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0279 - mae: 0.1750 - val_loss: 0.0386 - val_mae: 0.2064\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0279 - mae: 0.1750 - val_loss: 0.0371 - val_mae: 0.2030\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0278 - mae: 0.1746 - val_loss: 0.0523 - val_mae: 0.2548\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0279 - mae: 0.1751 - val_loss: 0.0348 - val_mae: 0.1969\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0278 - mae: 0.1746 - val_loss: 0.0479 - val_mae: 0.2365\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0278 - mae: 0.1748 - val_loss: 0.0500 - val_mae: 0.2433\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0279 - mae: 0.1747 - val_loss: 0.0873 - val_mae: 0.3386\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0279 - mae: 0.1749 - val_loss: 0.0382 - val_mae: 0.2031\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0279 - mae: 0.1752 - val_loss: 0.0426 - val_mae: 0.2131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:35:11,613] Trial 19 finished with value: 0.04260082542896271 and parameters: {'learning_rate': 0.0004446723528614575, 'dropout_rate': 0.1326836885918844, 'batch_size': 32, 'epochs': 100}. Best is trial 16 with value: 0.03297249600291252.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "15/15 [==============================] - 4s 57ms/step - loss: 0.4293 - mae: 0.8544 - val_loss: 1.4430 - val_mae: 2.0512\n",
            "Epoch 2/50\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.4300 - mae: 0.8546 - val_loss: 0.7851 - val_mae: 1.3243\n",
            "Epoch 3/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.4203 - mae: 0.8407 - val_loss: 0.5059 - val_mae: 0.9927\n",
            "Epoch 4/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.4195 - mae: 0.8394 - val_loss: 0.3551 - val_mae: 0.7927\n",
            "Epoch 5/50\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.4099 - mae: 0.8263 - val_loss: 0.2769 - val_mae: 0.6733\n",
            "Epoch 6/50\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.4122 - mae: 0.8309 - val_loss: 0.2396 - val_mae: 0.6131\n",
            "Epoch 7/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.4087 - mae: 0.8238 - val_loss: 0.2134 - val_mae: 0.5754\n",
            "Epoch 8/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3998 - mae: 0.8125 - val_loss: 0.1969 - val_mae: 0.5504\n",
            "Epoch 9/50\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.4035 - mae: 0.8171 - val_loss: 0.2040 - val_mae: 0.5636\n",
            "Epoch 10/50\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.3965 - mae: 0.8070 - val_loss: 0.2038 - val_mae: 0.5644\n",
            "Epoch 11/50\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.3945 - mae: 0.8043 - val_loss: 0.2197 - val_mae: 0.5903\n",
            "Epoch 12/50\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.3932 - mae: 0.8025 - val_loss: 0.2094 - val_mae: 0.5755\n",
            "Epoch 13/50\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.3767 - mae: 0.7812 - val_loss: 0.2292 - val_mae: 0.6072\n",
            "Epoch 14/50\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.3897 - mae: 0.7977 - val_loss: 0.2432 - val_mae: 0.6283\n",
            "Epoch 15/50\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.3811 - mae: 0.7850 - val_loss: 0.2348 - val_mae: 0.6151\n",
            "Epoch 16/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3780 - mae: 0.7816 - val_loss: 0.2416 - val_mae: 0.6285\n",
            "Epoch 17/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3759 - mae: 0.7787 - val_loss: 0.2267 - val_mae: 0.6023\n",
            "Epoch 18/50\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.3756 - mae: 0.7782 - val_loss: 0.2110 - val_mae: 0.5735\n",
            "Epoch 19/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3695 - mae: 0.7684 - val_loss: 0.2095 - val_mae: 0.5712\n",
            "Epoch 20/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3688 - mae: 0.7671 - val_loss: 0.2081 - val_mae: 0.5667\n",
            "Epoch 21/50\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.3691 - mae: 0.7661 - val_loss: 0.2133 - val_mae: 0.5730\n",
            "Epoch 22/50\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.3613 - mae: 0.7568 - val_loss: 0.2040 - val_mae: 0.5552\n",
            "Epoch 23/50\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.3623 - mae: 0.7578 - val_loss: 0.2013 - val_mae: 0.5487\n",
            "Epoch 24/50\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.3538 - mae: 0.7473 - val_loss: 0.1907 - val_mae: 0.5301\n",
            "Epoch 25/50\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.3539 - mae: 0.7457 - val_loss: 0.1977 - val_mae: 0.5417\n",
            "Epoch 26/50\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.3450 - mae: 0.7333 - val_loss: 0.1947 - val_mae: 0.5359\n",
            "Epoch 27/50\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.3455 - mae: 0.7344 - val_loss: 0.2021 - val_mae: 0.5471\n",
            "Epoch 28/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3380 - mae: 0.7246 - val_loss: 0.1893 - val_mae: 0.5256\n",
            "Epoch 29/50\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.3472 - mae: 0.7373 - val_loss: 0.1941 - val_mae: 0.5324\n",
            "Epoch 30/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3420 - mae: 0.7295 - val_loss: 0.1932 - val_mae: 0.5299\n",
            "Epoch 31/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3396 - mae: 0.7251 - val_loss: 0.1949 - val_mae: 0.5340\n",
            "Epoch 32/50\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.3335 - mae: 0.7187 - val_loss: 0.1946 - val_mae: 0.5353\n",
            "Epoch 33/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3351 - mae: 0.7194 - val_loss: 0.1907 - val_mae: 0.5303\n",
            "Epoch 34/50\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.3358 - mae: 0.7205 - val_loss: 0.1897 - val_mae: 0.5303\n",
            "Epoch 35/50\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.3341 - mae: 0.7181 - val_loss: 0.1800 - val_mae: 0.5155\n",
            "Epoch 36/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3294 - mae: 0.7108 - val_loss: 0.1879 - val_mae: 0.5251\n",
            "Epoch 37/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3236 - mae: 0.7010 - val_loss: 0.1791 - val_mae: 0.5128\n",
            "Epoch 38/50\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.3196 - mae: 0.6976 - val_loss: 0.1757 - val_mae: 0.5058\n",
            "Epoch 39/50\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.3204 - mae: 0.6967 - val_loss: 0.1734 - val_mae: 0.5055\n",
            "Epoch 40/50\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.3165 - mae: 0.6919 - val_loss: 0.1684 - val_mae: 0.5025\n",
            "Epoch 41/50\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.3142 - mae: 0.6883 - val_loss: 0.1681 - val_mae: 0.5007\n",
            "Epoch 42/50\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.3177 - mae: 0.6945 - val_loss: 0.1714 - val_mae: 0.5040\n",
            "Epoch 43/50\n",
            "15/15 [==============================] - 0s 27ms/step - loss: 0.3102 - mae: 0.6831 - val_loss: 0.1713 - val_mae: 0.5022\n",
            "Epoch 44/50\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.3142 - mae: 0.6901 - val_loss: 0.1702 - val_mae: 0.4990\n",
            "Epoch 45/50\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.3067 - mae: 0.6787 - val_loss: 0.1664 - val_mae: 0.4915\n",
            "Epoch 46/50\n",
            "15/15 [==============================] - 0s 27ms/step - loss: 0.3064 - mae: 0.6774 - val_loss: 0.1608 - val_mae: 0.4837\n",
            "Epoch 47/50\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.3048 - mae: 0.6759 - val_loss: 0.1587 - val_mae: 0.4782\n",
            "Epoch 48/50\n",
            "15/15 [==============================] - 0s 30ms/step - loss: 0.3004 - mae: 0.6698 - val_loss: 0.1572 - val_mae: 0.4768\n",
            "Epoch 49/50\n",
            "15/15 [==============================] - 0s 30ms/step - loss: 0.3037 - mae: 0.6734 - val_loss: 0.1579 - val_mae: 0.4758\n",
            "Epoch 50/50\n",
            "15/15 [==============================] - 0s 27ms/step - loss: 0.2987 - mae: 0.6667 - val_loss: 0.1596 - val_mae: 0.4805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:35:35,428] Trial 20 finished with value: 0.15962734818458557 and parameters: {'learning_rate': 2.5792459158849688e-05, 'dropout_rate': 0.22544523790565615, 'batch_size': 256, 'epochs': 50}. Best is trial 16 with value: 0.03297249600291252.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 3s 8ms/step - loss: 0.4437 - mae: 0.8769 - val_loss: 0.6082 - val_mae: 1.1009\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.4168 - mae: 0.8420 - val_loss: 0.2262 - val_mae: 0.5850\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.3980 - mae: 0.8164 - val_loss: 0.1967 - val_mae: 0.5462\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3734 - mae: 0.7831 - val_loss: 0.1933 - val_mae: 0.5326\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3527 - mae: 0.7546 - val_loss: 0.2025 - val_mae: 0.5401\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.3345 - mae: 0.7305 - val_loss: 0.2006 - val_mae: 0.5437\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3153 - mae: 0.7032 - val_loss: 0.1954 - val_mae: 0.5297\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2997 - mae: 0.6810 - val_loss: 0.1869 - val_mae: 0.5248\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2851 - mae: 0.6586 - val_loss: 0.1781 - val_mae: 0.5031\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2696 - mae: 0.6369 - val_loss: 0.1653 - val_mae: 0.4779\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2568 - mae: 0.6188 - val_loss: 0.1458 - val_mae: 0.4506\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2443 - mae: 0.5991 - val_loss: 0.1342 - val_mae: 0.4279\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2298 - mae: 0.5769 - val_loss: 0.1342 - val_mae: 0.4272\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2194 - mae: 0.5612 - val_loss: 0.1359 - val_mae: 0.4307\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2077 - mae: 0.5433 - val_loss: 0.1154 - val_mae: 0.3986\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1983 - mae: 0.5295 - val_loss: 0.1152 - val_mae: 0.4002\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1872 - mae: 0.5124 - val_loss: 0.1150 - val_mae: 0.3940\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1793 - mae: 0.4985 - val_loss: 0.1022 - val_mae: 0.3727\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1705 - mae: 0.4855 - val_loss: 0.1062 - val_mae: 0.3794\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1630 - mae: 0.4723 - val_loss: 0.1007 - val_mae: 0.3685\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1584 - mae: 0.4644 - val_loss: 0.0988 - val_mae: 0.3641\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1509 - mae: 0.4514 - val_loss: 0.0896 - val_mae: 0.3471\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1449 - mae: 0.4413 - val_loss: 0.0874 - val_mae: 0.3427\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1404 - mae: 0.4328 - val_loss: 0.0846 - val_mae: 0.3355\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1356 - mae: 0.4245 - val_loss: 0.0861 - val_mae: 0.3387\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1308 - mae: 0.4167 - val_loss: 0.0780 - val_mae: 0.3208\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1286 - mae: 0.4107 - val_loss: 0.0770 - val_mae: 0.3195\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1228 - mae: 0.4002 - val_loss: 0.0746 - val_mae: 0.3140\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1192 - mae: 0.3940 - val_loss: 0.0738 - val_mae: 0.3142\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1158 - mae: 0.3873 - val_loss: 0.0703 - val_mae: 0.3037\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1125 - mae: 0.3817 - val_loss: 0.0680 - val_mae: 0.3007\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1091 - mae: 0.3748 - val_loss: 0.0666 - val_mae: 0.2973\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1062 - mae: 0.3688 - val_loss: 0.0670 - val_mae: 0.2960\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1035 - mae: 0.3634 - val_loss: 0.0619 - val_mae: 0.2869\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1008 - mae: 0.3583 - val_loss: 0.0608 - val_mae: 0.2852\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0990 - mae: 0.3544 - val_loss: 0.0612 - val_mae: 0.2814\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0945 - mae: 0.3447 - val_loss: 0.0588 - val_mae: 0.2789\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0921 - mae: 0.3395 - val_loss: 0.0563 - val_mae: 0.2722\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0907 - mae: 0.3359 - val_loss: 0.0550 - val_mae: 0.2700\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0891 - mae: 0.3337 - val_loss: 0.0543 - val_mae: 0.2673\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0873 - mae: 0.3288 - val_loss: 0.0531 - val_mae: 0.2644\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0861 - mae: 0.3271 - val_loss: 0.0516 - val_mae: 0.2610\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0845 - mae: 0.3231 - val_loss: 0.0512 - val_mae: 0.2602\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0804 - mae: 0.3155 - val_loss: 0.0493 - val_mae: 0.2549\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0785 - mae: 0.3105 - val_loss: 0.0495 - val_mae: 0.2556\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0769 - mae: 0.3070 - val_loss: 0.0469 - val_mae: 0.2505\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0765 - mae: 0.3070 - val_loss: 0.0465 - val_mae: 0.2491\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0734 - mae: 0.2990 - val_loss: 0.0466 - val_mae: 0.2486\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0742 - mae: 0.2998 - val_loss: 0.0457 - val_mae: 0.2464\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0719 - mae: 0.2956 - val_loss: 0.0444 - val_mae: 0.2428\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0710 - mae: 0.2934 - val_loss: 0.0431 - val_mae: 0.2402\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0696 - mae: 0.2909 - val_loss: 0.0430 - val_mae: 0.2394\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0696 - mae: 0.2898 - val_loss: 0.0430 - val_mae: 0.2399\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0676 - mae: 0.2853 - val_loss: 0.0427 - val_mae: 0.2376\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0660 - mae: 0.2830 - val_loss: 0.0423 - val_mae: 0.2367\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0668 - mae: 0.2832 - val_loss: 0.0417 - val_mae: 0.2342\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0653 - mae: 0.2803 - val_loss: 0.0415 - val_mae: 0.2338\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0653 - mae: 0.2794 - val_loss: 0.0413 - val_mae: 0.2330\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0642 - mae: 0.2776 - val_loss: 0.0407 - val_mae: 0.2317\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0635 - mae: 0.2760 - val_loss: 0.0405 - val_mae: 0.2316\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0624 - mae: 0.2737 - val_loss: 0.0405 - val_mae: 0.2308\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0624 - mae: 0.2724 - val_loss: 0.0403 - val_mae: 0.2302\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0611 - mae: 0.2705 - val_loss: 0.0399 - val_mae: 0.2296\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0614 - mae: 0.2707 - val_loss: 0.0401 - val_mae: 0.2298\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0601 - mae: 0.2685 - val_loss: 0.0397 - val_mae: 0.2285\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0588 - mae: 0.2656 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0592 - mae: 0.2666 - val_loss: 0.0391 - val_mae: 0.2263\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0585 - mae: 0.2647 - val_loss: 0.0389 - val_mae: 0.2262\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0582 - mae: 0.2633 - val_loss: 0.0393 - val_mae: 0.2263\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0575 - mae: 0.2628 - val_loss: 0.0389 - val_mae: 0.2257\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0571 - mae: 0.2617 - val_loss: 0.0394 - val_mae: 0.2268\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0564 - mae: 0.2590 - val_loss: 0.0391 - val_mae: 0.2262\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0561 - mae: 0.2588 - val_loss: 0.0393 - val_mae: 0.2264\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0554 - mae: 0.2578 - val_loss: 0.0385 - val_mae: 0.2249\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0553 - mae: 0.2580 - val_loss: 0.0390 - val_mae: 0.2255\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0539 - mae: 0.2552 - val_loss: 0.0369 - val_mae: 0.2197\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0553 - mae: 0.2574 - val_loss: 0.0375 - val_mae: 0.2216\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0545 - mae: 0.2547 - val_loss: 0.0383 - val_mae: 0.2236\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0538 - mae: 0.2535 - val_loss: 0.0377 - val_mae: 0.2216\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0542 - mae: 0.2556 - val_loss: 0.0378 - val_mae: 0.2216\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0526 - mae: 0.2509 - val_loss: 0.0373 - val_mae: 0.2202\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0526 - mae: 0.2507 - val_loss: 0.0369 - val_mae: 0.2191\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0523 - mae: 0.2512 - val_loss: 0.0372 - val_mae: 0.2196\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0525 - mae: 0.2516 - val_loss: 0.0366 - val_mae: 0.2178\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0517 - mae: 0.2496 - val_loss: 0.0364 - val_mae: 0.2160\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0505 - mae: 0.2475 - val_loss: 0.0355 - val_mae: 0.2146\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0513 - mae: 0.2481 - val_loss: 0.0359 - val_mae: 0.2155\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0502 - mae: 0.2465 - val_loss: 0.0358 - val_mae: 0.2137\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0503 - mae: 0.2462 - val_loss: 0.0361 - val_mae: 0.2151\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0491 - mae: 0.2441 - val_loss: 0.0359 - val_mae: 0.2151\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0497 - mae: 0.2444 - val_loss: 0.0366 - val_mae: 0.2170\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0493 - mae: 0.2435 - val_loss: 0.0360 - val_mae: 0.2146\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0490 - mae: 0.2428 - val_loss: 0.0356 - val_mae: 0.2150\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0484 - mae: 0.2416 - val_loss: 0.0356 - val_mae: 0.2141\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0480 - mae: 0.2402 - val_loss: 0.0373 - val_mae: 0.2184\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0477 - mae: 0.2392 - val_loss: 0.0368 - val_mae: 0.2175\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0473 - mae: 0.2383 - val_loss: 0.0359 - val_mae: 0.2151\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0463 - mae: 0.2363 - val_loss: 0.0345 - val_mae: 0.2106\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0473 - mae: 0.2384 - val_loss: 0.0360 - val_mae: 0.2146\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0459 - mae: 0.2355 - val_loss: 0.0349 - val_mae: 0.2110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:36:56,123] Trial 21 finished with value: 0.034863099455833435 and parameters: {'learning_rate': 3.948888430730839e-05, 'dropout_rate': 0.18354359678554677, 'batch_size': 32, 'epochs': 100}. Best is trial 16 with value: 0.03297249600291252.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 3s 8ms/step - loss: 0.3846 - mae: 0.8056 - val_loss: 0.9882 - val_mae: 1.5179\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3109 - mae: 0.7070 - val_loss: 0.2678 - val_mae: 0.6575\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2581 - mae: 0.6327 - val_loss: 0.2060 - val_mae: 0.5543\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2169 - mae: 0.5707 - val_loss: 0.1781 - val_mae: 0.5068\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1849 - mae: 0.5200 - val_loss: 0.1528 - val_mae: 0.4735\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1576 - mae: 0.4736 - val_loss: 0.1160 - val_mae: 0.3957\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1368 - mae: 0.4357 - val_loss: 0.1027 - val_mae: 0.3692\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1196 - mae: 0.4030 - val_loss: 0.0796 - val_mae: 0.3244\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1058 - mae: 0.3751 - val_loss: 0.0727 - val_mae: 0.3004\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0953 - mae: 0.3526 - val_loss: 0.0726 - val_mae: 0.2976\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0840 - mae: 0.3288 - val_loss: 0.0577 - val_mae: 0.2667\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0766 - mae: 0.3125 - val_loss: 0.0541 - val_mae: 0.2654\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0695 - mae: 0.2959 - val_loss: 0.0581 - val_mae: 0.2686\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0637 - mae: 0.2820 - val_loss: 0.0494 - val_mae: 0.2553\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0582 - mae: 0.2698 - val_loss: 0.0423 - val_mae: 0.2363\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0545 - mae: 0.2608 - val_loss: 0.0514 - val_mae: 0.2464\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0514 - mae: 0.2518 - val_loss: 0.0410 - val_mae: 0.2254\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0491 - mae: 0.2467 - val_loss: 0.0356 - val_mae: 0.2115\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0479 - mae: 0.2425 - val_loss: 0.0374 - val_mae: 0.2172\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0455 - mae: 0.2358 - val_loss: 0.0337 - val_mae: 0.2040\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0448 - mae: 0.2326 - val_loss: 0.0448 - val_mae: 0.2321\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0438 - mae: 0.2292 - val_loss: 0.0342 - val_mae: 0.2060\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0424 - mae: 0.2256 - val_loss: 0.0342 - val_mae: 0.2038\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0415 - mae: 0.2227 - val_loss: 0.0369 - val_mae: 0.2123\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0404 - mae: 0.2193 - val_loss: 0.0335 - val_mae: 0.2016\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0399 - mae: 0.2169 - val_loss: 0.0410 - val_mae: 0.2209\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0391 - mae: 0.2149 - val_loss: 0.0325 - val_mae: 0.2001\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0382 - mae: 0.2127 - val_loss: 0.0642 - val_mae: 0.2768\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0379 - mae: 0.2111 - val_loss: 0.0340 - val_mae: 0.2032\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0379 - mae: 0.2108 - val_loss: 0.0345 - val_mae: 0.2021\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0370 - mae: 0.2084 - val_loss: 0.0340 - val_mae: 0.2024\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0365 - mae: 0.2065 - val_loss: 0.0402 - val_mae: 0.2180\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0361 - mae: 0.2052 - val_loss: 0.0452 - val_mae: 0.2259\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0356 - mae: 0.2037 - val_loss: 0.0397 - val_mae: 0.2170\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0353 - mae: 0.2026 - val_loss: 0.0517 - val_mae: 0.2476\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0351 - mae: 0.2019 - val_loss: 0.0334 - val_mae: 0.1970\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0346 - mae: 0.2006 - val_loss: 0.0316 - val_mae: 0.1945\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0342 - mae: 0.1994 - val_loss: 0.0367 - val_mae: 0.2083\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0338 - mae: 0.1982 - val_loss: 0.0361 - val_mae: 0.2090\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0341 - mae: 0.1983 - val_loss: 0.0341 - val_mae: 0.2015\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0337 - mae: 0.1972 - val_loss: 0.0379 - val_mae: 0.2088\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0334 - mae: 0.1967 - val_loss: 0.0353 - val_mae: 0.2045\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0331 - mae: 0.1951 - val_loss: 0.0386 - val_mae: 0.2130\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0329 - mae: 0.1944 - val_loss: 0.0351 - val_mae: 0.2050\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0327 - mae: 0.1941 - val_loss: 0.0336 - val_mae: 0.2029\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0326 - mae: 0.1938 - val_loss: 0.0374 - val_mae: 0.2090\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0324 - mae: 0.1927 - val_loss: 0.0315 - val_mae: 0.1916\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0321 - mae: 0.1918 - val_loss: 0.0337 - val_mae: 0.1972\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0320 - mae: 0.1913 - val_loss: 0.0326 - val_mae: 0.1990\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0318 - mae: 0.1908 - val_loss: 0.0338 - val_mae: 0.1990\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0317 - mae: 0.1904 - val_loss: 0.0308 - val_mae: 0.1942\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0315 - mae: 0.1898 - val_loss: 0.0328 - val_mae: 0.1985\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0313 - mae: 0.1891 - val_loss: 0.0334 - val_mae: 0.1948\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0312 - mae: 0.1889 - val_loss: 0.0363 - val_mae: 0.2086\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0311 - mae: 0.1880 - val_loss: 0.0308 - val_mae: 0.1898\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0310 - mae: 0.1881 - val_loss: 0.0362 - val_mae: 0.2080\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0310 - mae: 0.1880 - val_loss: 0.0360 - val_mae: 0.2034\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0309 - mae: 0.1875 - val_loss: 0.0351 - val_mae: 0.1994\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0309 - mae: 0.1876 - val_loss: 0.0307 - val_mae: 0.1867\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0307 - mae: 0.1868 - val_loss: 0.0366 - val_mae: 0.2059\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0304 - mae: 0.1859 - val_loss: 0.0489 - val_mae: 0.2335\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0304 - mae: 0.1858 - val_loss: 0.0369 - val_mae: 0.2040\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0303 - mae: 0.1855 - val_loss: 0.0353 - val_mae: 0.2002\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0305 - mae: 0.1859 - val_loss: 0.0407 - val_mae: 0.2134\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0303 - mae: 0.1852 - val_loss: 0.0365 - val_mae: 0.2018\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0300 - mae: 0.1841 - val_loss: 0.0351 - val_mae: 0.1968\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0302 - mae: 0.1852 - val_loss: 0.0341 - val_mae: 0.1955\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0299 - mae: 0.1840 - val_loss: 0.0355 - val_mae: 0.2017\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0301 - mae: 0.1848 - val_loss: 0.0373 - val_mae: 0.2038\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0300 - mae: 0.1840 - val_loss: 0.0368 - val_mae: 0.2080\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0301 - mae: 0.1844 - val_loss: 0.0338 - val_mae: 0.1954\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0299 - mae: 0.1839 - val_loss: 0.0441 - val_mae: 0.2297\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0298 - mae: 0.1834 - val_loss: 0.0323 - val_mae: 0.1936\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0297 - mae: 0.1835 - val_loss: 0.0328 - val_mae: 0.2020\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0298 - mae: 0.1830 - val_loss: 0.0303 - val_mae: 0.1904\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0296 - mae: 0.1825 - val_loss: 0.0324 - val_mae: 0.1921\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0297 - mae: 0.1833 - val_loss: 0.0385 - val_mae: 0.2163\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0298 - mae: 0.1832 - val_loss: 0.0312 - val_mae: 0.1919\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0297 - mae: 0.1829 - val_loss: 0.0473 - val_mae: 0.2297\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0299 - mae: 0.1835 - val_loss: 0.0371 - val_mae: 0.2043\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0296 - mae: 0.1828 - val_loss: 0.0358 - val_mae: 0.2037\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0297 - mae: 0.1828 - val_loss: 0.0510 - val_mae: 0.2454\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0296 - mae: 0.1823 - val_loss: 0.0312 - val_mae: 0.1955\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0294 - mae: 0.1819 - val_loss: 0.0310 - val_mae: 0.1902\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0294 - mae: 0.1814 - val_loss: 0.0345 - val_mae: 0.2044\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0293 - mae: 0.1815 - val_loss: 0.0319 - val_mae: 0.1889\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0293 - mae: 0.1814 - val_loss: 0.0330 - val_mae: 0.1928\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0292 - mae: 0.1806 - val_loss: 0.0353 - val_mae: 0.2001\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0293 - mae: 0.1814 - val_loss: 0.0323 - val_mae: 0.2024\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0290 - mae: 0.1805 - val_loss: 0.0417 - val_mae: 0.2161\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0289 - mae: 0.1798 - val_loss: 0.0334 - val_mae: 0.1992\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0289 - mae: 0.1800 - val_loss: 0.0292 - val_mae: 0.1827\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0290 - mae: 0.1802 - val_loss: 0.0382 - val_mae: 0.2108\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0290 - mae: 0.1800 - val_loss: 0.0360 - val_mae: 0.2115\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0291 - mae: 0.1804 - val_loss: 0.0319 - val_mae: 0.1922\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0290 - mae: 0.1799 - val_loss: 0.0329 - val_mae: 0.2015\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0289 - mae: 0.1802 - val_loss: 0.0297 - val_mae: 0.1868\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0290 - mae: 0.1803 - val_loss: 0.0337 - val_mae: 0.1937\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0289 - mae: 0.1798 - val_loss: 0.0369 - val_mae: 0.2051\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0287 - mae: 0.1788 - val_loss: 0.0323 - val_mae: 0.1922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:38:20,898] Trial 22 finished with value: 0.03226765617728233 and parameters: {'learning_rate': 0.0001339856742510523, 'dropout_rate': 0.11105621859310802, 'batch_size': 32, 'epochs': 100}. Best is trial 22 with value: 0.03226765617728233.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 4s 8ms/step - loss: 0.4352 - mae: 0.8679 - val_loss: 0.5949 - val_mae: 1.0758\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3673 - mae: 0.7791 - val_loss: 0.2584 - val_mae: 0.6311\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3102 - mae: 0.7004 - val_loss: 0.3142 - val_mae: 0.7205\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2663 - mae: 0.6380 - val_loss: 0.2891 - val_mae: 0.6791\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2264 - mae: 0.5794 - val_loss: 0.1725 - val_mae: 0.4957\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1926 - mae: 0.5263 - val_loss: 0.1342 - val_mae: 0.4329\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1642 - mae: 0.4787 - val_loss: 0.1257 - val_mae: 0.4144\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1416 - mae: 0.4393 - val_loss: 0.0884 - val_mae: 0.3349\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1237 - mae: 0.4073 - val_loss: 0.0733 - val_mae: 0.3066\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1102 - mae: 0.3818 - val_loss: 0.0713 - val_mae: 0.3001\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0989 - mae: 0.3595 - val_loss: 0.0603 - val_mae: 0.2755\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0892 - mae: 0.3383 - val_loss: 0.0536 - val_mae: 0.2603\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0811 - mae: 0.3220 - val_loss: 0.0500 - val_mae: 0.2527\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0736 - mae: 0.3048 - val_loss: 0.0457 - val_mae: 0.2418\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0681 - mae: 0.2923 - val_loss: 0.0439 - val_mae: 0.2381\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0634 - mae: 0.2814 - val_loss: 0.0439 - val_mae: 0.2401\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0603 - mae: 0.2740 - val_loss: 0.0414 - val_mae: 0.2328\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0572 - mae: 0.2663 - val_loss: 0.0405 - val_mae: 0.2305\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0544 - mae: 0.2595 - val_loss: 0.0381 - val_mae: 0.2220\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0518 - mae: 0.2527 - val_loss: 0.0423 - val_mae: 0.2335\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0499 - mae: 0.2477 - val_loss: 0.0414 - val_mae: 0.2305\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0481 - mae: 0.2425 - val_loss: 0.0510 - val_mae: 0.2467\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0465 - mae: 0.2381 - val_loss: 0.0385 - val_mae: 0.2224\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0451 - mae: 0.2344 - val_loss: 0.0361 - val_mae: 0.2161\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0436 - mae: 0.2303 - val_loss: 0.0394 - val_mae: 0.2216\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0425 - mae: 0.2271 - val_loss: 0.0349 - val_mae: 0.2113\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0415 - mae: 0.2239 - val_loss: 0.0343 - val_mae: 0.2089\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0403 - mae: 0.2203 - val_loss: 0.0358 - val_mae: 0.2104\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2183 - val_loss: 0.0343 - val_mae: 0.2088\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0389 - mae: 0.2159 - val_loss: 0.0331 - val_mae: 0.2027\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.0382 - mae: 0.2140 - val_loss: 0.0387 - val_mae: 0.2158\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 2s 15ms/step - loss: 0.0375 - mae: 0.2117 - val_loss: 0.0453 - val_mae: 0.2285\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0369 - mae: 0.2099 - val_loss: 0.0398 - val_mae: 0.2163\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0362 - mae: 0.2077 - val_loss: 0.0334 - val_mae: 0.2049\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0360 - mae: 0.2067 - val_loss: 0.0371 - val_mae: 0.2096\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0352 - mae: 0.2044 - val_loss: 0.0336 - val_mae: 0.2021\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0349 - mae: 0.2033 - val_loss: 0.0437 - val_mae: 0.2252\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0345 - mae: 0.2017 - val_loss: 0.0343 - val_mae: 0.2017\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0343 - mae: 0.2010 - val_loss: 0.0322 - val_mae: 0.1982\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0339 - mae: 0.1995 - val_loss: 0.0383 - val_mae: 0.2139\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0339 - mae: 0.1998 - val_loss: 0.0356 - val_mae: 0.2083\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0335 - mae: 0.1986 - val_loss: 0.0328 - val_mae: 0.2057\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0330 - mae: 0.1972 - val_loss: 0.0351 - val_mae: 0.2049\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0332 - mae: 0.1972 - val_loss: 0.0313 - val_mae: 0.1987\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0329 - mae: 0.1965 - val_loss: 0.0443 - val_mae: 0.2234\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0328 - mae: 0.1964 - val_loss: 0.0361 - val_mae: 0.2065\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0326 - mae: 0.1954 - val_loss: 0.0372 - val_mae: 0.2067\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0323 - mae: 0.1944 - val_loss: 0.0335 - val_mae: 0.1999\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0322 - mae: 0.1941 - val_loss: 0.0307 - val_mae: 0.1958\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0323 - mae: 0.1942 - val_loss: 0.0339 - val_mae: 0.1982\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0318 - mae: 0.1925 - val_loss: 0.0365 - val_mae: 0.2100\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0318 - mae: 0.1926 - val_loss: 0.0324 - val_mae: 0.1962\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0318 - mae: 0.1926 - val_loss: 0.0398 - val_mae: 0.2174\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0318 - mae: 0.1926 - val_loss: 0.0358 - val_mae: 0.2072\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0316 - mae: 0.1913 - val_loss: 0.0361 - val_mae: 0.2063\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0316 - mae: 0.1916 - val_loss: 0.0297 - val_mae: 0.1910\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0313 - mae: 0.1904 - val_loss: 0.0316 - val_mae: 0.1933\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0311 - mae: 0.1900 - val_loss: 0.0329 - val_mae: 0.1951\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0313 - mae: 0.1908 - val_loss: 0.0328 - val_mae: 0.1942\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0313 - mae: 0.1904 - val_loss: 0.0303 - val_mae: 0.1953\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0312 - mae: 0.1899 - val_loss: 0.0304 - val_mae: 0.1938\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0308 - mae: 0.1886 - val_loss: 0.0378 - val_mae: 0.2122\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0309 - mae: 0.1888 - val_loss: 0.0339 - val_mae: 0.1966\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0309 - mae: 0.1894 - val_loss: 0.0343 - val_mae: 0.2070\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0310 - mae: 0.1894 - val_loss: 0.0415 - val_mae: 0.2207\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0307 - mae: 0.1888 - val_loss: 0.0348 - val_mae: 0.2083\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0308 - mae: 0.1883 - val_loss: 0.0319 - val_mae: 0.1923\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0307 - mae: 0.1882 - val_loss: 0.0300 - val_mae: 0.1906\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0306 - mae: 0.1877 - val_loss: 0.0334 - val_mae: 0.1970\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0306 - mae: 0.1878 - val_loss: 0.0397 - val_mae: 0.2161\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0305 - mae: 0.1873 - val_loss: 0.0343 - val_mae: 0.2131\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0307 - mae: 0.1883 - val_loss: 0.0388 - val_mae: 0.2121\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0304 - mae: 0.1873 - val_loss: 0.0331 - val_mae: 0.1942\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0304 - mae: 0.1871 - val_loss: 0.0331 - val_mae: 0.1977\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0304 - mae: 0.1872 - val_loss: 0.0298 - val_mae: 0.1915\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0303 - mae: 0.1865 - val_loss: 0.0377 - val_mae: 0.2060\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0303 - mae: 0.1868 - val_loss: 0.0367 - val_mae: 0.2022\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0304 - mae: 0.1872 - val_loss: 0.0314 - val_mae: 0.2011\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0304 - mae: 0.1870 - val_loss: 0.0323 - val_mae: 0.1974\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0303 - mae: 0.1864 - val_loss: 0.0379 - val_mae: 0.2109\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0301 - mae: 0.1858 - val_loss: 0.0337 - val_mae: 0.1990\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0301 - mae: 0.1860 - val_loss: 0.0498 - val_mae: 0.2386\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0301 - mae: 0.1862 - val_loss: 0.0312 - val_mae: 0.1925\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0301 - mae: 0.1859 - val_loss: 0.0368 - val_mae: 0.2137\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0298 - mae: 0.1847 - val_loss: 0.0328 - val_mae: 0.2072\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0301 - mae: 0.1856 - val_loss: 0.0325 - val_mae: 0.2040\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0300 - mae: 0.1852 - val_loss: 0.0321 - val_mae: 0.2049\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0300 - mae: 0.1855 - val_loss: 0.0355 - val_mae: 0.2064\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0301 - mae: 0.1858 - val_loss: 0.0311 - val_mae: 0.1888\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0299 - mae: 0.1852 - val_loss: 0.0337 - val_mae: 0.2029\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0299 - mae: 0.1847 - val_loss: 0.0352 - val_mae: 0.1986\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0299 - mae: 0.1850 - val_loss: 0.0295 - val_mae: 0.1839\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0298 - mae: 0.1845 - val_loss: 0.0302 - val_mae: 0.1913\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0298 - mae: 0.1845 - val_loss: 0.0465 - val_mae: 0.2258\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0298 - mae: 0.1847 - val_loss: 0.0306 - val_mae: 0.1890\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0300 - mae: 0.1850 - val_loss: 0.0313 - val_mae: 0.1896\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0299 - mae: 0.1847 - val_loss: 0.0320 - val_mae: 0.2018\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0301 - mae: 0.1854 - val_loss: 0.0289 - val_mae: 0.1855\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0300 - mae: 0.1855 - val_loss: 0.0369 - val_mae: 0.2080\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0298 - mae: 0.1843 - val_loss: 0.0426 - val_mae: 0.2249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:39:42,116] Trial 23 finished with value: 0.042594946920871735 and parameters: {'learning_rate': 0.00014717111403399652, 'dropout_rate': 0.20511663003120917, 'batch_size': 32, 'epochs': 100}. Best is trial 22 with value: 0.03226765617728233.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 3s 8ms/step - loss: 0.3908 - mae: 0.8024 - val_loss: 1.7123 - val_mae: 2.2616\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2551 - mae: 0.6091 - val_loss: 0.3999 - val_mae: 0.8051\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1697 - mae: 0.4717 - val_loss: 0.1622 - val_mae: 0.4723\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1198 - mae: 0.3845 - val_loss: 0.0698 - val_mae: 0.3032\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0896 - mae: 0.3290 - val_loss: 0.0517 - val_mae: 0.2568\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0720 - mae: 0.2949 - val_loss: 0.0476 - val_mae: 0.2484\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0604 - mae: 0.2716 - val_loss: 0.0423 - val_mae: 0.2361\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0530 - mae: 0.2545 - val_loss: 0.0403 - val_mae: 0.2320\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0472 - mae: 0.2402 - val_loss: 0.0410 - val_mae: 0.2313\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0439 - mae: 0.2315 - val_loss: 0.0411 - val_mae: 0.2283\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0411 - mae: 0.2235 - val_loss: 0.0581 - val_mae: 0.2551\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0391 - mae: 0.2174 - val_loss: 0.0800 - val_mae: 0.3032\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0369 - mae: 0.2109 - val_loss: 0.0608 - val_mae: 0.2599\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0360 - mae: 0.2080 - val_loss: 0.0353 - val_mae: 0.2109\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0351 - mae: 0.2054 - val_loss: 0.0478 - val_mae: 0.2326\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0346 - mae: 0.2036 - val_loss: 0.0616 - val_mae: 0.2603\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0341 - mae: 0.2020 - val_loss: 0.0349 - val_mae: 0.2081\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0338 - mae: 0.2010 - val_loss: 0.0362 - val_mae: 0.2095\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0333 - mae: 0.1991 - val_loss: 0.0509 - val_mae: 0.2390\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0331 - mae: 0.1989 - val_loss: 0.0474 - val_mae: 0.2317\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0329 - mae: 0.1978 - val_loss: 0.0631 - val_mae: 0.2642\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0328 - mae: 0.1971 - val_loss: 0.0339 - val_mae: 0.2049\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0327 - mae: 0.1966 - val_loss: 0.0388 - val_mae: 0.2151\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0324 - mae: 0.1963 - val_loss: 0.0499 - val_mae: 0.2366\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0325 - mae: 0.1963 - val_loss: 0.0327 - val_mae: 0.2013\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0324 - mae: 0.1957 - val_loss: 0.0517 - val_mae: 0.2406\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0320 - mae: 0.1944 - val_loss: 0.0567 - val_mae: 0.2596\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 0.0321 - mae: 0.1946 - val_loss: 0.0392 - val_mae: 0.2174\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0320 - mae: 0.1942 - val_loss: 0.0384 - val_mae: 0.2132\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0320 - mae: 0.1941 - val_loss: 0.0394 - val_mae: 0.2149\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0319 - mae: 0.1937 - val_loss: 0.0367 - val_mae: 0.2106\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0321 - mae: 0.1946 - val_loss: 0.0536 - val_mae: 0.2525\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0321 - mae: 0.1944 - val_loss: 0.0480 - val_mae: 0.2372\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0319 - mae: 0.1936 - val_loss: 0.1313 - val_mae: 0.4177\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0322 - mae: 0.1947 - val_loss: 0.0553 - val_mae: 0.2562\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0320 - mae: 0.1940 - val_loss: 0.1218 - val_mae: 0.4036\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0317 - mae: 0.1931 - val_loss: 0.0808 - val_mae: 0.3129\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0318 - mae: 0.1935 - val_loss: 0.1044 - val_mae: 0.3644\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0319 - mae: 0.1932 - val_loss: 0.0438 - val_mae: 0.2323\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0317 - mae: 0.1927 - val_loss: 0.1510 - val_mae: 0.4543\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0317 - mae: 0.1926 - val_loss: 0.0651 - val_mae: 0.2832\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0316 - mae: 0.1923 - val_loss: 0.0397 - val_mae: 0.2173\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0311 - mae: 0.1905 - val_loss: 0.0976 - val_mae: 0.3515\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0315 - mae: 0.1914 - val_loss: 0.0436 - val_mae: 0.2287\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0315 - mae: 0.1917 - val_loss: 0.0350 - val_mae: 0.2053\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0314 - mae: 0.1916 - val_loss: 0.0408 - val_mae: 0.2335\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0315 - mae: 0.1916 - val_loss: 0.0495 - val_mae: 0.2611\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0314 - mae: 0.1914 - val_loss: 0.0708 - val_mae: 0.2923\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0312 - mae: 0.1907 - val_loss: 0.0820 - val_mae: 0.3178\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0313 - mae: 0.1911 - val_loss: 0.0576 - val_mae: 0.2616\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0312 - mae: 0.1905 - val_loss: 0.0665 - val_mae: 0.2825\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0310 - mae: 0.1898 - val_loss: 0.0429 - val_mae: 0.2315\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0312 - mae: 0.1905 - val_loss: 0.0855 - val_mae: 0.3238\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0313 - mae: 0.1908 - val_loss: 0.0955 - val_mae: 0.3482\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0309 - mae: 0.1893 - val_loss: 0.0350 - val_mae: 0.2069\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0311 - mae: 0.1901 - val_loss: 0.0359 - val_mae: 0.2087\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0312 - mae: 0.1902 - val_loss: 0.0595 - val_mae: 0.2648\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0313 - mae: 0.1907 - val_loss: 0.0439 - val_mae: 0.2282\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0314 - mae: 0.1914 - val_loss: 0.0405 - val_mae: 0.2204\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0313 - mae: 0.1909 - val_loss: 0.0411 - val_mae: 0.2257\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0314 - mae: 0.1913 - val_loss: 0.0549 - val_mae: 0.2552\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0313 - mae: 0.1908 - val_loss: 0.0392 - val_mae: 0.2190\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0314 - mae: 0.1910 - val_loss: 0.0364 - val_mae: 0.2078\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0314 - mae: 0.1911 - val_loss: 0.0381 - val_mae: 0.2112\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0312 - mae: 0.1901 - val_loss: 0.0721 - val_mae: 0.2954\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0310 - mae: 0.1897 - val_loss: 0.0465 - val_mae: 0.2530\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0313 - mae: 0.1907 - val_loss: 0.0367 - val_mae: 0.2051\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0311 - mae: 0.1898 - val_loss: 0.0638 - val_mae: 0.2759\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0312 - mae: 0.1903 - val_loss: 0.0420 - val_mae: 0.2312\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0312 - mae: 0.1903 - val_loss: 0.0378 - val_mae: 0.2197\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0309 - mae: 0.1892 - val_loss: 0.0371 - val_mae: 0.2091\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0309 - mae: 0.1887 - val_loss: 0.0439 - val_mae: 0.2282\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0311 - mae: 0.1900 - val_loss: 0.0391 - val_mae: 0.2143\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0307 - mae: 0.1883 - val_loss: 0.0407 - val_mae: 0.2211\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0310 - mae: 0.1893 - val_loss: 0.0413 - val_mae: 0.2223\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0310 - mae: 0.1893 - val_loss: 0.0370 - val_mae: 0.2103\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0311 - mae: 0.1895 - val_loss: 0.0390 - val_mae: 0.2127\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0309 - mae: 0.1893 - val_loss: 0.0457 - val_mae: 0.2458\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0311 - mae: 0.1895 - val_loss: 0.0320 - val_mae: 0.1937\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0307 - mae: 0.1882 - val_loss: 0.0350 - val_mae: 0.2040\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0310 - mae: 0.1893 - val_loss: 0.0375 - val_mae: 0.2097\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0307 - mae: 0.1879 - val_loss: 0.0460 - val_mae: 0.2368\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0307 - mae: 0.1881 - val_loss: 0.0424 - val_mae: 0.2369\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0308 - mae: 0.1882 - val_loss: 0.0587 - val_mae: 0.2784\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0309 - mae: 0.1889 - val_loss: 0.0814 - val_mae: 0.3154\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0309 - mae: 0.1889 - val_loss: 0.1331 - val_mae: 0.4219\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0309 - mae: 0.1891 - val_loss: 0.0606 - val_mae: 0.2879\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0311 - mae: 0.1893 - val_loss: 0.0714 - val_mae: 0.2944\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0308 - mae: 0.1888 - val_loss: 0.0325 - val_mae: 0.1934\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0308 - mae: 0.1882 - val_loss: 0.0657 - val_mae: 0.2812\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0311 - mae: 0.1896 - val_loss: 0.0367 - val_mae: 0.2148\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0312 - mae: 0.1903 - val_loss: 0.0405 - val_mae: 0.2218\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0311 - mae: 0.1898 - val_loss: 0.0487 - val_mae: 0.2403\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0310 - mae: 0.1897 - val_loss: 0.0424 - val_mae: 0.2267\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0309 - mae: 0.1892 - val_loss: 0.0572 - val_mae: 0.2612\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0307 - mae: 0.1880 - val_loss: 0.0420 - val_mae: 0.2199\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0310 - mae: 0.1895 - val_loss: 0.0314 - val_mae: 0.1937\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0309 - mae: 0.1890 - val_loss: 0.0444 - val_mae: 0.2233\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.0312 - mae: 0.1900 - val_loss: 0.0557 - val_mae: 0.2813\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0315 - mae: 0.1909 - val_loss: 0.0372 - val_mae: 0.2250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:41:06,628] Trial 24 finished with value: 0.03719763457775116 and parameters: {'learning_rate': 0.0004391171817038893, 'dropout_rate': 0.30233421592500154, 'batch_size': 32, 'epochs': 100}. Best is trial 22 with value: 0.03226765617728233.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 3s 8ms/step - loss: 0.5432 - mae: 0.9965 - val_loss: 0.7603 - val_mae: 1.2655\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5435 - mae: 0.9963 - val_loss: 0.2051 - val_mae: 0.5564\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5458 - mae: 0.9998 - val_loss: 0.2223 - val_mae: 0.5866\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5429 - mae: 0.9954 - val_loss: 0.2168 - val_mae: 0.5750\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5482 - mae: 1.0024 - val_loss: 0.2180 - val_mae: 0.5757\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5402 - mae: 0.9930 - val_loss: 0.2189 - val_mae: 0.5769\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.5462 - mae: 1.0004 - val_loss: 0.2154 - val_mae: 0.5720\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.5467 - mae: 1.0002 - val_loss: 0.2151 - val_mae: 0.5712\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.5453 - mae: 0.9985 - val_loss: 0.2164 - val_mae: 0.5730\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.5394 - mae: 0.9918 - val_loss: 0.2176 - val_mae: 0.5744\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5429 - mae: 0.9948 - val_loss: 0.2164 - val_mae: 0.5729\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5447 - mae: 0.9981 - val_loss: 0.2146 - val_mae: 0.5694\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5382 - mae: 0.9897 - val_loss: 0.2157 - val_mae: 0.5716\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5390 - mae: 0.9915 - val_loss: 0.2152 - val_mae: 0.5703\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5380 - mae: 0.9896 - val_loss: 0.2140 - val_mae: 0.5688\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5370 - mae: 0.9880 - val_loss: 0.2187 - val_mae: 0.5767\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5375 - mae: 0.9893 - val_loss: 0.2170 - val_mae: 0.5734\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5385 - mae: 0.9897 - val_loss: 0.2159 - val_mae: 0.5714\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5309 - mae: 0.9815 - val_loss: 0.2156 - val_mae: 0.5710\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5346 - mae: 0.9862 - val_loss: 0.2161 - val_mae: 0.5717\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5403 - mae: 0.9929 - val_loss: 0.2160 - val_mae: 0.5720\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5323 - mae: 0.9827 - val_loss: 0.2137 - val_mae: 0.5692\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5345 - mae: 0.9864 - val_loss: 0.2127 - val_mae: 0.5677\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.5350 - mae: 0.9850 - val_loss: 0.2140 - val_mae: 0.5688\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5384 - mae: 0.9904 - val_loss: 0.2131 - val_mae: 0.5683\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5417 - mae: 0.9929 - val_loss: 0.2106 - val_mae: 0.5641\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5313 - mae: 0.9814 - val_loss: 0.2123 - val_mae: 0.5670\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5273 - mae: 0.9769 - val_loss: 0.2120 - val_mae: 0.5659\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.5328 - mae: 0.9826 - val_loss: 0.2122 - val_mae: 0.5664\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5314 - mae: 0.9807 - val_loss: 0.2099 - val_mae: 0.5632\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5252 - mae: 0.9743 - val_loss: 0.2082 - val_mae: 0.5600\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5307 - mae: 0.9809 - val_loss: 0.2100 - val_mae: 0.5623\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5348 - mae: 0.9858 - val_loss: 0.2100 - val_mae: 0.5631\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5261 - mae: 0.9737 - val_loss: 0.2093 - val_mae: 0.5622\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5267 - mae: 0.9769 - val_loss: 0.2108 - val_mae: 0.5650\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5248 - mae: 0.9737 - val_loss: 0.2093 - val_mae: 0.5624\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5273 - mae: 0.9759 - val_loss: 0.2096 - val_mae: 0.5617\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5250 - mae: 0.9738 - val_loss: 0.2091 - val_mae: 0.5616\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5209 - mae: 0.9687 - val_loss: 0.2071 - val_mae: 0.5587\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5206 - mae: 0.9675 - val_loss: 0.2058 - val_mae: 0.5572\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5232 - mae: 0.9712 - val_loss: 0.2068 - val_mae: 0.5582\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5192 - mae: 0.9666 - val_loss: 0.2089 - val_mae: 0.5613\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5211 - mae: 0.9691 - val_loss: 0.2063 - val_mae: 0.5580\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.5219 - mae: 0.9705 - val_loss: 0.2082 - val_mae: 0.5602\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.5230 - mae: 0.9718 - val_loss: 0.2045 - val_mae: 0.5544\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5211 - mae: 0.9691 - val_loss: 0.2056 - val_mae: 0.5562\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.5236 - mae: 0.9719 - val_loss: 0.2057 - val_mae: 0.5565\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.5267 - mae: 0.9749 - val_loss: 0.2074 - val_mae: 0.5592\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5230 - mae: 0.9721 - val_loss: 0.2058 - val_mae: 0.5573\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5196 - mae: 0.9669 - val_loss: 0.2050 - val_mae: 0.5558\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.5145 - mae: 0.9611 - val_loss: 0.2033 - val_mae: 0.5533\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.5222 - mae: 0.9702 - val_loss: 0.2047 - val_mae: 0.5553\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.5149 - mae: 0.9611 - val_loss: 0.2041 - val_mae: 0.5545\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5197 - mae: 0.9665 - val_loss: 0.2046 - val_mae: 0.5554\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.5171 - mae: 0.9638 - val_loss: 0.2044 - val_mae: 0.5553\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5173 - mae: 0.9646 - val_loss: 0.2017 - val_mae: 0.5508\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5164 - mae: 0.9624 - val_loss: 0.2019 - val_mae: 0.5513\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.5149 - mae: 0.9601 - val_loss: 0.2016 - val_mae: 0.5504\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.5109 - mae: 0.9565 - val_loss: 0.2024 - val_mae: 0.5522\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.5088 - mae: 0.9541 - val_loss: 0.2036 - val_mae: 0.5536\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.5149 - mae: 0.9617 - val_loss: 0.2034 - val_mae: 0.5534\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.5088 - mae: 0.9542 - val_loss: 0.1998 - val_mae: 0.5475\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5170 - mae: 0.9631 - val_loss: 0.2007 - val_mae: 0.5491\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5131 - mae: 0.9587 - val_loss: 0.2019 - val_mae: 0.5517\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.5093 - mae: 0.9541 - val_loss: 0.2008 - val_mae: 0.5497\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5098 - mae: 0.9537 - val_loss: 0.1993 - val_mae: 0.5465\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.5103 - mae: 0.9548 - val_loss: 0.1957 - val_mae: 0.5422\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.5068 - mae: 0.9513 - val_loss: 0.1973 - val_mae: 0.5442\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.5115 - mae: 0.9564 - val_loss: 0.1979 - val_mae: 0.5443\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5092 - mae: 0.9547 - val_loss: 0.2015 - val_mae: 0.5508\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5054 - mae: 0.9490 - val_loss: 0.1994 - val_mae: 0.5477\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5078 - mae: 0.9527 - val_loss: 0.1972 - val_mae: 0.5437\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5169 - mae: 0.9620 - val_loss: 0.1976 - val_mae: 0.5446\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5041 - mae: 0.9483 - val_loss: 0.1979 - val_mae: 0.5450\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5035 - mae: 0.9480 - val_loss: 0.1994 - val_mae: 0.5477\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5013 - mae: 0.9447 - val_loss: 0.1982 - val_mae: 0.5461\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5036 - mae: 0.9467 - val_loss: 0.1966 - val_mae: 0.5437\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.5005 - mae: 0.9434 - val_loss: 0.1956 - val_mae: 0.5419\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.4997 - mae: 0.9428 - val_loss: 0.1938 - val_mae: 0.5390\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.5022 - mae: 0.9454 - val_loss: 0.1930 - val_mae: 0.5375\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5045 - mae: 0.9479 - val_loss: 0.1943 - val_mae: 0.5394\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.5046 - mae: 0.9486 - val_loss: 0.1954 - val_mae: 0.5416\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5004 - mae: 0.9426 - val_loss: 0.1965 - val_mae: 0.5430\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4986 - mae: 0.9417 - val_loss: 0.1951 - val_mae: 0.5411\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4993 - mae: 0.9415 - val_loss: 0.1948 - val_mae: 0.5403\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.5015 - mae: 0.9446 - val_loss: 0.1938 - val_mae: 0.5389\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4963 - mae: 0.9376 - val_loss: 0.1954 - val_mae: 0.5415\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4914 - mae: 0.9317 - val_loss: 0.1960 - val_mae: 0.5420\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4943 - mae: 0.9348 - val_loss: 0.1931 - val_mae: 0.5379\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.4942 - mae: 0.9361 - val_loss: 0.1926 - val_mae: 0.5371\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.4957 - mae: 0.9371 - val_loss: 0.1948 - val_mae: 0.5398\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5010 - mae: 0.9419 - val_loss: 0.1950 - val_mae: 0.5397\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4947 - mae: 0.9355 - val_loss: 0.1902 - val_mae: 0.5333\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4978 - mae: 0.9399 - val_loss: 0.1943 - val_mae: 0.5394\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4963 - mae: 0.9376 - val_loss: 0.1923 - val_mae: 0.5361\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.4911 - mae: 0.9313 - val_loss: 0.1916 - val_mae: 0.5346\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4900 - mae: 0.9303 - val_loss: 0.1922 - val_mae: 0.5360\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4935 - mae: 0.9342 - val_loss: 0.1927 - val_mae: 0.5367\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4914 - mae: 0.9319 - val_loss: 0.1905 - val_mae: 0.5329\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4943 - mae: 0.9358 - val_loss: 0.1915 - val_mae: 0.5355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:42:31,286] Trial 25 finished with value: 0.191468745470047 and parameters: {'learning_rate': 1.406309327650407e-06, 'dropout_rate': 0.41762228113116273, 'batch_size': 32, 'epochs': 100}. Best is trial 22 with value: 0.03226765617728233.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 3s 8ms/step - loss: 0.4851 - mae: 0.9295 - val_loss: 1.1914 - val_mae: 1.7483\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.4663 - mae: 0.9053 - val_loss: 0.2462 - val_mae: 0.6071\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4537 - mae: 0.8885 - val_loss: 0.2693 - val_mae: 0.6401\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4427 - mae: 0.8755 - val_loss: 0.2794 - val_mae: 0.6663\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4282 - mae: 0.8566 - val_loss: 0.2655 - val_mae: 0.6454\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4166 - mae: 0.8426 - val_loss: 0.2466 - val_mae: 0.6095\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4048 - mae: 0.8266 - val_loss: 0.2416 - val_mae: 0.6064\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3935 - mae: 0.8111 - val_loss: 0.2398 - val_mae: 0.6046\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3830 - mae: 0.7979 - val_loss: 0.2277 - val_mae: 0.5845\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3760 - mae: 0.7878 - val_loss: 0.2209 - val_mae: 0.5700\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3646 - mae: 0.7735 - val_loss: 0.2222 - val_mae: 0.5742\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3522 - mae: 0.7561 - val_loss: 0.2077 - val_mae: 0.5523\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3440 - mae: 0.7447 - val_loss: 0.2163 - val_mae: 0.5702\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3359 - mae: 0.7336 - val_loss: 0.2053 - val_mae: 0.5488\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3238 - mae: 0.7178 - val_loss: 0.1929 - val_mae: 0.5234\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3185 - mae: 0.7096 - val_loss: 0.1918 - val_mae: 0.5296\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3062 - mae: 0.6929 - val_loss: 0.1902 - val_mae: 0.5325\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3005 - mae: 0.6852 - val_loss: 0.1859 - val_mae: 0.5200\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2935 - mae: 0.6753 - val_loss: 0.1780 - val_mae: 0.5064\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2866 - mae: 0.6648 - val_loss: 0.1748 - val_mae: 0.4960\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2781 - mae: 0.6524 - val_loss: 0.1697 - val_mae: 0.4915\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2732 - mae: 0.6454 - val_loss: 0.1697 - val_mae: 0.4938\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2656 - mae: 0.6337 - val_loss: 0.1612 - val_mae: 0.4747\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2604 - mae: 0.6269 - val_loss: 0.1569 - val_mae: 0.4741\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2546 - mae: 0.6179 - val_loss: 0.1527 - val_mae: 0.4643\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2474 - mae: 0.6076 - val_loss: 0.1542 - val_mae: 0.4647\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2434 - mae: 0.6017 - val_loss: 0.1551 - val_mae: 0.4672\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2369 - mae: 0.5911 - val_loss: 0.1500 - val_mae: 0.4525\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.2324 - mae: 0.5845 - val_loss: 0.1462 - val_mae: 0.4481\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2273 - mae: 0.5765 - val_loss: 0.1442 - val_mae: 0.4477\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2204 - mae: 0.5671 - val_loss: 0.1471 - val_mae: 0.4533\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2168 - mae: 0.5608 - val_loss: 0.1393 - val_mae: 0.4395\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2111 - mae: 0.5524 - val_loss: 0.1344 - val_mae: 0.4313\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2102 - mae: 0.5508 - val_loss: 0.1332 - val_mae: 0.4299\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2043 - mae: 0.5417 - val_loss: 0.1281 - val_mae: 0.4188\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1986 - mae: 0.5335 - val_loss: 0.1260 - val_mae: 0.4146\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1977 - mae: 0.5309 - val_loss: 0.1249 - val_mae: 0.4115\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1908 - mae: 0.5200 - val_loss: 0.1226 - val_mae: 0.4073\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1871 - mae: 0.5140 - val_loss: 0.1188 - val_mae: 0.4000\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1843 - mae: 0.5102 - val_loss: 0.1152 - val_mae: 0.3958\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1791 - mae: 0.5013 - val_loss: 0.1135 - val_mae: 0.3898\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1773 - mae: 0.4985 - val_loss: 0.1108 - val_mae: 0.3846\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1720 - mae: 0.4901 - val_loss: 0.1092 - val_mae: 0.3796\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1697 - mae: 0.4860 - val_loss: 0.1061 - val_mae: 0.3748\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1663 - mae: 0.4802 - val_loss: 0.1046 - val_mae: 0.3720\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1643 - mae: 0.4770 - val_loss: 0.1059 - val_mae: 0.3761\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1614 - mae: 0.4717 - val_loss: 0.1018 - val_mae: 0.3666\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1570 - mae: 0.4648 - val_loss: 0.0997 - val_mae: 0.3622\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1530 - mae: 0.4581 - val_loss: 0.0992 - val_mae: 0.3605\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1512 - mae: 0.4551 - val_loss: 0.0972 - val_mae: 0.3561\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1480 - mae: 0.4498 - val_loss: 0.0954 - val_mae: 0.3552\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1447 - mae: 0.4426 - val_loss: 0.0927 - val_mae: 0.3454\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1432 - mae: 0.4411 - val_loss: 0.0916 - val_mae: 0.3438\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1414 - mae: 0.4373 - val_loss: 0.0904 - val_mae: 0.3420\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1381 - mae: 0.4315 - val_loss: 0.0889 - val_mae: 0.3392\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1370 - mae: 0.4288 - val_loss: 0.0862 - val_mae: 0.3333\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1327 - mae: 0.4223 - val_loss: 0.0855 - val_mae: 0.3307\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1321 - mae: 0.4207 - val_loss: 0.0845 - val_mae: 0.3291\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.1305 - mae: 0.4170 - val_loss: 0.0828 - val_mae: 0.3255\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1278 - mae: 0.4124 - val_loss: 0.0809 - val_mae: 0.3214\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.1255 - mae: 0.4076 - val_loss: 0.0796 - val_mae: 0.3166\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.1240 - mae: 0.4054 - val_loss: 0.0783 - val_mae: 0.3144\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1214 - mae: 0.4004 - val_loss: 0.0783 - val_mae: 0.3148\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1204 - mae: 0.3986 - val_loss: 0.0768 - val_mae: 0.3106\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1193 - mae: 0.3961 - val_loss: 0.0756 - val_mae: 0.3072\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1168 - mae: 0.3911 - val_loss: 0.0749 - val_mae: 0.3077\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1143 - mae: 0.3869 - val_loss: 0.0734 - val_mae: 0.3041\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1131 - mae: 0.3842 - val_loss: 0.0722 - val_mae: 0.3001\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1120 - mae: 0.3825 - val_loss: 0.0711 - val_mae: 0.2981\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1083 - mae: 0.3757 - val_loss: 0.0693 - val_mae: 0.2932\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1073 - mae: 0.3730 - val_loss: 0.0681 - val_mae: 0.2906\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1059 - mae: 0.3706 - val_loss: 0.0675 - val_mae: 0.2885\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1051 - mae: 0.3685 - val_loss: 0.0664 - val_mae: 0.2867\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1031 - mae: 0.3650 - val_loss: 0.0657 - val_mae: 0.2842\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1023 - mae: 0.3622 - val_loss: 0.0643 - val_mae: 0.2814\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1013 - mae: 0.3602 - val_loss: 0.0639 - val_mae: 0.2805\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0993 - mae: 0.3572 - val_loss: 0.0629 - val_mae: 0.2774\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0980 - mae: 0.3541 - val_loss: 0.0625 - val_mae: 0.2770\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 0.0958 - mae: 0.3499 - val_loss: 0.0620 - val_mae: 0.2756\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0943 - mae: 0.3467 - val_loss: 0.0606 - val_mae: 0.2722\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0925 - mae: 0.3433 - val_loss: 0.0598 - val_mae: 0.2703\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0919 - mae: 0.3417 - val_loss: 0.0589 - val_mae: 0.2694\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0917 - mae: 0.3410 - val_loss: 0.0583 - val_mae: 0.2676\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0907 - mae: 0.3387 - val_loss: 0.0574 - val_mae: 0.2656\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0892 - mae: 0.3358 - val_loss: 0.0570 - val_mae: 0.2646\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0871 - mae: 0.3317 - val_loss: 0.0556 - val_mae: 0.2611\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0872 - mae: 0.3321 - val_loss: 0.0551 - val_mae: 0.2594\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0864 - mae: 0.3299 - val_loss: 0.0546 - val_mae: 0.2590\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0854 - mae: 0.3274 - val_loss: 0.0540 - val_mae: 0.2568\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0843 - mae: 0.3255 - val_loss: 0.0535 - val_mae: 0.2554\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0831 - mae: 0.3233 - val_loss: 0.0528 - val_mae: 0.2536\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0814 - mae: 0.3203 - val_loss: 0.0521 - val_mae: 0.2535\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0817 - mae: 0.3196 - val_loss: 0.0513 - val_mae: 0.2511\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0796 - mae: 0.3154 - val_loss: 0.0507 - val_mae: 0.2494\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0810 - mae: 0.3179 - val_loss: 0.0502 - val_mae: 0.2485\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0785 - mae: 0.3138 - val_loss: 0.0498 - val_mae: 0.2474\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0789 - mae: 0.3138 - val_loss: 0.0494 - val_mae: 0.2459\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0774 - mae: 0.3111 - val_loss: 0.0493 - val_mae: 0.2458\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0771 - mae: 0.3101 - val_loss: 0.0487 - val_mae: 0.2442\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0758 - mae: 0.3081 - val_loss: 0.0479 - val_mae: 0.2431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:44:02,641] Trial 26 finished with value: 0.04794546216726303 and parameters: {'learning_rate': 1.848084566373174e-05, 'dropout_rate': 0.1736420835998629, 'batch_size': 32, 'epochs': 100}. Best is trial 22 with value: 0.03226765617728233.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 5s 9ms/step - loss: 0.4950 - mae: 0.9406 - val_loss: 0.6867 - val_mae: 1.2135\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4684 - mae: 0.9081 - val_loss: 0.2166 - val_mae: 0.5643\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 2s 13ms/step - loss: 0.4329 - mae: 0.8612 - val_loss: 0.1660 - val_mae: 0.4877\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 0.4021 - mae: 0.8201 - val_loss: 0.1525 - val_mae: 0.4607\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3695 - mae: 0.7774 - val_loss: 0.1779 - val_mae: 0.5050\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3387 - mae: 0.7342 - val_loss: 0.1717 - val_mae: 0.5011\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3136 - mae: 0.6998 - val_loss: 0.1386 - val_mae: 0.4276\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2947 - mae: 0.6734 - val_loss: 0.1219 - val_mae: 0.4057\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2718 - mae: 0.6401 - val_loss: 0.1159 - val_mae: 0.3939\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2517 - mae: 0.6112 - val_loss: 0.1192 - val_mae: 0.3940\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2354 - mae: 0.5873 - val_loss: 0.1060 - val_mae: 0.3737\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2181 - mae: 0.5603 - val_loss: 0.0943 - val_mae: 0.3541\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2031 - mae: 0.5376 - val_loss: 0.0905 - val_mae: 0.3444\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1909 - mae: 0.5171 - val_loss: 0.0926 - val_mae: 0.3457\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1785 - mae: 0.4980 - val_loss: 0.0835 - val_mae: 0.3247\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1668 - mae: 0.4789 - val_loss: 0.0854 - val_mae: 0.3286\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1562 - mae: 0.4617 - val_loss: 0.0807 - val_mae: 0.3204\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1478 - mae: 0.4479 - val_loss: 0.0656 - val_mae: 0.2958\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.1405 - mae: 0.4339 - val_loss: 0.0654 - val_mae: 0.2923\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.1322 - mae: 0.4185 - val_loss: 0.0635 - val_mae: 0.2865\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1256 - mae: 0.4073 - val_loss: 0.0629 - val_mae: 0.2837\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1190 - mae: 0.3959 - val_loss: 0.0604 - val_mae: 0.2763\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1133 - mae: 0.3845 - val_loss: 0.0570 - val_mae: 0.2698\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1076 - mae: 0.3731 - val_loss: 0.0560 - val_mae: 0.2658\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1016 - mae: 0.3614 - val_loss: 0.0519 - val_mae: 0.2579\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0979 - mae: 0.3548 - val_loss: 0.0490 - val_mae: 0.2520\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0925 - mae: 0.3433 - val_loss: 0.0477 - val_mae: 0.2489\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0871 - mae: 0.3330 - val_loss: 0.0461 - val_mae: 0.2455\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0849 - mae: 0.3281 - val_loss: 0.0448 - val_mae: 0.2425\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0818 - mae: 0.3201 - val_loss: 0.0443 - val_mae: 0.2409\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0778 - mae: 0.3130 - val_loss: 0.0436 - val_mae: 0.2399\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0750 - mae: 0.3062 - val_loss: 0.0417 - val_mae: 0.2336\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0717 - mae: 0.2992 - val_loss: 0.0421 - val_mae: 0.2355\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0698 - mae: 0.2931 - val_loss: 0.0411 - val_mae: 0.2332\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0675 - mae: 0.2881 - val_loss: 0.0427 - val_mae: 0.2354\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.0649 - mae: 0.2824 - val_loss: 0.0406 - val_mae: 0.2296\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0634 - mae: 0.2777 - val_loss: 0.0375 - val_mae: 0.2207\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0611 - mae: 0.2721 - val_loss: 0.0376 - val_mae: 0.2209\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0591 - mae: 0.2672 - val_loss: 0.0371 - val_mae: 0.2189\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0577 - mae: 0.2639 - val_loss: 0.0402 - val_mae: 0.2270\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0555 - mae: 0.2589 - val_loss: 0.0362 - val_mae: 0.2143\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0545 - mae: 0.2569 - val_loss: 0.0404 - val_mae: 0.2266\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0539 - mae: 0.2542 - val_loss: 0.0399 - val_mae: 0.2249\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0523 - mae: 0.2497 - val_loss: 0.0349 - val_mae: 0.2105\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0517 - mae: 0.2483 - val_loss: 0.0357 - val_mae: 0.2134\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0504 - mae: 0.2454 - val_loss: 0.0376 - val_mae: 0.2177\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0497 - mae: 0.2431 - val_loss: 0.0412 - val_mae: 0.2265\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0482 - mae: 0.2395 - val_loss: 0.0371 - val_mae: 0.2164\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0481 - mae: 0.2383 - val_loss: 0.0342 - val_mae: 0.2078\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0471 - mae: 0.2363 - val_loss: 0.0345 - val_mae: 0.2082\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0460 - mae: 0.2330 - val_loss: 0.0343 - val_mae: 0.2082\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0459 - mae: 0.2329 - val_loss: 0.0365 - val_mae: 0.2107\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0450 - mae: 0.2307 - val_loss: 0.0338 - val_mae: 0.2054\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.0439 - mae: 0.2282 - val_loss: 0.0338 - val_mae: 0.2063\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0434 - mae: 0.2262 - val_loss: 0.0335 - val_mae: 0.2047\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0440 - mae: 0.2270 - val_loss: 0.0388 - val_mae: 0.2150\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0424 - mae: 0.2240 - val_loss: 0.0336 - val_mae: 0.2036\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0420 - mae: 0.2225 - val_loss: 0.0329 - val_mae: 0.2016\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0417 - mae: 0.2212 - val_loss: 0.0368 - val_mae: 0.2132\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0411 - mae: 0.2205 - val_loss: 0.0376 - val_mae: 0.2154\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0405 - mae: 0.2187 - val_loss: 0.0331 - val_mae: 0.2030\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0402 - mae: 0.2179 - val_loss: 0.0345 - val_mae: 0.2073\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0395 - mae: 0.2164 - val_loss: 0.0350 - val_mae: 0.2084\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0392 - mae: 0.2153 - val_loss: 0.0334 - val_mae: 0.2033\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0390 - mae: 0.2148 - val_loss: 0.0344 - val_mae: 0.2048\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0385 - mae: 0.2128 - val_loss: 0.0375 - val_mae: 0.2146\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0385 - mae: 0.2132 - val_loss: 0.0392 - val_mae: 0.2156\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0384 - mae: 0.2122 - val_loss: 0.0327 - val_mae: 0.2008\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0381 - mae: 0.2114 - val_loss: 0.0325 - val_mae: 0.2002\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0380 - mae: 0.2118 - val_loss: 0.0328 - val_mae: 0.2017\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0371 - mae: 0.2094 - val_loss: 0.0340 - val_mae: 0.2026\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0374 - mae: 0.2098 - val_loss: 0.0368 - val_mae: 0.2092\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0372 - mae: 0.2093 - val_loss: 0.0343 - val_mae: 0.2053\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0365 - mae: 0.2079 - val_loss: 0.0402 - val_mae: 0.2173\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0364 - mae: 0.2072 - val_loss: 0.0334 - val_mae: 0.2020\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0364 - mae: 0.2072 - val_loss: 0.0325 - val_mae: 0.2003\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0360 - mae: 0.2062 - val_loss: 0.0333 - val_mae: 0.2018\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0363 - mae: 0.2068 - val_loss: 0.0344 - val_mae: 0.2048\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0357 - mae: 0.2056 - val_loss: 0.0339 - val_mae: 0.2026\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0356 - mae: 0.2050 - val_loss: 0.0347 - val_mae: 0.2042\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0350 - mae: 0.2031 - val_loss: 0.0325 - val_mae: 0.1993\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0354 - mae: 0.2044 - val_loss: 0.0372 - val_mae: 0.2108\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0353 - mae: 0.2040 - val_loss: 0.0338 - val_mae: 0.2020\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0351 - mae: 0.2032 - val_loss: 0.0334 - val_mae: 0.2015\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0349 - mae: 0.2030 - val_loss: 0.0378 - val_mae: 0.2115\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0351 - mae: 0.2032 - val_loss: 0.0323 - val_mae: 0.1978\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0349 - mae: 0.2029 - val_loss: 0.0358 - val_mae: 0.2091\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0346 - mae: 0.2020 - val_loss: 0.0333 - val_mae: 0.2009\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0346 - mae: 0.2018 - val_loss: 0.0322 - val_mae: 0.1983\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0345 - mae: 0.2017 - val_loss: 0.0323 - val_mae: 0.1977\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0346 - mae: 0.2018 - val_loss: 0.0404 - val_mae: 0.2173\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0343 - mae: 0.2012 - val_loss: 0.0316 - val_mae: 0.1957\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0341 - mae: 0.2003 - val_loss: 0.0341 - val_mae: 0.2036\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0339 - mae: 0.1997 - val_loss: 0.0368 - val_mae: 0.2091\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0340 - mae: 0.1999 - val_loss: 0.0333 - val_mae: 0.2004\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0338 - mae: 0.1996 - val_loss: 0.0321 - val_mae: 0.1966\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0338 - mae: 0.1997 - val_loss: 0.0333 - val_mae: 0.1997\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0337 - mae: 0.1987 - val_loss: 0.0343 - val_mae: 0.2028\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0338 - mae: 0.1992 - val_loss: 0.0324 - val_mae: 0.1982\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0333 - mae: 0.1978 - val_loss: 0.0338 - val_mae: 0.2015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:46:29,173] Trial 27 finished with value: 0.03383134305477142 and parameters: {'learning_rate': 6.712767641758024e-05, 'dropout_rate': 0.26671304517934646, 'batch_size': 32, 'epochs': 100}. Best is trial 22 with value: 0.03226765617728233.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 3s 8ms/step - loss: 0.3112 - mae: 0.7013 - val_loss: 0.2760 - val_mae: 0.6654\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1208 - mae: 0.4028 - val_loss: 0.0735 - val_mae: 0.3110\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0613 - mae: 0.2794 - val_loss: 0.0406 - val_mae: 0.2317\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0440 - mae: 0.2340 - val_loss: 0.0394 - val_mae: 0.2275\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0375 - mae: 0.2136 - val_loss: 0.1933 - val_mae: 0.5353\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0345 - mae: 0.2026 - val_loss: 0.0702 - val_mae: 0.2967\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0335 - mae: 0.1996 - val_loss: 0.1261 - val_mae: 0.4154\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0330 - mae: 0.1975 - val_loss: 0.1284 - val_mae: 0.4188\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0329 - mae: 0.1975 - val_loss: 0.0491 - val_mae: 0.2459\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0323 - mae: 0.1951 - val_loss: 0.0484 - val_mae: 0.2453\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0322 - mae: 0.1947 - val_loss: 0.0614 - val_mae: 0.2721\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0320 - mae: 0.1937 - val_loss: 0.0519 - val_mae: 0.2421\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0318 - mae: 0.1933 - val_loss: 0.1048 - val_mae: 0.3435\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0317 - mae: 0.1926 - val_loss: 0.0827 - val_mae: 0.3097\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0318 - mae: 0.1930 - val_loss: 0.0709 - val_mae: 0.3013\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0315 - mae: 0.1920 - val_loss: 0.0683 - val_mae: 0.3048\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0315 - mae: 0.1919 - val_loss: 0.0352 - val_mae: 0.2157\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0315 - mae: 0.1920 - val_loss: 0.0365 - val_mae: 0.2214\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0314 - mae: 0.1915 - val_loss: 0.0528 - val_mae: 0.2489\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0314 - mae: 0.1917 - val_loss: 0.1304 - val_mae: 0.4160\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0315 - mae: 0.1917 - val_loss: 0.0950 - val_mae: 0.3253\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0312 - mae: 0.1908 - val_loss: 0.1380 - val_mae: 0.4295\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0317 - mae: 0.1923 - val_loss: 0.0490 - val_mae: 0.2503\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0314 - mae: 0.1914 - val_loss: 0.0380 - val_mae: 0.2182\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0313 - mae: 0.1907 - val_loss: 0.0560 - val_mae: 0.2601\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0312 - mae: 0.1905 - val_loss: 0.0891 - val_mae: 0.3114\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0312 - mae: 0.1904 - val_loss: 0.1281 - val_mae: 0.3845\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0311 - mae: 0.1899 - val_loss: 0.0809 - val_mae: 0.2981\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0309 - mae: 0.1892 - val_loss: 0.1018 - val_mae: 0.3381\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0313 - mae: 0.1910 - val_loss: 0.0442 - val_mae: 0.2491\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0313 - mae: 0.1908 - val_loss: 0.0981 - val_mae: 0.3402\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0313 - mae: 0.1905 - val_loss: 0.0682 - val_mae: 0.2855\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0311 - mae: 0.1901 - val_loss: 0.0392 - val_mae: 0.2122\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0309 - mae: 0.1892 - val_loss: 0.0399 - val_mae: 0.2203\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0311 - mae: 0.1899 - val_loss: 0.0715 - val_mae: 0.2947\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0311 - mae: 0.1899 - val_loss: 0.0568 - val_mae: 0.2647\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0310 - mae: 0.1894 - val_loss: 0.0435 - val_mae: 0.2237\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0312 - mae: 0.1901 - val_loss: 0.1037 - val_mae: 0.3518\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0307 - mae: 0.1883 - val_loss: 0.0354 - val_mae: 0.1982\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0310 - mae: 0.1890 - val_loss: 0.0417 - val_mae: 0.2187\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0315 - mae: 0.1909 - val_loss: 0.0601 - val_mae: 0.2685\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0313 - mae: 0.1905 - val_loss: 0.0453 - val_mae: 0.2295\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0314 - mae: 0.1911 - val_loss: 0.0569 - val_mae: 0.2756\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0313 - mae: 0.1908 - val_loss: 0.0795 - val_mae: 0.2979\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0311 - mae: 0.1898 - val_loss: 0.0757 - val_mae: 0.3043\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0315 - mae: 0.1910 - val_loss: 0.1093 - val_mae: 0.3699\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0312 - mae: 0.1899 - val_loss: 0.1232 - val_mae: 0.4013\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0311 - mae: 0.1895 - val_loss: 0.0982 - val_mae: 0.3354\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0314 - mae: 0.1908 - val_loss: 0.0808 - val_mae: 0.3218\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0316 - mae: 0.1917 - val_loss: 0.0741 - val_mae: 0.3188\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0315 - mae: 0.1916 - val_loss: 0.0341 - val_mae: 0.2004\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0318 - mae: 0.1925 - val_loss: 0.0531 - val_mae: 0.2424\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0313 - mae: 0.1907 - val_loss: 0.0341 - val_mae: 0.2043\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0313 - mae: 0.1904 - val_loss: 0.1037 - val_mae: 0.3660\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0320 - mae: 0.1929 - val_loss: 0.0540 - val_mae: 0.2786\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0322 - mae: 0.1944 - val_loss: 0.0864 - val_mae: 0.3255\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0319 - mae: 0.1927 - val_loss: 0.0420 - val_mae: 0.2218\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0318 - mae: 0.1925 - val_loss: 0.0476 - val_mae: 0.2372\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0318 - mae: 0.1923 - val_loss: 0.0412 - val_mae: 0.2157\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0314 - mae: 0.1906 - val_loss: 0.0492 - val_mae: 0.2596\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0328 - mae: 0.1962 - val_loss: 0.0383 - val_mae: 0.2196\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0345 - mae: 0.2033 - val_loss: 0.0449 - val_mae: 0.2336\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0346 - mae: 0.2042 - val_loss: 0.0444 - val_mae: 0.2336\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0346 - mae: 0.2044 - val_loss: 0.0824 - val_mae: 0.3253\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0344 - mae: 0.2038 - val_loss: 0.0854 - val_mae: 0.3310\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0342 - mae: 0.2031 - val_loss: 0.0545 - val_mae: 0.2553\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0341 - mae: 0.2027 - val_loss: 0.0427 - val_mae: 0.2252\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0339 - mae: 0.2016 - val_loss: 0.0381 - val_mae: 0.2153\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0383 - mae: 0.2192 - val_loss: 0.0713 - val_mae: 0.3015\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0382 - mae: 0.2200 - val_loss: 0.0906 - val_mae: 0.3532\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0370 - mae: 0.2155 - val_loss: 0.0867 - val_mae: 0.3354\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0349 - mae: 0.2064 - val_loss: 0.0741 - val_mae: 0.3026\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0348 - mae: 0.2056 - val_loss: 0.0540 - val_mae: 0.2460\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0347 - mae: 0.2049 - val_loss: 0.0968 - val_mae: 0.3532\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0346 - mae: 0.2045 - val_loss: 0.0495 - val_mae: 0.2385\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0351 - mae: 0.2067 - val_loss: 0.0350 - val_mae: 0.2075\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0346 - mae: 0.2048 - val_loss: 0.0588 - val_mae: 0.2639\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0349 - mae: 0.2056 - val_loss: 0.1119 - val_mae: 0.3778\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0389 - mae: 0.2215 - val_loss: 0.0527 - val_mae: 0.2565\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0384 - mae: 0.2220 - val_loss: 0.0711 - val_mae: 0.3069\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0361 - mae: 0.2117 - val_loss: 0.0727 - val_mae: 0.3047\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0354 - mae: 0.2081 - val_loss: 0.0662 - val_mae: 0.2855\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0350 - mae: 0.2063 - val_loss: 0.0369 - val_mae: 0.2129\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0349 - mae: 0.2058 - val_loss: 0.0466 - val_mae: 0.2319\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0346 - mae: 0.2048 - val_loss: 0.0444 - val_mae: 0.2268\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0380 - mae: 0.2172 - val_loss: 0.0613 - val_mae: 0.2701\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0451 - val_mae: 0.2415\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0428 - val_mae: 0.2372\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0408 - val_mae: 0.2344\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0401 - val_mae: 0.2321\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0397 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0401 - val_mae: 0.2284\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0401 - val_mae: 0.2285\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0403 - val_mae: 0.2288\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0400 - val_mae: 0.2283\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0400 - val_mae: 0.2281\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0401 - val_mae: 0.2289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:47:55,516] Trial 28 finished with value: 0.04013834148645401 and parameters: {'learning_rate': 0.000990021635210037, 'dropout_rate': 0.2850255498980993, 'batch_size': 32, 'epochs': 100}. Best is trial 22 with value: 0.03226765617728233.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 4s 10ms/step - loss: 0.4895 - mae: 0.9343 - val_loss: 0.4863 - val_mae: 0.9305\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4255 - mae: 0.8529 - val_loss: 0.2679 - val_mae: 0.6736\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3664 - mae: 0.7763 - val_loss: 0.2543 - val_mae: 0.6232\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.3160 - mae: 0.7065 - val_loss: 0.1882 - val_mae: 0.5472\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2672 - mae: 0.6381 - val_loss: 0.1979 - val_mae: 0.5617\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2292 - mae: 0.5810 - val_loss: 0.1002 - val_mae: 0.3682\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1935 - mae: 0.5256 - val_loss: 0.1011 - val_mae: 0.3671\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1618 - mae: 0.4738 - val_loss: 0.0929 - val_mae: 0.3474\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1371 - mae: 0.4309 - val_loss: 0.0796 - val_mae: 0.3179\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1173 - mae: 0.3946 - val_loss: 0.0596 - val_mae: 0.2786\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0990 - mae: 0.3599 - val_loss: 0.0516 - val_mae: 0.2632\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0865 - mae: 0.3341 - val_loss: 0.0482 - val_mae: 0.2542\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0762 - mae: 0.3133 - val_loss: 0.0461 - val_mae: 0.2483\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0677 - mae: 0.2943 - val_loss: 0.0443 - val_mae: 0.2423\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0612 - mae: 0.2793 - val_loss: 0.0419 - val_mae: 0.2358\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0569 - mae: 0.2688 - val_loss: 0.0409 - val_mae: 0.2334\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0531 - mae: 0.2594 - val_loss: 0.0403 - val_mae: 0.2312\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0502 - mae: 0.2519 - val_loss: 0.0403 - val_mae: 0.2314\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0477 - mae: 0.2451 - val_loss: 0.0402 - val_mae: 0.2312\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0459 - mae: 0.2406 - val_loss: 0.0397 - val_mae: 0.2296\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0447 - mae: 0.2374 - val_loss: 0.0398 - val_mae: 0.2298\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0437 - mae: 0.2345 - val_loss: 0.0402 - val_mae: 0.2298\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0421 - mae: 0.2301 - val_loss: 0.0407 - val_mae: 0.2314\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0414 - mae: 0.2281 - val_loss: 0.0387 - val_mae: 0.2252\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0404 - mae: 0.2254 - val_loss: 0.0374 - val_mae: 0.2201\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2226 - val_loss: 0.0383 - val_mae: 0.2231\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0387 - mae: 0.2199 - val_loss: 0.0388 - val_mae: 0.2236\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0377 - mae: 0.2165 - val_loss: 0.0352 - val_mae: 0.2127\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0370 - mae: 0.2141 - val_loss: 0.0361 - val_mae: 0.2146\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0363 - mae: 0.2115 - val_loss: 0.0396 - val_mae: 0.2213\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0358 - mae: 0.2096 - val_loss: 0.0739 - val_mae: 0.3084\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0357 - mae: 0.2090 - val_loss: 0.0444 - val_mae: 0.2330\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0354 - mae: 0.2079 - val_loss: 0.0376 - val_mae: 0.2149\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0349 - mae: 0.2062 - val_loss: 0.0410 - val_mae: 0.2235\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0348 - mae: 0.2055 - val_loss: 0.0363 - val_mae: 0.2132\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0346 - mae: 0.2050 - val_loss: 0.0525 - val_mae: 0.2525\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0344 - mae: 0.2043 - val_loss: 0.0492 - val_mae: 0.2432\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0341 - mae: 0.2033 - val_loss: 0.0636 - val_mae: 0.2786\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0342 - mae: 0.2035 - val_loss: 0.0389 - val_mae: 0.2170\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0339 - mae: 0.2027 - val_loss: 0.0328 - val_mae: 0.2027\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0339 - mae: 0.2025 - val_loss: 0.0352 - val_mae: 0.2119\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0340 - mae: 0.2027 - val_loss: 0.0532 - val_mae: 0.2533\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0336 - mae: 0.2016 - val_loss: 0.0466 - val_mae: 0.2364\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0335 - mae: 0.2014 - val_loss: 0.0907 - val_mae: 0.3399\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0340 - mae: 0.2026 - val_loss: 0.0365 - val_mae: 0.2133\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0339 - mae: 0.2025 - val_loss: 0.0387 - val_mae: 0.2170\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0336 - mae: 0.2012 - val_loss: 0.0387 - val_mae: 0.2164\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0335 - mae: 0.2011 - val_loss: 0.0409 - val_mae: 0.2231\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0332 - mae: 0.1997 - val_loss: 0.0605 - val_mae: 0.2691\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0331 - mae: 0.1995 - val_loss: 0.0320 - val_mae: 0.1975\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0332 - mae: 0.1999 - val_loss: 0.0332 - val_mae: 0.2013\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0333 - mae: 0.2000 - val_loss: 0.0401 - val_mae: 0.2190\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0330 - mae: 0.1992 - val_loss: 0.0409 - val_mae: 0.2210\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0329 - mae: 0.1985 - val_loss: 0.0585 - val_mae: 0.2648\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0328 - mae: 0.1984 - val_loss: 0.0388 - val_mae: 0.2160\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0329 - mae: 0.1984 - val_loss: 0.0352 - val_mae: 0.2069\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0329 - mae: 0.1981 - val_loss: 0.0352 - val_mae: 0.2064\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0325 - mae: 0.1973 - val_loss: 0.0325 - val_mae: 0.1987\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0326 - mae: 0.1973 - val_loss: 0.0328 - val_mae: 0.1994\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0327 - mae: 0.1977 - val_loss: 0.0435 - val_mae: 0.2283\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0329 - mae: 0.1980 - val_loss: 0.0332 - val_mae: 0.2004\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0326 - mae: 0.1972 - val_loss: 0.0353 - val_mae: 0.2058\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0325 - mae: 0.1968 - val_loss: 0.0355 - val_mae: 0.2056\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0324 - mae: 0.1963 - val_loss: 0.0328 - val_mae: 0.1985\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0323 - mae: 0.1961 - val_loss: 0.0410 - val_mae: 0.2206\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0321 - mae: 0.1955 - val_loss: 0.0377 - val_mae: 0.2109\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0323 - mae: 0.1960 - val_loss: 0.0387 - val_mae: 0.2124\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0321 - mae: 0.1951 - val_loss: 0.0382 - val_mae: 0.2116\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0322 - mae: 0.1955 - val_loss: 0.0308 - val_mae: 0.1916\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0322 - mae: 0.1954 - val_loss: 0.0376 - val_mae: 0.2119\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0320 - mae: 0.1947 - val_loss: 0.0341 - val_mae: 0.2013\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0319 - mae: 0.1942 - val_loss: 0.0383 - val_mae: 0.2111\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0319 - mae: 0.1940 - val_loss: 0.0303 - val_mae: 0.1902\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0320 - mae: 0.1945 - val_loss: 0.0447 - val_mae: 0.2260\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0319 - mae: 0.1943 - val_loss: 0.0379 - val_mae: 0.2123\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0321 - mae: 0.1949 - val_loss: 0.0336 - val_mae: 0.2011\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0321 - mae: 0.1951 - val_loss: 0.0350 - val_mae: 0.2039\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0323 - mae: 0.1955 - val_loss: 0.0362 - val_mae: 0.2077\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0321 - mae: 0.1952 - val_loss: 0.0323 - val_mae: 0.1960\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0320 - mae: 0.1947 - val_loss: 0.0411 - val_mae: 0.2193\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0321 - mae: 0.1949 - val_loss: 0.0318 - val_mae: 0.1962\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0320 - mae: 0.1945 - val_loss: 0.0400 - val_mae: 0.2173\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0317 - mae: 0.1935 - val_loss: 0.0331 - val_mae: 0.1989\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0319 - mae: 0.1942 - val_loss: 0.0348 - val_mae: 0.2030\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0318 - mae: 0.1937 - val_loss: 0.0369 - val_mae: 0.2074\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0318 - mae: 0.1938 - val_loss: 0.0366 - val_mae: 0.2089\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0320 - mae: 0.1946 - val_loss: 0.0328 - val_mae: 0.1983\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0319 - mae: 0.1941 - val_loss: 0.0453 - val_mae: 0.2330\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0321 - mae: 0.1946 - val_loss: 0.0349 - val_mae: 0.2049\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0319 - mae: 0.1942 - val_loss: 0.0370 - val_mae: 0.2087\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0319 - mae: 0.1939 - val_loss: 0.0365 - val_mae: 0.2092\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0322 - mae: 0.1951 - val_loss: 0.0327 - val_mae: 0.1970\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0319 - mae: 0.1947 - val_loss: 0.0315 - val_mae: 0.1933\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0319 - mae: 0.1943 - val_loss: 0.0345 - val_mae: 0.2025\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0320 - mae: 0.1945 - val_loss: 0.0425 - val_mae: 0.2305\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0319 - mae: 0.1940 - val_loss: 0.0353 - val_mae: 0.2043\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0320 - mae: 0.1947 - val_loss: 0.0391 - val_mae: 0.2150\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0317 - mae: 0.1935 - val_loss: 0.0353 - val_mae: 0.2034\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0322 - mae: 0.1949 - val_loss: 0.0932 - val_mae: 0.3410\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0322 - mae: 0.1953 - val_loss: 0.0389 - val_mae: 0.2138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:49:20,579] Trial 29 finished with value: 0.03891934081912041 and parameters: {'learning_rate': 0.00018055780628765836, 'dropout_rate': 0.3778320217319273, 'batch_size': 32, 'epochs': 100}. Best is trial 22 with value: 0.03226765617728233.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 4s 9ms/step - loss: 0.6316 - mae: 1.0990 - val_loss: 0.2335 - val_mae: 0.5848\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.6000 - mae: 1.0618 - val_loss: 0.2329 - val_mae: 0.5969\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5589 - mae: 1.0125 - val_loss: 0.2035 - val_mae: 0.5324\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5284 - mae: 0.9726 - val_loss: 0.1961 - val_mae: 0.5213\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4943 - mae: 0.9300 - val_loss: 0.1850 - val_mae: 0.5047\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4659 - mae: 0.8965 - val_loss: 0.1768 - val_mae: 0.4926\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4329 - mae: 0.8548 - val_loss: 0.1550 - val_mae: 0.4539\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4044 - mae: 0.8160 - val_loss: 0.1590 - val_mae: 0.4708\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3808 - mae: 0.7849 - val_loss: 0.1450 - val_mae: 0.4413\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3620 - mae: 0.7599 - val_loss: 0.1348 - val_mae: 0.4320\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3370 - mae: 0.7254 - val_loss: 0.1314 - val_mae: 0.4264\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3053 - mae: 0.6819 - val_loss: 0.1227 - val_mae: 0.4099\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2885 - mae: 0.6591 - val_loss: 0.1127 - val_mae: 0.3939\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2710 - mae: 0.6330 - val_loss: 0.1046 - val_mae: 0.3755\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.2509 - mae: 0.6041 - val_loss: 0.0980 - val_mae: 0.3627\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.2312 - mae: 0.5750 - val_loss: 0.0928 - val_mae: 0.3548\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.2173 - mae: 0.5536 - val_loss: 0.0860 - val_mae: 0.3380\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1981 - mae: 0.5232 - val_loss: 0.0814 - val_mae: 0.3294\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1873 - mae: 0.5066 - val_loss: 0.0777 - val_mae: 0.3231\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1711 - mae: 0.4808 - val_loss: 0.0727 - val_mae: 0.3098\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1591 - mae: 0.4609 - val_loss: 0.0698 - val_mae: 0.3034\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1454 - mae: 0.4372 - val_loss: 0.0663 - val_mae: 0.2955\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1376 - mae: 0.4248 - val_loss: 0.0652 - val_mae: 0.2933\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1278 - mae: 0.4067 - val_loss: 0.0635 - val_mae: 0.2894\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1193 - mae: 0.3917 - val_loss: 0.0601 - val_mae: 0.2813\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1115 - mae: 0.3773 - val_loss: 0.0584 - val_mae: 0.2775\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1033 - mae: 0.3620 - val_loss: 0.0569 - val_mae: 0.2739\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0966 - mae: 0.3503 - val_loss: 0.0542 - val_mae: 0.2682\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0900 - mae: 0.3368 - val_loss: 0.0524 - val_mae: 0.2624\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0846 - mae: 0.3262 - val_loss: 0.0507 - val_mae: 0.2597\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0792 - mae: 0.3159 - val_loss: 0.0497 - val_mae: 0.2576\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0750 - mae: 0.3068 - val_loss: 0.0481 - val_mae: 0.2531\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0706 - mae: 0.2982 - val_loss: 0.0472 - val_mae: 0.2508\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0672 - mae: 0.2909 - val_loss: 0.0461 - val_mae: 0.2487\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0634 - mae: 0.2832 - val_loss: 0.0451 - val_mae: 0.2464\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0607 - mae: 0.2770 - val_loss: 0.0447 - val_mae: 0.2452\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0577 - mae: 0.2706 - val_loss: 0.0437 - val_mae: 0.2427\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0553 - mae: 0.2655 - val_loss: 0.0431 - val_mae: 0.2412\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0538 - mae: 0.2618 - val_loss: 0.0425 - val_mae: 0.2398\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0521 - mae: 0.2575 - val_loss: 0.0421 - val_mae: 0.2386\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0502 - mae: 0.2533 - val_loss: 0.0419 - val_mae: 0.2381\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0491 - mae: 0.2506 - val_loss: 0.0416 - val_mae: 0.2373\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0478 - mae: 0.2475 - val_loss: 0.0412 - val_mae: 0.2362\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0467 - mae: 0.2448 - val_loss: 0.0410 - val_mae: 0.2352\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0459 - mae: 0.2429 - val_loss: 0.0409 - val_mae: 0.2348\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0451 - mae: 0.2404 - val_loss: 0.0407 - val_mae: 0.2341\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0442 - mae: 0.2385 - val_loss: 0.0404 - val_mae: 0.2334\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0438 - mae: 0.2372 - val_loss: 0.0404 - val_mae: 0.2330\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0435 - mae: 0.2362 - val_loss: 0.0403 - val_mae: 0.2326\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0431 - mae: 0.2351 - val_loss: 0.0402 - val_mae: 0.2322\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0427 - mae: 0.2342 - val_loss: 0.0402 - val_mae: 0.2319\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0422 - mae: 0.2328 - val_loss: 0.0401 - val_mae: 0.2316\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0419 - mae: 0.2323 - val_loss: 0.0401 - val_mae: 0.2313\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0415 - mae: 0.2313 - val_loss: 0.0400 - val_mae: 0.2310\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0417 - mae: 0.2314 - val_loss: 0.0400 - val_mae: 0.2307\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0413 - mae: 0.2308 - val_loss: 0.0400 - val_mae: 0.2307\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0411 - mae: 0.2303 - val_loss: 0.0400 - val_mae: 0.2307\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0410 - mae: 0.2300 - val_loss: 0.0400 - val_mae: 0.2306\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2299 - val_loss: 0.0400 - val_mae: 0.2305\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0406 - mae: 0.2294 - val_loss: 0.0399 - val_mae: 0.2303\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0406 - mae: 0.2291 - val_loss: 0.0399 - val_mae: 0.2301\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0406 - mae: 0.2291 - val_loss: 0.0399 - val_mae: 0.2299\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0405 - mae: 0.2289 - val_loss: 0.0399 - val_mae: 0.2299\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0404 - mae: 0.2287 - val_loss: 0.0399 - val_mae: 0.2298\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0403 - mae: 0.2282 - val_loss: 0.0399 - val_mae: 0.2296\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0403 - mae: 0.2283 - val_loss: 0.0399 - val_mae: 0.2295\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0402 - mae: 0.2282 - val_loss: 0.0398 - val_mae: 0.2293\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0401 - mae: 0.2279 - val_loss: 0.0398 - val_mae: 0.2293\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0400 - mae: 0.2278 - val_loss: 0.0398 - val_mae: 0.2294\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0400 - mae: 0.2279 - val_loss: 0.0398 - val_mae: 0.2293\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0401 - mae: 0.2279 - val_loss: 0.0398 - val_mae: 0.2293\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0399 - mae: 0.2278 - val_loss: 0.0398 - val_mae: 0.2290\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0400 - mae: 0.2277 - val_loss: 0.0398 - val_mae: 0.2291\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0399 - mae: 0.2276 - val_loss: 0.0398 - val_mae: 0.2291\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0399 - mae: 0.2274 - val_loss: 0.0398 - val_mae: 0.2289\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0398 - mae: 0.2273 - val_loss: 0.0398 - val_mae: 0.2288\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0398 - mae: 0.2272 - val_loss: 0.0398 - val_mae: 0.2289\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0398 - val_mae: 0.2288\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2288\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0398 - mae: 0.2271 - val_loss: 0.0397 - val_mae: 0.2289\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0397 - val_mae: 0.2287\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0398 - val_mae: 0.2287\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0397 - val_mae: 0.2286\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0397 - val_mae: 0.2285\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0397 - val_mae: 0.2285\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0397 - val_mae: 0.2284\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0395 - mae: 0.2265 - val_loss: 0.0397 - val_mae: 0.2285\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0397 - val_mae: 0.2283\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0397 - val_mae: 0.2285\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2265 - val_loss: 0.0397 - val_mae: 0.2282\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0397 - val_mae: 0.2283\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0395 - mae: 0.2263 - val_loss: 0.0397 - val_mae: 0.2282\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0395 - mae: 0.2263 - val_loss: 0.0397 - val_mae: 0.2281\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0394 - mae: 0.2262 - val_loss: 0.0397 - val_mae: 0.2280\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0395 - mae: 0.2261 - val_loss: 0.0396 - val_mae: 0.2280\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2265 - val_loss: 0.0396 - val_mae: 0.2279\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0394 - mae: 0.2261 - val_loss: 0.0396 - val_mae: 0.2278\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0394 - mae: 0.2261 - val_loss: 0.0396 - val_mae: 0.2278\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0394 - mae: 0.2259 - val_loss: 0.0396 - val_mae: 0.2278\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0394 - mae: 0.2258 - val_loss: 0.0396 - val_mae: 0.2277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:50:45,891] Trial 30 finished with value: 0.03956570103764534 and parameters: {'learning_rate': 8.824578871940976e-05, 'dropout_rate': 0.5413526244651145, 'batch_size': 32, 'epochs': 100}. Best is trial 22 with value: 0.03226765617728233.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 4s 12ms/step - loss: 0.4579 - mae: 0.8980 - val_loss: 0.6669 - val_mae: 1.1552\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4311 - mae: 0.8635 - val_loss: 0.2482 - val_mae: 0.6332\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4116 - mae: 0.8380 - val_loss: 0.2265 - val_mae: 0.5919\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3846 - mae: 0.8021 - val_loss: 0.2212 - val_mae: 0.5843\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3648 - mae: 0.7757 - val_loss: 0.2061 - val_mae: 0.5570\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3459 - mae: 0.7507 - val_loss: 0.1896 - val_mae: 0.5291\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3258 - mae: 0.7215 - val_loss: 0.1671 - val_mae: 0.4930\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3070 - mae: 0.6960 - val_loss: 0.1616 - val_mae: 0.4847\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2917 - mae: 0.6737 - val_loss: 0.1771 - val_mae: 0.5114\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2761 - mae: 0.6509 - val_loss: 0.1478 - val_mae: 0.4557\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2612 - mae: 0.6297 - val_loss: 0.1542 - val_mae: 0.4647\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2469 - mae: 0.6080 - val_loss: 0.1340 - val_mae: 0.4260\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2354 - mae: 0.5904 - val_loss: 0.1237 - val_mae: 0.4097\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2217 - mae: 0.5697 - val_loss: 0.1190 - val_mae: 0.4022\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2104 - mae: 0.5521 - val_loss: 0.1079 - val_mae: 0.3801\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1995 - mae: 0.5346 - val_loss: 0.1064 - val_mae: 0.3770\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1900 - mae: 0.5182 - val_loss: 0.0978 - val_mae: 0.3619\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1789 - mae: 0.5002 - val_loss: 0.0947 - val_mae: 0.3549\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1708 - mae: 0.4866 - val_loss: 0.0885 - val_mae: 0.3400\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1628 - mae: 0.4733 - val_loss: 0.0863 - val_mae: 0.3372\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1546 - mae: 0.4581 - val_loss: 0.0797 - val_mae: 0.3235\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1468 - mae: 0.4449 - val_loss: 0.0781 - val_mae: 0.3172\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1401 - mae: 0.4321 - val_loss: 0.0819 - val_mae: 0.3230\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1338 - mae: 0.4205 - val_loss: 0.0750 - val_mae: 0.3114\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1285 - mae: 0.4101 - val_loss: 0.0700 - val_mae: 0.2983\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1222 - mae: 0.3982 - val_loss: 0.0700 - val_mae: 0.2984\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1167 - mae: 0.3873 - val_loss: 0.0688 - val_mae: 0.2957\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1124 - mae: 0.3793 - val_loss: 0.0619 - val_mae: 0.2837\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1102 - mae: 0.3741 - val_loss: 0.0675 - val_mae: 0.2906\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1045 - mae: 0.3633 - val_loss: 0.0648 - val_mae: 0.2853\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1014 - mae: 0.3560 - val_loss: 0.0600 - val_mae: 0.2763\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0983 - mae: 0.3497 - val_loss: 0.0557 - val_mae: 0.2676\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0948 - mae: 0.3429 - val_loss: 0.0537 - val_mae: 0.2637\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0917 - mae: 0.3356 - val_loss: 0.0526 - val_mae: 0.2632\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0905 - mae: 0.3330 - val_loss: 0.0532 - val_mae: 0.2616\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0858 - mae: 0.3240 - val_loss: 0.0513 - val_mae: 0.2566\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0836 - mae: 0.3182 - val_loss: 0.0494 - val_mae: 0.2540\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0809 - mae: 0.3134 - val_loss: 0.0507 - val_mae: 0.2544\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0782 - mae: 0.3072 - val_loss: 0.0487 - val_mae: 0.2494\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0765 - mae: 0.3030 - val_loss: 0.0471 - val_mae: 0.2462\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0752 - mae: 0.3004 - val_loss: 0.0461 - val_mae: 0.2444\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0719 - mae: 0.2935 - val_loss: 0.0449 - val_mae: 0.2410\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0707 - mae: 0.2910 - val_loss: 0.0445 - val_mae: 0.2394\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0693 - mae: 0.2873 - val_loss: 0.0449 - val_mae: 0.2401\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0670 - mae: 0.2836 - val_loss: 0.0438 - val_mae: 0.2376\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0657 - mae: 0.2791 - val_loss: 0.0414 - val_mae: 0.2336\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0656 - mae: 0.2785 - val_loss: 0.0408 - val_mae: 0.2316\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0637 - mae: 0.2737 - val_loss: 0.0402 - val_mae: 0.2305\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0630 - mae: 0.2735 - val_loss: 0.0399 - val_mae: 0.2303\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0611 - mae: 0.2685 - val_loss: 0.0396 - val_mae: 0.2291\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0592 - mae: 0.2645 - val_loss: 0.0395 - val_mae: 0.2293\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0597 - mae: 0.2637 - val_loss: 0.0389 - val_mae: 0.2253\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0588 - mae: 0.2620 - val_loss: 0.0385 - val_mae: 0.2252\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0576 - mae: 0.2589 - val_loss: 0.0378 - val_mae: 0.2234\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0562 - mae: 0.2569 - val_loss: 0.0381 - val_mae: 0.2217\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0551 - mae: 0.2537 - val_loss: 0.0374 - val_mae: 0.2216\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0552 - mae: 0.2536 - val_loss: 0.0380 - val_mae: 0.2238\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 2s 13ms/step - loss: 0.0550 - mae: 0.2521 - val_loss: 0.0373 - val_mae: 0.2206\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0538 - mae: 0.2492 - val_loss: 0.0369 - val_mae: 0.2187\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0534 - mae: 0.2482 - val_loss: 0.0365 - val_mae: 0.2172\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.0525 - mae: 0.2457 - val_loss: 0.0366 - val_mae: 0.2166\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0513 - mae: 0.2433 - val_loss: 0.0367 - val_mae: 0.2183\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0505 - mae: 0.2420 - val_loss: 0.0367 - val_mae: 0.2178\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0493 - mae: 0.2386 - val_loss: 0.0358 - val_mae: 0.2148\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0501 - mae: 0.2403 - val_loss: 0.0358 - val_mae: 0.2135\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0492 - mae: 0.2390 - val_loss: 0.0354 - val_mae: 0.2129\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0490 - mae: 0.2375 - val_loss: 0.0363 - val_mae: 0.2165\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0482 - mae: 0.2356 - val_loss: 0.0362 - val_mae: 0.2141\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0474 - mae: 0.2339 - val_loss: 0.0368 - val_mae: 0.2163\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0482 - mae: 0.2359 - val_loss: 0.0349 - val_mae: 0.2110\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0470 - mae: 0.2329 - val_loss: 0.0398 - val_mae: 0.2199\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0465 - mae: 0.2318 - val_loss: 0.0368 - val_mae: 0.2142\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0461 - mae: 0.2306 - val_loss: 0.0358 - val_mae: 0.2125\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0459 - mae: 0.2308 - val_loss: 0.0376 - val_mae: 0.2167\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0455 - mae: 0.2291 - val_loss: 0.0392 - val_mae: 0.2193\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0460 - mae: 0.2300 - val_loss: 0.0349 - val_mae: 0.2102\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0443 - mae: 0.2260 - val_loss: 0.0343 - val_mae: 0.2086\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0441 - mae: 0.2261 - val_loss: 0.0341 - val_mae: 0.2079\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0440 - mae: 0.2258 - val_loss: 0.0345 - val_mae: 0.2090\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0439 - mae: 0.2253 - val_loss: 0.0345 - val_mae: 0.2090\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0437 - mae: 0.2247 - val_loss: 0.0350 - val_mae: 0.2101\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0430 - mae: 0.2236 - val_loss: 0.0346 - val_mae: 0.2083\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0432 - mae: 0.2239 - val_loss: 0.0354 - val_mae: 0.2119\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0428 - mae: 0.2224 - val_loss: 0.0347 - val_mae: 0.2098\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0424 - mae: 0.2222 - val_loss: 0.0361 - val_mae: 0.2118\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0422 - mae: 0.2211 - val_loss: 0.0356 - val_mae: 0.2120\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0418 - mae: 0.2212 - val_loss: 0.0352 - val_mae: 0.2099\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0419 - mae: 0.2209 - val_loss: 0.0343 - val_mae: 0.2070\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0413 - mae: 0.2197 - val_loss: 0.0351 - val_mae: 0.2092\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0409 - mae: 0.2190 - val_loss: 0.0343 - val_mae: 0.2077\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0407 - mae: 0.2185 - val_loss: 0.0348 - val_mae: 0.2086\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0406 - mae: 0.2180 - val_loss: 0.0345 - val_mae: 0.2075\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0407 - mae: 0.2178 - val_loss: 0.0366 - val_mae: 0.2120\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0402 - mae: 0.2166 - val_loss: 0.0349 - val_mae: 0.2088\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0399 - mae: 0.2163 - val_loss: 0.0347 - val_mae: 0.2081\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0398 - mae: 0.2157 - val_loss: 0.0336 - val_mae: 0.2052\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0400 - mae: 0.2164 - val_loss: 0.0348 - val_mae: 0.2095\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0395 - mae: 0.2151 - val_loss: 0.0341 - val_mae: 0.2062\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0397 - mae: 0.2160 - val_loss: 0.0347 - val_mae: 0.2078\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0392 - mae: 0.2145 - val_loss: 0.0352 - val_mae: 0.2086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:52:10,456] Trial 31 finished with value: 0.03516251966357231 and parameters: {'learning_rate': 5.097704239464771e-05, 'dropout_rate': 0.24464158097468208, 'batch_size': 32, 'epochs': 100}. Best is trial 22 with value: 0.03226765617728233.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 3s 9ms/step - loss: 0.4330 - mae: 0.8600 - val_loss: 0.7043 - val_mae: 1.1936\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4219 - mae: 0.8453 - val_loss: 0.1960 - val_mae: 0.5270\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4116 - mae: 0.8314 - val_loss: 0.1843 - val_mae: 0.5107\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4035 - mae: 0.8192 - val_loss: 0.2082 - val_mae: 0.5464\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3940 - mae: 0.8061 - val_loss: 0.2110 - val_mae: 0.5482\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3815 - mae: 0.7901 - val_loss: 0.2086 - val_mae: 0.5440\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3760 - mae: 0.7820 - val_loss: 0.1914 - val_mae: 0.5179\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.3650 - mae: 0.7673 - val_loss: 0.1910 - val_mae: 0.5157\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3598 - mae: 0.7583 - val_loss: 0.1871 - val_mae: 0.5081\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3542 - mae: 0.7512 - val_loss: 0.1886 - val_mae: 0.5121\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3408 - mae: 0.7330 - val_loss: 0.1777 - val_mae: 0.4933\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3393 - mae: 0.7303 - val_loss: 0.1782 - val_mae: 0.4928\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3280 - mae: 0.7144 - val_loss: 0.1767 - val_mae: 0.4921\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3209 - mae: 0.7040 - val_loss: 0.1705 - val_mae: 0.4790\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3160 - mae: 0.6976 - val_loss: 0.1689 - val_mae: 0.4801\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3095 - mae: 0.6875 - val_loss: 0.1618 - val_mae: 0.4665\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3017 - mae: 0.6772 - val_loss: 0.1562 - val_mae: 0.4585\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2957 - mae: 0.6683 - val_loss: 0.1562 - val_mae: 0.4579\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2916 - mae: 0.6612 - val_loss: 0.1514 - val_mae: 0.4490\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2865 - mae: 0.6545 - val_loss: 0.1500 - val_mae: 0.4480\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2805 - mae: 0.6447 - val_loss: 0.1439 - val_mae: 0.4350\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2724 - mae: 0.6335 - val_loss: 0.1448 - val_mae: 0.4365\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.2692 - mae: 0.6276 - val_loss: 0.1423 - val_mae: 0.4341\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.2631 - mae: 0.6192 - val_loss: 0.1322 - val_mae: 0.4166\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2591 - mae: 0.6134 - val_loss: 0.1363 - val_mae: 0.4259\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.2534 - mae: 0.6055 - val_loss: 0.1318 - val_mae: 0.4172\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2487 - mae: 0.5988 - val_loss: 0.1330 - val_mae: 0.4149\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2438 - mae: 0.5909 - val_loss: 0.1319 - val_mae: 0.4123\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2398 - mae: 0.5852 - val_loss: 0.1312 - val_mae: 0.4124\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2363 - mae: 0.5794 - val_loss: 0.1278 - val_mae: 0.4076\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2304 - mae: 0.5706 - val_loss: 0.1232 - val_mae: 0.4017\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2283 - mae: 0.5676 - val_loss: 0.1184 - val_mae: 0.3900\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2224 - mae: 0.5584 - val_loss: 0.1203 - val_mae: 0.3918\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2189 - mae: 0.5529 - val_loss: 0.1177 - val_mae: 0.3878\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2132 - mae: 0.5444 - val_loss: 0.1162 - val_mae: 0.3841\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2097 - mae: 0.5388 - val_loss: 0.1137 - val_mae: 0.3813\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2060 - mae: 0.5343 - val_loss: 0.1132 - val_mae: 0.3809\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2021 - mae: 0.5272 - val_loss: 0.1103 - val_mae: 0.3755\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1982 - mae: 0.5213 - val_loss: 0.1079 - val_mae: 0.3723\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1918 - mae: 0.5108 - val_loss: 0.1066 - val_mae: 0.3706\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1906 - mae: 0.5092 - val_loss: 0.1053 - val_mae: 0.3678\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1870 - mae: 0.5032 - val_loss: 0.1043 - val_mae: 0.3653\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1840 - mae: 0.4996 - val_loss: 0.1025 - val_mae: 0.3635\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1809 - mae: 0.4935 - val_loss: 0.0995 - val_mae: 0.3580\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1753 - mae: 0.4856 - val_loss: 0.0973 - val_mae: 0.3528\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1735 - mae: 0.4831 - val_loss: 0.0963 - val_mae: 0.3513\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1698 - mae: 0.4768 - val_loss: 0.0944 - val_mae: 0.3485\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1665 - mae: 0.4720 - val_loss: 0.0953 - val_mae: 0.3521\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1642 - mae: 0.4679 - val_loss: 0.0932 - val_mae: 0.3460\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1615 - mae: 0.4629 - val_loss: 0.0926 - val_mae: 0.3458\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1588 - mae: 0.4582 - val_loss: 0.0910 - val_mae: 0.3429\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1557 - mae: 0.4543 - val_loss: 0.0903 - val_mae: 0.3422\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1533 - mae: 0.4502 - val_loss: 0.0904 - val_mae: 0.3432\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1504 - mae: 0.4463 - val_loss: 0.0893 - val_mae: 0.3409\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1486 - mae: 0.4428 - val_loss: 0.0888 - val_mae: 0.3394\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1449 - mae: 0.4367 - val_loss: 0.0887 - val_mae: 0.3395\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1434 - mae: 0.4346 - val_loss: 0.0855 - val_mae: 0.3337\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1415 - mae: 0.4316 - val_loss: 0.0846 - val_mae: 0.3314\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1387 - mae: 0.4271 - val_loss: 0.0831 - val_mae: 0.3289\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1372 - mae: 0.4242 - val_loss: 0.0831 - val_mae: 0.3287\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1340 - mae: 0.4187 - val_loss: 0.0813 - val_mae: 0.3250\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1317 - mae: 0.4150 - val_loss: 0.0806 - val_mae: 0.3234\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1296 - mae: 0.4115 - val_loss: 0.0789 - val_mae: 0.3202\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1294 - mae: 0.4108 - val_loss: 0.0798 - val_mae: 0.3221\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1270 - mae: 0.4067 - val_loss: 0.0771 - val_mae: 0.3158\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1233 - mae: 0.4012 - val_loss: 0.0783 - val_mae: 0.3195\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1235 - mae: 0.4010 - val_loss: 0.0777 - val_mae: 0.3183\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1220 - mae: 0.3984 - val_loss: 0.0743 - val_mae: 0.3110\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1208 - mae: 0.3951 - val_loss: 0.0736 - val_mae: 0.3093\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1174 - mae: 0.3904 - val_loss: 0.0723 - val_mae: 0.3061\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1164 - mae: 0.3882 - val_loss: 0.0723 - val_mae: 0.3060\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1140 - mae: 0.3836 - val_loss: 0.0724 - val_mae: 0.3060\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1135 - mae: 0.3828 - val_loss: 0.0705 - val_mae: 0.3023\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1116 - mae: 0.3797 - val_loss: 0.0715 - val_mae: 0.3040\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1116 - mae: 0.3787 - val_loss: 0.0707 - val_mae: 0.3028\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1099 - mae: 0.3760 - val_loss: 0.0691 - val_mae: 0.2991\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1098 - mae: 0.3756 - val_loss: 0.0688 - val_mae: 0.2982\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1064 - mae: 0.3694 - val_loss: 0.0683 - val_mae: 0.2972\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1075 - mae: 0.3708 - val_loss: 0.0665 - val_mae: 0.2930\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1029 - mae: 0.3632 - val_loss: 0.0662 - val_mae: 0.2926\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1042 - mae: 0.3648 - val_loss: 0.0655 - val_mae: 0.2909\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1039 - mae: 0.3637 - val_loss: 0.0651 - val_mae: 0.2895\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1021 - mae: 0.3604 - val_loss: 0.0638 - val_mae: 0.2863\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0998 - mae: 0.3570 - val_loss: 0.0632 - val_mae: 0.2854\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0986 - mae: 0.3546 - val_loss: 0.0635 - val_mae: 0.2862\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0977 - mae: 0.3518 - val_loss: 0.0629 - val_mae: 0.2856\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0981 - mae: 0.3526 - val_loss: 0.0617 - val_mae: 0.2824\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0964 - mae: 0.3500 - val_loss: 0.0608 - val_mae: 0.2805\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0958 - mae: 0.3482 - val_loss: 0.0605 - val_mae: 0.2800\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0947 - mae: 0.3467 - val_loss: 0.0603 - val_mae: 0.2795\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0946 - mae: 0.3453 - val_loss: 0.0608 - val_mae: 0.2807\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0918 - mae: 0.3413 - val_loss: 0.0611 - val_mae: 0.2815\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0917 - mae: 0.3399 - val_loss: 0.0601 - val_mae: 0.2790\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0917 - mae: 0.3401 - val_loss: 0.0592 - val_mae: 0.2769\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0896 - mae: 0.3359 - val_loss: 0.0581 - val_mae: 0.2736\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0905 - mae: 0.3366 - val_loss: 0.0573 - val_mae: 0.2721\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0875 - mae: 0.3315 - val_loss: 0.0567 - val_mae: 0.2701\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0891 - mae: 0.3335 - val_loss: 0.0557 - val_mae: 0.2680\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0864 - mae: 0.3300 - val_loss: 0.0560 - val_mae: 0.2694\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0853 - mae: 0.3277 - val_loss: 0.0556 - val_mae: 0.2686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:53:31,462] Trial 32 finished with value: 0.05564828962087631 and parameters: {'learning_rate': 1.5325675994752396e-05, 'dropout_rate': 0.15494242679384612, 'batch_size': 32, 'epochs': 100}. Best is trial 22 with value: 0.03226765617728233.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 3s 7ms/step - loss: 0.3818 - mae: 0.8018 - val_loss: 1.1813 - val_mae: 1.7203\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3426 - mae: 0.7487 - val_loss: 0.2960 - val_mae: 0.6870\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2993 - mae: 0.6884 - val_loss: 0.2608 - val_mae: 0.6550\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2635 - mae: 0.6363 - val_loss: 0.1971 - val_mae: 0.5425\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2335 - mae: 0.5914 - val_loss: 0.2055 - val_mae: 0.5527\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2093 - mae: 0.5542 - val_loss: 0.1903 - val_mae: 0.5219\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1860 - mae: 0.5173 - val_loss: 0.1580 - val_mae: 0.4740\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1725 - mae: 0.4943 - val_loss: 0.1424 - val_mae: 0.4415\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1572 - mae: 0.4684 - val_loss: 0.1283 - val_mae: 0.4157\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1453 - mae: 0.4470 - val_loss: 0.1114 - val_mae: 0.3831\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1358 - mae: 0.4288 - val_loss: 0.0992 - val_mae: 0.3602\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1271 - mae: 0.4130 - val_loss: 0.0945 - val_mae: 0.3499\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1196 - mae: 0.3986 - val_loss: 0.0919 - val_mae: 0.3430\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1147 - mae: 0.3885 - val_loss: 0.0880 - val_mae: 0.3338\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1102 - mae: 0.3795 - val_loss: 0.0847 - val_mae: 0.3268\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1036 - mae: 0.3662 - val_loss: 0.0795 - val_mae: 0.3141\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0986 - mae: 0.3565 - val_loss: 0.0710 - val_mae: 0.3004\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0938 - mae: 0.3462 - val_loss: 0.0709 - val_mae: 0.2988\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0910 - mae: 0.3392 - val_loss: 0.0716 - val_mae: 0.2941\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0859 - mae: 0.3292 - val_loss: 0.0722 - val_mae: 0.2971\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0832 - mae: 0.3230 - val_loss: 0.0590 - val_mae: 0.2757\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0792 - mae: 0.3131 - val_loss: 0.0606 - val_mae: 0.2717\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0769 - mae: 0.3091 - val_loss: 0.0537 - val_mae: 0.2723\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0741 - mae: 0.3022 - val_loss: 0.0508 - val_mae: 0.2531\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0698 - mae: 0.2918 - val_loss: 0.0486 - val_mae: 0.2563\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0673 - mae: 0.2868 - val_loss: 0.0461 - val_mae: 0.2436\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0663 - mae: 0.2825 - val_loss: 0.0452 - val_mae: 0.2455\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0627 - mae: 0.2757 - val_loss: 0.0450 - val_mae: 0.2412\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0617 - mae: 0.2745 - val_loss: 0.0432 - val_mae: 0.2367\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0610 - mae: 0.2718 - val_loss: 0.0419 - val_mae: 0.2354\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0577 - mae: 0.2644 - val_loss: 0.0389 - val_mae: 0.2251\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0572 - mae: 0.2623 - val_loss: 0.0378 - val_mae: 0.2204\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0546 - mae: 0.2568 - val_loss: 0.0372 - val_mae: 0.2195\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0532 - mae: 0.2539 - val_loss: 0.0359 - val_mae: 0.2140\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0522 - mae: 0.2504 - val_loss: 0.0372 - val_mae: 0.2159\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0515 - mae: 0.2488 - val_loss: 0.0356 - val_mae: 0.2122\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0506 - mae: 0.2464 - val_loss: 0.0352 - val_mae: 0.2115\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0505 - mae: 0.2450 - val_loss: 0.0360 - val_mae: 0.2112\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0499 - mae: 0.2431 - val_loss: 0.0347 - val_mae: 0.2106\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0481 - mae: 0.2397 - val_loss: 0.0344 - val_mae: 0.2066\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0484 - mae: 0.2390 - val_loss: 0.0336 - val_mae: 0.2060\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0476 - mae: 0.2369 - val_loss: 0.0362 - val_mae: 0.2098\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0467 - mae: 0.2352 - val_loss: 0.0332 - val_mae: 0.2038\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0469 - mae: 0.2357 - val_loss: 0.0334 - val_mae: 0.2032\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0461 - mae: 0.2328 - val_loss: 0.0329 - val_mae: 0.2023\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0458 - mae: 0.2319 - val_loss: 0.0362 - val_mae: 0.2120\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0451 - mae: 0.2297 - val_loss: 0.0332 - val_mae: 0.2019\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0447 - mae: 0.2291 - val_loss: 0.0350 - val_mae: 0.2073\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0445 - mae: 0.2285 - val_loss: 0.0372 - val_mae: 0.2116\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0431 - mae: 0.2249 - val_loss: 0.0350 - val_mae: 0.2080\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0432 - mae: 0.2254 - val_loss: 0.0331 - val_mae: 0.2020\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0432 - mae: 0.2246 - val_loss: 0.0350 - val_mae: 0.2097\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0423 - mae: 0.2227 - val_loss: 0.0325 - val_mae: 0.1986\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0422 - mae: 0.2220 - val_loss: 0.0361 - val_mae: 0.2095\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0416 - mae: 0.2207 - val_loss: 0.0329 - val_mae: 0.1990\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0420 - mae: 0.2215 - val_loss: 0.0354 - val_mae: 0.2047\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0423 - mae: 0.2218 - val_loss: 0.0314 - val_mae: 0.1951\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0413 - mae: 0.2194 - val_loss: 0.0511 - val_mae: 0.2484\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0414 - mae: 0.2197 - val_loss: 0.0397 - val_mae: 0.2208\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0404 - mae: 0.2170 - val_loss: 0.0338 - val_mae: 0.2026\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0409 - mae: 0.2179 - val_loss: 0.0358 - val_mae: 0.2078\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0401 - mae: 0.2155 - val_loss: 0.0314 - val_mae: 0.1947\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0399 - mae: 0.2156 - val_loss: 0.0395 - val_mae: 0.2129\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0400 - mae: 0.2152 - val_loss: 0.0381 - val_mae: 0.2097\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2140 - val_loss: 0.0316 - val_mae: 0.1951\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0395 - mae: 0.2145 - val_loss: 0.0315 - val_mae: 0.1943\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0387 - mae: 0.2118 - val_loss: 0.0349 - val_mae: 0.2054\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0386 - mae: 0.2122 - val_loss: 0.0331 - val_mae: 0.1989\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0382 - mae: 0.2101 - val_loss: 0.0319 - val_mae: 0.1965\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0386 - mae: 0.2115 - val_loss: 0.0331 - val_mae: 0.1998\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0382 - mae: 0.2109 - val_loss: 0.0355 - val_mae: 0.2027\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0374 - mae: 0.2084 - val_loss: 0.0303 - val_mae: 0.1903\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0381 - mae: 0.2100 - val_loss: 0.0335 - val_mae: 0.2000\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0375 - mae: 0.2088 - val_loss: 0.0312 - val_mae: 0.1921\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0377 - mae: 0.2091 - val_loss: 0.0316 - val_mae: 0.1937\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0370 - mae: 0.2066 - val_loss: 0.0344 - val_mae: 0.2028\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0369 - mae: 0.2067 - val_loss: 0.0331 - val_mae: 0.1989\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0369 - mae: 0.2065 - val_loss: 0.0318 - val_mae: 0.1941\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0366 - mae: 0.2058 - val_loss: 0.0321 - val_mae: 0.1962\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0365 - mae: 0.2053 - val_loss: 0.0308 - val_mae: 0.1936\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0364 - mae: 0.2054 - val_loss: 0.0306 - val_mae: 0.1902\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0362 - mae: 0.2044 - val_loss: 0.0315 - val_mae: 0.1920\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0357 - mae: 0.2032 - val_loss: 0.0311 - val_mae: 0.1911\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0358 - mae: 0.2033 - val_loss: 0.0301 - val_mae: 0.1884\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0363 - mae: 0.2047 - val_loss: 0.0301 - val_mae: 0.1883\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0355 - mae: 0.2026 - val_loss: 0.0305 - val_mae: 0.1898\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0353 - mae: 0.2018 - val_loss: 0.0358 - val_mae: 0.2051\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0353 - mae: 0.2019 - val_loss: 0.0336 - val_mae: 0.1987\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0350 - mae: 0.2012 - val_loss: 0.0348 - val_mae: 0.2017\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0349 - mae: 0.2007 - val_loss: 0.0312 - val_mae: 0.1912\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0351 - mae: 0.2011 - val_loss: 0.0298 - val_mae: 0.1876\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0343 - mae: 0.1994 - val_loss: 0.0311 - val_mae: 0.1906\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0347 - mae: 0.1999 - val_loss: 0.0315 - val_mae: 0.1938\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0347 - mae: 0.1996 - val_loss: 0.0368 - val_mae: 0.2078\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0347 - mae: 0.1998 - val_loss: 0.0306 - val_mae: 0.1899\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0343 - mae: 0.1989 - val_loss: 0.0352 - val_mae: 0.2007\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0339 - mae: 0.1977 - val_loss: 0.0315 - val_mae: 0.1924\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0341 - mae: 0.1979 - val_loss: 0.0297 - val_mae: 0.1859\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0340 - mae: 0.1978 - val_loss: 0.0326 - val_mae: 0.1939\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0338 - mae: 0.1975 - val_loss: 0.0308 - val_mae: 0.1891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:54:56,526] Trial 33 finished with value: 0.030832109972834587 and parameters: {'learning_rate': 6.24312152612077e-05, 'dropout_rate': 0.10252234219024534, 'batch_size': 32, 'epochs': 100}. Best is trial 33 with value: 0.030832109972834587.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - 4s 58ms/step - loss: 0.3704 - mae: 0.7776 - val_loss: 7.5957 - val_mae: 8.2768\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.3324 - mae: 0.7242 - val_loss: 6.0216 - val_mae: 6.6971\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.3081 - mae: 0.6887 - val_loss: 5.5126 - val_mae: 6.1702\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.2894 - mae: 0.6617 - val_loss: 4.4447 - val_mae: 5.0765\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.2725 - mae: 0.6365 - val_loss: 3.6160 - val_mae: 4.2450\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.2598 - mae: 0.6195 - val_loss: 2.9576 - val_mae: 3.5970\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.2417 - mae: 0.5911 - val_loss: 2.1019 - val_mae: 2.7365\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.2305 - mae: 0.5760 - val_loss: 1.6883 - val_mae: 2.3120\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.2195 - mae: 0.5600 - val_loss: 1.3627 - val_mae: 1.9494\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.2072 - mae: 0.5406 - val_loss: 1.1015 - val_mae: 1.6605\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.2014 - mae: 0.5321 - val_loss: 1.0201 - val_mae: 1.5419\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.1920 - mae: 0.5174 - val_loss: 0.8017 - val_mae: 1.2901\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.1846 - mae: 0.5053 - val_loss: 0.6000 - val_mae: 1.0467\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.1716 - mae: 0.4842 - val_loss: 0.4168 - val_mae: 0.8409\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.1685 - mae: 0.4803 - val_loss: 0.4559 - val_mae: 0.8912\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.1627 - mae: 0.4688 - val_loss: 0.2238 - val_mae: 0.5775\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.1575 - mae: 0.4598 - val_loss: 0.1758 - val_mae: 0.5001\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.1514 - mae: 0.4514 - val_loss: 0.1774 - val_mae: 0.5028\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.1476 - mae: 0.4446 - val_loss: 0.2196 - val_mae: 0.5809\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.1429 - mae: 0.4344 - val_loss: 0.2632 - val_mae: 0.6443\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.1400 - mae: 0.4301 - val_loss: 0.2444 - val_mae: 0.6145\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.1335 - mae: 0.4191 - val_loss: 0.1767 - val_mae: 0.5024\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.1305 - mae: 0.4128 - val_loss: 0.1872 - val_mae: 0.5231\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.1263 - mae: 0.4044 - val_loss: 0.1149 - val_mae: 0.3937\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.1234 - mae: 0.3995 - val_loss: 0.1065 - val_mae: 0.3811\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.1196 - mae: 0.3930 - val_loss: 0.1065 - val_mae: 0.3879\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.1214 - mae: 0.3941 - val_loss: 0.1181 - val_mae: 0.4060\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.1116 - mae: 0.3785 - val_loss: 0.1030 - val_mae: 0.3749\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.1126 - mae: 0.3792 - val_loss: 0.1349 - val_mae: 0.4331\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.1107 - mae: 0.3739 - val_loss: 0.1048 - val_mae: 0.3728\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.1068 - mae: 0.3672 - val_loss: 0.0977 - val_mae: 0.3572\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.1007 - mae: 0.3573 - val_loss: 0.1029 - val_mae: 0.3661\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.1032 - mae: 0.3590 - val_loss: 0.0862 - val_mae: 0.3304\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.0988 - mae: 0.3517 - val_loss: 0.0796 - val_mae: 0.3177\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.1002 - mae: 0.3523 - val_loss: 0.0776 - val_mae: 0.3142\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.0938 - mae: 0.3421 - val_loss: 0.0779 - val_mae: 0.3131\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0916 - mae: 0.3369 - val_loss: 0.0861 - val_mae: 0.3326\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0907 - mae: 0.3344 - val_loss: 0.0876 - val_mae: 0.3370\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.0882 - mae: 0.3290 - val_loss: 0.0746 - val_mae: 0.3087\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.0856 - mae: 0.3245 - val_loss: 0.0742 - val_mae: 0.3087\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0854 - mae: 0.3234 - val_loss: 0.0637 - val_mae: 0.2866\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.0847 - mae: 0.3214 - val_loss: 0.0604 - val_mae: 0.2779\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.0840 - mae: 0.3187 - val_loss: 0.0706 - val_mae: 0.3082\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.0798 - mae: 0.3100 - val_loss: 0.0691 - val_mae: 0.3024\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.0805 - mae: 0.3110 - val_loss: 0.0637 - val_mae: 0.2912\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.0741 - mae: 0.2989 - val_loss: 0.0669 - val_mae: 0.2993\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 0s 29ms/step - loss: 0.0795 - mae: 0.3054 - val_loss: 0.0621 - val_mae: 0.2836\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 0s 29ms/step - loss: 0.0761 - mae: 0.2988 - val_loss: 0.0625 - val_mae: 0.2788\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 0s 30ms/step - loss: 0.0729 - mae: 0.2933 - val_loss: 0.0547 - val_mae: 0.2584\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 0s 30ms/step - loss: 0.0707 - mae: 0.2883 - val_loss: 0.0573 - val_mae: 0.2663\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 0s 30ms/step - loss: 0.0703 - mae: 0.2867 - val_loss: 0.0588 - val_mae: 0.2722\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.0673 - mae: 0.2811 - val_loss: 0.0560 - val_mae: 0.2612\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.0664 - mae: 0.2793 - val_loss: 0.0519 - val_mae: 0.2533\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.0666 - mae: 0.2780 - val_loss: 0.0486 - val_mae: 0.2470\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.0641 - mae: 0.2732 - val_loss: 0.0509 - val_mae: 0.2575\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.0639 - mae: 0.2723 - val_loss: 0.0505 - val_mae: 0.2552\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.0626 - mae: 0.2695 - val_loss: 0.0490 - val_mae: 0.2436\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.0614 - mae: 0.2661 - val_loss: 0.0460 - val_mae: 0.2381\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0603 - mae: 0.2636 - val_loss: 0.0441 - val_mae: 0.2359\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.0595 - mae: 0.2638 - val_loss: 0.0446 - val_mae: 0.2362\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.0574 - mae: 0.2571 - val_loss: 0.0447 - val_mae: 0.2378\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0596 - mae: 0.2612 - val_loss: 0.0477 - val_mae: 0.2442\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.0566 - mae: 0.2555 - val_loss: 0.0418 - val_mae: 0.2307\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.0552 - mae: 0.2530 - val_loss: 0.0438 - val_mae: 0.2353\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.0584 - mae: 0.2556 - val_loss: 0.0418 - val_mae: 0.2360\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0567 - mae: 0.2545 - val_loss: 0.0429 - val_mae: 0.2355\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0562 - mae: 0.2521 - val_loss: 0.0382 - val_mae: 0.2213\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0535 - mae: 0.2484 - val_loss: 0.0385 - val_mae: 0.2242\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.0555 - mae: 0.2512 - val_loss: 0.0391 - val_mae: 0.2236\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.0531 - mae: 0.2467 - val_loss: 0.0382 - val_mae: 0.2200\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0503 - mae: 0.2434 - val_loss: 0.0396 - val_mae: 0.2245\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.0557 - mae: 0.2499 - val_loss: 0.0385 - val_mae: 0.2213\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.0527 - mae: 0.2452 - val_loss: 0.0390 - val_mae: 0.2268\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0530 - mae: 0.2463 - val_loss: 0.0383 - val_mae: 0.2213\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0509 - mae: 0.2430 - val_loss: 0.0445 - val_mae: 0.2374\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.0516 - mae: 0.2440 - val_loss: 0.0450 - val_mae: 0.2374\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.0514 - mae: 0.2419 - val_loss: 0.0464 - val_mae: 0.2407\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.0521 - mae: 0.2422 - val_loss: 0.0446 - val_mae: 0.2359\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.0506 - mae: 0.2416 - val_loss: 0.0406 - val_mae: 0.2262\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0498 - mae: 0.2386 - val_loss: 0.0426 - val_mae: 0.2361\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.0483 - mae: 0.2371 - val_loss: 0.0414 - val_mae: 0.2309\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.0490 - mae: 0.2375 - val_loss: 0.0399 - val_mae: 0.2241\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.0480 - mae: 0.2352 - val_loss: 0.0388 - val_mae: 0.2199\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.0507 - mae: 0.2383 - val_loss: 0.0410 - val_mae: 0.2347\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.0483 - mae: 0.2340 - val_loss: 0.0392 - val_mae: 0.2265\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.0469 - mae: 0.2325 - val_loss: 0.0575 - val_mae: 0.2815\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.0471 - mae: 0.2320 - val_loss: 0.0557 - val_mae: 0.2797\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.0473 - mae: 0.2330 - val_loss: 0.0437 - val_mae: 0.2402\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.0471 - mae: 0.2315 - val_loss: 0.0348 - val_mae: 0.2103\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.0477 - mae: 0.2328 - val_loss: 0.0392 - val_mae: 0.2280\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0458 - mae: 0.2312 - val_loss: 0.0466 - val_mae: 0.2453\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.0479 - mae: 0.2313 - val_loss: 0.0619 - val_mae: 0.2923\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.0467 - mae: 0.2308 - val_loss: 0.0664 - val_mae: 0.2970\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.0466 - mae: 0.2308 - val_loss: 0.0530 - val_mae: 0.2633\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.0456 - mae: 0.2278 - val_loss: 0.0462 - val_mae: 0.2445\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.0452 - mae: 0.2277 - val_loss: 0.0516 - val_mae: 0.2576\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 0s 29ms/step - loss: 0.0456 - mae: 0.2269 - val_loss: 0.0538 - val_mae: 0.2625\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.0424 - mae: 0.2217 - val_loss: 0.0357 - val_mae: 0.2145\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 0s 30ms/step - loss: 0.0452 - mae: 0.2263 - val_loss: 0.0409 - val_mae: 0.2330\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 0s 27ms/step - loss: 0.0442 - mae: 0.2241 - val_loss: 0.0436 - val_mae: 0.2325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:55:40,702] Trial 34 finished with value: 0.04355980455875397 and parameters: {'learning_rate': 0.0001917771533057372, 'dropout_rate': 0.10437601547960944, 'batch_size': 256, 'epochs': 100}. Best is trial 33 with value: 0.030832109972834587.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "120/120 [==============================] - 3s 8ms/step - loss: 1.3059 - mae: 1.8111 - val_loss: 0.2221 - val_mae: 0.6040\n",
            "Epoch 2/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 1.2042 - mae: 1.7010 - val_loss: 0.2127 - val_mae: 0.5956\n",
            "Epoch 3/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 1.1847 - mae: 1.6784 - val_loss: 0.2079 - val_mae: 0.5850\n",
            "Epoch 4/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 1.1420 - mae: 1.6316 - val_loss: 0.2026 - val_mae: 0.5747\n",
            "Epoch 5/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 1.0844 - mae: 1.5689 - val_loss: 0.1948 - val_mae: 0.5599\n",
            "Epoch 6/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 1.0040 - mae: 1.4794 - val_loss: 0.1836 - val_mae: 0.5402\n",
            "Epoch 7/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.9912 - mae: 1.4638 - val_loss: 0.1748 - val_mae: 0.5223\n",
            "Epoch 8/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.9704 - mae: 1.4405 - val_loss: 0.1675 - val_mae: 0.5077\n",
            "Epoch 9/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.8940 - mae: 1.3529 - val_loss: 0.1621 - val_mae: 0.4950\n",
            "Epoch 10/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.8617 - mae: 1.3168 - val_loss: 0.1542 - val_mae: 0.4791\n",
            "Epoch 11/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.7957 - mae: 1.2432 - val_loss: 0.1461 - val_mae: 0.4621\n",
            "Epoch 12/50\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.7387 - mae: 1.1772 - val_loss: 0.1394 - val_mae: 0.4476\n",
            "Epoch 13/50\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.7258 - mae: 1.1589 - val_loss: 0.1324 - val_mae: 0.4316\n",
            "Epoch 14/50\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.6974 - mae: 1.1254 - val_loss: 0.1274 - val_mae: 0.4212\n",
            "Epoch 15/50\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.6602 - mae: 1.0828 - val_loss: 0.1213 - val_mae: 0.4063\n",
            "Epoch 16/50\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.6279 - mae: 1.0431 - val_loss: 0.1155 - val_mae: 0.3934\n",
            "Epoch 17/50\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5858 - mae: 0.9948 - val_loss: 0.1112 - val_mae: 0.3845\n",
            "Epoch 18/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.5531 - mae: 0.9549 - val_loss: 0.1065 - val_mae: 0.3739\n",
            "Epoch 19/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5132 - mae: 0.9063 - val_loss: 0.1019 - val_mae: 0.3643\n",
            "Epoch 20/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4717 - mae: 0.8539 - val_loss: 0.0982 - val_mae: 0.3557\n",
            "Epoch 21/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4521 - mae: 0.8280 - val_loss: 0.0932 - val_mae: 0.3444\n",
            "Epoch 22/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.4352 - mae: 0.8079 - val_loss: 0.0891 - val_mae: 0.3347\n",
            "Epoch 23/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.4086 - mae: 0.7737 - val_loss: 0.0858 - val_mae: 0.3273\n",
            "Epoch 24/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3794 - mae: 0.7355 - val_loss: 0.0829 - val_mae: 0.3193\n",
            "Epoch 25/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3483 - mae: 0.6954 - val_loss: 0.0799 - val_mae: 0.3131\n",
            "Epoch 26/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3404 - mae: 0.6871 - val_loss: 0.0771 - val_mae: 0.3071\n",
            "Epoch 27/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3140 - mae: 0.6497 - val_loss: 0.0744 - val_mae: 0.3005\n",
            "Epoch 28/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2869 - mae: 0.6144 - val_loss: 0.0717 - val_mae: 0.2950\n",
            "Epoch 29/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2683 - mae: 0.5890 - val_loss: 0.0695 - val_mae: 0.2904\n",
            "Epoch 30/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2499 - mae: 0.5625 - val_loss: 0.0672 - val_mae: 0.2857\n",
            "Epoch 31/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2391 - mae: 0.5444 - val_loss: 0.0653 - val_mae: 0.2814\n",
            "Epoch 32/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2217 - mae: 0.5213 - val_loss: 0.0634 - val_mae: 0.2777\n",
            "Epoch 33/50\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.2046 - mae: 0.4973 - val_loss: 0.0617 - val_mae: 0.2742\n",
            "Epoch 34/50\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1931 - mae: 0.4817 - val_loss: 0.0602 - val_mae: 0.2712\n",
            "Epoch 35/50\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1740 - mae: 0.4549 - val_loss: 0.0587 - val_mae: 0.2685\n",
            "Epoch 36/50\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1698 - mae: 0.4463 - val_loss: 0.0572 - val_mae: 0.2657\n",
            "Epoch 37/50\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1542 - mae: 0.4231 - val_loss: 0.0560 - val_mae: 0.2636\n",
            "Epoch 38/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1392 - mae: 0.4017 - val_loss: 0.0548 - val_mae: 0.2613\n",
            "Epoch 39/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1243 - mae: 0.3796 - val_loss: 0.0535 - val_mae: 0.2590\n",
            "Epoch 40/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1226 - mae: 0.3763 - val_loss: 0.0526 - val_mae: 0.2572\n",
            "Epoch 41/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1121 - mae: 0.3595 - val_loss: 0.0517 - val_mae: 0.2558\n",
            "Epoch 42/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1059 - mae: 0.3504 - val_loss: 0.0509 - val_mae: 0.2542\n",
            "Epoch 43/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0985 - mae: 0.3379 - val_loss: 0.0500 - val_mae: 0.2526\n",
            "Epoch 44/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0915 - mae: 0.3270 - val_loss: 0.0493 - val_mae: 0.2515\n",
            "Epoch 45/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0838 - mae: 0.3139 - val_loss: 0.0485 - val_mae: 0.2503\n",
            "Epoch 46/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0770 - mae: 0.3023 - val_loss: 0.0478 - val_mae: 0.2493\n",
            "Epoch 47/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0744 - mae: 0.2978 - val_loss: 0.0472 - val_mae: 0.2483\n",
            "Epoch 48/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0695 - mae: 0.2904 - val_loss: 0.0467 - val_mae: 0.2476\n",
            "Epoch 49/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0645 - mae: 0.2812 - val_loss: 0.0461 - val_mae: 0.2467\n",
            "Epoch 50/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0603 - mae: 0.2739 - val_loss: 0.0456 - val_mae: 0.2459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:56:19,634] Trial 35 finished with value: 0.04559272155165672 and parameters: {'learning_rate': 8.193534827221171e-05, 'dropout_rate': 0.8847193380779347, 'batch_size': 32, 'epochs': 50}. Best is trial 33 with value: 0.030832109972834587.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 4s 11ms/step - loss: 0.7920 - mae: 1.2660 - val_loss: 0.1273 - val_mae: 0.4289\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4857 - mae: 0.8962 - val_loss: 0.0866 - val_mae: 0.3369\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2691 - mae: 0.6146 - val_loss: 0.0598 - val_mae: 0.2774\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1435 - mae: 0.4242 - val_loss: 0.0509 - val_mae: 0.2583\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0733 - mae: 0.3017 - val_loss: 0.0447 - val_mae: 0.2459\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0483 - mae: 0.2498 - val_loss: 0.0420 - val_mae: 0.2391\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0421 - mae: 0.2363 - val_loss: 0.0406 - val_mae: 0.2350\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0406 - mae: 0.2321 - val_loss: 0.0401 - val_mae: 0.2321\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0402 - mae: 0.2299 - val_loss: 0.0399 - val_mae: 0.2301\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0398 - mae: 0.2286 - val_loss: 0.0399 - val_mae: 0.2290\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0398 - mae: 0.2274 - val_loss: 0.0399 - val_mae: 0.2296\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2275 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0398 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2293\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2285\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2285\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 2s 15ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.0398 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2288\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 2s 13ms/step - loss: 0.0397 - mae: 0.2273 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 0.0398 - mae: 0.2271 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0397 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2285\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0398 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2287\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2285\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2271 - val_loss: 0.0398 - val_mae: 0.2279\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2286\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2287\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2285\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2285\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2272 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0397 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2285\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2288\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2286\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2292\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2285\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0397 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2285\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0397 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2265 - val_loss: 0.0398 - val_mae: 0.2287\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2284\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2265 - val_loss: 0.0398 - val_mae: 0.2287\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2286\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2285\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2287\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2286\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2265 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2285\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2265 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2265 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0395 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:57:42,086] Trial 36 finished with value: 0.03982503339648247 and parameters: {'learning_rate': 0.0007363204737421515, 'dropout_rate': 0.7648231955433428, 'batch_size': 32, 'epochs': 100}. Best is trial 33 with value: 0.030832109972834587.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - 3s 35ms/step - loss: 0.5131 - mae: 0.9574 - val_loss: 4.8516 - val_mae: 5.5055\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.5158 - mae: 0.9617 - val_loss: 3.1073 - val_mae: 3.7595\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.5209 - mae: 0.9669 - val_loss: 2.1751 - val_mae: 2.8163\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.5103 - mae: 0.9528 - val_loss: 1.6037 - val_mae: 2.2295\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.5048 - mae: 0.9480 - val_loss: 1.1799 - val_mae: 1.7821\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.5020 - mae: 0.9423 - val_loss: 0.8586 - val_mae: 1.4256\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.5079 - mae: 0.9510 - val_loss: 0.6272 - val_mae: 1.1439\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.5000 - mae: 0.9404 - val_loss: 0.4628 - val_mae: 0.9297\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.4940 - mae: 0.9350 - val_loss: 0.3543 - val_mae: 0.7872\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.5053 - mae: 0.9466 - val_loss: 0.2856 - val_mae: 0.6944\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.5099 - mae: 0.9522 - val_loss: 0.2332 - val_mae: 0.6163\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.4956 - mae: 0.9340 - val_loss: 0.2114 - val_mae: 0.5811\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.5035 - mae: 0.9441 - val_loss: 0.1970 - val_mae: 0.5562\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.4998 - mae: 0.9396 - val_loss: 0.1882 - val_mae: 0.5392\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.4932 - mae: 0.9324 - val_loss: 0.1869 - val_mae: 0.5353\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.4934 - mae: 0.9319 - val_loss: 0.1832 - val_mae: 0.5276\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.4885 - mae: 0.9269 - val_loss: 0.1816 - val_mae: 0.5228\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.4864 - mae: 0.9228 - val_loss: 0.1774 - val_mae: 0.5128\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.4894 - mae: 0.9264 - val_loss: 0.1805 - val_mae: 0.5157\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.4922 - mae: 0.9306 - val_loss: 0.1872 - val_mae: 0.5276\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.4854 - mae: 0.9220 - val_loss: 0.1898 - val_mae: 0.5320\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.4867 - mae: 0.9236 - val_loss: 0.1975 - val_mae: 0.5469\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.4896 - mae: 0.9271 - val_loss: 0.2014 - val_mae: 0.5559\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.4889 - mae: 0.9266 - val_loss: 0.2021 - val_mae: 0.5558\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.4839 - mae: 0.9207 - val_loss: 0.2060 - val_mae: 0.5642\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.4857 - mae: 0.9235 - val_loss: 0.2137 - val_mae: 0.5794\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.4810 - mae: 0.9170 - val_loss: 0.2173 - val_mae: 0.5850\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.4734 - mae: 0.9058 - val_loss: 0.2168 - val_mae: 0.5862\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.4776 - mae: 0.9111 - val_loss: 0.2121 - val_mae: 0.5773\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.4730 - mae: 0.9059 - val_loss: 0.2132 - val_mae: 0.5795\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.4744 - mae: 0.9082 - val_loss: 0.2130 - val_mae: 0.5789\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.4683 - mae: 0.8998 - val_loss: 0.2120 - val_mae: 0.5749\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.4699 - mae: 0.9024 - val_loss: 0.2143 - val_mae: 0.5814\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 0s 27ms/step - loss: 0.4719 - mae: 0.9034 - val_loss: 0.2115 - val_mae: 0.5743\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.4667 - mae: 0.8977 - val_loss: 0.2043 - val_mae: 0.5581\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.4699 - mae: 0.9011 - val_loss: 0.2096 - val_mae: 0.5682\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.4653 - mae: 0.8954 - val_loss: 0.2100 - val_mae: 0.5702\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.4666 - mae: 0.8968 - val_loss: 0.2162 - val_mae: 0.5822\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.4662 - mae: 0.8963 - val_loss: 0.2125 - val_mae: 0.5759\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.4634 - mae: 0.8934 - val_loss: 0.2089 - val_mae: 0.5703\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.4598 - mae: 0.8902 - val_loss: 0.2071 - val_mae: 0.5668\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.4633 - mae: 0.8924 - val_loss: 0.2100 - val_mae: 0.5722\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.4581 - mae: 0.8861 - val_loss: 0.2088 - val_mae: 0.5705\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.4591 - mae: 0.8871 - val_loss: 0.2058 - val_mae: 0.5650\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.4621 - mae: 0.8910 - val_loss: 0.2006 - val_mae: 0.5553\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.4596 - mae: 0.8874 - val_loss: 0.1981 - val_mae: 0.5487\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.4610 - mae: 0.8907 - val_loss: 0.1968 - val_mae: 0.5448\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.4562 - mae: 0.8826 - val_loss: 0.2002 - val_mae: 0.5511\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.4552 - mae: 0.8823 - val_loss: 0.2012 - val_mae: 0.5521\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.4491 - mae: 0.8740 - val_loss: 0.1981 - val_mae: 0.5460\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.4523 - mae: 0.8775 - val_loss: 0.1939 - val_mae: 0.5370\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.4487 - mae: 0.8738 - val_loss: 0.1934 - val_mae: 0.5377\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.4504 - mae: 0.8747 - val_loss: 0.1948 - val_mae: 0.5418\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.4507 - mae: 0.8764 - val_loss: 0.2001 - val_mae: 0.5515\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.4463 - mae: 0.8681 - val_loss: 0.1981 - val_mae: 0.5481\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.4408 - mae: 0.8632 - val_loss: 0.1971 - val_mae: 0.5460\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.4431 - mae: 0.8671 - val_loss: 0.1912 - val_mae: 0.5347\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.4366 - mae: 0.8575 - val_loss: 0.1892 - val_mae: 0.5327\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.4431 - mae: 0.8656 - val_loss: 0.1901 - val_mae: 0.5358\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.4487 - mae: 0.8717 - val_loss: 0.1937 - val_mae: 0.5435\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.4438 - mae: 0.8649 - val_loss: 0.1939 - val_mae: 0.5437\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.4404 - mae: 0.8623 - val_loss: 0.1957 - val_mae: 0.5471\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.4397 - mae: 0.8615 - val_loss: 0.1918 - val_mae: 0.5404\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.4377 - mae: 0.8587 - val_loss: 0.1886 - val_mae: 0.5337\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.4371 - mae: 0.8580 - val_loss: 0.1848 - val_mae: 0.5258\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.4382 - mae: 0.8595 - val_loss: 0.1831 - val_mae: 0.5224\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.4298 - mae: 0.8469 - val_loss: 0.1860 - val_mae: 0.5282\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.4304 - mae: 0.8477 - val_loss: 0.1909 - val_mae: 0.5384\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.4320 - mae: 0.8517 - val_loss: 0.1911 - val_mae: 0.5392\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.4389 - mae: 0.8590 - val_loss: 0.1912 - val_mae: 0.5383\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.4269 - mae: 0.8439 - val_loss: 0.1866 - val_mae: 0.5280\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.4308 - mae: 0.8483 - val_loss: 0.1885 - val_mae: 0.5324\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.4269 - mae: 0.8441 - val_loss: 0.1880 - val_mae: 0.5313\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.4249 - mae: 0.8398 - val_loss: 0.1853 - val_mae: 0.5267\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.4227 - mae: 0.8400 - val_loss: 0.1894 - val_mae: 0.5354\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.4278 - mae: 0.8432 - val_loss: 0.1894 - val_mae: 0.5355\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.4262 - mae: 0.8439 - val_loss: 0.1889 - val_mae: 0.5343\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.4210 - mae: 0.8355 - val_loss: 0.1877 - val_mae: 0.5317\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 0s 28ms/step - loss: 0.4207 - mae: 0.8371 - val_loss: 0.1853 - val_mae: 0.5275\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.4240 - mae: 0.8401 - val_loss: 0.1804 - val_mae: 0.5166\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.4148 - mae: 0.8271 - val_loss: 0.1764 - val_mae: 0.5085\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.4187 - mae: 0.8333 - val_loss: 0.1801 - val_mae: 0.5160\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.4233 - mae: 0.8375 - val_loss: 0.1796 - val_mae: 0.5138\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.4234 - mae: 0.8391 - val_loss: 0.1823 - val_mae: 0.5191\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.4185 - mae: 0.8324 - val_loss: 0.1816 - val_mae: 0.5183\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.4128 - mae: 0.8238 - val_loss: 0.1802 - val_mae: 0.5155\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.4147 - mae: 0.8270 - val_loss: 0.1818 - val_mae: 0.5202\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.4144 - mae: 0.8272 - val_loss: 0.1822 - val_mae: 0.5219\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.4181 - mae: 0.8324 - val_loss: 0.1813 - val_mae: 0.5199\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.4128 - mae: 0.8246 - val_loss: 0.1811 - val_mae: 0.5197\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.4055 - mae: 0.8159 - val_loss: 0.1799 - val_mae: 0.5174\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.4075 - mae: 0.8168 - val_loss: 0.1788 - val_mae: 0.5156\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.4077 - mae: 0.8177 - val_loss: 0.1829 - val_mae: 0.5241\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.4055 - mae: 0.8161 - val_loss: 0.1822 - val_mae: 0.5222\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.4062 - mae: 0.8151 - val_loss: 0.1814 - val_mae: 0.5206\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.4081 - mae: 0.8181 - val_loss: 0.1824 - val_mae: 0.5222\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.4047 - mae: 0.8135 - val_loss: 0.1831 - val_mae: 0.5229\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3999 - mae: 0.8076 - val_loss: 0.1776 - val_mae: 0.5113\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.3987 - mae: 0.8056 - val_loss: 0.1845 - val_mae: 0.5246\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.3990 - mae: 0.8071 - val_loss: 0.1815 - val_mae: 0.5190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:58:25,559] Trial 37 finished with value: 0.18149249255657196 and parameters: {'learning_rate': 1.2264194415532094e-05, 'dropout_rate': 0.32680570560434724, 'batch_size': 256, 'epochs': 100}. Best is trial 33 with value: 0.030832109972834587.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 4s 11ms/step - loss: 0.4416 - mae: 0.8761 - val_loss: 0.6538 - val_mae: 1.1191\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4036 - mae: 0.8271 - val_loss: 0.1926 - val_mae: 0.5405\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3742 - mae: 0.7879 - val_loss: 0.1975 - val_mae: 0.5462\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3500 - mae: 0.7553 - val_loss: 0.1854 - val_mae: 0.5194\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3242 - mae: 0.7196 - val_loss: 0.1876 - val_mae: 0.5276\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.3024 - mae: 0.6895 - val_loss: 0.1873 - val_mae: 0.5298\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2813 - mae: 0.6598 - val_loss: 0.1804 - val_mae: 0.5208\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2620 - mae: 0.6307 - val_loss: 0.1485 - val_mae: 0.4613\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2463 - mae: 0.6075 - val_loss: 0.1506 - val_mae: 0.4682\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.2282 - mae: 0.5788 - val_loss: 0.1418 - val_mae: 0.4497\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2114 - mae: 0.5538 - val_loss: 0.1241 - val_mae: 0.4171\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2001 - mae: 0.5359 - val_loss: 0.1114 - val_mae: 0.3897\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1865 - mae: 0.5139 - val_loss: 0.1242 - val_mae: 0.4212\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1777 - mae: 0.4992 - val_loss: 0.0996 - val_mae: 0.3679\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1658 - mae: 0.4789 - val_loss: 0.1127 - val_mae: 0.3980\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1584 - mae: 0.4658 - val_loss: 0.1001 - val_mae: 0.3733\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1495 - mae: 0.4499 - val_loss: 0.0936 - val_mae: 0.3574\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1418 - mae: 0.4358 - val_loss: 0.0808 - val_mae: 0.3254\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1341 - mae: 0.4218 - val_loss: 0.0801 - val_mae: 0.3258\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1282 - mae: 0.4098 - val_loss: 0.0792 - val_mae: 0.3240\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1225 - mae: 0.3991 - val_loss: 0.0706 - val_mae: 0.3034\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1169 - mae: 0.3884 - val_loss: 0.0682 - val_mae: 0.2999\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1094 - mae: 0.3745 - val_loss: 0.0655 - val_mae: 0.2935\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1062 - mae: 0.3677 - val_loss: 0.0621 - val_mae: 0.2845\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1022 - mae: 0.3585 - val_loss: 0.0579 - val_mae: 0.2743\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0974 - mae: 0.3490 - val_loss: 0.0568 - val_mae: 0.2716\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0934 - mae: 0.3414 - val_loss: 0.0528 - val_mae: 0.2626\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0897 - mae: 0.3334 - val_loss: 0.0513 - val_mae: 0.2598\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0861 - mae: 0.3256 - val_loss: 0.0507 - val_mae: 0.2557\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0836 - mae: 0.3199 - val_loss: 0.0487 - val_mae: 0.2511\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0804 - mae: 0.3119 - val_loss: 0.0468 - val_mae: 0.2449\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0805 - mae: 0.3109 - val_loss: 0.0445 - val_mae: 0.2394\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0754 - mae: 0.3007 - val_loss: 0.0436 - val_mae: 0.2390\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0749 - mae: 0.2985 - val_loss: 0.0424 - val_mae: 0.2338\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0723 - mae: 0.2920 - val_loss: 0.0420 - val_mae: 0.2317\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0697 - mae: 0.2872 - val_loss: 0.0401 - val_mae: 0.2268\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0680 - mae: 0.2829 - val_loss: 0.0397 - val_mae: 0.2266\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0667 - mae: 0.2789 - val_loss: 0.0390 - val_mae: 0.2248\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0655 - mae: 0.2756 - val_loss: 0.0373 - val_mae: 0.2185\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0629 - mae: 0.2704 - val_loss: 0.0365 - val_mae: 0.2154\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0618 - mae: 0.2681 - val_loss: 0.0391 - val_mae: 0.2211\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0605 - mae: 0.2639 - val_loss: 0.0359 - val_mae: 0.2133\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0604 - mae: 0.2638 - val_loss: 0.0353 - val_mae: 0.2118\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0599 - mae: 0.2624 - val_loss: 0.0385 - val_mae: 0.2222\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0579 - mae: 0.2577 - val_loss: 0.0347 - val_mae: 0.2102\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0588 - mae: 0.2580 - val_loss: 0.0345 - val_mae: 0.2078\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0575 - mae: 0.2560 - val_loss: 0.0364 - val_mae: 0.2155\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0561 - mae: 0.2531 - val_loss: 0.0351 - val_mae: 0.2123\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0553 - mae: 0.2518 - val_loss: 0.0340 - val_mae: 0.2078\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0553 - mae: 0.2512 - val_loss: 0.0341 - val_mae: 0.2085\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0535 - mae: 0.2469 - val_loss: 0.0336 - val_mae: 0.2060\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0527 - mae: 0.2455 - val_loss: 0.0337 - val_mae: 0.2067\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0517 - mae: 0.2437 - val_loss: 0.0359 - val_mae: 0.2136\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0515 - mae: 0.2429 - val_loss: 0.0339 - val_mae: 0.2070\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0519 - mae: 0.2428 - val_loss: 0.0335 - val_mae: 0.2040\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0503 - mae: 0.2393 - val_loss: 0.0355 - val_mae: 0.2119\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0501 - mae: 0.2389 - val_loss: 0.0330 - val_mae: 0.2034\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0497 - mae: 0.2379 - val_loss: 0.0357 - val_mae: 0.2124\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0494 - mae: 0.2375 - val_loss: 0.0332 - val_mae: 0.2031\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0485 - mae: 0.2352 - val_loss: 0.0360 - val_mae: 0.2126\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0476 - mae: 0.2332 - val_loss: 0.0353 - val_mae: 0.2105\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0475 - mae: 0.2330 - val_loss: 0.0342 - val_mae: 0.2072\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0475 - mae: 0.2328 - val_loss: 0.0343 - val_mae: 0.2080\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0469 - mae: 0.2312 - val_loss: 0.0328 - val_mae: 0.2027\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0470 - mae: 0.2318 - val_loss: 0.0333 - val_mae: 0.2027\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0468 - mae: 0.2307 - val_loss: 0.0337 - val_mae: 0.2059\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0457 - mae: 0.2291 - val_loss: 0.0356 - val_mae: 0.2080\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0461 - mae: 0.2290 - val_loss: 0.0392 - val_mae: 0.2173\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0459 - mae: 0.2290 - val_loss: 0.0338 - val_mae: 0.2052\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0450 - mae: 0.2263 - val_loss: 0.0335 - val_mae: 0.2053\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0449 - mae: 0.2269 - val_loss: 0.0324 - val_mae: 0.2007\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0439 - mae: 0.2250 - val_loss: 0.0322 - val_mae: 0.1997\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0445 - mae: 0.2251 - val_loss: 0.0348 - val_mae: 0.2057\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0444 - mae: 0.2247 - val_loss: 0.0326 - val_mae: 0.2011\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0435 - mae: 0.2235 - val_loss: 0.0319 - val_mae: 0.1984\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0431 - mae: 0.2221 - val_loss: 0.0336 - val_mae: 0.2023\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0428 - mae: 0.2217 - val_loss: 0.0341 - val_mae: 0.2033\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0426 - mae: 0.2211 - val_loss: 0.0322 - val_mae: 0.1988\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0423 - mae: 0.2202 - val_loss: 0.0367 - val_mae: 0.2132\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0419 - mae: 0.2199 - val_loss: 0.0319 - val_mae: 0.1982\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0415 - mae: 0.2183 - val_loss: 0.0324 - val_mae: 0.2002\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0411 - mae: 0.2176 - val_loss: 0.0325 - val_mae: 0.2001\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0408 - mae: 0.2173 - val_loss: 0.0319 - val_mae: 0.1979\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0412 - mae: 0.2178 - val_loss: 0.0341 - val_mae: 0.2032\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0405 - mae: 0.2166 - val_loss: 0.0333 - val_mae: 0.2013\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0407 - mae: 0.2166 - val_loss: 0.0339 - val_mae: 0.2053\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0401 - mae: 0.2156 - val_loss: 0.0330 - val_mae: 0.2022\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0403 - mae: 0.2157 - val_loss: 0.0355 - val_mae: 0.2094\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0404 - mae: 0.2160 - val_loss: 0.0314 - val_mae: 0.1956\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0399 - mae: 0.2145 - val_loss: 0.0358 - val_mae: 0.2071\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0396 - mae: 0.2138 - val_loss: 0.0346 - val_mae: 0.2069\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0391 - mae: 0.2127 - val_loss: 0.0324 - val_mae: 0.1997\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0384 - mae: 0.2116 - val_loss: 0.0335 - val_mae: 0.2029\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0387 - mae: 0.2120 - val_loss: 0.0337 - val_mae: 0.2036\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0387 - mae: 0.2115 - val_loss: 0.0314 - val_mae: 0.1957\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0389 - mae: 0.2117 - val_loss: 0.0320 - val_mae: 0.1966\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0386 - mae: 0.2116 - val_loss: 0.0321 - val_mae: 0.1980\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0386 - mae: 0.2117 - val_loss: 0.0316 - val_mae: 0.1961\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0386 - mae: 0.2116 - val_loss: 0.0325 - val_mae: 0.2001\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0380 - mae: 0.2101 - val_loss: 0.0318 - val_mae: 0.1974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 18:59:50,415] Trial 38 finished with value: 0.031763650476932526 and parameters: {'learning_rate': 5.448632172459822e-05, 'dropout_rate': 0.21064483303410522, 'batch_size': 32, 'epochs': 100}. Best is trial 33 with value: 0.030832109972834587.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "120/120 [==============================] - 3s 8ms/step - loss: 0.4443 - mae: 0.8788 - val_loss: 1.1415 - val_mae: 1.6997\n",
            "Epoch 2/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.4397 - mae: 0.8733 - val_loss: 0.3737 - val_mae: 0.8062\n",
            "Epoch 3/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4358 - mae: 0.8681 - val_loss: 0.2801 - val_mae: 0.6710\n",
            "Epoch 4/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4307 - mae: 0.8616 - val_loss: 0.2546 - val_mae: 0.6316\n",
            "Epoch 5/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.4313 - mae: 0.8616 - val_loss: 0.2531 - val_mae: 0.6296\n",
            "Epoch 6/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.4322 - mae: 0.8635 - val_loss: 0.2488 - val_mae: 0.6233\n",
            "Epoch 7/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.4282 - mae: 0.8587 - val_loss: 0.2458 - val_mae: 0.6195\n",
            "Epoch 8/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.4219 - mae: 0.8510 - val_loss: 0.2458 - val_mae: 0.6199\n",
            "Epoch 9/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4223 - mae: 0.8500 - val_loss: 0.2436 - val_mae: 0.6136\n",
            "Epoch 10/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4216 - mae: 0.8495 - val_loss: 0.2406 - val_mae: 0.6079\n",
            "Epoch 11/50\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4171 - mae: 0.8440 - val_loss: 0.2401 - val_mae: 0.6089\n",
            "Epoch 12/50\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4148 - mae: 0.8400 - val_loss: 0.2381 - val_mae: 0.6058\n",
            "Epoch 13/50\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4085 - mae: 0.8327 - val_loss: 0.2378 - val_mae: 0.6040\n",
            "Epoch 14/50\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4084 - mae: 0.8318 - val_loss: 0.2357 - val_mae: 0.6013\n",
            "Epoch 15/50\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4079 - mae: 0.8319 - val_loss: 0.2316 - val_mae: 0.5955\n",
            "Epoch 16/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.4032 - mae: 0.8250 - val_loss: 0.2298 - val_mae: 0.5940\n",
            "Epoch 17/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4002 - mae: 0.8215 - val_loss: 0.2277 - val_mae: 0.5907\n",
            "Epoch 18/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3998 - mae: 0.8213 - val_loss: 0.2249 - val_mae: 0.5858\n",
            "Epoch 19/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3978 - mae: 0.8176 - val_loss: 0.2241 - val_mae: 0.5838\n",
            "Epoch 20/50\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.3964 - mae: 0.8167 - val_loss: 0.2246 - val_mae: 0.5842\n",
            "Epoch 21/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3940 - mae: 0.8141 - val_loss: 0.2240 - val_mae: 0.5836\n",
            "Epoch 22/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3893 - mae: 0.8068 - val_loss: 0.2249 - val_mae: 0.5853\n",
            "Epoch 23/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3876 - mae: 0.8042 - val_loss: 0.2227 - val_mae: 0.5823\n",
            "Epoch 24/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3838 - mae: 0.8004 - val_loss: 0.2234 - val_mae: 0.5830\n",
            "Epoch 25/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3832 - mae: 0.7988 - val_loss: 0.2205 - val_mae: 0.5777\n",
            "Epoch 26/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3793 - mae: 0.7947 - val_loss: 0.2203 - val_mae: 0.5785\n",
            "Epoch 27/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3766 - mae: 0.7907 - val_loss: 0.2144 - val_mae: 0.5694\n",
            "Epoch 28/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3747 - mae: 0.7884 - val_loss: 0.2126 - val_mae: 0.5670\n",
            "Epoch 29/50\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.3734 - mae: 0.7868 - val_loss: 0.2130 - val_mae: 0.5693\n",
            "Epoch 30/50\n",
            "120/120 [==============================] - 2s 16ms/step - loss: 0.3697 - mae: 0.7811 - val_loss: 0.2119 - val_mae: 0.5663\n",
            "Epoch 31/50\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3670 - mae: 0.7772 - val_loss: 0.2130 - val_mae: 0.5689\n",
            "Epoch 32/50\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.3669 - mae: 0.7782 - val_loss: 0.2158 - val_mae: 0.5728\n",
            "Epoch 33/50\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.3596 - mae: 0.7684 - val_loss: 0.2117 - val_mae: 0.5670\n",
            "Epoch 34/50\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3602 - mae: 0.7697 - val_loss: 0.2120 - val_mae: 0.5678\n",
            "Epoch 35/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3580 - mae: 0.7657 - val_loss: 0.2066 - val_mae: 0.5584\n",
            "Epoch 36/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3563 - mae: 0.7633 - val_loss: 0.2094 - val_mae: 0.5638\n",
            "Epoch 37/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3533 - mae: 0.7591 - val_loss: 0.2049 - val_mae: 0.5576\n",
            "Epoch 38/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3541 - mae: 0.7602 - val_loss: 0.2036 - val_mae: 0.5550\n",
            "Epoch 39/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3493 - mae: 0.7538 - val_loss: 0.2040 - val_mae: 0.5557\n",
            "Epoch 40/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3457 - mae: 0.7495 - val_loss: 0.2007 - val_mae: 0.5506\n",
            "Epoch 41/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3431 - mae: 0.7459 - val_loss: 0.1996 - val_mae: 0.5491\n",
            "Epoch 42/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3386 - mae: 0.7399 - val_loss: 0.2006 - val_mae: 0.5503\n",
            "Epoch 43/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3412 - mae: 0.7422 - val_loss: 0.2023 - val_mae: 0.5529\n",
            "Epoch 44/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3397 - mae: 0.7412 - val_loss: 0.2002 - val_mae: 0.5505\n",
            "Epoch 45/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3352 - mae: 0.7351 - val_loss: 0.1988 - val_mae: 0.5478\n",
            "Epoch 46/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3337 - mae: 0.7331 - val_loss: 0.1910 - val_mae: 0.5354\n",
            "Epoch 47/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3322 - mae: 0.7304 - val_loss: 0.1926 - val_mae: 0.5380\n",
            "Epoch 48/50\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3282 - mae: 0.7243 - val_loss: 0.1961 - val_mae: 0.5441\n",
            "Epoch 49/50\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3255 - mae: 0.7227 - val_loss: 0.1937 - val_mae: 0.5404\n",
            "Epoch 50/50\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3246 - mae: 0.7214 - val_loss: 0.1918 - val_mae: 0.5369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 19:00:31,688] Trial 39 finished with value: 0.19177094101905823 and parameters: {'learning_rate': 4.0444306743211795e-06, 'dropout_rate': 0.14123460145147043, 'batch_size': 32, 'epochs': 50}. Best is trial 33 with value: 0.030832109972834587.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 4s 9ms/step - loss: 0.3956 - mae: 0.8094 - val_loss: 0.5677 - val_mae: 1.0447\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2801 - mae: 0.6479 - val_loss: 0.3714 - val_mae: 0.7824\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2051 - mae: 0.5360 - val_loss: 0.2327 - val_mae: 0.5741\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1570 - mae: 0.4577 - val_loss: 0.0940 - val_mae: 0.3473\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1233 - mae: 0.3995 - val_loss: 0.1011 - val_mae: 0.3674\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.1035 - mae: 0.3624 - val_loss: 0.0605 - val_mae: 0.2817\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0887 - mae: 0.3331 - val_loss: 0.0530 - val_mae: 0.2610\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0785 - mae: 0.3106 - val_loss: 0.0438 - val_mae: 0.2421\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0702 - mae: 0.2930 - val_loss: 0.0413 - val_mae: 0.2348\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0645 - mae: 0.2807 - val_loss: 0.0402 - val_mae: 0.2304\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0609 - mae: 0.2707 - val_loss: 0.0395 - val_mae: 0.2282\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0573 - mae: 0.2626 - val_loss: 0.0394 - val_mae: 0.2272\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0532 - mae: 0.2526 - val_loss: 0.0397 - val_mae: 0.2262\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0515 - mae: 0.2474 - val_loss: 0.0379 - val_mae: 0.2232\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0486 - mae: 0.2402 - val_loss: 0.0412 - val_mae: 0.2295\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0458 - mae: 0.2332 - val_loss: 0.0390 - val_mae: 0.2217\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0437 - mae: 0.2279 - val_loss: 0.0420 - val_mae: 0.2294\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0432 - mae: 0.2255 - val_loss: 0.0430 - val_mae: 0.2311\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0417 - mae: 0.2221 - val_loss: 0.0434 - val_mae: 0.2317\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0402 - mae: 0.2185 - val_loss: 0.0357 - val_mae: 0.2111\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0391 - mae: 0.2155 - val_loss: 0.0351 - val_mae: 0.2079\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0384 - mae: 0.2140 - val_loss: 0.0344 - val_mae: 0.2093\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0378 - mae: 0.2123 - val_loss: 0.0348 - val_mae: 0.2055\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0374 - mae: 0.2110 - val_loss: 0.0346 - val_mae: 0.2091\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0365 - mae: 0.2085 - val_loss: 0.0373 - val_mae: 0.2145\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0362 - mae: 0.2080 - val_loss: 0.0353 - val_mae: 0.2082\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0358 - mae: 0.2066 - val_loss: 0.0459 - val_mae: 0.2287\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0351 - mae: 0.2046 - val_loss: 0.0401 - val_mae: 0.2179\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0349 - mae: 0.2038 - val_loss: 0.0434 - val_mae: 0.2244\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0344 - mae: 0.2024 - val_loss: 0.0334 - val_mae: 0.2019\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0341 - mae: 0.2013 - val_loss: 0.0329 - val_mae: 0.2008\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0341 - mae: 0.2011 - val_loss: 0.0377 - val_mae: 0.2117\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0335 - mae: 0.1992 - val_loss: 0.0524 - val_mae: 0.2501\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0330 - mae: 0.1974 - val_loss: 0.0452 - val_mae: 0.2318\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0330 - mae: 0.1972 - val_loss: 0.0376 - val_mae: 0.2115\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0327 - mae: 0.1963 - val_loss: 0.0444 - val_mae: 0.2246\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0325 - mae: 0.1950 - val_loss: 0.0355 - val_mae: 0.2045\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0322 - mae: 0.1942 - val_loss: 0.0358 - val_mae: 0.2064\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0321 - mae: 0.1933 - val_loss: 0.0504 - val_mae: 0.2436\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0320 - mae: 0.1932 - val_loss: 0.0400 - val_mae: 0.2164\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0317 - mae: 0.1916 - val_loss: 0.0438 - val_mae: 0.2267\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0316 - mae: 0.1919 - val_loss: 0.0310 - val_mae: 0.1910\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0312 - mae: 0.1902 - val_loss: 0.0311 - val_mae: 0.1907\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0312 - mae: 0.1900 - val_loss: 0.0313 - val_mae: 0.1929\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0311 - mae: 0.1896 - val_loss: 0.0367 - val_mae: 0.2055\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0311 - mae: 0.1896 - val_loss: 0.0805 - val_mae: 0.3142\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0311 - mae: 0.1895 - val_loss: 0.0629 - val_mae: 0.2745\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0310 - mae: 0.1891 - val_loss: 0.0722 - val_mae: 0.2965\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0312 - mae: 0.1897 - val_loss: 0.0630 - val_mae: 0.2730\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0309 - mae: 0.1888 - val_loss: 0.0323 - val_mae: 0.1943\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0308 - mae: 0.1883 - val_loss: 0.0430 - val_mae: 0.2229\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0307 - mae: 0.1880 - val_loss: 0.0307 - val_mae: 0.1951\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0304 - mae: 0.1870 - val_loss: 0.0354 - val_mae: 0.2137\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0307 - mae: 0.1882 - val_loss: 0.0479 - val_mae: 0.2318\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0309 - mae: 0.1884 - val_loss: 0.0376 - val_mae: 0.2079\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0305 - mae: 0.1871 - val_loss: 0.0346 - val_mae: 0.2000\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0309 - mae: 0.1884 - val_loss: 0.0389 - val_mae: 0.2154\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0308 - mae: 0.1886 - val_loss: 0.0327 - val_mae: 0.1952\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0306 - mae: 0.1876 - val_loss: 0.0321 - val_mae: 0.1971\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0306 - mae: 0.1877 - val_loss: 0.0377 - val_mae: 0.2118\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0304 - mae: 0.1868 - val_loss: 0.0432 - val_mae: 0.2319\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0304 - mae: 0.1868 - val_loss: 0.0342 - val_mae: 0.2040\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0304 - mae: 0.1870 - val_loss: 0.0550 - val_mae: 0.2492\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0303 - mae: 0.1866 - val_loss: 0.0327 - val_mae: 0.2049\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0303 - mae: 0.1868 - val_loss: 0.0392 - val_mae: 0.2111\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0302 - mae: 0.1861 - val_loss: 0.0389 - val_mae: 0.2135\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0301 - mae: 0.1859 - val_loss: 0.0355 - val_mae: 0.2025\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0301 - mae: 0.1859 - val_loss: 0.0679 - val_mae: 0.2925\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0302 - mae: 0.1861 - val_loss: 0.0423 - val_mae: 0.2385\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0302 - mae: 0.1862 - val_loss: 0.0386 - val_mae: 0.2198\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0301 - mae: 0.1858 - val_loss: 0.0399 - val_mae: 0.2165\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0299 - mae: 0.1853 - val_loss: 0.0330 - val_mae: 0.1932\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0301 - mae: 0.1857 - val_loss: 0.0363 - val_mae: 0.2058\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0303 - mae: 0.1864 - val_loss: 0.0336 - val_mae: 0.2012\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0301 - mae: 0.1858 - val_loss: 0.0434 - val_mae: 0.2250\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0300 - mae: 0.1857 - val_loss: 0.0521 - val_mae: 0.2423\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0300 - mae: 0.1854 - val_loss: 0.0325 - val_mae: 0.1975\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0300 - mae: 0.1854 - val_loss: 0.0335 - val_mae: 0.2067\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0300 - mae: 0.1855 - val_loss: 0.0371 - val_mae: 0.2152\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0301 - mae: 0.1855 - val_loss: 0.0330 - val_mae: 0.1943\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 0.0302 - mae: 0.1863 - val_loss: 0.0419 - val_mae: 0.2206\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0301 - mae: 0.1861 - val_loss: 0.0321 - val_mae: 0.2003\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0301 - mae: 0.1859 - val_loss: 0.0337 - val_mae: 0.2034\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0301 - mae: 0.1856 - val_loss: 0.0455 - val_mae: 0.2248\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.0300 - mae: 0.1854 - val_loss: 0.0430 - val_mae: 0.2206\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 2s 13ms/step - loss: 0.0300 - mae: 0.1854 - val_loss: 0.0346 - val_mae: 0.2134\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 2s 16ms/step - loss: 0.0302 - mae: 0.1861 - val_loss: 0.0344 - val_mae: 0.1980\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 2s 14ms/step - loss: 0.0303 - mae: 0.1864 - val_loss: 0.0363 - val_mae: 0.2027\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0300 - mae: 0.1853 - val_loss: 0.0355 - val_mae: 0.2013\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0301 - mae: 0.1854 - val_loss: 0.0527 - val_mae: 0.2490\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0298 - mae: 0.1847 - val_loss: 0.0415 - val_mae: 0.2192\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0300 - mae: 0.1853 - val_loss: 0.0398 - val_mae: 0.2210\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0301 - mae: 0.1853 - val_loss: 0.0393 - val_mae: 0.2125\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0300 - mae: 0.1853 - val_loss: 0.0343 - val_mae: 0.1973\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0302 - mae: 0.1859 - val_loss: 0.0376 - val_mae: 0.2062\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0304 - mae: 0.1865 - val_loss: 0.0481 - val_mae: 0.2429\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0301 - mae: 0.1856 - val_loss: 0.0382 - val_mae: 0.2249\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0301 - mae: 0.1856 - val_loss: 0.0326 - val_mae: 0.1980\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0299 - mae: 0.1847 - val_loss: 0.0310 - val_mae: 0.1902\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0302 - mae: 0.1858 - val_loss: 0.0360 - val_mae: 0.2016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 19:01:56,748] Trial 40 finished with value: 0.035994354635477066 and parameters: {'learning_rate': 0.00026146777092219304, 'dropout_rate': 0.20494273016743847, 'batch_size': 32, 'epochs': 100}. Best is trial 33 with value: 0.030832109972834587.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 4s 11ms/step - loss: 0.4593 - mae: 0.8935 - val_loss: 0.5673 - val_mae: 1.0349\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4328 - mae: 0.8585 - val_loss: 0.2067 - val_mae: 0.5621\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4071 - mae: 0.8254 - val_loss: 0.2569 - val_mae: 0.6279\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3868 - mae: 0.7985 - val_loss: 0.1877 - val_mae: 0.5199\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3695 - mae: 0.7740 - val_loss: 0.1703 - val_mae: 0.4878\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3492 - mae: 0.7460 - val_loss: 0.1896 - val_mae: 0.5265\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.3298 - mae: 0.7212 - val_loss: 0.1545 - val_mae: 0.4594\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.3099 - mae: 0.6941 - val_loss: 0.1901 - val_mae: 0.5324\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2948 - mae: 0.6720 - val_loss: 0.1647 - val_mae: 0.4801\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2779 - mae: 0.6471 - val_loss: 0.1390 - val_mae: 0.4407\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2627 - mae: 0.6254 - val_loss: 0.1323 - val_mae: 0.4210\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2452 - mae: 0.6003 - val_loss: 0.1316 - val_mae: 0.4177\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2337 - mae: 0.5825 - val_loss: 0.1169 - val_mae: 0.3901\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2194 - mae: 0.5609 - val_loss: 0.1222 - val_mae: 0.4011\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2049 - mae: 0.5382 - val_loss: 0.1191 - val_mae: 0.4016\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1947 - mae: 0.5220 - val_loss: 0.1084 - val_mae: 0.3814\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1832 - mae: 0.5042 - val_loss: 0.1023 - val_mae: 0.3741\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1747 - mae: 0.4894 - val_loss: 0.0935 - val_mae: 0.3473\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1630 - mae: 0.4706 - val_loss: 0.0926 - val_mae: 0.3423\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1541 - mae: 0.4557 - val_loss: 0.0870 - val_mae: 0.3348\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1473 - mae: 0.4435 - val_loss: 0.0844 - val_mae: 0.3286\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1392 - mae: 0.4301 - val_loss: 0.0807 - val_mae: 0.3231\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1314 - mae: 0.4159 - val_loss: 0.0765 - val_mae: 0.3106\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1272 - mae: 0.4080 - val_loss: 0.0746 - val_mae: 0.3108\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1198 - mae: 0.3952 - val_loss: 0.0699 - val_mae: 0.2943\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1166 - mae: 0.3886 - val_loss: 0.0664 - val_mae: 0.2874\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1096 - mae: 0.3761 - val_loss: 0.0655 - val_mae: 0.2863\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1071 - mae: 0.3705 - val_loss: 0.0641 - val_mae: 0.2820\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1014 - mae: 0.3589 - val_loss: 0.0616 - val_mae: 0.2766\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0973 - mae: 0.3514 - val_loss: 0.0600 - val_mae: 0.2728\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0943 - mae: 0.3449 - val_loss: 0.0567 - val_mae: 0.2657\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0909 - mae: 0.3383 - val_loss: 0.0552 - val_mae: 0.2630\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0889 - mae: 0.3339 - val_loss: 0.0521 - val_mae: 0.2575\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0842 - mae: 0.3245 - val_loss: 0.0516 - val_mae: 0.2537\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0818 - mae: 0.3197 - val_loss: 0.0503 - val_mae: 0.2513\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0776 - mae: 0.3112 - val_loss: 0.0490 - val_mae: 0.2484\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0767 - mae: 0.3080 - val_loss: 0.0474 - val_mae: 0.2446\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0742 - mae: 0.3034 - val_loss: 0.0459 - val_mae: 0.2406\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0716 - mae: 0.2973 - val_loss: 0.0451 - val_mae: 0.2385\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0692 - mae: 0.2920 - val_loss: 0.0435 - val_mae: 0.2361\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.0678 - mae: 0.2882 - val_loss: 0.0426 - val_mae: 0.2339\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0662 - mae: 0.2848 - val_loss: 0.0422 - val_mae: 0.2322\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0640 - mae: 0.2799 - val_loss: 0.0410 - val_mae: 0.2304\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0618 - mae: 0.2745 - val_loss: 0.0398 - val_mae: 0.2267\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0602 - mae: 0.2712 - val_loss: 0.0426 - val_mae: 0.2321\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0591 - mae: 0.2674 - val_loss: 0.0395 - val_mae: 0.2245\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0575 - mae: 0.2638 - val_loss: 0.0383 - val_mae: 0.2229\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0564 - mae: 0.2609 - val_loss: 0.0374 - val_mae: 0.2190\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0549 - mae: 0.2574 - val_loss: 0.0390 - val_mae: 0.2247\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0540 - mae: 0.2547 - val_loss: 0.0372 - val_mae: 0.2185\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0525 - mae: 0.2507 - val_loss: 0.0401 - val_mae: 0.2263\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0525 - mae: 0.2503 - val_loss: 0.0354 - val_mae: 0.2126\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0507 - mae: 0.2460 - val_loss: 0.0345 - val_mae: 0.2095\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0503 - mae: 0.2442 - val_loss: 0.0359 - val_mae: 0.2135\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0494 - mae: 0.2424 - val_loss: 0.0410 - val_mae: 0.2263\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0487 - mae: 0.2407 - val_loss: 0.0356 - val_mae: 0.2125\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0483 - mae: 0.2391 - val_loss: 0.0342 - val_mae: 0.2082\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0479 - mae: 0.2379 - val_loss: 0.0341 - val_mae: 0.2066\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0468 - mae: 0.2349 - val_loss: 0.0405 - val_mae: 0.2244\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0467 - mae: 0.2345 - val_loss: 0.0369 - val_mae: 0.2157\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0456 - mae: 0.2319 - val_loss: 0.0333 - val_mae: 0.2039\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.0450 - mae: 0.2308 - val_loss: 0.0331 - val_mae: 0.2032\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0444 - mae: 0.2288 - val_loss: 0.0332 - val_mae: 0.2040\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0445 - mae: 0.2286 - val_loss: 0.0333 - val_mae: 0.2037\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0436 - mae: 0.2271 - val_loss: 0.0339 - val_mae: 0.2047\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0439 - mae: 0.2267 - val_loss: 0.0340 - val_mae: 0.2034\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0428 - mae: 0.2242 - val_loss: 0.0355 - val_mae: 0.2074\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0424 - mae: 0.2231 - val_loss: 0.0329 - val_mae: 0.2017\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0423 - mae: 0.2228 - val_loss: 0.0326 - val_mae: 0.2007\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0417 - mae: 0.2210 - val_loss: 0.0324 - val_mae: 0.1996\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0415 - mae: 0.2209 - val_loss: 0.0329 - val_mae: 0.2014\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0415 - mae: 0.2205 - val_loss: 0.0365 - val_mae: 0.2121\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0409 - mae: 0.2190 - val_loss: 0.0325 - val_mae: 0.1994\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0406 - mae: 0.2181 - val_loss: 0.0337 - val_mae: 0.2040\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0400 - mae: 0.2170 - val_loss: 0.0360 - val_mae: 0.2074\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0400 - mae: 0.2164 - val_loss: 0.0322 - val_mae: 0.1987\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0395 - mae: 0.2154 - val_loss: 0.0347 - val_mae: 0.2043\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0390 - mae: 0.2143 - val_loss: 0.0320 - val_mae: 0.1969\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0388 - mae: 0.2135 - val_loss: 0.0339 - val_mae: 0.2013\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0386 - mae: 0.2131 - val_loss: 0.0322 - val_mae: 0.1979\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0383 - mae: 0.2121 - val_loss: 0.0323 - val_mae: 0.1981\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0386 - mae: 0.2125 - val_loss: 0.0323 - val_mae: 0.1984\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0379 - mae: 0.2113 - val_loss: 0.0319 - val_mae: 0.1967\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0376 - mae: 0.2101 - val_loss: 0.0327 - val_mae: 0.1995\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0379 - mae: 0.2107 - val_loss: 0.0334 - val_mae: 0.2020\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0374 - mae: 0.2096 - val_loss: 0.0320 - val_mae: 0.1972\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0374 - mae: 0.2092 - val_loss: 0.0368 - val_mae: 0.2122\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0369 - mae: 0.2083 - val_loss: 0.0319 - val_mae: 0.1963\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0370 - mae: 0.2086 - val_loss: 0.0325 - val_mae: 0.1991\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0370 - mae: 0.2081 - val_loss: 0.0328 - val_mae: 0.1993\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0365 - mae: 0.2067 - val_loss: 0.0323 - val_mae: 0.1976\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0365 - mae: 0.2070 - val_loss: 0.0336 - val_mae: 0.2024\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0360 - mae: 0.2063 - val_loss: 0.0340 - val_mae: 0.2038\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0361 - mae: 0.2059 - val_loss: 0.0320 - val_mae: 0.1968\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0365 - mae: 0.2068 - val_loss: 0.0313 - val_mae: 0.1943\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0356 - mae: 0.2046 - val_loss: 0.0322 - val_mae: 0.1978\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0355 - mae: 0.2045 - val_loss: 0.0322 - val_mae: 0.1964\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0355 - mae: 0.2046 - val_loss: 0.0322 - val_mae: 0.1980\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0354 - mae: 0.2041 - val_loss: 0.0317 - val_mae: 0.1959\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0351 - mae: 0.2032 - val_loss: 0.0354 - val_mae: 0.2076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 19:03:21,407] Trial 41 finished with value: 0.03542760759592056 and parameters: {'learning_rate': 5.498840419344768e-05, 'dropout_rate': 0.24856490864293918, 'batch_size': 32, 'epochs': 100}. Best is trial 33 with value: 0.030832109972834587.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 3s 8ms/step - loss: 0.4260 - mae: 0.8561 - val_loss: 0.6035 - val_mae: 1.1008\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3724 - mae: 0.7852 - val_loss: 0.3605 - val_mae: 0.7699\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.3236 - mae: 0.7183 - val_loss: 0.2972 - val_mae: 0.6845\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 0.2869 - mae: 0.6667 - val_loss: 0.2566 - val_mae: 0.6183\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2540 - mae: 0.6192 - val_loss: 0.2662 - val_mae: 0.6343\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.2245 - mae: 0.5753 - val_loss: 0.1667 - val_mae: 0.4842\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.2002 - mae: 0.5378 - val_loss: 0.1523 - val_mae: 0.4570\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1820 - mae: 0.5085 - val_loss: 0.1379 - val_mae: 0.4301\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1648 - mae: 0.4810 - val_loss: 0.1244 - val_mae: 0.4063\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1518 - mae: 0.4589 - val_loss: 0.1164 - val_mae: 0.3938\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1408 - mae: 0.4405 - val_loss: 0.1015 - val_mae: 0.3648\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1306 - mae: 0.4225 - val_loss: 0.0957 - val_mae: 0.3524\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1215 - mae: 0.4057 - val_loss: 0.0887 - val_mae: 0.3413\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1140 - mae: 0.3918 - val_loss: 0.0863 - val_mae: 0.3346\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1076 - mae: 0.3790 - val_loss: 0.0773 - val_mae: 0.3154\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1020 - mae: 0.3682 - val_loss: 0.0709 - val_mae: 0.3053\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0957 - mae: 0.3553 - val_loss: 0.0698 - val_mae: 0.2986\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0901 - mae: 0.3438 - val_loss: 0.0646 - val_mae: 0.2887\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0859 - mae: 0.3344 - val_loss: 0.0573 - val_mae: 0.2755\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0807 - mae: 0.3233 - val_loss: 0.0588 - val_mae: 0.2739\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0768 - mae: 0.3144 - val_loss: 0.0518 - val_mae: 0.2614\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0719 - mae: 0.3029 - val_loss: 0.0509 - val_mae: 0.2578\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0698 - mae: 0.2984 - val_loss: 0.0462 - val_mae: 0.2456\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0668 - mae: 0.2904 - val_loss: 0.0458 - val_mae: 0.2434\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0631 - mae: 0.2818 - val_loss: 0.0421 - val_mae: 0.2348\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0607 - mae: 0.2755 - val_loss: 0.0460 - val_mae: 0.2421\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0586 - mae: 0.2702 - val_loss: 0.0417 - val_mae: 0.2309\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0578 - mae: 0.2669 - val_loss: 0.0477 - val_mae: 0.2395\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0546 - mae: 0.2594 - val_loss: 0.0489 - val_mae: 0.2415\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0530 - mae: 0.2547 - val_loss: 0.0410 - val_mae: 0.2243\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0523 - mae: 0.2529 - val_loss: 0.0361 - val_mae: 0.2152\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0505 - mae: 0.2482 - val_loss: 0.0352 - val_mae: 0.2121\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0500 - mae: 0.2465 - val_loss: 0.0401 - val_mae: 0.2219\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0494 - mae: 0.2443 - val_loss: 0.0377 - val_mae: 0.2169\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0475 - mae: 0.2404 - val_loss: 0.0348 - val_mae: 0.2098\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0478 - mae: 0.2404 - val_loss: 0.0429 - val_mae: 0.2274\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0462 - mae: 0.2363 - val_loss: 0.0444 - val_mae: 0.2303\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0454 - mae: 0.2343 - val_loss: 0.0406 - val_mae: 0.2205\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0450 - mae: 0.2333 - val_loss: 0.0332 - val_mae: 0.2030\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0443 - mae: 0.2306 - val_loss: 0.0340 - val_mae: 0.2060\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0440 - mae: 0.2293 - val_loss: 0.0403 - val_mae: 0.2198\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0430 - mae: 0.2266 - val_loss: 0.0328 - val_mae: 0.2021\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0426 - mae: 0.2259 - val_loss: 0.0407 - val_mae: 0.2165\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0418 - mae: 0.2234 - val_loss: 0.0331 - val_mae: 0.2015\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0412 - mae: 0.2213 - val_loss: 0.0361 - val_mae: 0.2067\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0412 - mae: 0.2210 - val_loss: 0.0351 - val_mae: 0.2066\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0405 - mae: 0.2193 - val_loss: 0.0436 - val_mae: 0.2278\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0403 - mae: 0.2183 - val_loss: 0.0338 - val_mae: 0.2045\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0398 - mae: 0.2170 - val_loss: 0.0333 - val_mae: 0.2003\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0401 - mae: 0.2175 - val_loss: 0.0351 - val_mae: 0.2043\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0390 - mae: 0.2143 - val_loss: 0.0322 - val_mae: 0.1967\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0389 - mae: 0.2143 - val_loss: 0.0310 - val_mae: 0.1938\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0385 - mae: 0.2125 - val_loss: 0.0327 - val_mae: 0.1975\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0378 - mae: 0.2106 - val_loss: 0.0320 - val_mae: 0.1964\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0374 - mae: 0.2098 - val_loss: 0.0339 - val_mae: 0.2001\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0368 - mae: 0.2080 - val_loss: 0.0340 - val_mae: 0.2018\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0370 - mae: 0.2079 - val_loss: 0.0423 - val_mae: 0.2240\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0365 - mae: 0.2067 - val_loss: 0.0427 - val_mae: 0.2221\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0365 - mae: 0.2065 - val_loss: 0.0320 - val_mae: 0.1977\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0359 - mae: 0.2044 - val_loss: 0.0425 - val_mae: 0.2244\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0358 - mae: 0.2041 - val_loss: 0.0355 - val_mae: 0.2040\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0357 - mae: 0.2040 - val_loss: 0.0341 - val_mae: 0.2014\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0356 - mae: 0.2032 - val_loss: 0.0311 - val_mae: 0.1926\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0349 - mae: 0.2012 - val_loss: 0.0329 - val_mae: 0.1947\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0351 - mae: 0.2016 - val_loss: 0.0305 - val_mae: 0.1918\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0347 - mae: 0.2006 - val_loss: 0.0563 - val_mae: 0.2579\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0347 - mae: 0.2003 - val_loss: 0.0396 - val_mae: 0.2189\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0342 - mae: 0.1992 - val_loss: 0.0349 - val_mae: 0.2006\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0342 - mae: 0.1988 - val_loss: 0.0342 - val_mae: 0.2008\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0340 - mae: 0.1975 - val_loss: 0.0320 - val_mae: 0.1924\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0334 - mae: 0.1961 - val_loss: 0.0295 - val_mae: 0.1854\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0335 - mae: 0.1965 - val_loss: 0.0356 - val_mae: 0.2009\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0335 - mae: 0.1961 - val_loss: 0.0322 - val_mae: 0.1936\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0331 - mae: 0.1952 - val_loss: 0.0298 - val_mae: 0.1882\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0329 - mae: 0.1940 - val_loss: 0.0338 - val_mae: 0.1964\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0329 - mae: 0.1940 - val_loss: 0.0297 - val_mae: 0.1865\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0325 - mae: 0.1928 - val_loss: 0.0338 - val_mae: 0.1981\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0325 - mae: 0.1931 - val_loss: 0.0304 - val_mae: 0.1915\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0324 - mae: 0.1926 - val_loss: 0.0312 - val_mae: 0.1957\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0321 - mae: 0.1912 - val_loss: 0.0373 - val_mae: 0.2052\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0322 - mae: 0.1916 - val_loss: 0.0343 - val_mae: 0.2009\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0323 - mae: 0.1919 - val_loss: 0.0357 - val_mae: 0.2091\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0319 - mae: 0.1907 - val_loss: 0.0308 - val_mae: 0.1880\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0317 - mae: 0.1899 - val_loss: 0.0305 - val_mae: 0.1903\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0317 - mae: 0.1898 - val_loss: 0.0305 - val_mae: 0.1889\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0316 - mae: 0.1891 - val_loss: 0.0321 - val_mae: 0.1992\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0315 - mae: 0.1895 - val_loss: 0.0309 - val_mae: 0.1880\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0317 - mae: 0.1896 - val_loss: 0.0298 - val_mae: 0.1880\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0314 - mae: 0.1888 - val_loss: 0.0339 - val_mae: 0.1983\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0311 - mae: 0.1877 - val_loss: 0.0333 - val_mae: 0.2027\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0312 - mae: 0.1881 - val_loss: 0.0295 - val_mae: 0.1871\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0312 - mae: 0.1880 - val_loss: 0.0322 - val_mae: 0.1904\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0310 - mae: 0.1874 - val_loss: 0.0299 - val_mae: 0.1846\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0310 - mae: 0.1874 - val_loss: 0.0318 - val_mae: 0.1914\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0310 - mae: 0.1870 - val_loss: 0.0397 - val_mae: 0.2134\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0306 - mae: 0.1860 - val_loss: 0.0284 - val_mae: 0.1825\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0309 - mae: 0.1870 - val_loss: 0.0287 - val_mae: 0.1836\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.0307 - mae: 0.1860 - val_loss: 0.0336 - val_mae: 0.1922\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0304 - mae: 0.1852 - val_loss: 0.0302 - val_mae: 0.1856\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0305 - mae: 0.1856 - val_loss: 0.0293 - val_mae: 0.1816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 19:04:48,935] Trial 42 finished with value: 0.029339225962758064 and parameters: {'learning_rate': 7.27897485581272e-05, 'dropout_rate': 0.10373251980675244, 'batch_size': 32, 'epochs': 100}. Best is trial 42 with value: 0.029339225962758064.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 3s 8ms/step - loss: 0.4404 - mae: 0.8752 - val_loss: 0.2314 - val_mae: 0.6037\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4238 - mae: 0.8538 - val_loss: 0.2413 - val_mae: 0.6235\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4091 - mae: 0.8345 - val_loss: 0.2484 - val_mae: 0.6297\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3916 - mae: 0.8120 - val_loss: 0.2284 - val_mae: 0.5975\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.3766 - mae: 0.7910 - val_loss: 0.2319 - val_mae: 0.5916\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3655 - mae: 0.7763 - val_loss: 0.2238 - val_mae: 0.5820\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.3539 - mae: 0.7602 - val_loss: 0.2236 - val_mae: 0.5763\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.3373 - mae: 0.7376 - val_loss: 0.2246 - val_mae: 0.5882\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3249 - mae: 0.7221 - val_loss: 0.2092 - val_mae: 0.5603\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3117 - mae: 0.7033 - val_loss: 0.2098 - val_mae: 0.5573\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.3030 - mae: 0.6903 - val_loss: 0.2048 - val_mae: 0.5498\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2885 - mae: 0.6703 - val_loss: 0.1851 - val_mae: 0.5242\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2800 - mae: 0.6579 - val_loss: 0.1759 - val_mae: 0.5087\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2665 - mae: 0.6386 - val_loss: 0.1712 - val_mae: 0.4967\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2587 - mae: 0.6275 - val_loss: 0.1653 - val_mae: 0.4891\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2509 - mae: 0.6156 - val_loss: 0.1666 - val_mae: 0.4902\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2409 - mae: 0.6004 - val_loss: 0.1533 - val_mae: 0.4667\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2349 - mae: 0.5917 - val_loss: 0.1564 - val_mae: 0.4729\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.2250 - mae: 0.5768 - val_loss: 0.1466 - val_mae: 0.4566\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.2184 - mae: 0.5663 - val_loss: 0.1440 - val_mae: 0.4517\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2102 - mae: 0.5541 - val_loss: 0.1418 - val_mae: 0.4479\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2051 - mae: 0.5461 - val_loss: 0.1370 - val_mae: 0.4384\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1994 - mae: 0.5372 - val_loss: 0.1289 - val_mae: 0.4237\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1926 - mae: 0.5258 - val_loss: 0.1263 - val_mae: 0.4156\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1864 - mae: 0.5154 - val_loss: 0.1260 - val_mae: 0.4155\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1816 - mae: 0.5075 - val_loss: 0.1250 - val_mae: 0.4139\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.1752 - mae: 0.4978 - val_loss: 0.1201 - val_mae: 0.4031\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1698 - mae: 0.4881 - val_loss: 0.1179 - val_mae: 0.3981\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.1663 - mae: 0.4811 - val_loss: 0.1146 - val_mae: 0.3913\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1614 - mae: 0.4737 - val_loss: 0.1089 - val_mae: 0.3792\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1565 - mae: 0.4658 - val_loss: 0.1045 - val_mae: 0.3706\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1523 - mae: 0.4585 - val_loss: 0.1042 - val_mae: 0.3707\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1496 - mae: 0.4527 - val_loss: 0.1003 - val_mae: 0.3636\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1456 - mae: 0.4468 - val_loss: 0.0983 - val_mae: 0.3603\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1430 - mae: 0.4420 - val_loss: 0.0947 - val_mae: 0.3511\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1405 - mae: 0.4366 - val_loss: 0.0928 - val_mae: 0.3460\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1366 - mae: 0.4296 - val_loss: 0.0938 - val_mae: 0.3517\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1318 - mae: 0.4213 - val_loss: 0.0914 - val_mae: 0.3483\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1301 - mae: 0.4183 - val_loss: 0.0900 - val_mae: 0.3427\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1281 - mae: 0.4148 - val_loss: 0.0857 - val_mae: 0.3312\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1262 - mae: 0.4106 - val_loss: 0.0852 - val_mae: 0.3307\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1226 - mae: 0.4039 - val_loss: 0.0829 - val_mae: 0.3257\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1204 - mae: 0.3997 - val_loss: 0.0819 - val_mae: 0.3247\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1170 - mae: 0.3944 - val_loss: 0.0793 - val_mae: 0.3171\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1161 - mae: 0.3911 - val_loss: 0.0780 - val_mae: 0.3152\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1131 - mae: 0.3859 - val_loss: 0.0778 - val_mae: 0.3150\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1116 - mae: 0.3832 - val_loss: 0.0774 - val_mae: 0.3140\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1096 - mae: 0.3790 - val_loss: 0.0749 - val_mae: 0.3091\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1076 - mae: 0.3754 - val_loss: 0.0739 - val_mae: 0.3079\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1056 - mae: 0.3710 - val_loss: 0.0720 - val_mae: 0.3027\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1045 - mae: 0.3694 - val_loss: 0.0708 - val_mae: 0.2997\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1021 - mae: 0.3644 - val_loss: 0.0692 - val_mae: 0.2967\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0994 - mae: 0.3588 - val_loss: 0.0682 - val_mae: 0.2952\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0987 - mae: 0.3571 - val_loss: 0.0678 - val_mae: 0.2942\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0973 - mae: 0.3547 - val_loss: 0.0657 - val_mae: 0.2879\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0965 - mae: 0.3533 - val_loss: 0.0649 - val_mae: 0.2874\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0936 - mae: 0.3467 - val_loss: 0.0639 - val_mae: 0.2848\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 0.0929 - mae: 0.3455 - val_loss: 0.0628 - val_mae: 0.2815\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0898 - mae: 0.3399 - val_loss: 0.0619 - val_mae: 0.2809\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.0889 - mae: 0.3380 - val_loss: 0.0609 - val_mae: 0.2793\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0886 - mae: 0.3361 - val_loss: 0.0600 - val_mae: 0.2773\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0854 - mae: 0.3301 - val_loss: 0.0590 - val_mae: 0.2735\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0853 - mae: 0.3298 - val_loss: 0.0579 - val_mae: 0.2716\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0841 - mae: 0.3271 - val_loss: 0.0574 - val_mae: 0.2706\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0825 - mae: 0.3243 - val_loss: 0.0566 - val_mae: 0.2691\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0811 - mae: 0.3214 - val_loss: 0.0560 - val_mae: 0.2674\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0798 - mae: 0.3182 - val_loss: 0.0551 - val_mae: 0.2647\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0783 - mae: 0.3152 - val_loss: 0.0546 - val_mae: 0.2643\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0782 - mae: 0.3146 - val_loss: 0.0532 - val_mae: 0.2604\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0779 - mae: 0.3130 - val_loss: 0.0524 - val_mae: 0.2580\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0755 - mae: 0.3087 - val_loss: 0.0514 - val_mae: 0.2573\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0752 - mae: 0.3073 - val_loss: 0.0505 - val_mae: 0.2556\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0745 - mae: 0.3064 - val_loss: 0.0494 - val_mae: 0.2528\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0722 - mae: 0.3014 - val_loss: 0.0494 - val_mae: 0.2526\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0718 - mae: 0.3003 - val_loss: 0.0493 - val_mae: 0.2516\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.0699 - mae: 0.2960 - val_loss: 0.0474 - val_mae: 0.2480\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0704 - mae: 0.2970 - val_loss: 0.0474 - val_mae: 0.2469\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0699 - mae: 0.2962 - val_loss: 0.0464 - val_mae: 0.2454\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0686 - mae: 0.2929 - val_loss: 0.0459 - val_mae: 0.2439\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0656 - mae: 0.2868 - val_loss: 0.0453 - val_mae: 0.2422\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0668 - mae: 0.2892 - val_loss: 0.0447 - val_mae: 0.2410\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0660 - mae: 0.2871 - val_loss: 0.0442 - val_mae: 0.2391\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0645 - mae: 0.2836 - val_loss: 0.0443 - val_mae: 0.2390\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0639 - mae: 0.2831 - val_loss: 0.0434 - val_mae: 0.2370\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0635 - mae: 0.2810 - val_loss: 0.0423 - val_mae: 0.2344\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0633 - mae: 0.2801 - val_loss: 0.0423 - val_mae: 0.2349\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0625 - mae: 0.2784 - val_loss: 0.0414 - val_mae: 0.2324\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0605 - mae: 0.2744 - val_loss: 0.0410 - val_mae: 0.2316\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0610 - mae: 0.2749 - val_loss: 0.0416 - val_mae: 0.2327\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0605 - mae: 0.2727 - val_loss: 0.0406 - val_mae: 0.2304\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0593 - mae: 0.2708 - val_loss: 0.0397 - val_mae: 0.2278\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0589 - mae: 0.2701 - val_loss: 0.0405 - val_mae: 0.2292\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0577 - mae: 0.2670 - val_loss: 0.0393 - val_mae: 0.2263\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.0583 - mae: 0.2684 - val_loss: 0.0387 - val_mae: 0.2244\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0577 - mae: 0.2664 - val_loss: 0.0397 - val_mae: 0.2273\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0576 - mae: 0.2664 - val_loss: 0.0394 - val_mae: 0.2264\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0574 - mae: 0.2653 - val_loss: 0.0387 - val_mae: 0.2245\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.0559 - mae: 0.2627 - val_loss: 0.0380 - val_mae: 0.2221\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.0563 - mae: 0.2633 - val_loss: 0.0381 - val_mae: 0.2225\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.0554 - mae: 0.2611 - val_loss: 0.0381 - val_mae: 0.2224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 19:06:25,086] Trial 43 finished with value: 0.03806569427251816 and parameters: {'learning_rate': 2.4020914210686392e-05, 'dropout_rate': 0.1427046556854712, 'batch_size': 32, 'epochs': 100}. Best is trial 42 with value: 0.029339225962758064.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 6s 12ms/step - loss: 0.4231 - mae: 0.8555 - val_loss: 0.8249 - val_mae: 1.3539\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.4159 - mae: 0.8464 - val_loss: 0.2184 - val_mae: 0.5779\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4127 - mae: 0.8426 - val_loss: 0.3270 - val_mae: 0.7375\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 2s 17ms/step - loss: 0.4056 - mae: 0.8338 - val_loss: 0.2849 - val_mae: 0.6734\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 2s 14ms/step - loss: 0.4010 - mae: 0.8274 - val_loss: 0.2867 - val_mae: 0.6790\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.3938 - mae: 0.8171 - val_loss: 0.2794 - val_mae: 0.6689\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 0.3878 - mae: 0.8086 - val_loss: 0.2746 - val_mae: 0.6572\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3824 - mae: 0.8030 - val_loss: 0.2639 - val_mae: 0.6425\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3746 - mae: 0.7925 - val_loss: 0.2685 - val_mae: 0.6495\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3690 - mae: 0.7850 - val_loss: 0.2601 - val_mae: 0.6371\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3590 - mae: 0.7705 - val_loss: 0.2488 - val_mae: 0.6204\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3610 - mae: 0.7738 - val_loss: 0.2366 - val_mae: 0.6035\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3507 - mae: 0.7598 - val_loss: 0.2356 - val_mae: 0.6018\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3457 - mae: 0.7518 - val_loss: 0.2389 - val_mae: 0.6093\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3392 - mae: 0.7449 - val_loss: 0.2155 - val_mae: 0.5727\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3365 - mae: 0.7409 - val_loss: 0.2257 - val_mae: 0.5882\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.3310 - mae: 0.7331 - val_loss: 0.2160 - val_mae: 0.5724\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.3244 - mae: 0.7250 - val_loss: 0.2060 - val_mae: 0.5581\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 2s 17ms/step - loss: 0.3223 - mae: 0.7223 - val_loss: 0.2070 - val_mae: 0.5600\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.3134 - mae: 0.7088 - val_loss: 0.2067 - val_mae: 0.5610\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3116 - mae: 0.7066 - val_loss: 0.2037 - val_mae: 0.5570\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.3057 - mae: 0.6983 - val_loss: 0.1987 - val_mae: 0.5487\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2985 - mae: 0.6885 - val_loss: 0.1903 - val_mae: 0.5360\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2975 - mae: 0.6864 - val_loss: 0.1913 - val_mae: 0.5383\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2908 - mae: 0.6779 - val_loss: 0.1901 - val_mae: 0.5380\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2889 - mae: 0.6754 - val_loss: 0.1828 - val_mae: 0.5260\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2860 - mae: 0.6709 - val_loss: 0.1824 - val_mae: 0.5251\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.2825 - mae: 0.6654 - val_loss: 0.1828 - val_mae: 0.5254\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.2769 - mae: 0.6576 - val_loss: 0.1799 - val_mae: 0.5233\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.2746 - mae: 0.6541 - val_loss: 0.1799 - val_mae: 0.5217\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 12ms/step - loss: 0.2698 - mae: 0.6474 - val_loss: 0.1749 - val_mae: 0.5134\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.2669 - mae: 0.6429 - val_loss: 0.1732 - val_mae: 0.5095\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.2617 - mae: 0.6353 - val_loss: 0.1683 - val_mae: 0.5023\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.2586 - mae: 0.6301 - val_loss: 0.1689 - val_mae: 0.5034\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.2553 - mae: 0.6256 - val_loss: 0.1643 - val_mae: 0.4933\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.2523 - mae: 0.6210 - val_loss: 0.1664 - val_mae: 0.4974\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.2512 - mae: 0.6192 - val_loss: 0.1648 - val_mae: 0.4953\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.2460 - mae: 0.6110 - val_loss: 0.1646 - val_mae: 0.4942\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.2432 - mae: 0.6079 - val_loss: 0.1610 - val_mae: 0.4894\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2399 - mae: 0.6022 - val_loss: 0.1542 - val_mae: 0.4765\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2402 - mae: 0.6031 - val_loss: 0.1560 - val_mae: 0.4800\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.2370 - mae: 0.5984 - val_loss: 0.1567 - val_mae: 0.4798\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.2340 - mae: 0.5930 - val_loss: 0.1503 - val_mae: 0.4684\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.2291 - mae: 0.5860 - val_loss: 0.1510 - val_mae: 0.4688\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.2289 - mae: 0.5857 - val_loss: 0.1474 - val_mae: 0.4628\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.2251 - mae: 0.5801 - val_loss: 0.1496 - val_mae: 0.4673\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2209 - mae: 0.5741 - val_loss: 0.1454 - val_mae: 0.4596\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2207 - mae: 0.5729 - val_loss: 0.1445 - val_mae: 0.4582\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2159 - mae: 0.5653 - val_loss: 0.1457 - val_mae: 0.4593\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2148 - mae: 0.5642 - val_loss: 0.1419 - val_mae: 0.4532\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2128 - mae: 0.5612 - val_loss: 0.1429 - val_mae: 0.4538\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2101 - mae: 0.5570 - val_loss: 0.1399 - val_mae: 0.4493\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.2085 - mae: 0.5536 - val_loss: 0.1395 - val_mae: 0.4468\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.2052 - mae: 0.5488 - val_loss: 0.1380 - val_mae: 0.4440\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2030 - mae: 0.5457 - val_loss: 0.1349 - val_mae: 0.4383\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.2004 - mae: 0.5414 - val_loss: 0.1348 - val_mae: 0.4387\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1994 - mae: 0.5396 - val_loss: 0.1305 - val_mae: 0.4325\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1955 - mae: 0.5337 - val_loss: 0.1283 - val_mae: 0.4263\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1952 - mae: 0.5334 - val_loss: 0.1312 - val_mae: 0.4330\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1919 - mae: 0.5277 - val_loss: 0.1292 - val_mae: 0.4287\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1919 - mae: 0.5276 - val_loss: 0.1316 - val_mae: 0.4328\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1880 - mae: 0.5218 - val_loss: 0.1263 - val_mae: 0.4235\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1867 - mae: 0.5199 - val_loss: 0.1242 - val_mae: 0.4193\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1837 - mae: 0.5148 - val_loss: 0.1238 - val_mae: 0.4180\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1832 - mae: 0.5137 - val_loss: 0.1237 - val_mae: 0.4184\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1815 - mae: 0.5108 - val_loss: 0.1240 - val_mae: 0.4177\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1792 - mae: 0.5070 - val_loss: 0.1214 - val_mae: 0.4117\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1762 - mae: 0.5026 - val_loss: 0.1199 - val_mae: 0.4100\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1753 - mae: 0.5010 - val_loss: 0.1161 - val_mae: 0.4036\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1732 - mae: 0.4965 - val_loss: 0.1187 - val_mae: 0.4078\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1718 - mae: 0.4950 - val_loss: 0.1177 - val_mae: 0.4051\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1699 - mae: 0.4924 - val_loss: 0.1150 - val_mae: 0.3996\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1696 - mae: 0.4910 - val_loss: 0.1149 - val_mae: 0.4004\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1666 - mae: 0.4868 - val_loss: 0.1121 - val_mae: 0.3949\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1659 - mae: 0.4852 - val_loss: 0.1110 - val_mae: 0.3922\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1638 - mae: 0.4814 - val_loss: 0.1101 - val_mae: 0.3908\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1620 - mae: 0.4782 - val_loss: 0.1101 - val_mae: 0.3904\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1626 - mae: 0.4788 - val_loss: 0.1093 - val_mae: 0.3895\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1597 - mae: 0.4748 - val_loss: 0.1072 - val_mae: 0.3853\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1583 - mae: 0.4725 - val_loss: 0.1075 - val_mae: 0.3848\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1559 - mae: 0.4687 - val_loss: 0.1054 - val_mae: 0.3814\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.1559 - mae: 0.4670 - val_loss: 0.1044 - val_mae: 0.3788\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1534 - mae: 0.4636 - val_loss: 0.1032 - val_mae: 0.3766\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1525 - mae: 0.4623 - val_loss: 0.1029 - val_mae: 0.3757\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1516 - mae: 0.4595 - val_loss: 0.1032 - val_mae: 0.3752\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1499 - mae: 0.4577 - val_loss: 0.1011 - val_mae: 0.3719\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1490 - mae: 0.4556 - val_loss: 0.1011 - val_mae: 0.3719\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1478 - mae: 0.4535 - val_loss: 0.1022 - val_mae: 0.3734\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1462 - mae: 0.4514 - val_loss: 0.0993 - val_mae: 0.3679\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.1459 - mae: 0.4503 - val_loss: 0.0996 - val_mae: 0.3676\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1430 - mae: 0.4453 - val_loss: 0.0977 - val_mae: 0.3638\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1430 - mae: 0.4454 - val_loss: 0.0988 - val_mae: 0.3654\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.1424 - mae: 0.4435 - val_loss: 0.0961 - val_mae: 0.3604\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1398 - mae: 0.4388 - val_loss: 0.0953 - val_mae: 0.3590\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1392 - mae: 0.4387 - val_loss: 0.0961 - val_mae: 0.3601\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.1400 - mae: 0.4391 - val_loss: 0.0960 - val_mae: 0.3602\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1370 - mae: 0.4337 - val_loss: 0.0934 - val_mae: 0.3552\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.1358 - mae: 0.4319 - val_loss: 0.0940 - val_mae: 0.3550\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.1347 - mae: 0.4303 - val_loss: 0.0923 - val_mae: 0.3517\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.1350 - mae: 0.4306 - val_loss: 0.0916 - val_mae: 0.3509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 19:08:52,100] Trial 44 finished with value: 0.09163260459899902 and parameters: {'learning_rate': 8.059687628362711e-06, 'dropout_rate': 0.10117333456718294, 'batch_size': 32, 'epochs': 100}. Best is trial 42 with value: 0.029339225962758064.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "60/60 [==============================] - 4s 17ms/step - loss: 0.1663 - mae: 0.4647 - val_loss: 3.4137 - val_mae: 4.0709\n",
            "Epoch 2/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0642 - mae: 0.2770 - val_loss: 0.2805 - val_mae: 0.6820\n",
            "Epoch 3/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0476 - mae: 0.2384 - val_loss: 0.0518 - val_mae: 0.2647\n",
            "Epoch 4/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0398 - mae: 0.2165 - val_loss: 0.0548 - val_mae: 0.2604\n",
            "Epoch 5/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0357 - mae: 0.2062 - val_loss: 0.0887 - val_mae: 0.3179\n",
            "Epoch 6/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0338 - mae: 0.2002 - val_loss: 0.0423 - val_mae: 0.2203\n",
            "Epoch 7/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0331 - mae: 0.1978 - val_loss: 0.1078 - val_mae: 0.3548\n",
            "Epoch 8/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0325 - mae: 0.1960 - val_loss: 0.0474 - val_mae: 0.2325\n",
            "Epoch 9/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0322 - mae: 0.1945 - val_loss: 0.1377 - val_mae: 0.4035\n",
            "Epoch 10/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0318 - mae: 0.1930 - val_loss: 0.0893 - val_mae: 0.3160\n",
            "Epoch 11/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0316 - mae: 0.1923 - val_loss: 0.5136 - val_mae: 0.9036\n",
            "Epoch 12/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0316 - mae: 0.1919 - val_loss: 0.5410 - val_mae: 0.9248\n",
            "Epoch 13/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0314 - mae: 0.1908 - val_loss: 0.6514 - val_mae: 1.0622\n",
            "Epoch 14/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.0315 - mae: 0.1914 - val_loss: 0.5354 - val_mae: 0.9269\n",
            "Epoch 15/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.0310 - mae: 0.1895 - val_loss: 0.2412 - val_mae: 0.5478\n",
            "Epoch 16/200\n",
            "60/60 [==============================] - 1s 16ms/step - loss: 0.0308 - mae: 0.1883 - val_loss: 0.0714 - val_mae: 0.2848\n",
            "Epoch 17/200\n",
            "60/60 [==============================] - 1s 16ms/step - loss: 0.0309 - mae: 0.1886 - val_loss: 0.0824 - val_mae: 0.3001\n",
            "Epoch 18/200\n",
            "60/60 [==============================] - 1s 16ms/step - loss: 0.0306 - mae: 0.1876 - val_loss: 0.4448 - val_mae: 0.8843\n",
            "Epoch 19/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0303 - mae: 0.1863 - val_loss: 0.1803 - val_mae: 0.5009\n",
            "Epoch 20/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0303 - mae: 0.1863 - val_loss: 0.1739 - val_mae: 0.4393\n",
            "Epoch 21/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0304 - mae: 0.1868 - val_loss: 0.0457 - val_mae: 0.2266\n",
            "Epoch 22/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0302 - mae: 0.1860 - val_loss: 0.0443 - val_mae: 0.2291\n",
            "Epoch 23/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0302 - mae: 0.1857 - val_loss: 0.0401 - val_mae: 0.2105\n",
            "Epoch 24/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0299 - mae: 0.1840 - val_loss: 0.0537 - val_mae: 0.2709\n",
            "Epoch 25/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0299 - mae: 0.1850 - val_loss: 0.0455 - val_mae: 0.2527\n",
            "Epoch 26/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0301 - mae: 0.1852 - val_loss: 0.0968 - val_mae: 0.3463\n",
            "Epoch 27/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0299 - mae: 0.1845 - val_loss: 0.1053 - val_mae: 0.3591\n",
            "Epoch 28/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0301 - mae: 0.1850 - val_loss: 0.0812 - val_mae: 0.3024\n",
            "Epoch 29/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0299 - mae: 0.1841 - val_loss: 0.0433 - val_mae: 0.2251\n",
            "Epoch 30/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0300 - mae: 0.1846 - val_loss: 0.1740 - val_mae: 0.4479\n",
            "Epoch 31/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0297 - mae: 0.1831 - val_loss: 0.1480 - val_mae: 0.4186\n",
            "Epoch 32/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0296 - mae: 0.1832 - val_loss: 0.0464 - val_mae: 0.2288\n",
            "Epoch 33/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0298 - mae: 0.1836 - val_loss: 0.1830 - val_mae: 0.4563\n",
            "Epoch 34/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0297 - mae: 0.1835 - val_loss: 0.0700 - val_mae: 0.3135\n",
            "Epoch 35/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.0298 - mae: 0.1835 - val_loss: 0.0448 - val_mae: 0.2484\n",
            "Epoch 36/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.0296 - mae: 0.1827 - val_loss: 0.0501 - val_mae: 0.2467\n",
            "Epoch 37/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.0298 - mae: 0.1832 - val_loss: 0.2495 - val_mae: 0.6057\n",
            "Epoch 38/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0297 - mae: 0.1835 - val_loss: 0.0840 - val_mae: 0.3203\n",
            "Epoch 39/200\n",
            "60/60 [==============================] - 1s 16ms/step - loss: 0.0299 - mae: 0.1840 - val_loss: 0.2107 - val_mae: 0.5514\n",
            "Epoch 40/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.0296 - mae: 0.1829 - val_loss: 0.0626 - val_mae: 0.2736\n",
            "Epoch 41/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0297 - mae: 0.1830 - val_loss: 0.3050 - val_mae: 0.6842\n",
            "Epoch 42/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0298 - mae: 0.1837 - val_loss: 0.1865 - val_mae: 0.5189\n",
            "Epoch 43/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0297 - mae: 0.1830 - val_loss: 0.0406 - val_mae: 0.2273\n",
            "Epoch 44/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0297 - mae: 0.1827 - val_loss: 0.1261 - val_mae: 0.4093\n",
            "Epoch 45/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0295 - mae: 0.1820 - val_loss: 0.0779 - val_mae: 0.3362\n",
            "Epoch 46/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0295 - mae: 0.1822 - val_loss: 0.0646 - val_mae: 0.3013\n",
            "Epoch 47/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0295 - mae: 0.1822 - val_loss: 0.0511 - val_mae: 0.2547\n",
            "Epoch 48/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0297 - mae: 0.1829 - val_loss: 0.1174 - val_mae: 0.4054\n",
            "Epoch 49/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0299 - mae: 0.1838 - val_loss: 0.1981 - val_mae: 0.5335\n",
            "Epoch 50/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0298 - mae: 0.1834 - val_loss: 0.0525 - val_mae: 0.2519\n",
            "Epoch 51/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0301 - mae: 0.1847 - val_loss: 0.1319 - val_mae: 0.4490\n",
            "Epoch 52/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0305 - mae: 0.1865 - val_loss: 0.0593 - val_mae: 0.2873\n",
            "Epoch 53/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0311 - mae: 0.1884 - val_loss: 0.0978 - val_mae: 0.3478\n",
            "Epoch 54/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0326 - mae: 0.1949 - val_loss: 0.0482 - val_mae: 0.2668\n",
            "Epoch 55/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0345 - mae: 0.2030 - val_loss: 0.0815 - val_mae: 0.3542\n",
            "Epoch 56/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0344 - mae: 0.2030 - val_loss: 0.1798 - val_mae: 0.5302\n",
            "Epoch 57/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0338 - mae: 0.2006 - val_loss: 0.0806 - val_mae: 0.3199\n",
            "Epoch 58/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0339 - mae: 0.2011 - val_loss: 0.0725 - val_mae: 0.3059\n",
            "Epoch 59/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0402 - mae: 0.2274 - val_loss: 0.0441 - val_mae: 0.2507\n",
            "Epoch 60/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0438 - val_mae: 0.2518\n",
            "Epoch 61/200\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0485 - val_mae: 0.2535\n",
            "Epoch 62/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0408 - val_mae: 0.2338\n",
            "Epoch 63/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0417 - val_mae: 0.2339\n",
            "Epoch 64/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0405 - val_mae: 0.2282\n",
            "Epoch 65/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0404 - val_mae: 0.2330\n",
            "Epoch 66/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2288\n",
            "Epoch 67/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2271\n",
            "Epoch 68/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2272\n",
            "Epoch 69/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 70/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 71/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 72/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0400 - val_mae: 0.2276\n",
            "Epoch 73/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0400 - val_mae: 0.2278\n",
            "Epoch 74/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 75/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0400 - val_mae: 0.2282\n",
            "Epoch 76/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0400 - val_mae: 0.2275\n",
            "Epoch 77/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0402 - val_mae: 0.2281\n",
            "Epoch 78/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0401 - val_mae: 0.2288\n",
            "Epoch 79/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 80/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 81/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 82/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2268\n",
            "Epoch 83/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0396 - mae: 0.2265 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 84/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2291\n",
            "Epoch 85/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2284\n",
            "Epoch 86/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 87/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 88/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 89/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2274\n",
            "Epoch 90/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2278\n",
            "Epoch 91/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2265 - val_loss: 0.0399 - val_mae: 0.2288\n",
            "Epoch 92/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 93/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 94/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 95/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2284\n",
            "Epoch 96/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2279\n",
            "Epoch 97/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 98/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 99/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 100/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2275\n",
            "Epoch 101/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 102/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2278\n",
            "Epoch 103/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 104/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2281\n",
            "Epoch 105/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2273\n",
            "Epoch 106/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 107/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 108/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 109/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2279\n",
            "Epoch 110/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2278\n",
            "Epoch 111/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 112/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 113/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 114/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2285\n",
            "Epoch 115/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 116/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 117/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2284\n",
            "Epoch 118/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 119/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2286\n",
            "Epoch 120/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 121/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 122/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 123/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 124/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 125/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2278\n",
            "Epoch 126/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2286\n",
            "Epoch 127/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2286\n",
            "Epoch 128/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 129/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0400 - val_mae: 0.2276\n",
            "Epoch 130/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2270\n",
            "Epoch 131/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 132/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2271\n",
            "Epoch 133/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 134/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 135/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0396 - mae: 0.2271 - val_loss: 0.0399 - val_mae: 0.2272\n",
            "Epoch 136/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 137/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2280\n",
            "Epoch 138/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2285\n",
            "Epoch 139/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 140/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 141/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 142/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2271\n",
            "Epoch 143/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 144/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 145/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2282\n",
            "Epoch 146/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2280\n",
            "Epoch 147/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2277\n",
            "Epoch 148/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 149/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 150/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2288\n",
            "Epoch 151/200\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2286\n",
            "Epoch 152/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 153/200\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2274\n",
            "Epoch 154/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 155/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 156/200\n",
            "60/60 [==============================] - 1s 18ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2283\n",
            "Epoch 157/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 158/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 159/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 160/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2276\n",
            "Epoch 161/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 162/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 163/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2279\n",
            "Epoch 164/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2272\n",
            "Epoch 165/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 166/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0399 - val_mae: 0.2273\n",
            "Epoch 167/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2276\n",
            "Epoch 168/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2278\n",
            "Epoch 169/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0398 - val_mae: 0.2273\n",
            "Epoch 170/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2265 - val_loss: 0.0398 - val_mae: 0.2284\n",
            "Epoch 171/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2277\n",
            "Epoch 172/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2282\n",
            "Epoch 173/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0396 - mae: 0.2265 - val_loss: 0.0398 - val_mae: 0.2289\n",
            "Epoch 174/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0398 - val_mae: 0.2279\n",
            "Epoch 175/200\n",
            "60/60 [==============================] - 1s 18ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2277\n",
            "Epoch 176/200\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2275\n",
            "Epoch 177/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0396 - mae: 0.2266 - val_loss: 0.0399 - val_mae: 0.2278\n",
            "Epoch 178/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 179/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0398 - val_mae: 0.2283\n",
            "Epoch 180/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 181/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0399 - val_mae: 0.2281\n",
            "Epoch 182/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0399 - val_mae: 0.2279\n",
            "Epoch 183/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0396 - mae: 0.2269 - val_loss: 0.0403 - val_mae: 0.2303\n",
            "Epoch 184/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0392 - mae: 0.2253 - val_loss: 0.0409 - val_mae: 0.2262\n",
            "Epoch 185/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0365 - mae: 0.2139 - val_loss: 0.1438 - val_mae: 0.4485\n",
            "Epoch 186/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0348 - mae: 0.2059 - val_loss: 0.0421 - val_mae: 0.2243\n",
            "Epoch 187/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0344 - mae: 0.2038 - val_loss: 0.0453 - val_mae: 0.2291\n",
            "Epoch 188/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0341 - mae: 0.2026 - val_loss: 0.1159 - val_mae: 0.3941\n",
            "Epoch 189/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0340 - mae: 0.2022 - val_loss: 0.0524 - val_mae: 0.2437\n",
            "Epoch 190/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0336 - mae: 0.2001 - val_loss: 0.1290 - val_mae: 0.4192\n",
            "Epoch 191/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0339 - mae: 0.2011 - val_loss: 0.0348 - val_mae: 0.2029\n",
            "Epoch 192/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0376 - mae: 0.2156 - val_loss: 0.0620 - val_mae: 0.2881\n",
            "Epoch 193/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0397 - mae: 0.2268 - val_loss: 0.0460 - val_mae: 0.2548\n",
            "Epoch 194/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0397 - mae: 0.2269 - val_loss: 0.0441 - val_mae: 0.2511\n",
            "Epoch 195/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0410 - val_mae: 0.2395\n",
            "Epoch 196/200\n",
            "60/60 [==============================] - 1s 18ms/step - loss: 0.0396 - mae: 0.2270 - val_loss: 0.0405 - val_mae: 0.2341\n",
            "Epoch 197/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0405 - val_mae: 0.2360\n",
            "Epoch 198/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0402 - val_mae: 0.2306\n",
            "Epoch 199/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0396 - mae: 0.2268 - val_loss: 0.0402 - val_mae: 0.2329\n",
            "Epoch 200/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.0396 - mae: 0.2267 - val_loss: 0.0400 - val_mae: 0.2308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 19:11:12,196] Trial 45 finished with value: 0.039991285651922226 and parameters: {'learning_rate': 0.003159769102360879, 'dropout_rate': 0.20130032905853917, 'batch_size': 64, 'epochs': 200}. Best is trial 42 with value: 0.029339225962758064.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "30/30 [==============================] - 3s 28ms/step - loss: 0.4073 - mae: 0.8270 - val_loss: 5.1048 - val_mae: 5.7522\n",
            "Epoch 2/100\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.3838 - mae: 0.7956 - val_loss: 3.0702 - val_mae: 3.6791\n",
            "Epoch 3/100\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.3615 - mae: 0.7644 - val_loss: 2.4976 - val_mae: 3.0859\n",
            "Epoch 4/100\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.3464 - mae: 0.7436 - val_loss: 1.8556 - val_mae: 2.3915\n",
            "Epoch 5/100\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.3285 - mae: 0.7190 - val_loss: 1.0695 - val_mae: 1.5750\n",
            "Epoch 6/100\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.3149 - mae: 0.6996 - val_loss: 0.7074 - val_mae: 1.1985\n",
            "Epoch 7/100\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.3029 - mae: 0.6822 - val_loss: 0.3877 - val_mae: 0.7979\n",
            "Epoch 8/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.2926 - mae: 0.6672 - val_loss: 0.5673 - val_mae: 1.0409\n",
            "Epoch 9/100\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.2772 - mae: 0.6454 - val_loss: 0.3983 - val_mae: 0.8248\n",
            "Epoch 10/100\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.2660 - mae: 0.6291 - val_loss: 0.3278 - val_mae: 0.7366\n",
            "Epoch 11/100\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.2578 - mae: 0.6162 - val_loss: 0.1995 - val_mae: 0.5468\n",
            "Epoch 12/100\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.2467 - mae: 0.6005 - val_loss: 0.1866 - val_mae: 0.5224\n",
            "Epoch 13/100\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.2380 - mae: 0.5880 - val_loss: 0.1851 - val_mae: 0.5222\n",
            "Epoch 14/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.2327 - mae: 0.5792 - val_loss: 0.1950 - val_mae: 0.5416\n",
            "Epoch 15/100\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.2208 - mae: 0.5611 - val_loss: 0.1632 - val_mae: 0.4887\n",
            "Epoch 16/100\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.2145 - mae: 0.5520 - val_loss: 0.1465 - val_mae: 0.4534\n",
            "Epoch 17/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.2082 - mae: 0.5424 - val_loss: 0.1643 - val_mae: 0.4937\n",
            "Epoch 18/100\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.1992 - mae: 0.5287 - val_loss: 0.1470 - val_mae: 0.4580\n",
            "Epoch 19/100\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.1921 - mae: 0.5175 - val_loss: 0.1497 - val_mae: 0.4690\n",
            "Epoch 20/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.1870 - mae: 0.5091 - val_loss: 0.1431 - val_mae: 0.4505\n",
            "Epoch 21/100\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.1810 - mae: 0.5000 - val_loss: 0.1298 - val_mae: 0.4252\n",
            "Epoch 22/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.1762 - mae: 0.4922 - val_loss: 0.1334 - val_mae: 0.4317\n",
            "Epoch 23/100\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.1709 - mae: 0.4828 - val_loss: 0.1300 - val_mae: 0.4204\n",
            "Epoch 24/100\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 0.1635 - mae: 0.4719 - val_loss: 0.1219 - val_mae: 0.4081\n",
            "Epoch 25/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1616 - mae: 0.4676 - val_loss: 0.1118 - val_mae: 0.3851\n",
            "Epoch 26/100\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.1556 - mae: 0.4577 - val_loss: 0.1076 - val_mae: 0.3825\n",
            "Epoch 27/100\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.1517 - mae: 0.4510 - val_loss: 0.1062 - val_mae: 0.3763\n",
            "Epoch 28/100\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.1476 - mae: 0.4436 - val_loss: 0.1134 - val_mae: 0.3910\n",
            "Epoch 29/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1446 - mae: 0.4383 - val_loss: 0.1060 - val_mae: 0.3848\n",
            "Epoch 30/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1398 - mae: 0.4292 - val_loss: 0.0963 - val_mae: 0.3581\n",
            "Epoch 31/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1352 - mae: 0.4208 - val_loss: 0.0950 - val_mae: 0.3529\n",
            "Epoch 32/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1323 - mae: 0.4159 - val_loss: 0.0922 - val_mae: 0.3485\n",
            "Epoch 33/100\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.1288 - mae: 0.4095 - val_loss: 0.0895 - val_mae: 0.3400\n",
            "Epoch 34/100\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.1261 - mae: 0.4038 - val_loss: 0.0887 - val_mae: 0.3332\n",
            "Epoch 35/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1224 - mae: 0.3985 - val_loss: 0.0868 - val_mae: 0.3318\n",
            "Epoch 36/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1197 - mae: 0.3929 - val_loss: 0.0847 - val_mae: 0.3270\n",
            "Epoch 37/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1167 - mae: 0.3870 - val_loss: 0.0810 - val_mae: 0.3164\n",
            "Epoch 38/100\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1140 - mae: 0.3823 - val_loss: 0.0771 - val_mae: 0.3109\n",
            "Epoch 39/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1114 - mae: 0.3764 - val_loss: 0.0774 - val_mae: 0.3071\n",
            "Epoch 40/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1100 - mae: 0.3736 - val_loss: 0.0754 - val_mae: 0.3067\n",
            "Epoch 41/100\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.1073 - mae: 0.3690 - val_loss: 0.0748 - val_mae: 0.3080\n",
            "Epoch 42/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1039 - mae: 0.3614 - val_loss: 0.0721 - val_mae: 0.2980\n",
            "Epoch 43/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.1019 - mae: 0.3580 - val_loss: 0.0698 - val_mae: 0.2896\n",
            "Epoch 44/100\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.0996 - mae: 0.3534 - val_loss: 0.0679 - val_mae: 0.2862\n",
            "Epoch 45/100\n",
            "30/30 [==============================] - 1s 24ms/step - loss: 0.0980 - mae: 0.3496 - val_loss: 0.0662 - val_mae: 0.2845\n",
            "Epoch 46/100\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0948 - mae: 0.3437 - val_loss: 0.0644 - val_mae: 0.2826\n",
            "Epoch 47/100\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.0940 - mae: 0.3416 - val_loss: 0.0633 - val_mae: 0.2796\n",
            "Epoch 48/100\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.0915 - mae: 0.3363 - val_loss: 0.0618 - val_mae: 0.2755\n",
            "Epoch 49/100\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.0919 - mae: 0.3360 - val_loss: 0.0605 - val_mae: 0.2690\n",
            "Epoch 50/100\n",
            "30/30 [==============================] - 1s 36ms/step - loss: 0.0877 - mae: 0.3294 - val_loss: 0.0599 - val_mae: 0.2687\n",
            "Epoch 51/100\n",
            "30/30 [==============================] - 1s 48ms/step - loss: 0.0878 - mae: 0.3282 - val_loss: 0.0587 - val_mae: 0.2651\n",
            "Epoch 52/100\n",
            "30/30 [==============================] - 1s 43ms/step - loss: 0.0840 - mae: 0.3210 - val_loss: 0.0560 - val_mae: 0.2638\n",
            "Epoch 53/100\n",
            "30/30 [==============================] - 1s 49ms/step - loss: 0.0844 - mae: 0.3207 - val_loss: 0.0564 - val_mae: 0.2598\n",
            "Epoch 54/100\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.0822 - mae: 0.3172 - val_loss: 0.0545 - val_mae: 0.2576\n",
            "Epoch 55/100\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0798 - mae: 0.3119 - val_loss: 0.0528 - val_mae: 0.2567\n",
            "Epoch 56/100\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0780 - mae: 0.3083 - val_loss: 0.0518 - val_mae: 0.2564\n",
            "Epoch 57/100\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0770 - mae: 0.3066 - val_loss: 0.0506 - val_mae: 0.2540\n",
            "Epoch 58/100\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0750 - mae: 0.3017 - val_loss: 0.0538 - val_mae: 0.2557\n",
            "Epoch 59/100\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0733 - mae: 0.2981 - val_loss: 0.0503 - val_mae: 0.2482\n",
            "Epoch 60/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0716 - mae: 0.2947 - val_loss: 0.0488 - val_mae: 0.2480\n",
            "Epoch 61/100\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0701 - mae: 0.2927 - val_loss: 0.0486 - val_mae: 0.2462\n",
            "Epoch 62/100\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0687 - mae: 0.2893 - val_loss: 0.0465 - val_mae: 0.2449\n",
            "Epoch 63/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0689 - mae: 0.2878 - val_loss: 0.0466 - val_mae: 0.2427\n",
            "Epoch 64/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0685 - mae: 0.2873 - val_loss: 0.0454 - val_mae: 0.2393\n",
            "Epoch 65/100\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0665 - mae: 0.2826 - val_loss: 0.0459 - val_mae: 0.2394\n",
            "Epoch 66/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0646 - mae: 0.2790 - val_loss: 0.0451 - val_mae: 0.2378\n",
            "Epoch 67/100\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0628 - mae: 0.2758 - val_loss: 0.0513 - val_mae: 0.2529\n",
            "Epoch 68/100\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0637 - mae: 0.2764 - val_loss: 0.0415 - val_mae: 0.2288\n",
            "Epoch 69/100\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0632 - mae: 0.2745 - val_loss: 0.0441 - val_mae: 0.2342\n",
            "Epoch 70/100\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0611 - mae: 0.2710 - val_loss: 0.0430 - val_mae: 0.2324\n",
            "Epoch 71/100\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0621 - mae: 0.2706 - val_loss: 0.0448 - val_mae: 0.2366\n",
            "Epoch 72/100\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0590 - mae: 0.2659 - val_loss: 0.0417 - val_mae: 0.2303\n",
            "Epoch 73/100\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0591 - mae: 0.2646 - val_loss: 0.0447 - val_mae: 0.2359\n",
            "Epoch 74/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0581 - mae: 0.2631 - val_loss: 0.0418 - val_mae: 0.2291\n",
            "Epoch 75/100\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0561 - mae: 0.2582 - val_loss: 0.0461 - val_mae: 0.2396\n",
            "Epoch 76/100\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0555 - mae: 0.2572 - val_loss: 0.0418 - val_mae: 0.2302\n",
            "Epoch 77/100\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0557 - mae: 0.2561 - val_loss: 0.0482 - val_mae: 0.2453\n",
            "Epoch 78/100\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.0553 - mae: 0.2551 - val_loss: 0.0407 - val_mae: 0.2260\n",
            "Epoch 79/100\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.0543 - mae: 0.2527 - val_loss: 0.0417 - val_mae: 0.2284\n",
            "Epoch 80/100\n",
            "30/30 [==============================] - 1s 20ms/step - loss: 0.0545 - mae: 0.2517 - val_loss: 0.0441 - val_mae: 0.2335\n",
            "Epoch 81/100\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0519 - mae: 0.2476 - val_loss: 0.0439 - val_mae: 0.2338\n",
            "Epoch 82/100\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0517 - mae: 0.2466 - val_loss: 0.0379 - val_mae: 0.2203\n",
            "Epoch 83/100\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0536 - mae: 0.2479 - val_loss: 0.0432 - val_mae: 0.2316\n",
            "Epoch 84/100\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0518 - mae: 0.2457 - val_loss: 0.0359 - val_mae: 0.2136\n",
            "Epoch 85/100\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0501 - mae: 0.2418 - val_loss: 0.0488 - val_mae: 0.2439\n",
            "Epoch 86/100\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.0505 - mae: 0.2417 - val_loss: 0.0401 - val_mae: 0.2220\n",
            "Epoch 87/100\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0497 - mae: 0.2401 - val_loss: 0.0361 - val_mae: 0.2141\n",
            "Epoch 88/100\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0500 - mae: 0.2403 - val_loss: 0.0491 - val_mae: 0.2446\n",
            "Epoch 89/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0503 - mae: 0.2403 - val_loss: 0.0428 - val_mae: 0.2278\n",
            "Epoch 90/100\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0492 - mae: 0.2374 - val_loss: 0.0393 - val_mae: 0.2189\n",
            "Epoch 91/100\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0504 - mae: 0.2392 - val_loss: 0.0388 - val_mae: 0.2174\n",
            "Epoch 92/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0494 - mae: 0.2374 - val_loss: 0.0421 - val_mae: 0.2263\n",
            "Epoch 93/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0490 - mae: 0.2359 - val_loss: 0.0515 - val_mae: 0.2487\n",
            "Epoch 94/100\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0471 - mae: 0.2332 - val_loss: 0.0391 - val_mae: 0.2173\n",
            "Epoch 95/100\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0471 - mae: 0.2326 - val_loss: 0.0669 - val_mae: 0.2823\n",
            "Epoch 96/100\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.0491 - mae: 0.2355 - val_loss: 0.0408 - val_mae: 0.2203\n",
            "Epoch 97/100\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.0465 - mae: 0.2306 - val_loss: 0.0367 - val_mae: 0.2109\n",
            "Epoch 98/100\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.0455 - mae: 0.2288 - val_loss: 0.0350 - val_mae: 0.2080\n",
            "Epoch 99/100\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0467 - mae: 0.2300 - val_loss: 0.0353 - val_mae: 0.2084\n",
            "Epoch 100/100\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.0465 - mae: 0.2300 - val_loss: 0.0340 - val_mae: 0.2040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 19:12:06,461] Trial 46 finished with value: 0.03400759771466255 and parameters: {'learning_rate': 0.00010563311589208274, 'dropout_rate': 0.14937161291841888, 'batch_size': 128, 'epochs': 100}. Best is trial 42 with value: 0.029339225962758064.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "15/15 [==============================] - 3s 41ms/step - loss: 0.3888 - mae: 0.8051 - val_loss: 4.7749 - val_mae: 5.3827\n",
            "Epoch 2/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.3395 - mae: 0.7385 - val_loss: 4.1616 - val_mae: 4.7622\n",
            "Epoch 3/150\n",
            "15/15 [==============================] - 0s 27ms/step - loss: 0.3163 - mae: 0.7065 - val_loss: 3.8204 - val_mae: 4.4488\n",
            "Epoch 4/150\n",
            "15/15 [==============================] - 0s 34ms/step - loss: 0.2956 - mae: 0.6737 - val_loss: 3.3183 - val_mae: 3.9372\n",
            "Epoch 5/150\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.2727 - mae: 0.6412 - val_loss: 2.6991 - val_mae: 3.2890\n",
            "Epoch 6/150\n",
            "15/15 [==============================] - 0s 30ms/step - loss: 0.2606 - mae: 0.6227 - val_loss: 2.2171 - val_mae: 2.8082\n",
            "Epoch 7/150\n",
            "15/15 [==============================] - 0s 29ms/step - loss: 0.2409 - mae: 0.5931 - val_loss: 1.6517 - val_mae: 2.2056\n",
            "Epoch 8/150\n",
            "15/15 [==============================] - 0s 28ms/step - loss: 0.2298 - mae: 0.5764 - val_loss: 1.6114 - val_mae: 2.1610\n",
            "Epoch 9/150\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.2168 - mae: 0.5568 - val_loss: 1.2665 - val_mae: 1.8037\n",
            "Epoch 10/150\n",
            "15/15 [==============================] - 0s 28ms/step - loss: 0.2076 - mae: 0.5417 - val_loss: 0.9965 - val_mae: 1.4984\n",
            "Epoch 11/150\n",
            "15/15 [==============================] - 0s 27ms/step - loss: 0.1953 - mae: 0.5221 - val_loss: 0.6559 - val_mae: 1.1216\n",
            "Epoch 12/150\n",
            "15/15 [==============================] - 0s 27ms/step - loss: 0.1879 - mae: 0.5104 - val_loss: 0.5049 - val_mae: 0.9571\n",
            "Epoch 13/150\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.1799 - mae: 0.4965 - val_loss: 0.2926 - val_mae: 0.6676\n",
            "Epoch 14/150\n",
            "15/15 [==============================] - 0s 27ms/step - loss: 0.1700 - mae: 0.4805 - val_loss: 0.3852 - val_mae: 0.8093\n",
            "Epoch 15/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.1620 - mae: 0.4671 - val_loss: 0.2461 - val_mae: 0.6206\n",
            "Epoch 16/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.1601 - mae: 0.4625 - val_loss: 0.2632 - val_mae: 0.6601\n",
            "Epoch 17/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.1531 - mae: 0.4506 - val_loss: 0.2058 - val_mae: 0.5602\n",
            "Epoch 18/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.1447 - mae: 0.4367 - val_loss: 0.1644 - val_mae: 0.4963\n",
            "Epoch 19/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.1441 - mae: 0.4343 - val_loss: 0.1504 - val_mae: 0.4624\n",
            "Epoch 20/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.1370 - mae: 0.4223 - val_loss: 0.1250 - val_mae: 0.4179\n",
            "Epoch 21/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.1306 - mae: 0.4107 - val_loss: 0.1294 - val_mae: 0.4279\n",
            "Epoch 22/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.1287 - mae: 0.4063 - val_loss: 0.1036 - val_mae: 0.3752\n",
            "Epoch 23/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.1250 - mae: 0.3998 - val_loss: 0.0997 - val_mae: 0.3735\n",
            "Epoch 24/150\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.1208 - mae: 0.3926 - val_loss: 0.1140 - val_mae: 0.3963\n",
            "Epoch 25/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.1194 - mae: 0.3884 - val_loss: 0.1018 - val_mae: 0.3744\n",
            "Epoch 26/150\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.1181 - mae: 0.3841 - val_loss: 0.0966 - val_mae: 0.3638\n",
            "Epoch 27/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.1110 - mae: 0.3731 - val_loss: 0.0940 - val_mae: 0.3552\n",
            "Epoch 28/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.1071 - mae: 0.3673 - val_loss: 0.0876 - val_mae: 0.3339\n",
            "Epoch 29/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.1057 - mae: 0.3633 - val_loss: 0.0862 - val_mae: 0.3270\n",
            "Epoch 30/150\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.1033 - mae: 0.3586 - val_loss: 0.0815 - val_mae: 0.3212\n",
            "Epoch 31/150\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.0985 - mae: 0.3489 - val_loss: 0.0668 - val_mae: 0.2915\n",
            "Epoch 32/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.0961 - mae: 0.3430 - val_loss: 0.0717 - val_mae: 0.2987\n",
            "Epoch 33/150\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.0939 - mae: 0.3388 - val_loss: 0.0818 - val_mae: 0.3165\n",
            "Epoch 34/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.0912 - mae: 0.3332 - val_loss: 0.0710 - val_mae: 0.2970\n",
            "Epoch 35/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.0883 - mae: 0.3273 - val_loss: 0.0620 - val_mae: 0.2799\n",
            "Epoch 36/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0865 - mae: 0.3230 - val_loss: 0.0567 - val_mae: 0.2692\n",
            "Epoch 37/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.0837 - mae: 0.3181 - val_loss: 0.0571 - val_mae: 0.2686\n",
            "Epoch 38/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.0816 - mae: 0.3141 - val_loss: 0.0546 - val_mae: 0.2650\n",
            "Epoch 39/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.0782 - mae: 0.3077 - val_loss: 0.0542 - val_mae: 0.2670\n",
            "Epoch 40/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.0782 - mae: 0.3054 - val_loss: 0.0532 - val_mae: 0.2637\n",
            "Epoch 41/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.0753 - mae: 0.2997 - val_loss: 0.0560 - val_mae: 0.2729\n",
            "Epoch 42/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.0735 - mae: 0.2950 - val_loss: 0.0497 - val_mae: 0.2561\n",
            "Epoch 43/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.0719 - mae: 0.2925 - val_loss: 0.0564 - val_mae: 0.2728\n",
            "Epoch 44/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.0722 - mae: 0.2916 - val_loss: 0.0474 - val_mae: 0.2480\n",
            "Epoch 45/150\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.0707 - mae: 0.2886 - val_loss: 0.0560 - val_mae: 0.2777\n",
            "Epoch 46/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.0682 - mae: 0.2834 - val_loss: 0.0497 - val_mae: 0.2565\n",
            "Epoch 47/150\n",
            "15/15 [==============================] - 0s 30ms/step - loss: 0.0677 - mae: 0.2808 - val_loss: 0.0616 - val_mae: 0.2846\n",
            "Epoch 48/150\n",
            "15/15 [==============================] - 0s 31ms/step - loss: 0.0640 - mae: 0.2749 - val_loss: 0.0590 - val_mae: 0.2785\n",
            "Epoch 49/150\n",
            "15/15 [==============================] - 0s 31ms/step - loss: 0.0627 - mae: 0.2711 - val_loss: 0.0582 - val_mae: 0.2802\n",
            "Epoch 50/150\n",
            "15/15 [==============================] - 0s 31ms/step - loss: 0.0613 - mae: 0.2687 - val_loss: 0.0549 - val_mae: 0.2731\n",
            "Epoch 51/150\n",
            "15/15 [==============================] - 0s 27ms/step - loss: 0.0620 - mae: 0.2681 - val_loss: 0.0603 - val_mae: 0.2951\n",
            "Epoch 52/150\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.0629 - mae: 0.2689 - val_loss: 0.0460 - val_mae: 0.2462\n",
            "Epoch 53/150\n",
            "15/15 [==============================] - 0s 29ms/step - loss: 0.0604 - mae: 0.2637 - val_loss: 0.0501 - val_mae: 0.2566\n",
            "Epoch 54/150\n",
            "15/15 [==============================] - 0s 31ms/step - loss: 0.0586 - mae: 0.2607 - val_loss: 0.0442 - val_mae: 0.2422\n",
            "Epoch 55/150\n",
            "15/15 [==============================] - 0s 32ms/step - loss: 0.0571 - mae: 0.2566 - val_loss: 0.0456 - val_mae: 0.2476\n",
            "Epoch 56/150\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.0566 - mae: 0.2556 - val_loss: 0.0574 - val_mae: 0.2801\n",
            "Epoch 57/150\n",
            "15/15 [==============================] - 0s 29ms/step - loss: 0.0551 - mae: 0.2527 - val_loss: 0.0476 - val_mae: 0.2529\n",
            "Epoch 58/150\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.0552 - mae: 0.2505 - val_loss: 0.0511 - val_mae: 0.2605\n",
            "Epoch 59/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0543 - mae: 0.2492 - val_loss: 0.0611 - val_mae: 0.2864\n",
            "Epoch 60/150\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.0543 - mae: 0.2482 - val_loss: 0.0474 - val_mae: 0.2482\n",
            "Epoch 61/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.0508 - mae: 0.2431 - val_loss: 0.0650 - val_mae: 0.2955\n",
            "Epoch 62/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0500 - mae: 0.2410 - val_loss: 0.0475 - val_mae: 0.2470\n",
            "Epoch 63/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.0518 - mae: 0.2416 - val_loss: 0.0642 - val_mae: 0.2963\n",
            "Epoch 64/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0518 - mae: 0.2404 - val_loss: 0.0621 - val_mae: 0.2878\n",
            "Epoch 65/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.0523 - mae: 0.2418 - val_loss: 0.0513 - val_mae: 0.2612\n",
            "Epoch 66/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.0506 - mae: 0.2384 - val_loss: 0.0667 - val_mae: 0.3054\n",
            "Epoch 67/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.0492 - mae: 0.2359 - val_loss: 0.0718 - val_mae: 0.3137\n",
            "Epoch 68/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.0483 - mae: 0.2343 - val_loss: 0.0834 - val_mae: 0.3360\n",
            "Epoch 69/150\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.0484 - mae: 0.2340 - val_loss: 0.1109 - val_mae: 0.3974\n",
            "Epoch 70/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.0481 - mae: 0.2331 - val_loss: 0.1311 - val_mae: 0.4394\n",
            "Epoch 71/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.0481 - mae: 0.2317 - val_loss: 0.0711 - val_mae: 0.3069\n",
            "Epoch 72/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.0466 - mae: 0.2303 - val_loss: 0.0744 - val_mae: 0.3198\n",
            "Epoch 73/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.0479 - mae: 0.2309 - val_loss: 0.0799 - val_mae: 0.3289\n",
            "Epoch 74/150\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.0472 - mae: 0.2299 - val_loss: 0.0692 - val_mae: 0.2975\n",
            "Epoch 75/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.0463 - mae: 0.2280 - val_loss: 0.0792 - val_mae: 0.3236\n",
            "Epoch 76/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0472 - mae: 0.2280 - val_loss: 0.0685 - val_mae: 0.2983\n",
            "Epoch 77/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0449 - mae: 0.2256 - val_loss: 0.0602 - val_mae: 0.2773\n",
            "Epoch 78/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0449 - mae: 0.2247 - val_loss: 0.0800 - val_mae: 0.3269\n",
            "Epoch 79/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.0457 - mae: 0.2254 - val_loss: 0.0507 - val_mae: 0.2521\n",
            "Epoch 80/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0443 - mae: 0.2226 - val_loss: 0.0487 - val_mae: 0.2473\n",
            "Epoch 81/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0442 - mae: 0.2223 - val_loss: 0.0614 - val_mae: 0.2812\n",
            "Epoch 82/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.0449 - mae: 0.2235 - val_loss: 0.0615 - val_mae: 0.2820\n",
            "Epoch 83/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0445 - mae: 0.2218 - val_loss: 0.0518 - val_mae: 0.2535\n",
            "Epoch 84/150\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.0435 - mae: 0.2203 - val_loss: 0.0858 - val_mae: 0.3357\n",
            "Epoch 85/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0436 - mae: 0.2208 - val_loss: 0.0879 - val_mae: 0.3436\n",
            "Epoch 86/150\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.0432 - mae: 0.2197 - val_loss: 0.0824 - val_mae: 0.3380\n",
            "Epoch 87/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0437 - mae: 0.2199 - val_loss: 0.0665 - val_mae: 0.3007\n",
            "Epoch 88/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0422 - mae: 0.2173 - val_loss: 0.1243 - val_mae: 0.4238\n",
            "Epoch 89/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0422 - mae: 0.2171 - val_loss: 0.0592 - val_mae: 0.2775\n",
            "Epoch 90/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.0439 - mae: 0.2193 - val_loss: 0.0511 - val_mae: 0.2515\n",
            "Epoch 91/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.0420 - mae: 0.2158 - val_loss: 0.0398 - val_mae: 0.2218\n",
            "Epoch 92/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.0418 - mae: 0.2166 - val_loss: 0.0628 - val_mae: 0.2827\n",
            "Epoch 93/150\n",
            "15/15 [==============================] - 0s 28ms/step - loss: 0.0424 - mae: 0.2163 - val_loss: 0.0544 - val_mae: 0.2556\n",
            "Epoch 94/150\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.0422 - mae: 0.2154 - val_loss: 0.0515 - val_mae: 0.2537\n",
            "Epoch 95/150\n",
            "15/15 [==============================] - 0s 27ms/step - loss: 0.0428 - mae: 0.2163 - val_loss: 0.0969 - val_mae: 0.3657\n",
            "Epoch 96/150\n",
            "15/15 [==============================] - 0s 30ms/step - loss: 0.0425 - mae: 0.2160 - val_loss: 0.1074 - val_mae: 0.3856\n",
            "Epoch 97/150\n",
            "15/15 [==============================] - 0s 27ms/step - loss: 0.0393 - mae: 0.2111 - val_loss: 0.0730 - val_mae: 0.3069\n",
            "Epoch 98/150\n",
            "15/15 [==============================] - 1s 37ms/step - loss: 0.0405 - mae: 0.2117 - val_loss: 0.0651 - val_mae: 0.2857\n",
            "Epoch 99/150\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.0398 - mae: 0.2110 - val_loss: 0.1498 - val_mae: 0.4726\n",
            "Epoch 100/150\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.0411 - mae: 0.2132 - val_loss: 0.1165 - val_mae: 0.4077\n",
            "Epoch 101/150\n",
            "15/15 [==============================] - 0s 30ms/step - loss: 0.0406 - mae: 0.2114 - val_loss: 0.1714 - val_mae: 0.5107\n",
            "Epoch 102/150\n",
            "15/15 [==============================] - 0s 27ms/step - loss: 0.0379 - mae: 0.2069 - val_loss: 0.0758 - val_mae: 0.3119\n",
            "Epoch 103/150\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.0391 - mae: 0.2090 - val_loss: 0.0504 - val_mae: 0.2489\n",
            "Epoch 104/150\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.0387 - mae: 0.2086 - val_loss: 0.0504 - val_mae: 0.2453\n",
            "Epoch 105/150\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.0390 - mae: 0.2088 - val_loss: 0.0516 - val_mae: 0.2483\n",
            "Epoch 106/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.0394 - mae: 0.2091 - val_loss: 0.0501 - val_mae: 0.2455\n",
            "Epoch 107/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.0414 - mae: 0.2107 - val_loss: 0.0507 - val_mae: 0.2489\n",
            "Epoch 108/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0379 - mae: 0.2064 - val_loss: 0.0502 - val_mae: 0.2467\n",
            "Epoch 109/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0396 - mae: 0.2078 - val_loss: 0.0428 - val_mae: 0.2266\n",
            "Epoch 110/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.0393 - mae: 0.2071 - val_loss: 0.0441 - val_mae: 0.2305\n",
            "Epoch 111/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0387 - mae: 0.2065 - val_loss: 0.0423 - val_mae: 0.2250\n",
            "Epoch 112/150\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.0404 - mae: 0.2085 - val_loss: 0.0385 - val_mae: 0.2140\n",
            "Epoch 113/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0384 - mae: 0.2057 - val_loss: 0.0388 - val_mae: 0.2140\n",
            "Epoch 114/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.0376 - mae: 0.2044 - val_loss: 0.0405 - val_mae: 0.2182\n",
            "Epoch 115/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.0376 - mae: 0.2032 - val_loss: 0.0385 - val_mae: 0.2129\n",
            "Epoch 116/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.0361 - mae: 0.2013 - val_loss: 0.0363 - val_mae: 0.2093\n",
            "Epoch 117/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.0372 - mae: 0.2036 - val_loss: 0.0372 - val_mae: 0.2107\n",
            "Epoch 118/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0368 - mae: 0.2023 - val_loss: 0.0428 - val_mae: 0.2264\n",
            "Epoch 119/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0381 - mae: 0.2034 - val_loss: 0.0385 - val_mae: 0.2151\n",
            "Epoch 120/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0377 - mae: 0.2028 - val_loss: 0.0349 - val_mae: 0.2080\n",
            "Epoch 121/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.0371 - mae: 0.2028 - val_loss: 0.0367 - val_mae: 0.2094\n",
            "Epoch 122/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.0368 - mae: 0.2013 - val_loss: 0.0610 - val_mae: 0.2781\n",
            "Epoch 123/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.0377 - mae: 0.2034 - val_loss: 0.0390 - val_mae: 0.2196\n",
            "Epoch 124/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.0374 - mae: 0.2013 - val_loss: 0.0436 - val_mae: 0.2302\n",
            "Epoch 125/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.0367 - mae: 0.2006 - val_loss: 0.0517 - val_mae: 0.2520\n",
            "Epoch 126/150\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.0356 - mae: 0.1992 - val_loss: 0.0512 - val_mae: 0.2515\n",
            "Epoch 127/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.0362 - mae: 0.2002 - val_loss: 0.0420 - val_mae: 0.2265\n",
            "Epoch 128/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.0363 - mae: 0.1999 - val_loss: 0.0382 - val_mae: 0.2156\n",
            "Epoch 129/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.0357 - mae: 0.1988 - val_loss: 0.0361 - val_mae: 0.2094\n",
            "Epoch 130/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.0363 - mae: 0.1995 - val_loss: 0.0363 - val_mae: 0.2084\n",
            "Epoch 131/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.0354 - mae: 0.1987 - val_loss: 0.0376 - val_mae: 0.2105\n",
            "Epoch 132/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.0348 - mae: 0.1975 - val_loss: 0.0400 - val_mae: 0.2159\n",
            "Epoch 133/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.0355 - mae: 0.1977 - val_loss: 0.0426 - val_mae: 0.2276\n",
            "Epoch 134/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0357 - mae: 0.1982 - val_loss: 0.0407 - val_mae: 0.2199\n",
            "Epoch 135/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0350 - mae: 0.1970 - val_loss: 0.0437 - val_mae: 0.2264\n",
            "Epoch 136/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.0363 - mae: 0.1996 - val_loss: 0.0516 - val_mae: 0.2496\n",
            "Epoch 137/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.0353 - mae: 0.1975 - val_loss: 0.0417 - val_mae: 0.2208\n",
            "Epoch 138/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.0352 - mae: 0.1969 - val_loss: 0.0476 - val_mae: 0.2363\n",
            "Epoch 139/150\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.0345 - mae: 0.1958 - val_loss: 0.0387 - val_mae: 0.2135\n",
            "Epoch 140/150\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.0352 - mae: 0.1970 - val_loss: 0.0394 - val_mae: 0.2204\n",
            "Epoch 141/150\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.0353 - mae: 0.1968 - val_loss: 0.0340 - val_mae: 0.1988\n",
            "Epoch 142/150\n",
            "15/15 [==============================] - 1s 36ms/step - loss: 0.0343 - mae: 0.1952 - val_loss: 0.0414 - val_mae: 0.2212\n",
            "Epoch 143/150\n",
            "15/15 [==============================] - 0s 31ms/step - loss: 0.0342 - mae: 0.1949 - val_loss: 0.0329 - val_mae: 0.1978\n",
            "Epoch 144/150\n",
            "15/15 [==============================] - 0s 29ms/step - loss: 0.0340 - mae: 0.1945 - val_loss: 0.0529 - val_mae: 0.2553\n",
            "Epoch 145/150\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.0347 - mae: 0.1955 - val_loss: 0.0356 - val_mae: 0.2057\n",
            "Epoch 146/150\n",
            "15/15 [==============================] - 0s 32ms/step - loss: 0.0355 - mae: 0.1973 - val_loss: 0.0385 - val_mae: 0.2117\n",
            "Epoch 147/150\n",
            "15/15 [==============================] - 0s 32ms/step - loss: 0.0336 - mae: 0.1939 - val_loss: 0.0351 - val_mae: 0.2034\n",
            "Epoch 148/150\n",
            "15/15 [==============================] - 1s 35ms/step - loss: 0.0335 - mae: 0.1936 - val_loss: 0.0357 - val_mae: 0.2053\n",
            "Epoch 149/150\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.0330 - mae: 0.1925 - val_loss: 0.0330 - val_mae: 0.1986\n",
            "Epoch 150/150\n",
            "15/15 [==============================] - 0s 27ms/step - loss: 0.0342 - mae: 0.1944 - val_loss: 0.0359 - val_mae: 0.2063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 19:13:31,106] Trial 47 finished with value: 0.035879578441381454 and parameters: {'learning_rate': 0.0002444852966897274, 'dropout_rate': 0.1051140173553178, 'batch_size': 256, 'epochs': 150}. Best is trial 42 with value: 0.029339225962758064.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "120/120 [==============================] - 3s 10ms/step - loss: 0.5283 - mae: 0.9754 - val_loss: 0.3715 - val_mae: 0.7814\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5318 - mae: 0.9804 - val_loss: 0.1878 - val_mae: 0.5329\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.5205 - mae: 0.9675 - val_loss: 0.1714 - val_mae: 0.4961\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.5259 - mae: 0.9730 - val_loss: 0.1774 - val_mae: 0.5089\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.5253 - mae: 0.9730 - val_loss: 0.1780 - val_mae: 0.5100\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.5206 - mae: 0.9671 - val_loss: 0.1781 - val_mae: 0.5116\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.5214 - mae: 0.9665 - val_loss: 0.1798 - val_mae: 0.5151\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5232 - mae: 0.9694 - val_loss: 0.1799 - val_mae: 0.5142\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5145 - mae: 0.9586 - val_loss: 0.1814 - val_mae: 0.5177\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5191 - mae: 0.9650 - val_loss: 0.1810 - val_mae: 0.5168\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5173 - mae: 0.9629 - val_loss: 0.1793 - val_mae: 0.5147\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.5208 - mae: 0.9666 - val_loss: 0.1786 - val_mae: 0.5123\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5083 - mae: 0.9504 - val_loss: 0.1756 - val_mae: 0.5068\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.5082 - mae: 0.9504 - val_loss: 0.1770 - val_mae: 0.5099\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.5126 - mae: 0.9564 - val_loss: 0.1756 - val_mae: 0.5072\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5079 - mae: 0.9507 - val_loss: 0.1757 - val_mae: 0.5062\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5076 - mae: 0.9503 - val_loss: 0.1748 - val_mae: 0.5055\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5096 - mae: 0.9530 - val_loss: 0.1769 - val_mae: 0.5096\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.5043 - mae: 0.9467 - val_loss: 0.1741 - val_mae: 0.5041\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.5077 - mae: 0.9495 - val_loss: 0.1733 - val_mae: 0.5034\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.5010 - mae: 0.9424 - val_loss: 0.1734 - val_mae: 0.5037\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.5016 - mae: 0.9418 - val_loss: 0.1733 - val_mae: 0.5027\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4954 - mae: 0.9341 - val_loss: 0.1716 - val_mae: 0.4997\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4963 - mae: 0.9363 - val_loss: 0.1713 - val_mae: 0.5003\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4993 - mae: 0.9398 - val_loss: 0.1711 - val_mae: 0.5004\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4944 - mae: 0.9327 - val_loss: 0.1697 - val_mae: 0.4971\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4923 - mae: 0.9310 - val_loss: 0.1698 - val_mae: 0.4970\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4957 - mae: 0.9341 - val_loss: 0.1693 - val_mae: 0.4964\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4907 - mae: 0.9281 - val_loss: 0.1690 - val_mae: 0.4958\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4938 - mae: 0.9319 - val_loss: 0.1668 - val_mae: 0.4920\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4894 - mae: 0.9272 - val_loss: 0.1662 - val_mae: 0.4906\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4869 - mae: 0.9233 - val_loss: 0.1652 - val_mae: 0.4884\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4881 - mae: 0.9249 - val_loss: 0.1666 - val_mae: 0.4899\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4860 - mae: 0.9221 - val_loss: 0.1655 - val_mae: 0.4891\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4822 - mae: 0.9176 - val_loss: 0.1656 - val_mae: 0.4896\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4808 - mae: 0.9167 - val_loss: 0.1651 - val_mae: 0.4886\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4886 - mae: 0.9264 - val_loss: 0.1635 - val_mae: 0.4868\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.4810 - mae: 0.9168 - val_loss: 0.1668 - val_mae: 0.4917\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.4720 - mae: 0.9046 - val_loss: 0.1641 - val_mae: 0.4877\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4823 - mae: 0.9177 - val_loss: 0.1647 - val_mae: 0.4890\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4752 - mae: 0.9099 - val_loss: 0.1641 - val_mae: 0.4863\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4794 - mae: 0.9142 - val_loss: 0.1632 - val_mae: 0.4844\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4755 - mae: 0.9094 - val_loss: 0.1639 - val_mae: 0.4864\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4731 - mae: 0.9075 - val_loss: 0.1634 - val_mae: 0.4852\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4797 - mae: 0.9140 - val_loss: 0.1647 - val_mae: 0.4880\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4768 - mae: 0.9103 - val_loss: 0.1645 - val_mae: 0.4882\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4719 - mae: 0.9040 - val_loss: 0.1627 - val_mae: 0.4846\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4775 - mae: 0.9109 - val_loss: 0.1627 - val_mae: 0.4840\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4709 - mae: 0.9017 - val_loss: 0.1623 - val_mae: 0.4833\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4675 - mae: 0.8986 - val_loss: 0.1611 - val_mae: 0.4813\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.4687 - mae: 0.9006 - val_loss: 0.1605 - val_mae: 0.4816\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.4612 - mae: 0.8912 - val_loss: 0.1601 - val_mae: 0.4811\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4634 - mae: 0.8941 - val_loss: 0.1574 - val_mae: 0.4756\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.4614 - mae: 0.8899 - val_loss: 0.1577 - val_mae: 0.4755\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4628 - mae: 0.8916 - val_loss: 0.1596 - val_mae: 0.4786\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4589 - mae: 0.8874 - val_loss: 0.1586 - val_mae: 0.4772\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4552 - mae: 0.8831 - val_loss: 0.1577 - val_mae: 0.4757\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4542 - mae: 0.8809 - val_loss: 0.1574 - val_mae: 0.4754\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4541 - mae: 0.8817 - val_loss: 0.1549 - val_mae: 0.4696\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4542 - mae: 0.8810 - val_loss: 0.1552 - val_mae: 0.4718\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4438 - mae: 0.8684 - val_loss: 0.1549 - val_mae: 0.4718\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4549 - mae: 0.8811 - val_loss: 0.1546 - val_mae: 0.4709\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4545 - mae: 0.8800 - val_loss: 0.1529 - val_mae: 0.4672\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4546 - mae: 0.8809 - val_loss: 0.1562 - val_mae: 0.4732\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4392 - mae: 0.8621 - val_loss: 0.1554 - val_mae: 0.4719\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4523 - mae: 0.8790 - val_loss: 0.1532 - val_mae: 0.4676\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4419 - mae: 0.8642 - val_loss: 0.1527 - val_mae: 0.4679\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.4373 - mae: 0.8596 - val_loss: 0.1532 - val_mae: 0.4688\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 11ms/step - loss: 0.4418 - mae: 0.8639 - val_loss: 0.1519 - val_mae: 0.4659\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4426 - mae: 0.8672 - val_loss: 0.1511 - val_mae: 0.4648\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4410 - mae: 0.8633 - val_loss: 0.1525 - val_mae: 0.4660\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4463 - mae: 0.8697 - val_loss: 0.1530 - val_mae: 0.4668\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4418 - mae: 0.8642 - val_loss: 0.1506 - val_mae: 0.4638\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4387 - mae: 0.8620 - val_loss: 0.1535 - val_mae: 0.4692\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4387 - mae: 0.8607 - val_loss: 0.1524 - val_mae: 0.4667\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4318 - mae: 0.8511 - val_loss: 0.1495 - val_mae: 0.4611\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4319 - mae: 0.8508 - val_loss: 0.1489 - val_mae: 0.4599\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4310 - mae: 0.8499 - val_loss: 0.1499 - val_mae: 0.4615\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4311 - mae: 0.8519 - val_loss: 0.1506 - val_mae: 0.4628\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4289 - mae: 0.8462 - val_loss: 0.1489 - val_mae: 0.4599\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4291 - mae: 0.8489 - val_loss: 0.1480 - val_mae: 0.4579\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4291 - mae: 0.8481 - val_loss: 0.1486 - val_mae: 0.4596\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.4255 - mae: 0.8431 - val_loss: 0.1465 - val_mae: 0.4561\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4209 - mae: 0.8374 - val_loss: 0.1455 - val_mae: 0.4539\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4209 - mae: 0.8370 - val_loss: 0.1481 - val_mae: 0.4588\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4214 - mae: 0.8380 - val_loss: 0.1487 - val_mae: 0.4602\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4244 - mae: 0.8410 - val_loss: 0.1460 - val_mae: 0.4550\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4213 - mae: 0.8372 - val_loss: 0.1463 - val_mae: 0.4567\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4230 - mae: 0.8392 - val_loss: 0.1457 - val_mae: 0.4541\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4183 - mae: 0.8331 - val_loss: 0.1451 - val_mae: 0.4533\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4210 - mae: 0.8363 - val_loss: 0.1452 - val_mae: 0.4539\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4155 - mae: 0.8292 - val_loss: 0.1453 - val_mae: 0.4535\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4190 - mae: 0.8347 - val_loss: 0.1444 - val_mae: 0.4516\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 0.4138 - mae: 0.8278 - val_loss: 0.1449 - val_mae: 0.4513\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 0.4133 - mae: 0.8271 - val_loss: 0.1461 - val_mae: 0.4541\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4106 - mae: 0.8221 - val_loss: 0.1439 - val_mae: 0.4506\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4105 - mae: 0.8246 - val_loss: 0.1443 - val_mae: 0.4513\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 0.4084 - mae: 0.8207 - val_loss: 0.1437 - val_mae: 0.4495\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 0.4079 - mae: 0.8188 - val_loss: 0.1437 - val_mae: 0.4497\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.4061 - mae: 0.8179 - val_loss: 0.1443 - val_mae: 0.4505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 19:15:55,549] Trial 48 finished with value: 0.1442839652299881 and parameters: {'learning_rate': 3.0257623767359626e-06, 'dropout_rate': 0.40402174825059667, 'batch_size': 32, 'epochs': 100}. Best is trial 42 with value: 0.029339225962758064.\n",
            "<ipython-input-87-dc6b60c2879f>:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
            "<ipython-input-87-dc6b60c2879f>:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "60/60 [==============================] - 4s 20ms/step - loss: 0.4595 - mae: 0.8909 - val_loss: 3.8348 - val_mae: 4.4462\n",
            "Epoch 2/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.4494 - mae: 0.8798 - val_loss: 1.6345 - val_mae: 2.2044\n",
            "Epoch 3/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.4437 - mae: 0.8717 - val_loss: 0.5529 - val_mae: 1.0252\n",
            "Epoch 4/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.4274 - mae: 0.8510 - val_loss: 0.1975 - val_mae: 0.5331\n",
            "Epoch 5/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.4262 - mae: 0.8489 - val_loss: 0.1930 - val_mae: 0.5461\n",
            "Epoch 6/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.4085 - mae: 0.8249 - val_loss: 0.1971 - val_mae: 0.5570\n",
            "Epoch 7/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.3987 - mae: 0.8123 - val_loss: 0.1886 - val_mae: 0.5378\n",
            "Epoch 8/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.3891 - mae: 0.7989 - val_loss: 0.1850 - val_mae: 0.5286\n",
            "Epoch 9/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.3822 - mae: 0.7892 - val_loss: 0.1767 - val_mae: 0.5087\n",
            "Epoch 10/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.3782 - mae: 0.7825 - val_loss: 0.1725 - val_mae: 0.4992\n",
            "Epoch 11/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.3677 - mae: 0.7687 - val_loss: 0.1589 - val_mae: 0.4767\n",
            "Epoch 12/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.3568 - mae: 0.7555 - val_loss: 0.1587 - val_mae: 0.4761\n",
            "Epoch 13/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.3553 - mae: 0.7522 - val_loss: 0.1568 - val_mae: 0.4755\n",
            "Epoch 14/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.3425 - mae: 0.7337 - val_loss: 0.1623 - val_mae: 0.4831\n",
            "Epoch 15/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.3374 - mae: 0.7263 - val_loss: 0.1548 - val_mae: 0.4758\n",
            "Epoch 16/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.3281 - mae: 0.7133 - val_loss: 0.1527 - val_mae: 0.4688\n",
            "Epoch 17/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.3185 - mae: 0.6989 - val_loss: 0.1487 - val_mae: 0.4624\n",
            "Epoch 18/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.3148 - mae: 0.6936 - val_loss: 0.1413 - val_mae: 0.4507\n",
            "Epoch 19/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.3106 - mae: 0.6871 - val_loss: 0.1472 - val_mae: 0.4591\n",
            "Epoch 20/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.3027 - mae: 0.6755 - val_loss: 0.1464 - val_mae: 0.4558\n",
            "Epoch 21/200\n",
            "60/60 [==============================] - 1s 16ms/step - loss: 0.3019 - mae: 0.6741 - val_loss: 0.1423 - val_mae: 0.4490\n",
            "Epoch 22/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.2924 - mae: 0.6593 - val_loss: 0.1347 - val_mae: 0.4349\n",
            "Epoch 23/200\n",
            "60/60 [==============================] - 1s 16ms/step - loss: 0.2844 - mae: 0.6483 - val_loss: 0.1399 - val_mae: 0.4450\n",
            "Epoch 24/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.2790 - mae: 0.6401 - val_loss: 0.1339 - val_mae: 0.4351\n",
            "Epoch 25/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.2715 - mae: 0.6292 - val_loss: 0.1221 - val_mae: 0.4111\n",
            "Epoch 26/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.2700 - mae: 0.6256 - val_loss: 0.1237 - val_mae: 0.4149\n",
            "Epoch 27/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.2659 - mae: 0.6209 - val_loss: 0.1192 - val_mae: 0.4071\n",
            "Epoch 28/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.2585 - mae: 0.6079 - val_loss: 0.1140 - val_mae: 0.3955\n",
            "Epoch 29/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.2517 - mae: 0.5987 - val_loss: 0.1195 - val_mae: 0.4051\n",
            "Epoch 30/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.2473 - mae: 0.5910 - val_loss: 0.1170 - val_mae: 0.4025\n",
            "Epoch 31/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.2434 - mae: 0.5852 - val_loss: 0.1159 - val_mae: 0.4001\n",
            "Epoch 32/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.2378 - mae: 0.5766 - val_loss: 0.1159 - val_mae: 0.4003\n",
            "Epoch 33/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.2341 - mae: 0.5709 - val_loss: 0.1186 - val_mae: 0.4053\n",
            "Epoch 34/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.2318 - mae: 0.5664 - val_loss: 0.1208 - val_mae: 0.4094\n",
            "Epoch 35/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.2253 - mae: 0.5575 - val_loss: 0.1228 - val_mae: 0.4136\n",
            "Epoch 36/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.2184 - mae: 0.5459 - val_loss: 0.1136 - val_mae: 0.3949\n",
            "Epoch 37/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.2182 - mae: 0.5449 - val_loss: 0.1079 - val_mae: 0.3826\n",
            "Epoch 38/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.2110 - mae: 0.5334 - val_loss: 0.1063 - val_mae: 0.3797\n",
            "Epoch 39/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.2093 - mae: 0.5309 - val_loss: 0.1011 - val_mae: 0.3680\n",
            "Epoch 40/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.2052 - mae: 0.5245 - val_loss: 0.1021 - val_mae: 0.3708\n",
            "Epoch 41/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.2010 - mae: 0.5175 - val_loss: 0.0950 - val_mae: 0.3556\n",
            "Epoch 42/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.1986 - mae: 0.5135 - val_loss: 0.0975 - val_mae: 0.3616\n",
            "Epoch 43/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.1937 - mae: 0.5044 - val_loss: 0.1031 - val_mae: 0.3721\n",
            "Epoch 44/200\n",
            "60/60 [==============================] - 1s 16ms/step - loss: 0.1893 - mae: 0.4982 - val_loss: 0.0941 - val_mae: 0.3541\n",
            "Epoch 45/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.1866 - mae: 0.4932 - val_loss: 0.0912 - val_mae: 0.3478\n",
            "Epoch 46/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.1838 - mae: 0.4888 - val_loss: 0.0894 - val_mae: 0.3434\n",
            "Epoch 47/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.1794 - mae: 0.4817 - val_loss: 0.0910 - val_mae: 0.3475\n",
            "Epoch 48/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.1793 - mae: 0.4817 - val_loss: 0.0905 - val_mae: 0.3456\n",
            "Epoch 49/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.1752 - mae: 0.4739 - val_loss: 0.0878 - val_mae: 0.3396\n",
            "Epoch 50/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.1723 - mae: 0.4690 - val_loss: 0.0859 - val_mae: 0.3354\n",
            "Epoch 51/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.1674 - mae: 0.4620 - val_loss: 0.0847 - val_mae: 0.3329\n",
            "Epoch 52/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1647 - mae: 0.4574 - val_loss: 0.0831 - val_mae: 0.3308\n",
            "Epoch 53/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.1641 - mae: 0.4537 - val_loss: 0.0806 - val_mae: 0.3254\n",
            "Epoch 54/200\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.1606 - mae: 0.4481 - val_loss: 0.0790 - val_mae: 0.3217\n",
            "Epoch 55/200\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.1570 - mae: 0.4438 - val_loss: 0.0802 - val_mae: 0.3233\n",
            "Epoch 56/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.1545 - mae: 0.4397 - val_loss: 0.0802 - val_mae: 0.3232\n",
            "Epoch 57/200\n",
            "60/60 [==============================] - 1s 19ms/step - loss: 0.1503 - mae: 0.4318 - val_loss: 0.0773 - val_mae: 0.3176\n",
            "Epoch 58/200\n",
            "60/60 [==============================] - 1s 22ms/step - loss: 0.1498 - mae: 0.4297 - val_loss: 0.0753 - val_mae: 0.3121\n",
            "Epoch 59/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.1468 - mae: 0.4256 - val_loss: 0.0756 - val_mae: 0.3134\n",
            "Epoch 60/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.1458 - mae: 0.4243 - val_loss: 0.0779 - val_mae: 0.3187\n",
            "Epoch 61/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.1430 - mae: 0.4197 - val_loss: 0.0769 - val_mae: 0.3162\n",
            "Epoch 62/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.1411 - mae: 0.4143 - val_loss: 0.0762 - val_mae: 0.3139\n",
            "Epoch 63/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.1401 - mae: 0.4136 - val_loss: 0.0753 - val_mae: 0.3123\n",
            "Epoch 64/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.1384 - mae: 0.4095 - val_loss: 0.0736 - val_mae: 0.3086\n",
            "Epoch 65/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.1315 - mae: 0.3983 - val_loss: 0.0727 - val_mae: 0.3063\n",
            "Epoch 66/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.1320 - mae: 0.4000 - val_loss: 0.0705 - val_mae: 0.3012\n",
            "Epoch 67/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.1301 - mae: 0.3952 - val_loss: 0.0682 - val_mae: 0.2963\n",
            "Epoch 68/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.1289 - mae: 0.3934 - val_loss: 0.0677 - val_mae: 0.2957\n",
            "Epoch 69/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.1262 - mae: 0.3894 - val_loss: 0.0677 - val_mae: 0.2964\n",
            "Epoch 70/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.1236 - mae: 0.3841 - val_loss: 0.0683 - val_mae: 0.2977\n",
            "Epoch 71/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.1246 - mae: 0.3848 - val_loss: 0.0671 - val_mae: 0.2942\n",
            "Epoch 72/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.1204 - mae: 0.3782 - val_loss: 0.0669 - val_mae: 0.2936\n",
            "Epoch 73/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.1212 - mae: 0.3791 - val_loss: 0.0670 - val_mae: 0.2938\n",
            "Epoch 74/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.1201 - mae: 0.3768 - val_loss: 0.0659 - val_mae: 0.2913\n",
            "Epoch 75/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.1174 - mae: 0.3727 - val_loss: 0.0648 - val_mae: 0.2893\n",
            "Epoch 76/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.1186 - mae: 0.3731 - val_loss: 0.0635 - val_mae: 0.2857\n",
            "Epoch 77/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.1143 - mae: 0.3684 - val_loss: 0.0626 - val_mae: 0.2837\n",
            "Epoch 78/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1137 - mae: 0.3651 - val_loss: 0.0620 - val_mae: 0.2817\n",
            "Epoch 79/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.1091 - mae: 0.3590 - val_loss: 0.0614 - val_mae: 0.2805\n",
            "Epoch 80/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.1070 - mae: 0.3555 - val_loss: 0.0607 - val_mae: 0.2788\n",
            "Epoch 81/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1100 - mae: 0.3588 - val_loss: 0.0596 - val_mae: 0.2762\n",
            "Epoch 82/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.1067 - mae: 0.3542 - val_loss: 0.0592 - val_mae: 0.2746\n",
            "Epoch 83/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1084 - mae: 0.3562 - val_loss: 0.0586 - val_mae: 0.2736\n",
            "Epoch 84/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.1049 - mae: 0.3498 - val_loss: 0.0583 - val_mae: 0.2730\n",
            "Epoch 85/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.1044 - mae: 0.3494 - val_loss: 0.0579 - val_mae: 0.2721\n",
            "Epoch 86/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.1022 - mae: 0.3457 - val_loss: 0.0570 - val_mae: 0.2694\n",
            "Epoch 87/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0998 - mae: 0.3417 - val_loss: 0.0567 - val_mae: 0.2693\n",
            "Epoch 88/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0986 - mae: 0.3395 - val_loss: 0.0563 - val_mae: 0.2689\n",
            "Epoch 89/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.0986 - mae: 0.3380 - val_loss: 0.0553 - val_mae: 0.2661\n",
            "Epoch 90/200\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.0981 - mae: 0.3369 - val_loss: 0.0550 - val_mae: 0.2651\n",
            "Epoch 91/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0949 - mae: 0.3328 - val_loss: 0.0551 - val_mae: 0.2656\n",
            "Epoch 92/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0948 - mae: 0.3322 - val_loss: 0.0543 - val_mae: 0.2639\n",
            "Epoch 93/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0954 - mae: 0.3319 - val_loss: 0.0541 - val_mae: 0.2632\n",
            "Epoch 94/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0937 - mae: 0.3298 - val_loss: 0.0537 - val_mae: 0.2625\n",
            "Epoch 95/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0927 - mae: 0.3274 - val_loss: 0.0529 - val_mae: 0.2603\n",
            "Epoch 96/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0890 - mae: 0.3218 - val_loss: 0.0523 - val_mae: 0.2590\n",
            "Epoch 97/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0892 - mae: 0.3213 - val_loss: 0.0518 - val_mae: 0.2578\n",
            "Epoch 98/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0905 - mae: 0.3231 - val_loss: 0.0513 - val_mae: 0.2567\n",
            "Epoch 99/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0904 - mae: 0.3225 - val_loss: 0.0512 - val_mae: 0.2564\n",
            "Epoch 100/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0880 - mae: 0.3199 - val_loss: 0.0508 - val_mae: 0.2553\n",
            "Epoch 101/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0868 - mae: 0.3172 - val_loss: 0.0504 - val_mae: 0.2542\n",
            "Epoch 102/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0865 - mae: 0.3155 - val_loss: 0.0501 - val_mae: 0.2535\n",
            "Epoch 103/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0844 - mae: 0.3130 - val_loss: 0.0499 - val_mae: 0.2533\n",
            "Epoch 104/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0843 - mae: 0.3105 - val_loss: 0.0494 - val_mae: 0.2519\n",
            "Epoch 105/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0836 - mae: 0.3107 - val_loss: 0.0490 - val_mae: 0.2507\n",
            "Epoch 106/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0843 - mae: 0.3106 - val_loss: 0.0485 - val_mae: 0.2495\n",
            "Epoch 107/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0821 - mae: 0.3072 - val_loss: 0.0482 - val_mae: 0.2489\n",
            "Epoch 108/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0822 - mae: 0.3066 - val_loss: 0.0478 - val_mae: 0.2479\n",
            "Epoch 109/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0809 - mae: 0.3044 - val_loss: 0.0474 - val_mae: 0.2469\n",
            "Epoch 110/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0809 - mae: 0.3039 - val_loss: 0.0472 - val_mae: 0.2462\n",
            "Epoch 111/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.0804 - mae: 0.3031 - val_loss: 0.0471 - val_mae: 0.2460\n",
            "Epoch 112/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.0787 - mae: 0.3000 - val_loss: 0.0465 - val_mae: 0.2445\n",
            "Epoch 113/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0796 - mae: 0.3001 - val_loss: 0.0463 - val_mae: 0.2441\n",
            "Epoch 114/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0787 - mae: 0.2992 - val_loss: 0.0459 - val_mae: 0.2431\n",
            "Epoch 115/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0759 - mae: 0.2957 - val_loss: 0.0456 - val_mae: 0.2422\n",
            "Epoch 116/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0770 - mae: 0.2964 - val_loss: 0.0454 - val_mae: 0.2418\n",
            "Epoch 117/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0757 - mae: 0.2946 - val_loss: 0.0451 - val_mae: 0.2410\n",
            "Epoch 118/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0737 - mae: 0.2910 - val_loss: 0.0447 - val_mae: 0.2402\n",
            "Epoch 119/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0758 - mae: 0.2929 - val_loss: 0.0446 - val_mae: 0.2398\n",
            "Epoch 120/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0734 - mae: 0.2897 - val_loss: 0.0443 - val_mae: 0.2391\n",
            "Epoch 121/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0759 - mae: 0.2927 - val_loss: 0.0441 - val_mae: 0.2385\n",
            "Epoch 122/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0733 - mae: 0.2898 - val_loss: 0.0438 - val_mae: 0.2379\n",
            "Epoch 123/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0744 - mae: 0.2894 - val_loss: 0.0437 - val_mae: 0.2375\n",
            "Epoch 124/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0726 - mae: 0.2876 - val_loss: 0.0435 - val_mae: 0.2371\n",
            "Epoch 125/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0721 - mae: 0.2860 - val_loss: 0.0433 - val_mae: 0.2365\n",
            "Epoch 126/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0715 - mae: 0.2859 - val_loss: 0.0430 - val_mae: 0.2359\n",
            "Epoch 127/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0714 - mae: 0.2864 - val_loss: 0.0429 - val_mae: 0.2355\n",
            "Epoch 128/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0715 - mae: 0.2840 - val_loss: 0.0428 - val_mae: 0.2352\n",
            "Epoch 129/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0716 - mae: 0.2848 - val_loss: 0.0426 - val_mae: 0.2348\n",
            "Epoch 130/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0706 - mae: 0.2828 - val_loss: 0.0424 - val_mae: 0.2344\n",
            "Epoch 131/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0699 - mae: 0.2817 - val_loss: 0.0423 - val_mae: 0.2340\n",
            "Epoch 132/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0690 - mae: 0.2808 - val_loss: 0.0422 - val_mae: 0.2337\n",
            "Epoch 133/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0703 - mae: 0.2812 - val_loss: 0.0420 - val_mae: 0.2332\n",
            "Epoch 134/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0687 - mae: 0.2793 - val_loss: 0.0419 - val_mae: 0.2329\n",
            "Epoch 135/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.0692 - mae: 0.2804 - val_loss: 0.0418 - val_mae: 0.2326\n",
            "Epoch 136/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.0697 - mae: 0.2808 - val_loss: 0.0417 - val_mae: 0.2323\n",
            "Epoch 137/200\n",
            "60/60 [==============================] - 1s 12ms/step - loss: 0.0685 - mae: 0.2786 - val_loss: 0.0415 - val_mae: 0.2319\n",
            "Epoch 138/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.0678 - mae: 0.2772 - val_loss: 0.0414 - val_mae: 0.2318\n",
            "Epoch 139/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.0679 - mae: 0.2778 - val_loss: 0.0413 - val_mae: 0.2315\n",
            "Epoch 140/200\n",
            "60/60 [==============================] - 1s 16ms/step - loss: 0.0667 - mae: 0.2761 - val_loss: 0.0412 - val_mae: 0.2313\n",
            "Epoch 141/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0668 - mae: 0.2756 - val_loss: 0.0411 - val_mae: 0.2311\n",
            "Epoch 142/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0652 - mae: 0.2728 - val_loss: 0.0410 - val_mae: 0.2308\n",
            "Epoch 143/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0662 - mae: 0.2749 - val_loss: 0.0410 - val_mae: 0.2307\n",
            "Epoch 144/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0662 - mae: 0.2743 - val_loss: 0.0409 - val_mae: 0.2306\n",
            "Epoch 145/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0659 - mae: 0.2743 - val_loss: 0.0409 - val_mae: 0.2304\n",
            "Epoch 146/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0657 - mae: 0.2734 - val_loss: 0.0408 - val_mae: 0.2302\n",
            "Epoch 147/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0635 - mae: 0.2709 - val_loss: 0.0407 - val_mae: 0.2299\n",
            "Epoch 148/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0653 - mae: 0.2721 - val_loss: 0.0406 - val_mae: 0.2299\n",
            "Epoch 149/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0632 - mae: 0.2699 - val_loss: 0.0406 - val_mae: 0.2298\n",
            "Epoch 150/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0650 - mae: 0.2727 - val_loss: 0.0405 - val_mae: 0.2297\n",
            "Epoch 151/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0642 - mae: 0.2709 - val_loss: 0.0405 - val_mae: 0.2296\n",
            "Epoch 152/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0651 - mae: 0.2717 - val_loss: 0.0405 - val_mae: 0.2294\n",
            "Epoch 153/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0635 - mae: 0.2686 - val_loss: 0.0404 - val_mae: 0.2294\n",
            "Epoch 154/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0638 - mae: 0.2694 - val_loss: 0.0404 - val_mae: 0.2293\n",
            "Epoch 155/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0632 - mae: 0.2695 - val_loss: 0.0404 - val_mae: 0.2293\n",
            "Epoch 156/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0640 - mae: 0.2697 - val_loss: 0.0404 - val_mae: 0.2293\n",
            "Epoch 157/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0629 - mae: 0.2688 - val_loss: 0.0403 - val_mae: 0.2293\n",
            "Epoch 158/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0635 - mae: 0.2685 - val_loss: 0.0403 - val_mae: 0.2292\n",
            "Epoch 159/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0631 - mae: 0.2686 - val_loss: 0.0402 - val_mae: 0.2292\n",
            "Epoch 160/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0615 - mae: 0.2663 - val_loss: 0.0402 - val_mae: 0.2290\n",
            "Epoch 161/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.0628 - mae: 0.2680 - val_loss: 0.0402 - val_mae: 0.2291\n",
            "Epoch 162/200\n",
            "60/60 [==============================] - 1s 17ms/step - loss: 0.0631 - mae: 0.2687 - val_loss: 0.0401 - val_mae: 0.2289\n",
            "Epoch 163/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.0613 - mae: 0.2661 - val_loss: 0.0401 - val_mae: 0.2289\n",
            "Epoch 164/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0622 - mae: 0.2673 - val_loss: 0.0401 - val_mae: 0.2290\n",
            "Epoch 165/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0626 - mae: 0.2675 - val_loss: 0.0401 - val_mae: 0.2290\n",
            "Epoch 166/200\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.0612 - mae: 0.2651 - val_loss: 0.0401 - val_mae: 0.2290\n",
            "Epoch 167/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0610 - mae: 0.2654 - val_loss: 0.0401 - val_mae: 0.2289\n",
            "Epoch 168/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0595 - mae: 0.2630 - val_loss: 0.0401 - val_mae: 0.2290\n",
            "Epoch 169/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0599 - mae: 0.2628 - val_loss: 0.0401 - val_mae: 0.2290\n",
            "Epoch 170/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0604 - mae: 0.2636 - val_loss: 0.0401 - val_mae: 0.2288\n",
            "Epoch 171/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0590 - mae: 0.2623 - val_loss: 0.0400 - val_mae: 0.2288\n",
            "Epoch 172/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0601 - mae: 0.2634 - val_loss: 0.0400 - val_mae: 0.2288\n",
            "Epoch 173/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0602 - mae: 0.2641 - val_loss: 0.0400 - val_mae: 0.2287\n",
            "Epoch 174/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0593 - mae: 0.2627 - val_loss: 0.0400 - val_mae: 0.2288\n",
            "Epoch 175/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0580 - mae: 0.2605 - val_loss: 0.0400 - val_mae: 0.2288\n",
            "Epoch 176/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0601 - mae: 0.2633 - val_loss: 0.0400 - val_mae: 0.2288\n",
            "Epoch 177/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0596 - mae: 0.2625 - val_loss: 0.0400 - val_mae: 0.2287\n",
            "Epoch 178/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0580 - mae: 0.2599 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 179/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0580 - mae: 0.2607 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 180/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0594 - mae: 0.2615 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 181/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0589 - mae: 0.2613 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 182/200\n",
            "60/60 [==============================] - 1s 16ms/step - loss: 0.0586 - mae: 0.2610 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 183/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.0571 - mae: 0.2586 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 184/200\n",
            "60/60 [==============================] - 1s 14ms/step - loss: 0.0578 - mae: 0.2605 - val_loss: 0.0399 - val_mae: 0.2288\n",
            "Epoch 185/200\n",
            "60/60 [==============================] - 1s 13ms/step - loss: 0.0578 - mae: 0.2595 - val_loss: 0.0399 - val_mae: 0.2288\n",
            "Epoch 186/200\n",
            "60/60 [==============================] - 1s 15ms/step - loss: 0.0602 - mae: 0.2635 - val_loss: 0.0399 - val_mae: 0.2288\n",
            "Epoch 187/200\n",
            "60/60 [==============================] - 1s 16ms/step - loss: 0.0569 - mae: 0.2582 - val_loss: 0.0399 - val_mae: 0.2288\n",
            "Epoch 188/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0585 - mae: 0.2607 - val_loss: 0.0399 - val_mae: 0.2288\n",
            "Epoch 189/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0572 - mae: 0.2591 - val_loss: 0.0399 - val_mae: 0.2288\n",
            "Epoch 190/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0568 - mae: 0.2579 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 191/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0573 - mae: 0.2584 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 192/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0575 - mae: 0.2587 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 193/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0560 - mae: 0.2569 - val_loss: 0.0399 - val_mae: 0.2285\n",
            "Epoch 194/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0576 - mae: 0.2591 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 195/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0579 - mae: 0.2590 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 196/200\n",
            "60/60 [==============================] - 1s 11ms/step - loss: 0.0571 - mae: 0.2579 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 197/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0558 - mae: 0.2562 - val_loss: 0.0399 - val_mae: 0.2288\n",
            "Epoch 198/200\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.0564 - mae: 0.2573 - val_loss: 0.0399 - val_mae: 0.2288\n",
            "Epoch 199/200\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.0564 - mae: 0.2573 - val_loss: 0.0399 - val_mae: 0.2287\n",
            "Epoch 200/200\n",
            "60/60 [==============================] - 1s 10ms/step - loss: 0.0562 - mae: 0.2567 - val_loss: 0.0398 - val_mae: 0.2285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-25 19:18:11,741] Trial 49 finished with value: 0.039832279086112976 and parameters: {'learning_rate': 3.103338336864286e-05, 'dropout_rate': 0.21829999280837137, 'batch_size': 64, 'epochs': 200}. Best is trial 42 with value: 0.029339225962758064.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial:\n",
            "Value: 0.029339225962758064\n",
            "Params: \n",
            "    learning_rate: 7.27897485581272e-05\n",
            "    dropout_rate: 0.10373251980675244\n",
            "    batch_size: 32\n",
            "    epochs: 100\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Funzione obiettivo per Optuna\n",
        "def objective(trial):\n",
        "    # Iperparametri da ottimizzare\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
        "    epochs = trial.suggest_categorical('epochs', [50, 100, 150, 200])\n",
        "\n",
        "    # Creazione del modello\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.InputLayer(input_shape=(136,)))\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(trial.suggest_uniform('dropout_rate', 0.1, 0.9)))\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(trial.suggest_uniform('dropout_rate', 0.1, 0.9)))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(trial.suggest_uniform('dropout_rate', 0.1, 0.9)))\n",
        "    model.add(layers.Dense(32, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(trial.suggest_uniform('dropout_rate', 0.1, 0.9)))\n",
        "    model.add(layers.Dense(16, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(trial.suggest_uniform('dropout_rate', 0.1, 0.9)))\n",
        "    model.add(layers.Dense(20, activation='linear'))\n",
        "\n",
        "    lr_schedule = ExponentialDecay(learning_rate, decay_steps=10000, decay_rate=0.9, staircase=True)\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=tf.keras.losses.LogCosh(), metrics=['mae'])\n",
        "\n",
        "    # Addestramento del modello\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
        "\n",
        "    # Valutazione delle prestazioni sul set di validazione\n",
        "    val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "    return val_loss\n",
        "\n",
        "# Creazione dell'oggetto Optuna Study\n",
        "study = optuna.create_study(direction='minimize')\n",
        "\n",
        "# Esecuzione dell'ottimizzazione degli iperparametri\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Stampa dei risultati migliori\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(f\"Value: {trial.value}\")\n",
        "print(\"Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Emotion Recognition"
      ],
      "metadata": {
        "id": "Dm2_BuBJ8edn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "loaded_model = load_model('data/au_pred_model.h5')"
      ],
      "metadata": {
        "id": "Fw3BaZfD8njL"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk0HG4RC8Xfz",
        "outputId": "49d63071-d4a8-4d69-9792-bffe0c44ab7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:09<00:00,  9.68s/it]\n"
          ]
        }
      ],
      "source": [
        "single_face_prediction = detector.detect_image('/content/will.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "6Q4SKrQ38h80",
        "outputId": "87fb2394-307c-47f1-f871-2794f3afb777"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       AU01      AU02      AU04      AU05      AU06  AU07      AU09    AU10  \\\n",
              "0  0.680683  0.492251  0.213703  0.300965  0.942601   1.0  0.469445  0.9928   \n",
              "\n",
              "   AU11      AU12      AU14      AU15      AU17  AU20      AU23      AU24  \\\n",
              "0   1.0  0.981605  0.645309  0.483527  0.201595   1.0  0.197912  0.009984   \n",
              "\n",
              "       AU25      AU26      AU28      AU43  \n",
              "0  0.999977  0.662218  0.066732  0.416628  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1973c65-392c-4d2a-812b-2e00055798d3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AU01</th>\n",
              "      <th>AU02</th>\n",
              "      <th>AU04</th>\n",
              "      <th>AU05</th>\n",
              "      <th>AU06</th>\n",
              "      <th>AU07</th>\n",
              "      <th>AU09</th>\n",
              "      <th>AU10</th>\n",
              "      <th>AU11</th>\n",
              "      <th>AU12</th>\n",
              "      <th>AU14</th>\n",
              "      <th>AU15</th>\n",
              "      <th>AU17</th>\n",
              "      <th>AU20</th>\n",
              "      <th>AU23</th>\n",
              "      <th>AU24</th>\n",
              "      <th>AU25</th>\n",
              "      <th>AU26</th>\n",
              "      <th>AU28</th>\n",
              "      <th>AU43</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.680683</td>\n",
              "      <td>0.492251</td>\n",
              "      <td>0.213703</td>\n",
              "      <td>0.300965</td>\n",
              "      <td>0.942601</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.469445</td>\n",
              "      <td>0.9928</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.981605</td>\n",
              "      <td>0.645309</td>\n",
              "      <td>0.483527</td>\n",
              "      <td>0.201595</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.197912</td>\n",
              "      <td>0.009984</td>\n",
              "      <td>0.999977</td>\n",
              "      <td>0.662218</td>\n",
              "      <td>0.066732</td>\n",
              "      <td>0.416628</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1973c65-392c-4d2a-812b-2e00055798d3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e1973c65-392c-4d2a-812b-2e00055798d3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e1973c65-392c-4d2a-812b-2e00055798d3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "single_face_prediction.aus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "SDCJPE9UwLWf",
        "outputId": "6bd53fa7-95af-4252-e0c7-9419d9ae1554"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      anger   disgust      fear  happiness   sadness  surprise   neutral\n",
              "0  0.008884  0.001947  0.000829   0.901518  0.001035  0.075105  0.010682"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fc8b067c-447c-443c-9e6c-a42416c3148f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>anger</th>\n",
              "      <th>disgust</th>\n",
              "      <th>fear</th>\n",
              "      <th>happiness</th>\n",
              "      <th>sadness</th>\n",
              "      <th>surprise</th>\n",
              "      <th>neutral</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.008884</td>\n",
              "      <td>0.001947</td>\n",
              "      <td>0.000829</td>\n",
              "      <td>0.901518</td>\n",
              "      <td>0.001035</td>\n",
              "      <td>0.075105</td>\n",
              "      <td>0.010682</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc8b067c-447c-443c-9e6c-a42416c3148f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fc8b067c-447c-443c-9e6c-a42416c3148f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fc8b067c-447c-443c-9e6c-a42416c3148f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "single_face_prediction.emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "SnEBNxkP8wXv",
        "outputId": "8be4c5ac-b5ff-4ae7-fb6e-871b4f5ff8bc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import dlib\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load the pre-trained face landmark predictor model\n",
        "predictor_path = \"data/shape_predictor_68_face_landmarks.dat\"\n",
        "predictor = dlib.shape_predictor(predictor_path)\n",
        "\n",
        "# Load the pre-trained face detector model\n",
        "detector_face = dlib.get_frontal_face_detector()\n",
        "\n",
        "# Load an image\n",
        "image_path = \"will.jpg\"\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Convert the image to grayscale (required by Dlib)\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Detect faces in the image\n",
        "faces = detector_face(gray)\n",
        "\n",
        "# Loop through each face and detect landmarks\n",
        "for face in faces:\n",
        "    landmarks = predictor(gray, face)\n",
        "\n",
        "    # Draw landmarks on the image\n",
        "    for i in range(68):\n",
        "        x, y = landmarks.part(i).x, landmarks.part(i).y\n",
        "        cv2.circle(image, (x, y), 2, (0, 255, 0), -1)  # Draw a green circle for each landmark\n",
        "        cv2.putText(image, str(i), (x + 5, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 0, 0), 1)\n",
        "\n",
        "    landmarks_coordinates = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(68)])\n",
        "    middle_point = np.mean(landmarks_coordinates, axis=0).astype(int)\n",
        "\n",
        "    # Draw a circle at the middle point on the image\n",
        "    cv2.circle(image, tuple(middle_point), 4, (0, 0, 255), -1)\n",
        "\n",
        "# Convert BGR image to RGB for displaying with pyplot\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display the result using pyplot\n",
        "plt.imshow(image_rgb)\n",
        "plt.title(\"Face Landmarks Detection\")\n",
        "plt.axis(\"off\")  # Turn off axis labels\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "cP7ajWLl9VAw"
      },
      "outputs": [],
      "source": [
        "# Supponiamo che landmarks_coordinates sia l'array delle coordinate\n",
        "landmarks_coordinates = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(68)])\n",
        "\n",
        "all_landmark = []\n",
        "vec_comb_lin_x = []\n",
        "vec_comb_lin_y = []\n",
        "\n",
        "# Dividi l'array in coordinate x e y\n",
        "land_x = landmarks_coordinates[:, 0]  # Estrae tutte le colonne 0 (coordinate x)\n",
        "land_y = landmarks_coordinates[:, 1]  # Estrae tutte le colonne 1 (coordinate y)\n",
        "\n",
        "for i in range(68):\n",
        "    sum_x = 0\n",
        "    sum_y = 0\n",
        "\n",
        "    for j in range(68):\n",
        "      sum_x = sum_x + land_x[i] * (1/land_x[j])\n",
        "      sum_y = sum_y + land_y[i] * (1/land_y[j])\n",
        "\n",
        "    vec_comb_lin_x.append(sum_x)\n",
        "    vec_comb_lin_y.append(sum_y)\n",
        "\n",
        "all_landmark = vec_comb_lin_x + vec_comb_lin_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "z2rTJjOA93FW"
      },
      "outputs": [],
      "source": [
        "columns_names = [f'Landmark_x_{i}' for i in range(68)]\n",
        "columns_names1 = [f'Landmark_y_{i}' for i in range(68)]\n",
        "\n",
        "all_columns_names = columns_names + columns_names1\n",
        "\n",
        "test_df = pd.DataFrame([all_landmark], columns=all_columns_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "86qGIfsZ8lLD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d138ed-270a-442f-83fc-3cf7e8729626"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.3120126 ,  0.32860428,  0.25508118,  0.19041109,  1.6431276 ,\n",
              "         1.4268074 ,  1.0517149 ,  1.6763566 ,  0.27564302,  1.9132034 ,\n",
              "         1.1997375 , -0.05137002, -0.00753626,  1.393965  , -0.15552217,\n",
              "        -0.16441983,  1.0105008 ,  0.45943496, -0.11880568,  0.5180964 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "pred = model.predict(test_df)\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dNJDOg9_v1b",
        "outputId": "c60f740a-a7d4-44f9-a489-c39eab313d57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.68068308 0.49225131 0.21370329 0.30096534 0.94260103 1.\n",
            " 0.46944541 0.9928     1.         0.98160458 0.64530909 0.48352721\n",
            " 0.20159455 1.         0.19791234 0.00998374 0.99997699 0.66221792\n",
            " 0.06673186 0.41662848]\n"
          ]
        }
      ],
      "source": [
        "aus_list = np.array([value[0] for key, value in single_face_prediction.aus.to_dict().items()])\n",
        "print(aus_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U72H9uHjUVNU",
        "outputId": "6a8e1603-f593-4181-f426-54dfd18ad7c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "bin_pred = (pred >= 0.475).astype(int)\n",
        "bin_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZKwEdgjUnau",
        "outputId": "00788fbf-f1e4-4089-a3c9-75b5e9d63895"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "bin_aus = (aus_list >= 0.5).astype(int)\n",
        "bin_aus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "0xIhFoVc3UxE"
      },
      "outputs": [],
      "source": [
        "emotions = {'happiness': [4, 9], 'sadness': [0, 2, 11], 'surprise': [0, 1, 3, 17], 'fear': [0, 1, 2, 3, 5, 13, 17], 'anger': [2, 3, 5, 14]}\n",
        "\n",
        "emotion = 'neutral'\n",
        "diff_temp = 0\n",
        "\n",
        "for key, value in emotions.items():\n",
        "  sum = 0\n",
        "  v_len = len(value)/2\n",
        "\n",
        "  for v in value:\n",
        "    if pred[0][v] >= 0.48:\n",
        "      sum = sum + 1\n",
        "\n",
        "  diff = sum - v_len\n",
        "\n",
        "  if diff >= diff_temp:\n",
        "    emotion = key\n",
        "    diff_temp = sum - v_len\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OYsD3HJ-8d_u",
        "outputId": "8f522a3d-c325-4e7e-ad79-5d74f87c6051"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'happiness'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "emotion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Emotion Confront"
      ],
      "metadata": {
        "id": "IcrV1Z9Gfj8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "loaded_model = load_model('data/au_pred_model.h5')"
      ],
      "metadata": {
        "id": "w1DEbWtPktqp"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "afew_keyframes_df = pd.read_csv('data/afew_keyframes.csv')"
      ],
      "metadata": {
        "id": "mN1f0NGm-Dfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(afew_keyframes_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWPezXYg-PFZ",
        "outputId": "8b5dad6c-8963-4c3a-ed59-98d2fc5fe77b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6000"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aus_names_list = merged_df.columns.to_list()[136:]"
      ],
      "metadata": {
        "id": "SYgSp2-CjPcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_emotion = merged_df\n",
        "y_emotion = merged_df[aus_names_list]"
      ],
      "metadata": {
        "id": "gGWxEEBGCc_u"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_emotion, y_emotion, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "ewlFjLNyCPgj"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.columns[137]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DJ2n6ND0CnWG",
        "outputId": "39339251-7294-4f82-dcec-0352e2b5a7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AU01'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 323
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_emotion(prediction):\n",
        "    emotions = {'happiness': [4, 9], 'sadness': [0, 2, 11], 'surprise': [0, 1, 3, 17], 'fear': [0, 1, 2, 3, 5, 13, 17], 'anger': [2, 3, 5, 14]}\n",
        "\n",
        "    emotion = 'neutral'\n",
        "    diff_temp = 0\n",
        "\n",
        "    for key, value in emotions.items():\n",
        "      sum = 0\n",
        "      v_len = len(value)/2\n",
        "\n",
        "      for v in value:\n",
        "        if prediction[v] >= 0.47:\n",
        "          sum = sum + 1\n",
        "\n",
        "      diff = sum - v_len\n",
        "\n",
        "      if diff >= diff_temp:\n",
        "        emotion = key\n",
        "        diff_temp = sum - v_len\n",
        "\n",
        "    return emotion"
      ],
      "metadata": {
        "id": "T2g2Ekm21A05"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction_emotion_pyfeat_au():\n",
        "  emotions_aus_list = []\n",
        "\n",
        "  for index, row in X_test.iterrows():\n",
        "    aus_list = row.to_list()[137:157]\n",
        "    true_pred = predict_emotion(aus_list)\n",
        "\n",
        "    emotions_aus_list.append(true_pred)\n",
        "\n",
        "  return emotions_aus_list"
      ],
      "metadata": {
        "id": "1pgNIJf9fOVy"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_fear_surprise(emotion_list):\n",
        "  # Cambia 'fear' con 'fear-surprise'\n",
        "  for i in range(len(emotion_list)):\n",
        "      if emotion_list[i] == 'fear' or emotion_list[i] == 'surprise':\n",
        "          emotion_list[i] = 'fear-surprise'\n",
        "\n",
        "  return emotion_list"
      ],
      "metadata": {
        "id": "Z6olxNyxYMMN"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_categorical_values(emotion_list):\n",
        "  # Dizionario di mapping per le categorie\n",
        "  mapping_categorie_numeriche = {\n",
        "      \"happiness\": 0,\n",
        "      \"fear-surprise\": 1,\n",
        "      \"sadness\": 2,\n",
        "      \"neutral\": 3,\n",
        "      \"anger\": 4,\n",
        "  }\n",
        "\n",
        "  # Funzione per categorizzare una stringa restituendo il numero corrispondente\n",
        "  def categorizza_stringa_numerica(stringa, mapping):\n",
        "      for categoria, numero in mapping.items():\n",
        "          if stringa in categoria:\n",
        "              return numero\n",
        "      return 0  # 0 per sconosciuto o nessuna corrispondenza\n",
        "\n",
        "  # Categorizza ogni stringa nella lista\n",
        "  emotions_au = [categorizza_stringa_numerica(stringa, mapping_categorie_numeriche) for stringa in emotion_list]\n",
        "\n",
        "  return emotions_au"
      ],
      "metadata": {
        "id": "GhhJp5UquEUj"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_emotion_our_au():\n",
        "  pred = []\n",
        "\n",
        "  for index, row in X_test.iterrows():\n",
        "\n",
        "    directory_path, file_name = os.path.split(row['image_path'])\n",
        "    directory_path = directory_path.replace('_resized', '')\n",
        "    json_name = directory_path.split('/')[3]\n",
        "    file_name = file_name.split('.')[0]\n",
        "\n",
        "    json_path = f'{directory_path}/{json_name}.json'\n",
        "    with open(json_path, 'r') as file:\n",
        "        json_data = json.load(file)\n",
        "\n",
        "    landmarks = json_data['frames'][file_name]['landmarks']\n",
        "\n",
        "    vec_comb_lin_x = []\n",
        "    vec_comb_lin_y = []\n",
        "\n",
        "    land_x = np.array([int(x) for x, _ in landmarks])\n",
        "    land_y = np.array([int(y) for _, y in landmarks])\n",
        "        #print(f\"Landmark {idx + 1}: ({x}, {y})\")\n",
        "\n",
        "    for i in range(68):\n",
        "      sum_x = 0\n",
        "      sum_y = 0\n",
        "\n",
        "      for j in range(68):\n",
        "        sum_x = sum_x + land_x[i] * (1/land_x[j])\n",
        "        sum_y = sum_y + land_y[i] * (1/land_y[j])\n",
        "\n",
        "      vec_comb_lin_x.append(sum_x)\n",
        "      vec_comb_lin_y.append(sum_y)\n",
        "\n",
        "    all_landmark = vec_comb_lin_x + vec_comb_lin_y\n",
        "\n",
        "    landmarks_df = pd.DataFrame([all_landmark], columns=all_columns_names)\n",
        "    prediction = loaded_model.predict(landmarks_df, verbose=False)\n",
        "\n",
        "    pred.append(predict_emotion(prediction[0]))\n",
        "\n",
        "  return pred"
      ],
      "metadata": {
        "id": "0nAjTP0kxhEJ"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "threshold = 0.40\n",
        "max = 0\n",
        "best_threshold = 0\n",
        "\n",
        "emotion_pyfeat_au_list = prediction_emotion_pyfeat_au()\n",
        "emotion_our_au_list = predict_emotion_our_au()\n",
        "\n",
        "merged_emotion_pyfeat_au_list = merge_fear_surprise(emotion_pyfeat_au_list)\n",
        "merged_emotion_our_au_list = merge_fear_surprise(emotion_our_au_list)\n",
        "\n",
        "cat_pyfeat_emotion = get_categorical_values(merged_emotion_pyfeat_au_list)\n",
        "cat_our_emotion = get_categorical_values(merged_emotion_our_au_list)\n",
        "\n",
        "accuracy = accuracy_score(cat_pyfeat_emotion, cat_our_emotion)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVtxIOJngZYj",
        "outputId": "6035ab11-28ce-4880-bb78-7bf56e81c3dd"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.43666666666666665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(cat_pyfeat_emotion, cat_our_emotion))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j5LJ8TaA3d1",
        "outputId": "1f2ba655-6dfa-4049-dd64-69e75155f0ce"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.54      0.61       135\n",
            "           1       0.46      0.48      0.47       376\n",
            "           2       0.33      0.45      0.38       260\n",
            "           3       0.05      0.02      0.03        89\n",
            "           4       0.49      0.45      0.47       340\n",
            "\n",
            "    accuracy                           0.44      1200\n",
            "   macro avg       0.40      0.39      0.39      1200\n",
            "weighted avg       0.44      0.44      0.43      1200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recompute AUs for the entire dataset(using our model)"
      ],
      "metadata": {
        "id": "KfnhnQmq-0E7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import dlib\n",
        "import pandas as pd\n",
        "\n",
        "def detect_landmarks(image_path):\n",
        "    # Load the pre-trained face detector model\n",
        "    detector_face = dlib.get_frontal_face_detector()\n",
        "    predictor_path = \"shape_predictor_68_face_landmarks.dat\"\n",
        "    predictor = dlib.shape_predictor(predictor_path)\n",
        "    # Load an image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Convert the image to grayscale (required by Dlib)\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the image\n",
        "    faces = detector_face(gray)\n",
        "\n",
        "    landmarks = predictor(gray, faces[0])\n",
        "    landmarks_coordinates = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(68)])\n",
        "\n",
        "    return landmarks_coordinates\n",
        "\n",
        "\n",
        "def landmarks_combination_df(landmarks):\n",
        "    columns_names = [f'Landmark_x_{i}' for i in range(68)]\n",
        "    columns_names1 = [f'Landmark_y_{i}' for i in range(68)]\n",
        "\n",
        "    all_columns_names = columns_names + columns_names1\n",
        "\n",
        "    vec_comb_lin_x = []\n",
        "    vec_comb_lin_y = []\n",
        "\n",
        "    # Dividi l'array in coordinate x e y\n",
        "    land_x = landmarks[:, 0]  # Estrae tutte le colonne 0 (coordinate x)\n",
        "    land_y = landmarks[:, 1]  # Estrae tutte le colonne 1 (coordinate y)\n",
        "\n",
        "    for i in range(68):\n",
        "        sum_x = 0\n",
        "        sum_y = 0\n",
        "\n",
        "        for j in range(68):\n",
        "            sum_x = sum_x + land_x[i] * (1 / land_x[j])\n",
        "            sum_y = sum_y + land_y[i] * (1 / land_y[j])\n",
        "\n",
        "        vec_comb_lin_x.append(sum_x)\n",
        "        vec_comb_lin_y.append(sum_y)\n",
        "\n",
        "    all_landmark = vec_comb_lin_x + vec_comb_lin_y\n",
        "\n",
        "    return pd.DataFrame([all_landmark], columns=all_columns_names)"
      ],
      "metadata": {
        "id": "lG2VzehJCC02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "def process_all_images(folder_path, start_folder=0, end_folder=None):\n",
        "    \"\"\"\n",
        "    Processa tutte le immagini in un dato percorso per estrarre le Action Units (AU).\n",
        "\n",
        "    :param folder_path: Percorso della cartella contenente le immagini.\n",
        "    :param start_folder: Indice della sotto-cartella da cui iniziare l'elaborazione.\n",
        "    :param end_folder: Indice della sotto-cartella in cui fermare l'elaborazione (esclusiva).\n",
        "    :return: DataFrame con i valori delle AU per ogni immagine processata.\n",
        "    \"\"\"\n",
        "    aus_results = []\n",
        "    au_pred_model = load_model('data/au_pred_model1.h5')\n",
        "    sorted_folders = sorted(os.listdir(folder_path))\n",
        "\n",
        "    # Se end_folder non è specificato, processa fino all'ultima cartella\n",
        "    if end_folder is None:\n",
        "        end_folder = len(sorted_folders)\n",
        "\n",
        "    # Processa solo le cartelle nell'intervallo specificato\n",
        "    for folder_index, folder_name in enumerate(sorted_folders[start_folder:end_folder], start_folder):\n",
        "        subfolder_path = os.path.join(folder_path, folder_name)\n",
        "        print(f\"Elaborazione della sotto-cartella: {subfolder_path}\")\n",
        "\n",
        "        if not os.path.isdir(subfolder_path):\n",
        "            print(f\"Non è una cartella: {subfolder_path}\")\n",
        "            continue\n",
        "\n",
        "        for image_name in os.listdir(subfolder_path):\n",
        "            image_path = os.path.join(subfolder_path, image_name)\n",
        "            if not image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                continue  # Process only image files\n",
        "\n",
        "            try:\n",
        "                # Esegui la detection delle AU\n",
        "                landmarks = detect_landmarks(image_path)\n",
        "                if len(landmarks) > 0:\n",
        "                    landmarks_df = landmarks_combination_df(landmarks)\n",
        "                    prediction = au_pred_model.predict(landmarks_df,verbose=0)\n",
        "                    au_values = prediction[0]  # Presumiamo che questo sia un array con le previsioni delle AU\n",
        "\n",
        "                # Crea un dizionario dalle previsioni delle AU utilizzando indici numerici come chiavi\n",
        "                au_dict = {'AU' + str(index + 1): value for index, value in enumerate(au_values)}\n",
        "\n",
        "                # Crea il dizionario finale aggiungendo il percorso dell'immagine\n",
        "                au_values_with_path = {'image_path': image_path, **au_dict}\n",
        "                aus_results.append(au_values_with_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Errore durante l'elaborazione di {image_path}: {e}\")\n",
        "\n",
        "    return pd.DataFrame(aus_results)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w2QIRKiwA_I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Percorso della cartella principale del dataset\n",
        "dataset_path = \"/content/dataset_resized/\"\n",
        "\n",
        "# Processa le immagini nel dataset\n",
        "# Puoi modificare 'max_folders' in base al numero di sotto-cartelle che vuoi processare\n",
        "aus_data = process_all_images(dataset_path,0,600)\n",
        "\n",
        "# Stampa i primi record per avere un'anteprima dei dati\n",
        "print(aus_data.head())"
      ],
      "metadata": {
        "id": "3s6Mxqf8Dwgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aus_data.to_csv('afew_0_600.csv')"
      ],
      "metadata": {
        "id": "ZpcrJxyg4R7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arousal and Valence Model"
      ],
      "metadata": {
        "id": "Q9LJvGbs9vz7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "GINFAodhEgPp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "new_aus_df = pd.read_csv('data/afew_keyframes.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "FF2ggPVIjhsE"
      },
      "outputs": [],
      "source": [
        "new_aus_df = new_aus_df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_aus_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "2EQSdFdrkpeG",
        "outputId": "e8f3284a-4eb3-48d4-8b8c-cbd33ba9a1f5"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  image_path      AU01      AU02      AU04  \\\n",
              "0     /content/dataset_resized/401/00015.png  0.311357  0.135927  0.798931   \n",
              "1     /content/dataset_resized/401/00016.png  0.291428  0.148411  0.881824   \n",
              "2     /content/dataset_resized/401/00018.png  0.340948  0.113658  0.831665   \n",
              "3     /content/dataset_resized/401/00031.png  0.291201  0.127205  0.756636   \n",
              "4     /content/dataset_resized/401/00046.png  0.360045  0.199449  0.859479   \n",
              "...                                      ...       ...       ...       ...   \n",
              "5995  /content/dataset_resized/200/00020.png  0.422657  0.050986  0.963368   \n",
              "5996  /content/dataset_resized/200/00021.png  0.421654  0.109469  0.965105   \n",
              "5997  /content/dataset_resized/200/00059.png  0.451566  0.175831  0.896503   \n",
              "5998  /content/dataset_resized/200/00067.png  0.608986  0.194725  0.704085   \n",
              "5999  /content/dataset_resized/200/00068.png  0.420602  0.128949  0.802174   \n",
              "\n",
              "          AU05      AU06  AU07      AU09      AU10  AU11  ...      AU17  AU20  \\\n",
              "0     0.328358  0.113616   1.0  0.493366  0.029253   0.0  ...  0.257176   1.0   \n",
              "1     0.365231  0.104657   1.0  0.360925  0.062726   0.0  ...  0.327704   1.0   \n",
              "2     0.340745  0.084021   0.0  0.353916  0.018393   0.0  ...  0.309095   1.0   \n",
              "3     0.326717  0.118116   0.0  0.433923  0.005935   0.0  ...  0.297202   1.0   \n",
              "4     0.252883  0.122171   1.0  0.447879  0.063832   0.0  ...  0.340059   1.0   \n",
              "...        ...       ...   ...       ...       ...   ...  ...       ...   ...   \n",
              "5995  0.253873  0.370320   1.0  0.270748  0.697472   1.0  ...  0.539658   0.0   \n",
              "5996  0.249666  0.180396   1.0  0.282195  0.023396   1.0  ...  0.545938   0.0   \n",
              "5997  0.529105  0.116464   0.0  0.126099  0.014512   1.0  ...  0.508652   0.0   \n",
              "5998  0.423611  0.285823   0.0  0.158707  0.843249   1.0  ...  0.545037   0.0   \n",
              "5999  0.316815  0.494270   1.0  0.225843  0.466679   1.0  ...  0.421382   1.0   \n",
              "\n",
              "          AU23      AU24      AU25      AU26      AU28      AU43  arousal  \\\n",
              "0     0.719543  0.268569  0.993881  0.885669  0.501640  0.786895      1.0   \n",
              "1     0.655670  0.382354  0.990813  0.953209  0.437312  0.711133      1.0   \n",
              "2     0.407399  0.346851  0.978948  0.869907  0.266384  0.741665      1.0   \n",
              "3     0.264385  0.346684  0.999072  0.740485  0.712587  0.701903      2.0   \n",
              "4     0.582021  0.231151  0.994853  0.659481  0.780542  0.535933      2.0   \n",
              "...        ...       ...       ...       ...       ...       ...      ...   \n",
              "5995  0.722313  0.064561  0.998532  0.746642  0.063520  0.549602      3.0   \n",
              "5996  0.603435  0.156507  0.986948  0.566380  0.103323  0.467108      3.0   \n",
              "5997  0.459939  0.352326  0.965488  0.837143  0.633322  0.098899      2.0   \n",
              "5998  0.535025  0.066318  0.992853  0.892109  0.421667  0.059766      2.0   \n",
              "5999  0.407309  0.047096  0.998882  0.789521  0.164114  0.124826      2.0   \n",
              "\n",
              "      valence  \n",
              "0        -1.0  \n",
              "1        -1.0  \n",
              "2        -1.0  \n",
              "3        -1.0  \n",
              "4        -1.0  \n",
              "...       ...  \n",
              "5995     -1.0  \n",
              "5996     -1.0  \n",
              "5997      0.0  \n",
              "5998      0.0  \n",
              "5999      0.0  \n",
              "\n",
              "[6000 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-50840519-ffef-495e-9ff7-86ab81243933\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>AU01</th>\n",
              "      <th>AU02</th>\n",
              "      <th>AU04</th>\n",
              "      <th>AU05</th>\n",
              "      <th>AU06</th>\n",
              "      <th>AU07</th>\n",
              "      <th>AU09</th>\n",
              "      <th>AU10</th>\n",
              "      <th>AU11</th>\n",
              "      <th>...</th>\n",
              "      <th>AU17</th>\n",
              "      <th>AU20</th>\n",
              "      <th>AU23</th>\n",
              "      <th>AU24</th>\n",
              "      <th>AU25</th>\n",
              "      <th>AU26</th>\n",
              "      <th>AU28</th>\n",
              "      <th>AU43</th>\n",
              "      <th>arousal</th>\n",
              "      <th>valence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/dataset_resized/401/00015.png</td>\n",
              "      <td>0.311357</td>\n",
              "      <td>0.135927</td>\n",
              "      <td>0.798931</td>\n",
              "      <td>0.328358</td>\n",
              "      <td>0.113616</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.493366</td>\n",
              "      <td>0.029253</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.257176</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.719543</td>\n",
              "      <td>0.268569</td>\n",
              "      <td>0.993881</td>\n",
              "      <td>0.885669</td>\n",
              "      <td>0.501640</td>\n",
              "      <td>0.786895</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/dataset_resized/401/00016.png</td>\n",
              "      <td>0.291428</td>\n",
              "      <td>0.148411</td>\n",
              "      <td>0.881824</td>\n",
              "      <td>0.365231</td>\n",
              "      <td>0.104657</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.360925</td>\n",
              "      <td>0.062726</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.327704</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.655670</td>\n",
              "      <td>0.382354</td>\n",
              "      <td>0.990813</td>\n",
              "      <td>0.953209</td>\n",
              "      <td>0.437312</td>\n",
              "      <td>0.711133</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/dataset_resized/401/00018.png</td>\n",
              "      <td>0.340948</td>\n",
              "      <td>0.113658</td>\n",
              "      <td>0.831665</td>\n",
              "      <td>0.340745</td>\n",
              "      <td>0.084021</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.353916</td>\n",
              "      <td>0.018393</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.309095</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.407399</td>\n",
              "      <td>0.346851</td>\n",
              "      <td>0.978948</td>\n",
              "      <td>0.869907</td>\n",
              "      <td>0.266384</td>\n",
              "      <td>0.741665</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/dataset_resized/401/00031.png</td>\n",
              "      <td>0.291201</td>\n",
              "      <td>0.127205</td>\n",
              "      <td>0.756636</td>\n",
              "      <td>0.326717</td>\n",
              "      <td>0.118116</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.433923</td>\n",
              "      <td>0.005935</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.297202</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.264385</td>\n",
              "      <td>0.346684</td>\n",
              "      <td>0.999072</td>\n",
              "      <td>0.740485</td>\n",
              "      <td>0.712587</td>\n",
              "      <td>0.701903</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/dataset_resized/401/00046.png</td>\n",
              "      <td>0.360045</td>\n",
              "      <td>0.199449</td>\n",
              "      <td>0.859479</td>\n",
              "      <td>0.252883</td>\n",
              "      <td>0.122171</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.447879</td>\n",
              "      <td>0.063832</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.340059</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.582021</td>\n",
              "      <td>0.231151</td>\n",
              "      <td>0.994853</td>\n",
              "      <td>0.659481</td>\n",
              "      <td>0.780542</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>/content/dataset_resized/200/00020.png</td>\n",
              "      <td>0.422657</td>\n",
              "      <td>0.050986</td>\n",
              "      <td>0.963368</td>\n",
              "      <td>0.253873</td>\n",
              "      <td>0.370320</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.270748</td>\n",
              "      <td>0.697472</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.539658</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.722313</td>\n",
              "      <td>0.064561</td>\n",
              "      <td>0.998532</td>\n",
              "      <td>0.746642</td>\n",
              "      <td>0.063520</td>\n",
              "      <td>0.549602</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>/content/dataset_resized/200/00021.png</td>\n",
              "      <td>0.421654</td>\n",
              "      <td>0.109469</td>\n",
              "      <td>0.965105</td>\n",
              "      <td>0.249666</td>\n",
              "      <td>0.180396</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.282195</td>\n",
              "      <td>0.023396</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.545938</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.603435</td>\n",
              "      <td>0.156507</td>\n",
              "      <td>0.986948</td>\n",
              "      <td>0.566380</td>\n",
              "      <td>0.103323</td>\n",
              "      <td>0.467108</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>/content/dataset_resized/200/00059.png</td>\n",
              "      <td>0.451566</td>\n",
              "      <td>0.175831</td>\n",
              "      <td>0.896503</td>\n",
              "      <td>0.529105</td>\n",
              "      <td>0.116464</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.126099</td>\n",
              "      <td>0.014512</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.508652</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.459939</td>\n",
              "      <td>0.352326</td>\n",
              "      <td>0.965488</td>\n",
              "      <td>0.837143</td>\n",
              "      <td>0.633322</td>\n",
              "      <td>0.098899</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>/content/dataset_resized/200/00067.png</td>\n",
              "      <td>0.608986</td>\n",
              "      <td>0.194725</td>\n",
              "      <td>0.704085</td>\n",
              "      <td>0.423611</td>\n",
              "      <td>0.285823</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.158707</td>\n",
              "      <td>0.843249</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.545037</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.535025</td>\n",
              "      <td>0.066318</td>\n",
              "      <td>0.992853</td>\n",
              "      <td>0.892109</td>\n",
              "      <td>0.421667</td>\n",
              "      <td>0.059766</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>/content/dataset_resized/200/00068.png</td>\n",
              "      <td>0.420602</td>\n",
              "      <td>0.128949</td>\n",
              "      <td>0.802174</td>\n",
              "      <td>0.316815</td>\n",
              "      <td>0.494270</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.225843</td>\n",
              "      <td>0.466679</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.421382</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.407309</td>\n",
              "      <td>0.047096</td>\n",
              "      <td>0.998882</td>\n",
              "      <td>0.789521</td>\n",
              "      <td>0.164114</td>\n",
              "      <td>0.124826</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 23 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50840519-ffef-495e-9ff7-86ab81243933')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-50840519-ffef-495e-9ff7-86ab81243933 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-50840519-ffef-495e-9ff7-86ab81243933');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8ed5048a-0a84-427d-9caa-85b8b5736204\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8ed5048a-0a84-427d-9caa-85b8b5736204')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8ed5048a-0a84-427d-9caa-85b8b5736204 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_emotion(prediction):\n",
        "    emotions = {'happiness': [4, 9], 'sadness': [0, 2, 11], 'surprise': [0, 1, 3, 17], 'fear': [0, 1, 2, 3, 5, 13, 17], 'anger': [2, 3, 5, 14]}\n",
        "\n",
        "    emotion = 'neutral'\n",
        "    diff_temp = 0\n",
        "\n",
        "    for key, value in emotions.items():\n",
        "      sum = 0\n",
        "      v_len = len(value)/2\n",
        "\n",
        "      for v in value:\n",
        "        if prediction[v] >= 0.48:\n",
        "          sum = sum + 1\n",
        "\n",
        "      diff = sum - v_len\n",
        "\n",
        "      if diff >= diff_temp:\n",
        "        emotion = key\n",
        "        diff_temp = sum - v_len\n",
        "\n",
        "    return emotion"
      ],
      "metadata": {
        "id": "NOWswhNqlmCt"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "arousal = []\n",
        "valence = []\n",
        "emotion = []\n",
        "\n",
        "for index, row in new_aus_df.iterrows():\n",
        "  directory_path, file_name = os.path.split(row['image_path'])\n",
        "  directory_path = directory_path.replace('_resized', '')\n",
        "  json_name = directory_path.split('/')[3]\n",
        "  file_name = file_name.split('.')[0]\n",
        "\n",
        "  json_path = f'{directory_path}/{json_name}.json'\n",
        "  with open(json_path, 'r') as file:\n",
        "      json_data = json.load(file)\n",
        "\n",
        "\n",
        "  arousal.append(json_data['frames'][file_name]['arousal'])\n",
        "  valence.append(json_data['frames'][file_name]['valence'])\n",
        "  emotion.append(predict_emotion(row.to_list()[2:]))"
      ],
      "metadata": {
        "id": "kHezJP_7OLiV"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_aus_df['arousal'] = arousal\n",
        "new_aus_df['valence'] = valence\n",
        "new_aus_df['emotion'] = emotion"
      ],
      "metadata": {
        "id": "ACQgjqd_ris_"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_aus_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "qLZi3RZUr_39",
        "outputId": "d8dc6871-ca96-4bf8-a373-05af64f82526"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  image_path      AU01      AU02      AU04  \\\n",
              "0     /content/dataset_resized/401/00015.png  0.311357  0.135927  0.798931   \n",
              "1     /content/dataset_resized/401/00016.png  0.291428  0.148411  0.881824   \n",
              "2     /content/dataset_resized/401/00018.png  0.340948  0.113658  0.831665   \n",
              "3     /content/dataset_resized/401/00031.png  0.291201  0.127205  0.756636   \n",
              "4     /content/dataset_resized/401/00046.png  0.360045  0.199449  0.859479   \n",
              "...                                      ...       ...       ...       ...   \n",
              "5995  /content/dataset_resized/200/00020.png  0.422657  0.050986  0.963368   \n",
              "5996  /content/dataset_resized/200/00021.png  0.421654  0.109469  0.965105   \n",
              "5997  /content/dataset_resized/200/00059.png  0.451566  0.175831  0.896503   \n",
              "5998  /content/dataset_resized/200/00067.png  0.608986  0.194725  0.704085   \n",
              "5999  /content/dataset_resized/200/00068.png  0.420602  0.128949  0.802174   \n",
              "\n",
              "          AU05      AU06  AU07      AU09      AU10  AU11  ...  AU20      AU23  \\\n",
              "0     0.328358  0.113616   1.0  0.493366  0.029253   0.0  ...   1.0  0.719543   \n",
              "1     0.365231  0.104657   1.0  0.360925  0.062726   0.0  ...   1.0  0.655670   \n",
              "2     0.340745  0.084021   0.0  0.353916  0.018393   0.0  ...   1.0  0.407399   \n",
              "3     0.326717  0.118116   0.0  0.433923  0.005935   0.0  ...   1.0  0.264385   \n",
              "4     0.252883  0.122171   1.0  0.447879  0.063832   0.0  ...   1.0  0.582021   \n",
              "...        ...       ...   ...       ...       ...   ...  ...   ...       ...   \n",
              "5995  0.253873  0.370320   1.0  0.270748  0.697472   1.0  ...   0.0  0.722313   \n",
              "5996  0.249666  0.180396   1.0  0.282195  0.023396   1.0  ...   0.0  0.603435   \n",
              "5997  0.529105  0.116464   0.0  0.126099  0.014512   1.0  ...   0.0  0.459939   \n",
              "5998  0.423611  0.285823   0.0  0.158707  0.843249   1.0  ...   0.0  0.535025   \n",
              "5999  0.316815  0.494270   1.0  0.225843  0.466679   1.0  ...   1.0  0.407309   \n",
              "\n",
              "          AU24      AU25      AU26      AU28      AU43  arousal  valence  \\\n",
              "0     0.268569  0.993881  0.885669  0.501640  0.786895      1.0     -1.0   \n",
              "1     0.382354  0.990813  0.953209  0.437312  0.711133      1.0     -1.0   \n",
              "2     0.346851  0.978948  0.869907  0.266384  0.741665      1.0     -1.0   \n",
              "3     0.346684  0.999072  0.740485  0.712587  0.701903      2.0     -1.0   \n",
              "4     0.231151  0.994853  0.659481  0.780542  0.535933      2.0     -1.0   \n",
              "...        ...       ...       ...       ...       ...      ...      ...   \n",
              "5995  0.064561  0.998532  0.746642  0.063520  0.549602      3.0     -1.0   \n",
              "5996  0.156507  0.986948  0.566380  0.103323  0.467108      3.0     -1.0   \n",
              "5997  0.352326  0.965488  0.837143  0.633322  0.098899      2.0      0.0   \n",
              "5998  0.066318  0.992853  0.892109  0.421667  0.059766      2.0      0.0   \n",
              "5999  0.047096  0.998882  0.789521  0.164114  0.124826      2.0      0.0   \n",
              "\n",
              "        emotion  \n",
              "0          fear  \n",
              "1     happiness  \n",
              "2       neutral  \n",
              "3      surprise  \n",
              "4      surprise  \n",
              "...         ...  \n",
              "5995  happiness  \n",
              "5996  happiness  \n",
              "5997    sadness  \n",
              "5998    neutral  \n",
              "5999   surprise  \n",
              "\n",
              "[6000 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5a3c314f-3f54-4cc5-9d2f-f25f76941a53\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>AU01</th>\n",
              "      <th>AU02</th>\n",
              "      <th>AU04</th>\n",
              "      <th>AU05</th>\n",
              "      <th>AU06</th>\n",
              "      <th>AU07</th>\n",
              "      <th>AU09</th>\n",
              "      <th>AU10</th>\n",
              "      <th>AU11</th>\n",
              "      <th>...</th>\n",
              "      <th>AU20</th>\n",
              "      <th>AU23</th>\n",
              "      <th>AU24</th>\n",
              "      <th>AU25</th>\n",
              "      <th>AU26</th>\n",
              "      <th>AU28</th>\n",
              "      <th>AU43</th>\n",
              "      <th>arousal</th>\n",
              "      <th>valence</th>\n",
              "      <th>emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/dataset_resized/401/00015.png</td>\n",
              "      <td>0.311357</td>\n",
              "      <td>0.135927</td>\n",
              "      <td>0.798931</td>\n",
              "      <td>0.328358</td>\n",
              "      <td>0.113616</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.493366</td>\n",
              "      <td>0.029253</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.719543</td>\n",
              "      <td>0.268569</td>\n",
              "      <td>0.993881</td>\n",
              "      <td>0.885669</td>\n",
              "      <td>0.501640</td>\n",
              "      <td>0.786895</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>fear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/dataset_resized/401/00016.png</td>\n",
              "      <td>0.291428</td>\n",
              "      <td>0.148411</td>\n",
              "      <td>0.881824</td>\n",
              "      <td>0.365231</td>\n",
              "      <td>0.104657</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.360925</td>\n",
              "      <td>0.062726</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.655670</td>\n",
              "      <td>0.382354</td>\n",
              "      <td>0.990813</td>\n",
              "      <td>0.953209</td>\n",
              "      <td>0.437312</td>\n",
              "      <td>0.711133</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>happiness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/dataset_resized/401/00018.png</td>\n",
              "      <td>0.340948</td>\n",
              "      <td>0.113658</td>\n",
              "      <td>0.831665</td>\n",
              "      <td>0.340745</td>\n",
              "      <td>0.084021</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.353916</td>\n",
              "      <td>0.018393</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.407399</td>\n",
              "      <td>0.346851</td>\n",
              "      <td>0.978948</td>\n",
              "      <td>0.869907</td>\n",
              "      <td>0.266384</td>\n",
              "      <td>0.741665</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/dataset_resized/401/00031.png</td>\n",
              "      <td>0.291201</td>\n",
              "      <td>0.127205</td>\n",
              "      <td>0.756636</td>\n",
              "      <td>0.326717</td>\n",
              "      <td>0.118116</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.433923</td>\n",
              "      <td>0.005935</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.264385</td>\n",
              "      <td>0.346684</td>\n",
              "      <td>0.999072</td>\n",
              "      <td>0.740485</td>\n",
              "      <td>0.712587</td>\n",
              "      <td>0.701903</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>surprise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/dataset_resized/401/00046.png</td>\n",
              "      <td>0.360045</td>\n",
              "      <td>0.199449</td>\n",
              "      <td>0.859479</td>\n",
              "      <td>0.252883</td>\n",
              "      <td>0.122171</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.447879</td>\n",
              "      <td>0.063832</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.582021</td>\n",
              "      <td>0.231151</td>\n",
              "      <td>0.994853</td>\n",
              "      <td>0.659481</td>\n",
              "      <td>0.780542</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>surprise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>/content/dataset_resized/200/00020.png</td>\n",
              "      <td>0.422657</td>\n",
              "      <td>0.050986</td>\n",
              "      <td>0.963368</td>\n",
              "      <td>0.253873</td>\n",
              "      <td>0.370320</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.270748</td>\n",
              "      <td>0.697472</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.722313</td>\n",
              "      <td>0.064561</td>\n",
              "      <td>0.998532</td>\n",
              "      <td>0.746642</td>\n",
              "      <td>0.063520</td>\n",
              "      <td>0.549602</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>happiness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>/content/dataset_resized/200/00021.png</td>\n",
              "      <td>0.421654</td>\n",
              "      <td>0.109469</td>\n",
              "      <td>0.965105</td>\n",
              "      <td>0.249666</td>\n",
              "      <td>0.180396</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.282195</td>\n",
              "      <td>0.023396</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.603435</td>\n",
              "      <td>0.156507</td>\n",
              "      <td>0.986948</td>\n",
              "      <td>0.566380</td>\n",
              "      <td>0.103323</td>\n",
              "      <td>0.467108</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>happiness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>/content/dataset_resized/200/00059.png</td>\n",
              "      <td>0.451566</td>\n",
              "      <td>0.175831</td>\n",
              "      <td>0.896503</td>\n",
              "      <td>0.529105</td>\n",
              "      <td>0.116464</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.126099</td>\n",
              "      <td>0.014512</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.459939</td>\n",
              "      <td>0.352326</td>\n",
              "      <td>0.965488</td>\n",
              "      <td>0.837143</td>\n",
              "      <td>0.633322</td>\n",
              "      <td>0.098899</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>/content/dataset_resized/200/00067.png</td>\n",
              "      <td>0.608986</td>\n",
              "      <td>0.194725</td>\n",
              "      <td>0.704085</td>\n",
              "      <td>0.423611</td>\n",
              "      <td>0.285823</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.158707</td>\n",
              "      <td>0.843249</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.535025</td>\n",
              "      <td>0.066318</td>\n",
              "      <td>0.992853</td>\n",
              "      <td>0.892109</td>\n",
              "      <td>0.421667</td>\n",
              "      <td>0.059766</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>/content/dataset_resized/200/00068.png</td>\n",
              "      <td>0.420602</td>\n",
              "      <td>0.128949</td>\n",
              "      <td>0.802174</td>\n",
              "      <td>0.316815</td>\n",
              "      <td>0.494270</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.225843</td>\n",
              "      <td>0.466679</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.407309</td>\n",
              "      <td>0.047096</td>\n",
              "      <td>0.998882</td>\n",
              "      <td>0.789521</td>\n",
              "      <td>0.164114</td>\n",
              "      <td>0.124826</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>surprise</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 24 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5a3c314f-3f54-4cc5-9d2f-f25f76941a53')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5a3c314f-3f54-4cc5-9d2f-f25f76941a53 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5a3c314f-3f54-4cc5-9d2f-f25f76941a53');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-75fd2f64-3ef0-4913-b13d-4b77f4b96119\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-75fd2f64-3ef0-4913-b13d-4b77f4b96119')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-75fd2f64-3ef0-4913-b13d-4b77f4b96119 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "# Creazione del modello con i nuovi parametri\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.InputLayer(input_shape=(26,)))\n",
        "\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.14627773253137555))\n",
        "\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.3104964013629035))\n",
        "\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.4514332549580662))\n",
        "\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.43080782555839275))\n",
        "\n",
        "# Output layer con 2 neuroni per le 2 variabili target e attivazione lineare\n",
        "model.add(layers.Dense(2, activation='linear'))\n",
        "\n",
        "# Definizione del programma di decadimento esponenziale del learning rate\n",
        "initial_learning_rate = 0.0005315067319714526\n",
        "lr_schedule = ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9,\n",
        "    staircase=True)\n",
        "\n",
        "# Cambio dell'ottimizzatore a RMSprop con learning rate schedule\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
        "\n",
        "# Cambio della loss function a 'huber_loss'\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=tf.keras.losses.LogCosh(),\n",
        "              metrics=['mae'])\n",
        "\n",
        "# Stampa della struttura del modello\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkO3DC5At1CC",
        "outputId": "7b59e71f-088e-4fa8-b1ba-77ac723a31d6"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_58\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_347 (Dense)           (None, 128)               3456      \n",
            "                                                                 \n",
            " batch_normalization_289 (B  (None, 128)               512       \n",
            " atchNormalization)                                              \n",
            "                                                                 \n",
            " dropout_289 (Dropout)       (None, 128)               0         \n",
            "                                                                 \n",
            " dense_348 (Dense)           (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_290 (B  (None, 64)                256       \n",
            " atchNormalization)                                              \n",
            "                                                                 \n",
            " dropout_290 (Dropout)       (None, 64)                0         \n",
            "                                                                 \n",
            " dense_349 (Dense)           (None, 32)                2080      \n",
            "                                                                 \n",
            " batch_normalization_291 (B  (None, 32)                128       \n",
            " atchNormalization)                                              \n",
            "                                                                 \n",
            " dropout_291 (Dropout)       (None, 32)                0         \n",
            "                                                                 \n",
            " dense_350 (Dense)           (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_292 (B  (None, 16)                64        \n",
            " atchNormalization)                                              \n",
            "                                                                 \n",
            " dropout_292 (Dropout)       (None, 16)                0         \n",
            "                                                                 \n",
            " dense_351 (Dense)           (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15314 (59.82 KB)\n",
            "Trainable params: 14834 (57.95 KB)\n",
            "Non-trainable params: 480 (1.88 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9qTnzNmmWun",
        "outputId": "0d5786d1-82f4-480d-959d-8af43eb0f7cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.24)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.4)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.0 alembic-1.13.1 colorlog-6.8.2 optuna-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "# Function to create and compile the model\n",
        "def create_model(trial):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.InputLayer(input_shape=(20,)))\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
        "    model.add(layers.Dense(32, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
        "    model.add(layers.Dense(16, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
        "    model.add(layers.Dense(2, activation='linear'))\n",
        "\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n",
        "    lr_schedule = ExponentialDecay(learning_rate, decay_steps=10000, decay_rate=0.9, staircase=True)\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=tf.keras.losses.LogCosh(), metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Objective function for Optuna\n",
        "def objective(trial):\n",
        "    model = create_model(trial)\n",
        "\n",
        "    # Training the model\n",
        "    history = model.fit(X_train, y_train, epochs=trial.suggest_categorical('epochs', [50, 100, 150, 200]),\n",
        "                        batch_size=trial.suggest_categorical('batch_size', [32, 64, 128, 256]),\n",
        "                        validation_split=0.2)\n",
        "\n",
        "    # Evaluation on the validation set\n",
        "    val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "    return val_loss\n",
        "\n",
        "# Create an Optuna study\n",
        "study = optuna.create_study(direction='minimize')\n",
        "\n",
        "# Run hyperparameter optimization\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Print the best trial results\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(f\"Value: {trial.value}\")\n",
        "print(\"Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv6lgxA3cg1n",
        "outputId": "e2f83a84-4818-47ce-b203-88be632e352f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:08:37,727] A new study created in memory with name: no-name-9ce514b9-757d-4f65-80b1-d8b4d8139c64\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "27/27 [==============================] - 3s 30ms/step - loss: 1.3159 - mae: 1.8296 - val_loss: 1.2483 - val_mae: 1.7112\n",
            "Epoch 2/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.1828 - mae: 1.6796 - val_loss: 1.1711 - val_mae: 1.6440\n",
            "Epoch 3/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.1491 - mae: 1.6406 - val_loss: 1.1413 - val_mae: 1.6131\n",
            "Epoch 4/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.1051 - mae: 1.5926 - val_loss: 1.1203 - val_mae: 1.6108\n",
            "Epoch 5/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.1028 - mae: 1.5901 - val_loss: 1.1495 - val_mae: 1.6621\n",
            "Epoch 6/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0877 - mae: 1.5738 - val_loss: 1.1187 - val_mae: 1.6310\n",
            "Epoch 7/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0955 - mae: 1.5821 - val_loss: 1.1265 - val_mae: 1.6410\n",
            "Epoch 8/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0823 - mae: 1.5679 - val_loss: 1.1350 - val_mae: 1.6495\n",
            "Epoch 9/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0761 - mae: 1.5610 - val_loss: 1.1446 - val_mae: 1.6501\n",
            "Epoch 10/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0635 - mae: 1.5444 - val_loss: 1.1380 - val_mae: 1.6452\n",
            "Epoch 11/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0620 - mae: 1.5469 - val_loss: 1.1203 - val_mae: 1.6191\n",
            "Epoch 12/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0645 - mae: 1.5472 - val_loss: 1.1163 - val_mae: 1.6094\n",
            "Epoch 13/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0661 - mae: 1.5502 - val_loss: 1.1327 - val_mae: 1.6142\n",
            "Epoch 14/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0647 - mae: 1.5477 - val_loss: 1.1324 - val_mae: 1.6380\n",
            "Epoch 15/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0819 - mae: 1.5662 - val_loss: 1.1380 - val_mae: 1.6387\n",
            "Epoch 16/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0609 - mae: 1.5475 - val_loss: 1.1318 - val_mae: 1.6228\n",
            "Epoch 17/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0601 - mae: 1.5436 - val_loss: 1.1155 - val_mae: 1.6165\n",
            "Epoch 18/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0505 - mae: 1.5354 - val_loss: 1.1420 - val_mae: 1.6356\n",
            "Epoch 19/150\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.0519 - mae: 1.5365 - val_loss: 1.1253 - val_mae: 1.6206\n",
            "Epoch 20/150\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.0657 - mae: 1.5519 - val_loss: 1.1117 - val_mae: 1.6094\n",
            "Epoch 21/150\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.0518 - mae: 1.5354 - val_loss: 1.1270 - val_mae: 1.6302\n",
            "Epoch 22/150\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.0463 - mae: 1.5305 - val_loss: 1.1275 - val_mae: 1.6282\n",
            "Epoch 23/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0517 - mae: 1.5356 - val_loss: 1.1170 - val_mae: 1.6213\n",
            "Epoch 24/150\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.0467 - mae: 1.5283 - val_loss: 1.1193 - val_mae: 1.6160\n",
            "Epoch 25/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0520 - mae: 1.5339 - val_loss: 1.1234 - val_mae: 1.6174\n",
            "Epoch 26/150\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.0484 - mae: 1.5329 - val_loss: 1.0908 - val_mae: 1.5902\n",
            "Epoch 27/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0394 - mae: 1.5229 - val_loss: 1.0963 - val_mae: 1.5864\n",
            "Epoch 28/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0429 - mae: 1.5272 - val_loss: 1.1124 - val_mae: 1.6088\n",
            "Epoch 29/150\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.0450 - mae: 1.5277 - val_loss: 1.1270 - val_mae: 1.6315\n",
            "Epoch 30/150\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.0414 - mae: 1.5244 - val_loss: 1.1037 - val_mae: 1.6038\n",
            "Epoch 31/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0419 - mae: 1.5250 - val_loss: 1.0916 - val_mae: 1.5884\n",
            "Epoch 32/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0397 - mae: 1.5271 - val_loss: 1.1329 - val_mae: 1.6233\n",
            "Epoch 33/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0374 - mae: 1.5189 - val_loss: 1.1148 - val_mae: 1.6081\n",
            "Epoch 34/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0404 - mae: 1.5256 - val_loss: 1.1157 - val_mae: 1.5934\n",
            "Epoch 35/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0519 - mae: 1.5366 - val_loss: 1.1152 - val_mae: 1.5910\n",
            "Epoch 36/150\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.0243 - mae: 1.5074 - val_loss: 1.1367 - val_mae: 1.6183\n",
            "Epoch 37/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0335 - mae: 1.5165 - val_loss: 1.1276 - val_mae: 1.6115\n",
            "Epoch 38/150\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 1.0291 - mae: 1.5114 - val_loss: 1.1162 - val_mae: 1.6142\n",
            "Epoch 39/150\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.0435 - mae: 1.5215 - val_loss: 1.0946 - val_mae: 1.5943\n",
            "Epoch 40/150\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 1.0381 - mae: 1.5238 - val_loss: 1.0801 - val_mae: 1.5682\n",
            "Epoch 41/150\n",
            "27/27 [==============================] - 1s 30ms/step - loss: 1.0301 - mae: 1.5157 - val_loss: 1.0790 - val_mae: 1.5643\n",
            "Epoch 42/150\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 1.0281 - mae: 1.5124 - val_loss: 1.0937 - val_mae: 1.5603\n",
            "Epoch 43/150\n",
            "27/27 [==============================] - 1s 20ms/step - loss: 1.0227 - mae: 1.5039 - val_loss: 1.0800 - val_mae: 1.5764\n",
            "Epoch 44/150\n",
            "27/27 [==============================] - 1s 22ms/step - loss: 1.0144 - mae: 1.4965 - val_loss: 1.0702 - val_mae: 1.5611\n",
            "Epoch 45/150\n",
            "27/27 [==============================] - 1s 25ms/step - loss: 1.0195 - mae: 1.5009 - val_loss: 1.1125 - val_mae: 1.6125\n",
            "Epoch 46/150\n",
            "27/27 [==============================] - 1s 33ms/step - loss: 1.0275 - mae: 1.5145 - val_loss: 1.0933 - val_mae: 1.5897\n",
            "Epoch 47/150\n",
            "27/27 [==============================] - 1s 21ms/step - loss: 1.0250 - mae: 1.5096 - val_loss: 1.0998 - val_mae: 1.5886\n",
            "Epoch 48/150\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.0209 - mae: 1.5022 - val_loss: 1.0759 - val_mae: 1.5742\n",
            "Epoch 49/150\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.0204 - mae: 1.4984 - val_loss: 1.0914 - val_mae: 1.5965\n",
            "Epoch 50/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0273 - mae: 1.5123 - val_loss: 1.0731 - val_mae: 1.5616\n",
            "Epoch 51/150\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.0326 - mae: 1.5146 - val_loss: 1.0602 - val_mae: 1.5385\n",
            "Epoch 52/150\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 1.0327 - mae: 1.5157 - val_loss: 1.0650 - val_mae: 1.5375\n",
            "Epoch 53/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0250 - mae: 1.5056 - val_loss: 1.0483 - val_mae: 1.5180\n",
            "Epoch 54/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0280 - mae: 1.5094 - val_loss: 1.0559 - val_mae: 1.5366\n",
            "Epoch 55/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0142 - mae: 1.5000 - val_loss: 1.0663 - val_mae: 1.5418\n",
            "Epoch 56/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0163 - mae: 1.5020 - val_loss: 1.0651 - val_mae: 1.5438\n",
            "Epoch 57/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0290 - mae: 1.5132 - val_loss: 1.0753 - val_mae: 1.5569\n",
            "Epoch 58/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0354 - mae: 1.5168 - val_loss: 1.0644 - val_mae: 1.5527\n",
            "Epoch 59/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0373 - mae: 1.5262 - val_loss: 1.0649 - val_mae: 1.5601\n",
            "Epoch 60/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0101 - mae: 1.4923 - val_loss: 1.1114 - val_mae: 1.6133\n",
            "Epoch 61/150\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.0359 - mae: 1.5170 - val_loss: 1.0964 - val_mae: 1.5888\n",
            "Epoch 62/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0142 - mae: 1.5016 - val_loss: 1.0872 - val_mae: 1.5874\n",
            "Epoch 63/150\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 1.0205 - mae: 1.5035 - val_loss: 1.0816 - val_mae: 1.5829\n",
            "Epoch 64/150\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.0309 - mae: 1.5150 - val_loss: 1.0997 - val_mae: 1.6050\n",
            "Epoch 65/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0250 - mae: 1.5093 - val_loss: 1.0920 - val_mae: 1.5867\n",
            "Epoch 66/150\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.0428 - mae: 1.5252 - val_loss: 1.0802 - val_mae: 1.5609\n",
            "Epoch 67/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0030 - mae: 1.4812 - val_loss: 1.0988 - val_mae: 1.5844\n",
            "Epoch 68/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0168 - mae: 1.4997 - val_loss: 1.0738 - val_mae: 1.5641\n",
            "Epoch 69/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0175 - mae: 1.5024 - val_loss: 1.0671 - val_mae: 1.5534\n",
            "Epoch 70/150\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 1.0163 - mae: 1.5005 - val_loss: 1.0624 - val_mae: 1.5592\n",
            "Epoch 71/150\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.0268 - mae: 1.5127 - val_loss: 1.0907 - val_mae: 1.6045\n",
            "Epoch 72/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0257 - mae: 1.5126 - val_loss: 1.1086 - val_mae: 1.6117\n",
            "Epoch 73/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0106 - mae: 1.4926 - val_loss: 1.0797 - val_mae: 1.5769\n",
            "Epoch 74/150\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.0243 - mae: 1.5065 - val_loss: 1.0705 - val_mae: 1.5630\n",
            "Epoch 75/150\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.0207 - mae: 1.5048 - val_loss: 1.0849 - val_mae: 1.5719\n",
            "Epoch 76/150\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 1.0140 - mae: 1.4962 - val_loss: 1.0837 - val_mae: 1.5680\n",
            "Epoch 77/150\n",
            "27/27 [==============================] - 1s 21ms/step - loss: 1.0103 - mae: 1.4947 - val_loss: 1.0751 - val_mae: 1.5656\n",
            "Epoch 78/150\n",
            "27/27 [==============================] - 1s 27ms/step - loss: 1.0125 - mae: 1.4968 - val_loss: 1.0876 - val_mae: 1.5930\n",
            "Epoch 79/150\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 1.0173 - mae: 1.5039 - val_loss: 1.0774 - val_mae: 1.5681\n",
            "Epoch 80/150\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.0177 - mae: 1.5013 - val_loss: 1.0637 - val_mae: 1.5456\n",
            "Epoch 81/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0043 - mae: 1.4841 - val_loss: 1.0686 - val_mae: 1.5614\n",
            "Epoch 82/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9969 - mae: 1.4835 - val_loss: 1.0620 - val_mae: 1.5512\n",
            "Epoch 83/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0129 - mae: 1.4958 - val_loss: 1.0647 - val_mae: 1.5475\n",
            "Epoch 84/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0160 - mae: 1.4981 - val_loss: 1.0452 - val_mae: 1.5367\n",
            "Epoch 85/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0195 - mae: 1.5018 - val_loss: 1.0665 - val_mae: 1.5550\n",
            "Epoch 86/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0103 - mae: 1.4902 - val_loss: 1.0625 - val_mae: 1.5646\n",
            "Epoch 87/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0094 - mae: 1.4922 - val_loss: 1.0575 - val_mae: 1.5562\n",
            "Epoch 88/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0057 - mae: 1.4898 - val_loss: 1.0682 - val_mae: 1.5616\n",
            "Epoch 89/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0177 - mae: 1.5008 - val_loss: 1.0771 - val_mae: 1.5756\n",
            "Epoch 90/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9997 - mae: 1.4829 - val_loss: 1.0529 - val_mae: 1.5305\n",
            "Epoch 91/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0173 - mae: 1.5014 - val_loss: 1.0530 - val_mae: 1.5465\n",
            "Epoch 92/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0113 - mae: 1.4936 - val_loss: 1.0535 - val_mae: 1.5494\n",
            "Epoch 93/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0232 - mae: 1.5080 - val_loss: 1.0533 - val_mae: 1.5472\n",
            "Epoch 94/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9846 - mae: 1.4702 - val_loss: 1.0632 - val_mae: 1.5474\n",
            "Epoch 95/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9943 - mae: 1.4781 - val_loss: 1.0603 - val_mae: 1.5486\n",
            "Epoch 96/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9979 - mae: 1.4803 - val_loss: 1.0392 - val_mae: 1.5150\n",
            "Epoch 97/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0045 - mae: 1.4860 - val_loss: 1.0478 - val_mae: 1.5185\n",
            "Epoch 98/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 0.9940 - mae: 1.4750 - val_loss: 1.0494 - val_mae: 1.5276\n",
            "Epoch 99/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0066 - mae: 1.4898 - val_loss: 1.0462 - val_mae: 1.5397\n",
            "Epoch 100/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0068 - mae: 1.4930 - val_loss: 1.0503 - val_mae: 1.5503\n",
            "Epoch 101/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0107 - mae: 1.4991 - val_loss: 1.0876 - val_mae: 1.5748\n",
            "Epoch 102/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0085 - mae: 1.4890 - val_loss: 1.0342 - val_mae: 1.5249\n",
            "Epoch 103/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0006 - mae: 1.4841 - val_loss: 1.0489 - val_mae: 1.5457\n",
            "Epoch 104/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0120 - mae: 1.4971 - val_loss: 1.0483 - val_mae: 1.5432\n",
            "Epoch 105/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0042 - mae: 1.4890 - val_loss: 1.0660 - val_mae: 1.5656\n",
            "Epoch 106/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9896 - mae: 1.4756 - val_loss: 1.0652 - val_mae: 1.5496\n",
            "Epoch 107/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9973 - mae: 1.4787 - val_loss: 1.0305 - val_mae: 1.5071\n",
            "Epoch 108/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0183 - mae: 1.5014 - val_loss: 1.0466 - val_mae: 1.5288\n",
            "Epoch 109/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0105 - mae: 1.4914 - val_loss: 1.0563 - val_mae: 1.5526\n",
            "Epoch 110/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0186 - mae: 1.5012 - val_loss: 1.0677 - val_mae: 1.5624\n",
            "Epoch 111/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0213 - mae: 1.5066 - val_loss: 1.0483 - val_mae: 1.5399\n",
            "Epoch 112/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0072 - mae: 1.4941 - val_loss: 1.0346 - val_mae: 1.5390\n",
            "Epoch 113/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0262 - mae: 1.5154 - val_loss: 1.0568 - val_mae: 1.5493\n",
            "Epoch 114/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9978 - mae: 1.4824 - val_loss: 1.0302 - val_mae: 1.5220\n",
            "Epoch 115/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0036 - mae: 1.4862 - val_loss: 1.0336 - val_mae: 1.5213\n",
            "Epoch 116/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0114 - mae: 1.4934 - val_loss: 1.0360 - val_mae: 1.5203\n",
            "Epoch 117/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0142 - mae: 1.4965 - val_loss: 1.0631 - val_mae: 1.5554\n",
            "Epoch 118/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9902 - mae: 1.4761 - val_loss: 1.0485 - val_mae: 1.5346\n",
            "Epoch 119/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9882 - mae: 1.4678 - val_loss: 1.0435 - val_mae: 1.5347\n",
            "Epoch 120/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9994 - mae: 1.4792 - val_loss: 1.0546 - val_mae: 1.5505\n",
            "Epoch 121/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0006 - mae: 1.4800 - val_loss: 1.0410 - val_mae: 1.5395\n",
            "Epoch 122/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9978 - mae: 1.4798 - val_loss: 1.0364 - val_mae: 1.5324\n",
            "Epoch 123/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0003 - mae: 1.4822 - val_loss: 1.0299 - val_mae: 1.5186\n",
            "Epoch 124/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9886 - mae: 1.4656 - val_loss: 1.0196 - val_mae: 1.4951\n",
            "Epoch 125/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0083 - mae: 1.4917 - val_loss: 1.0206 - val_mae: 1.5105\n",
            "Epoch 126/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0029 - mae: 1.4871 - val_loss: 1.0533 - val_mae: 1.5369\n",
            "Epoch 127/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0029 - mae: 1.4840 - val_loss: 1.0456 - val_mae: 1.5369\n",
            "Epoch 128/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9963 - mae: 1.4765 - val_loss: 1.0444 - val_mae: 1.5259\n",
            "Epoch 129/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9944 - mae: 1.4770 - val_loss: 1.0723 - val_mae: 1.5714\n",
            "Epoch 130/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0000 - mae: 1.4790 - val_loss: 1.0615 - val_mae: 1.5409\n",
            "Epoch 131/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0206 - mae: 1.4992 - val_loss: 1.0515 - val_mae: 1.5466\n",
            "Epoch 132/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0006 - mae: 1.4839 - val_loss: 1.0381 - val_mae: 1.5343\n",
            "Epoch 133/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9993 - mae: 1.4794 - val_loss: 1.0254 - val_mae: 1.5139\n",
            "Epoch 134/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9926 - mae: 1.4779 - val_loss: 1.0474 - val_mae: 1.5349\n",
            "Epoch 135/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0063 - mae: 1.4901 - val_loss: 1.0714 - val_mae: 1.5486\n",
            "Epoch 136/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 0.9972 - mae: 1.4756 - val_loss: 1.0317 - val_mae: 1.5074\n",
            "Epoch 137/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0126 - mae: 1.4943 - val_loss: 1.0452 - val_mae: 1.5356\n",
            "Epoch 138/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0128 - mae: 1.4955 - val_loss: 1.0521 - val_mae: 1.5324\n",
            "Epoch 139/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0170 - mae: 1.4984 - val_loss: 1.0731 - val_mae: 1.5633\n",
            "Epoch 140/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0200 - mae: 1.5068 - val_loss: 1.0600 - val_mae: 1.5491\n",
            "Epoch 141/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9796 - mae: 1.4595 - val_loss: 1.0726 - val_mae: 1.5667\n",
            "Epoch 142/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0059 - mae: 1.4894 - val_loss: 1.0373 - val_mae: 1.5312\n",
            "Epoch 143/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9970 - mae: 1.4776 - val_loss: 1.0603 - val_mae: 1.5564\n",
            "Epoch 144/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9981 - mae: 1.4799 - val_loss: 1.0295 - val_mae: 1.5224\n",
            "Epoch 145/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9956 - mae: 1.4744 - val_loss: 1.0510 - val_mae: 1.5401\n",
            "Epoch 146/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9960 - mae: 1.4758 - val_loss: 1.0351 - val_mae: 1.5148\n",
            "Epoch 147/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0085 - mae: 1.4924 - val_loss: 1.0280 - val_mae: 1.5037\n",
            "Epoch 148/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0080 - mae: 1.4914 - val_loss: 1.0374 - val_mae: 1.5341\n",
            "Epoch 149/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 0.9918 - mae: 1.4728 - val_loss: 1.0307 - val_mae: 1.5178\n",
            "Epoch 150/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9981 - mae: 1.4773 - val_loss: 1.0860 - val_mae: 1.5799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:09:25,998] Trial 0 finished with value: 1.08600652217865 and parameters: {'dropout_2': 0.7006430636057989, 'dropout_3': 0.6224749648646567, 'dropout_4': 0.23249422169819944, 'dropout_5': 0.19701895000514755, 'learning_rate': 0.007599153905170674, 'epochs': 150, 'batch_size': 128}. Best is trial 0 with value: 1.08600652217865.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "108/108 [==============================] - 3s 7ms/step - loss: 1.3400 - mae: 1.8284 - val_loss: 1.2640 - val_mae: 1.7297\n",
            "Epoch 2/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2582 - mae: 1.7480 - val_loss: 1.1970 - val_mae: 1.6615\n",
            "Epoch 3/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2232 - mae: 1.7144 - val_loss: 1.2062 - val_mae: 1.6644\n",
            "Epoch 4/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2051 - mae: 1.6962 - val_loss: 1.1956 - val_mae: 1.7197\n",
            "Epoch 5/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2039 - mae: 1.6938 - val_loss: 1.2256 - val_mae: 1.7283\n",
            "Epoch 6/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1892 - mae: 1.6776 - val_loss: 1.2370 - val_mae: 1.7141\n",
            "Epoch 7/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1965 - mae: 1.6834 - val_loss: 1.1985 - val_mae: 1.7301\n",
            "Epoch 8/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1965 - mae: 1.6864 - val_loss: 1.2874 - val_mae: 1.7319\n",
            "Epoch 9/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1906 - mae: 1.6732 - val_loss: 1.2272 - val_mae: 1.7414\n",
            "Epoch 10/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1820 - mae: 1.6676 - val_loss: 1.2021 - val_mae: 1.6436\n",
            "Epoch 11/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1742 - mae: 1.6615 - val_loss: 1.1873 - val_mae: 1.6519\n",
            "Epoch 12/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1830 - mae: 1.6693 - val_loss: 1.1874 - val_mae: 1.6484\n",
            "Epoch 13/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1816 - mae: 1.6736 - val_loss: 1.1414 - val_mae: 1.6151\n",
            "Epoch 14/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1719 - mae: 1.6577 - val_loss: 1.2168 - val_mae: 1.6755\n",
            "Epoch 15/100\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.1743 - mae: 1.6633 - val_loss: 1.2148 - val_mae: 1.7068\n",
            "Epoch 16/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1861 - mae: 1.6780 - val_loss: 1.2109 - val_mae: 1.7157\n",
            "Epoch 17/100\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.1711 - mae: 1.6617 - val_loss: 1.2446 - val_mae: 1.7738\n",
            "Epoch 18/100\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.1822 - mae: 1.6709 - val_loss: 1.2661 - val_mae: 1.8021\n",
            "Epoch 19/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1612 - mae: 1.6548 - val_loss: 1.3064 - val_mae: 1.8764\n",
            "Epoch 20/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1717 - mae: 1.6582 - val_loss: 1.1359 - val_mae: 1.6011\n",
            "Epoch 21/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1752 - mae: 1.6616 - val_loss: 1.1527 - val_mae: 1.6182\n",
            "Epoch 22/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1620 - mae: 1.6495 - val_loss: 1.2158 - val_mae: 1.7041\n",
            "Epoch 23/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1636 - mae: 1.6507 - val_loss: 1.1634 - val_mae: 1.6874\n",
            "Epoch 24/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1560 - mae: 1.6430 - val_loss: 1.1654 - val_mae: 1.6903\n",
            "Epoch 25/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1585 - mae: 1.6494 - val_loss: 1.2419 - val_mae: 1.7136\n",
            "Epoch 26/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1863 - mae: 1.6704 - val_loss: 1.1911 - val_mae: 1.6435\n",
            "Epoch 27/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1662 - mae: 1.6524 - val_loss: 1.2659 - val_mae: 1.7338\n",
            "Epoch 28/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1747 - mae: 1.6660 - val_loss: 1.1312 - val_mae: 1.6161\n",
            "Epoch 29/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1476 - mae: 1.6301 - val_loss: 1.2023 - val_mae: 1.7183\n",
            "Epoch 30/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1687 - mae: 1.6547 - val_loss: 1.2865 - val_mae: 1.7923\n",
            "Epoch 31/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1798 - mae: 1.6711 - val_loss: 1.2034 - val_mae: 1.6742\n",
            "Epoch 32/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1639 - mae: 1.6506 - val_loss: 1.1892 - val_mae: 1.7028\n",
            "Epoch 33/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1658 - mae: 1.6529 - val_loss: 1.2577 - val_mae: 1.7151\n",
            "Epoch 34/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1465 - mae: 1.6303 - val_loss: 1.2508 - val_mae: 1.7924\n",
            "Epoch 35/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1756 - mae: 1.6660 - val_loss: 1.3127 - val_mae: 1.7773\n",
            "Epoch 36/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1637 - mae: 1.6470 - val_loss: 1.2255 - val_mae: 1.6885\n",
            "Epoch 37/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1765 - mae: 1.6605 - val_loss: 1.1776 - val_mae: 1.6911\n",
            "Epoch 38/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1511 - mae: 1.6399 - val_loss: 1.1744 - val_mae: 1.6373\n",
            "Epoch 39/100\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.1628 - mae: 1.6473 - val_loss: 1.2269 - val_mae: 1.7641\n",
            "Epoch 40/100\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.1517 - mae: 1.6431 - val_loss: 1.1538 - val_mae: 1.6105\n",
            "Epoch 41/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1720 - mae: 1.6591 - val_loss: 1.2510 - val_mae: 1.7727\n",
            "Epoch 42/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1707 - mae: 1.6592 - val_loss: 1.1707 - val_mae: 1.6545\n",
            "Epoch 43/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1711 - mae: 1.6579 - val_loss: 1.2140 - val_mae: 1.6915\n",
            "Epoch 44/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1587 - mae: 1.6431 - val_loss: 1.1969 - val_mae: 1.7182\n",
            "Epoch 45/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1673 - mae: 1.6627 - val_loss: 1.1286 - val_mae: 1.6246\n",
            "Epoch 46/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1587 - mae: 1.6446 - val_loss: 1.1901 - val_mae: 1.7150\n",
            "Epoch 47/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1492 - mae: 1.6382 - val_loss: 1.1808 - val_mae: 1.6650\n",
            "Epoch 48/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1591 - mae: 1.6491 - val_loss: 1.2149 - val_mae: 1.6937\n",
            "Epoch 49/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1492 - mae: 1.6357 - val_loss: 1.1279 - val_mae: 1.6172\n",
            "Epoch 50/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1577 - mae: 1.6436 - val_loss: 1.1636 - val_mae: 1.6427\n",
            "Epoch 51/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1338 - mae: 1.6170 - val_loss: 1.1922 - val_mae: 1.6299\n",
            "Epoch 52/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1476 - mae: 1.6307 - val_loss: 1.2466 - val_mae: 1.7027\n",
            "Epoch 53/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1705 - mae: 1.6549 - val_loss: 1.1381 - val_mae: 1.6510\n",
            "Epoch 54/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1607 - mae: 1.6473 - val_loss: 1.1904 - val_mae: 1.7011\n",
            "Epoch 55/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1568 - mae: 1.6445 - val_loss: 1.1512 - val_mae: 1.5915\n",
            "Epoch 56/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1513 - mae: 1.6390 - val_loss: 1.2712 - val_mae: 1.7312\n",
            "Epoch 57/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1611 - mae: 1.6473 - val_loss: 1.1835 - val_mae: 1.7025\n",
            "Epoch 58/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1678 - mae: 1.6553 - val_loss: 1.1913 - val_mae: 1.6536\n",
            "Epoch 59/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1595 - mae: 1.6501 - val_loss: 1.1508 - val_mae: 1.6047\n",
            "Epoch 60/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1467 - mae: 1.6311 - val_loss: 1.2000 - val_mae: 1.7303\n",
            "Epoch 61/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1649 - mae: 1.6550 - val_loss: 1.1506 - val_mae: 1.6417\n",
            "Epoch 62/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1395 - mae: 1.6235 - val_loss: 1.2309 - val_mae: 1.7400\n",
            "Epoch 63/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1532 - mae: 1.6350 - val_loss: 1.2614 - val_mae: 1.7844\n",
            "Epoch 64/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1398 - mae: 1.6292 - val_loss: 1.2038 - val_mae: 1.6900\n",
            "Epoch 65/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1420 - mae: 1.6284 - val_loss: 1.1720 - val_mae: 1.6596\n",
            "Epoch 66/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1651 - mae: 1.6493 - val_loss: 1.1747 - val_mae: 1.6716\n",
            "Epoch 67/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1496 - mae: 1.6349 - val_loss: 1.1589 - val_mae: 1.6691\n",
            "Epoch 68/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1537 - mae: 1.6385 - val_loss: 1.2508 - val_mae: 1.7129\n",
            "Epoch 69/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1666 - mae: 1.6552 - val_loss: 1.2073 - val_mae: 1.6720\n",
            "Epoch 70/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1524 - mae: 1.6405 - val_loss: 1.1333 - val_mae: 1.5806\n",
            "Epoch 71/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1365 - mae: 1.6222 - val_loss: 1.1988 - val_mae: 1.6842\n",
            "Epoch 72/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1477 - mae: 1.6365 - val_loss: 1.1276 - val_mae: 1.5847\n",
            "Epoch 73/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1421 - mae: 1.6297 - val_loss: 1.2187 - val_mae: 1.7350\n",
            "Epoch 74/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1711 - mae: 1.6598 - val_loss: 1.1772 - val_mae: 1.6968\n",
            "Epoch 75/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1398 - mae: 1.6242 - val_loss: 1.1799 - val_mae: 1.7061\n",
            "Epoch 76/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1538 - mae: 1.6400 - val_loss: 1.2823 - val_mae: 1.7448\n",
            "Epoch 77/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1529 - mae: 1.6384 - val_loss: 1.2763 - val_mae: 1.7246\n",
            "Epoch 78/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1596 - mae: 1.6468 - val_loss: 1.1237 - val_mae: 1.6368\n",
            "Epoch 79/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1502 - mae: 1.6362 - val_loss: 1.1463 - val_mae: 1.6537\n",
            "Epoch 80/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1526 - mae: 1.6418 - val_loss: 1.1206 - val_mae: 1.6283\n",
            "Epoch 81/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1500 - mae: 1.6365 - val_loss: 1.1497 - val_mae: 1.6088\n",
            "Epoch 82/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1418 - mae: 1.6340 - val_loss: 1.2122 - val_mae: 1.6673\n",
            "Epoch 83/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1563 - mae: 1.6476 - val_loss: 1.1795 - val_mae: 1.6637\n",
            "Epoch 84/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1414 - mae: 1.6247 - val_loss: 1.1219 - val_mae: 1.6212\n",
            "Epoch 85/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1555 - mae: 1.6430 - val_loss: 1.1607 - val_mae: 1.6254\n",
            "Epoch 86/100\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.1606 - mae: 1.6479 - val_loss: 1.2006 - val_mae: 1.7264\n",
            "Epoch 87/100\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.1535 - mae: 1.6382 - val_loss: 1.2198 - val_mae: 1.6936\n",
            "Epoch 88/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1485 - mae: 1.6351 - val_loss: 1.2309 - val_mae: 1.6874\n",
            "Epoch 89/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1592 - mae: 1.6449 - val_loss: 1.1644 - val_mae: 1.6383\n",
            "Epoch 90/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1425 - mae: 1.6281 - val_loss: 1.2474 - val_mae: 1.7422\n",
            "Epoch 91/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1471 - mae: 1.6387 - val_loss: 1.2247 - val_mae: 1.7297\n",
            "Epoch 92/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1528 - mae: 1.6451 - val_loss: 1.1679 - val_mae: 1.6498\n",
            "Epoch 93/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1686 - mae: 1.6594 - val_loss: 1.1584 - val_mae: 1.6811\n",
            "Epoch 94/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1484 - mae: 1.6341 - val_loss: 1.2124 - val_mae: 1.7223\n",
            "Epoch 95/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1575 - mae: 1.6471 - val_loss: 1.2453 - val_mae: 1.7289\n",
            "Epoch 96/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1460 - mae: 1.6367 - val_loss: 1.1715 - val_mae: 1.6194\n",
            "Epoch 97/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1526 - mae: 1.6442 - val_loss: 1.1035 - val_mae: 1.5907\n",
            "Epoch 98/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1365 - mae: 1.6256 - val_loss: 1.1881 - val_mae: 1.6527\n",
            "Epoch 99/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1327 - mae: 1.6193 - val_loss: 1.1827 - val_mae: 1.6848\n",
            "Epoch 100/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1367 - mae: 1.6234 - val_loss: 1.1561 - val_mae: 1.6829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:10:26,488] Trial 1 finished with value: 1.1561261415481567 and parameters: {'dropout_2': 0.8818604491589114, 'dropout_3': 0.3359364727035885, 'dropout_4': 0.4693389984089641, 'dropout_5': 0.4451976562910088, 'learning_rate': 0.047140428906063035, 'epochs': 100, 'batch_size': 32}. Best is trial 0 with value: 1.08600652217865.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 3s 7ms/step - loss: 1.6970 - mae: 2.2649 - val_loss: 1.3972 - val_mae: 1.9625\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5383 - mae: 2.0962 - val_loss: 1.3870 - val_mae: 1.9596\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.4191 - mae: 1.9655 - val_loss: 1.2938 - val_mae: 1.8394\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3304 - mae: 1.8641 - val_loss: 1.2365 - val_mae: 1.7592\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.2702 - mae: 1.7994 - val_loss: 1.2309 - val_mae: 1.7513\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2322 - mae: 1.7503 - val_loss: 1.1809 - val_mae: 1.6825\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.2083 - mae: 1.7269 - val_loss: 1.1885 - val_mae: 1.6907\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1946 - mae: 1.7088 - val_loss: 1.1572 - val_mae: 1.6512\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1673 - mae: 1.6756 - val_loss: 1.1317 - val_mae: 1.6207\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1408 - mae: 1.6438 - val_loss: 1.1216 - val_mae: 1.6085\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1346 - mae: 1.6357 - val_loss: 1.1395 - val_mae: 1.6344\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1085 - mae: 1.6089 - val_loss: 1.1146 - val_mae: 1.6079\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1145 - mae: 1.6142 - val_loss: 1.1076 - val_mae: 1.5935\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0904 - mae: 1.5865 - val_loss: 1.1176 - val_mae: 1.6107\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0798 - mae: 1.5770 - val_loss: 1.1158 - val_mae: 1.6115\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0732 - mae: 1.5682 - val_loss: 1.0865 - val_mae: 1.5747\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0662 - mae: 1.5587 - val_loss: 1.0924 - val_mae: 1.5831\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0552 - mae: 1.5489 - val_loss: 1.0750 - val_mae: 1.5541\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 1s 12ms/step - loss: 1.0547 - mae: 1.5473 - val_loss: 1.0821 - val_mae: 1.5660\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0574 - mae: 1.5478 - val_loss: 1.0604 - val_mae: 1.5400\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0310 - mae: 1.5210 - val_loss: 1.0534 - val_mae: 1.5322\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0525 - mae: 1.5444 - val_loss: 1.0705 - val_mae: 1.5533\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0301 - mae: 1.5187 - val_loss: 1.0503 - val_mae: 1.5311\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0510 - mae: 1.5471 - val_loss: 1.0485 - val_mae: 1.5178\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0231 - mae: 1.5105 - val_loss: 1.0600 - val_mae: 1.5346\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0155 - mae: 1.5041 - val_loss: 1.0157 - val_mae: 1.4876\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0271 - mae: 1.5220 - val_loss: 1.0240 - val_mae: 1.4965\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0431 - mae: 1.5341 - val_loss: 1.0466 - val_mae: 1.5364\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0196 - mae: 1.5084 - val_loss: 1.0261 - val_mae: 1.5092\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0154 - mae: 1.5005 - val_loss: 1.0420 - val_mae: 1.5273\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0185 - mae: 1.5069 - val_loss: 1.0241 - val_mae: 1.5027\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0029 - mae: 1.4884 - val_loss: 1.0509 - val_mae: 1.5405\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9986 - mae: 1.4839 - val_loss: 1.0014 - val_mae: 1.4702\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9980 - mae: 1.4816 - val_loss: 1.0034 - val_mae: 1.4829\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9887 - mae: 1.4735 - val_loss: 1.0089 - val_mae: 1.4826\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9799 - mae: 1.4663 - val_loss: 1.0141 - val_mae: 1.4869\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9980 - mae: 1.4844 - val_loss: 1.0060 - val_mae: 1.4811\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0031 - mae: 1.4887 - val_loss: 1.0018 - val_mae: 1.4766\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9918 - mae: 1.4771 - val_loss: 1.0103 - val_mae: 1.4865\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9835 - mae: 1.4669 - val_loss: 1.0060 - val_mae: 1.4857\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9632 - mae: 1.4457 - val_loss: 1.0060 - val_mae: 1.4804\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9787 - mae: 1.4654 - val_loss: 1.0009 - val_mae: 1.4763\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9750 - mae: 1.4601 - val_loss: 1.0001 - val_mae: 1.4779\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9626 - mae: 1.4458 - val_loss: 1.0012 - val_mae: 1.4724\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9848 - mae: 1.4734 - val_loss: 1.0023 - val_mae: 1.4749\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9682 - mae: 1.4515 - val_loss: 0.9849 - val_mae: 1.4566\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9659 - mae: 1.4497 - val_loss: 0.9814 - val_mae: 1.4489\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9590 - mae: 1.4435 - val_loss: 0.9698 - val_mae: 1.4394\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9569 - mae: 1.4402 - val_loss: 0.9829 - val_mae: 1.4542\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9629 - mae: 1.4450 - val_loss: 0.9826 - val_mae: 1.4438\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9521 - mae: 1.4336 - val_loss: 0.9508 - val_mae: 1.4161\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9394 - mae: 1.4167 - val_loss: 0.9622 - val_mae: 1.4308\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9616 - mae: 1.4461 - val_loss: 0.9562 - val_mae: 1.4217\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9544 - mae: 1.4397 - val_loss: 1.0169 - val_mae: 1.4834\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9709 - mae: 1.4541 - val_loss: 0.9975 - val_mae: 1.4691\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9440 - mae: 1.4243 - val_loss: 1.0168 - val_mae: 1.4874\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9433 - mae: 1.4280 - val_loss: 0.9966 - val_mae: 1.4636\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9393 - mae: 1.4196 - val_loss: 0.9811 - val_mae: 1.4418\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9467 - mae: 1.4275 - val_loss: 0.9831 - val_mae: 1.4468\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9183 - mae: 1.3960 - val_loss: 0.9600 - val_mae: 1.4268\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9404 - mae: 1.4202 - val_loss: 0.9407 - val_mae: 1.4015\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9208 - mae: 1.3951 - val_loss: 0.9557 - val_mae: 1.4213\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9372 - mae: 1.4183 - val_loss: 0.9555 - val_mae: 1.4169\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9276 - mae: 1.4019 - val_loss: 0.9430 - val_mae: 1.4022\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9235 - mae: 1.3968 - val_loss: 0.9472 - val_mae: 1.4099\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9316 - mae: 1.4137 - val_loss: 0.9638 - val_mae: 1.4200\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9366 - mae: 1.4177 - val_loss: 0.9463 - val_mae: 1.4087\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9203 - mae: 1.3971 - val_loss: 0.9273 - val_mae: 1.3910\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9282 - mae: 1.4084 - val_loss: 0.9342 - val_mae: 1.3912\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9221 - mae: 1.4003 - val_loss: 0.9316 - val_mae: 1.3916\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9237 - mae: 1.4062 - val_loss: 0.9397 - val_mae: 1.3913\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9137 - mae: 1.3879 - val_loss: 0.9310 - val_mae: 1.3887\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9333 - mae: 1.4137 - val_loss: 0.9224 - val_mae: 1.3825\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9190 - mae: 1.3973 - val_loss: 0.9075 - val_mae: 1.3659\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9092 - mae: 1.3853 - val_loss: 0.9123 - val_mae: 1.3723\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9062 - mae: 1.3795 - val_loss: 0.9451 - val_mae: 1.4072\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9073 - mae: 1.3820 - val_loss: 0.9383 - val_mae: 1.3961\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8999 - mae: 1.3744 - val_loss: 0.9423 - val_mae: 1.3982\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9215 - mae: 1.3953 - val_loss: 0.9458 - val_mae: 1.4019\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8920 - mae: 1.3646 - val_loss: 0.9425 - val_mae: 1.4075\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9067 - mae: 1.3787 - val_loss: 0.9444 - val_mae: 1.4047\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9000 - mae: 1.3706 - val_loss: 0.9311 - val_mae: 1.3774\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9091 - mae: 1.3832 - val_loss: 0.9572 - val_mae: 1.4152\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9139 - mae: 1.3910 - val_loss: 0.9258 - val_mae: 1.3795\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9119 - mae: 1.3860 - val_loss: 0.9228 - val_mae: 1.3739\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8889 - mae: 1.3611 - val_loss: 0.9398 - val_mae: 1.3949\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9161 - mae: 1.3938 - val_loss: 0.9300 - val_mae: 1.3813\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8875 - mae: 1.3568 - val_loss: 0.9392 - val_mae: 1.3837\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8860 - mae: 1.3574 - val_loss: 0.9484 - val_mae: 1.3956\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8941 - mae: 1.3669 - val_loss: 0.9521 - val_mae: 1.4098\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8989 - mae: 1.3742 - val_loss: 0.9147 - val_mae: 1.3690\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8936 - mae: 1.3637 - val_loss: 0.9418 - val_mae: 1.3959\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8958 - mae: 1.3648 - val_loss: 0.9521 - val_mae: 1.4043\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8882 - mae: 1.3566 - val_loss: 0.9388 - val_mae: 1.3972\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8888 - mae: 1.3605 - val_loss: 0.9081 - val_mae: 1.3594\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8803 - mae: 1.3542 - val_loss: 0.9020 - val_mae: 1.3523\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8768 - mae: 1.3494 - val_loss: 0.9193 - val_mae: 1.3749\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8758 - mae: 1.3423 - val_loss: 0.9059 - val_mae: 1.3532\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8725 - mae: 1.3383 - val_loss: 0.8986 - val_mae: 1.3465\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8935 - mae: 1.3690 - val_loss: 0.9427 - val_mae: 1.3937\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8763 - mae: 1.3459 - val_loss: 0.9006 - val_mae: 1.3560\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8494 - mae: 1.3106 - val_loss: 0.9231 - val_mae: 1.3848\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8824 - mae: 1.3510 - val_loss: 0.9270 - val_mae: 1.3845\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8557 - mae: 1.3260 - val_loss: 0.9063 - val_mae: 1.3611\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8593 - mae: 1.3300 - val_loss: 0.9077 - val_mae: 1.3637\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8674 - mae: 1.3350 - val_loss: 0.9021 - val_mae: 1.3512\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8802 - mae: 1.3460 - val_loss: 0.8850 - val_mae: 1.3384\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8725 - mae: 1.3424 - val_loss: 0.8942 - val_mae: 1.3467\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8689 - mae: 1.3391 - val_loss: 0.9315 - val_mae: 1.3867\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8578 - mae: 1.3267 - val_loss: 0.9186 - val_mae: 1.3629\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8528 - mae: 1.3159 - val_loss: 0.9272 - val_mae: 1.3794\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8744 - mae: 1.3463 - val_loss: 0.9281 - val_mae: 1.3808\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8648 - mae: 1.3310 - val_loss: 0.9135 - val_mae: 1.3547\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8712 - mae: 1.3376 - val_loss: 0.8978 - val_mae: 1.3527\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8710 - mae: 1.3419 - val_loss: 0.8940 - val_mae: 1.3373\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8639 - mae: 1.3282 - val_loss: 0.9209 - val_mae: 1.3746\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8599 - mae: 1.3279 - val_loss: 0.9108 - val_mae: 1.3534\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8552 - mae: 1.3212 - val_loss: 0.8950 - val_mae: 1.3397\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8487 - mae: 1.3109 - val_loss: 0.8968 - val_mae: 1.3424\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8558 - mae: 1.3227 - val_loss: 0.9171 - val_mae: 1.3636\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8630 - mae: 1.3300 - val_loss: 0.9078 - val_mae: 1.3595\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8425 - mae: 1.3038 - val_loss: 0.9431 - val_mae: 1.3954\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8455 - mae: 1.3132 - val_loss: 0.8981 - val_mae: 1.3458\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8635 - mae: 1.3294 - val_loss: 0.9240 - val_mae: 1.3771\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8483 - mae: 1.3130 - val_loss: 0.8811 - val_mae: 1.3279\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8413 - mae: 1.3008 - val_loss: 0.9383 - val_mae: 1.3853\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8675 - mae: 1.3346 - val_loss: 0.9254 - val_mae: 1.3703\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8584 - mae: 1.3232 - val_loss: 0.9046 - val_mae: 1.3482\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8436 - mae: 1.3060 - val_loss: 0.9199 - val_mae: 1.3643\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8605 - mae: 1.3253 - val_loss: 0.9207 - val_mae: 1.3658\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8510 - mae: 1.3180 - val_loss: 0.9224 - val_mae: 1.3611\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8358 - mae: 1.2973 - val_loss: 0.8795 - val_mae: 1.3255\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8485 - mae: 1.3088 - val_loss: 0.8847 - val_mae: 1.3282\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8356 - mae: 1.2975 - val_loss: 0.8945 - val_mae: 1.3461\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8375 - mae: 1.3025 - val_loss: 0.9115 - val_mae: 1.3580\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8473 - mae: 1.3117 - val_loss: 0.8706 - val_mae: 1.3139\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8256 - mae: 1.2889 - val_loss: 0.8766 - val_mae: 1.3239\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8199 - mae: 1.2827 - val_loss: 0.8857 - val_mae: 1.3284\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8447 - mae: 1.3092 - val_loss: 0.9023 - val_mae: 1.3451\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8501 - mae: 1.3152 - val_loss: 0.8667 - val_mae: 1.3139\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8279 - mae: 1.2895 - val_loss: 0.8988 - val_mae: 1.3421\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8401 - mae: 1.3032 - val_loss: 0.8899 - val_mae: 1.3248\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8404 - mae: 1.3057 - val_loss: 0.8914 - val_mae: 1.3290\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8343 - mae: 1.2950 - val_loss: 0.9183 - val_mae: 1.3659\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8588 - mae: 1.3211 - val_loss: 0.8938 - val_mae: 1.3296\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8315 - mae: 1.2943 - val_loss: 0.8872 - val_mae: 1.3281\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 1s 10ms/step - loss: 0.8424 - mae: 1.3039 - val_loss: 0.8918 - val_mae: 1.3315\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8367 - mae: 1.2988 - val_loss: 0.9045 - val_mae: 1.3519\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 1s 10ms/step - loss: 0.8309 - mae: 1.2908 - val_loss: 0.8824 - val_mae: 1.3237\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8161 - mae: 1.2774 - val_loss: 0.8972 - val_mae: 1.3431\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8058 - mae: 1.2638 - val_loss: 0.9144 - val_mae: 1.3628\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8025 - mae: 1.2591 - val_loss: 0.8467 - val_mae: 1.2900\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8343 - mae: 1.2978 - val_loss: 0.8355 - val_mae: 1.2780\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8244 - mae: 1.2847 - val_loss: 0.8707 - val_mae: 1.3154\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8279 - mae: 1.2881 - val_loss: 0.8725 - val_mae: 1.3131\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8548 - mae: 1.3209 - val_loss: 0.8699 - val_mae: 1.3104\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8389 - mae: 1.3011 - val_loss: 0.8443 - val_mae: 1.2787\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8044 - mae: 1.2603 - val_loss: 0.8478 - val_mae: 1.2887\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8158 - mae: 1.2699 - val_loss: 0.8700 - val_mae: 1.3136\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8323 - mae: 1.2977 - val_loss: 0.8570 - val_mae: 1.2974\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8348 - mae: 1.2950 - val_loss: 0.8424 - val_mae: 1.2806\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8147 - mae: 1.2722 - val_loss: 0.8869 - val_mae: 1.3294\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8355 - mae: 1.2988 - val_loss: 0.8768 - val_mae: 1.3173\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8197 - mae: 1.2777 - val_loss: 0.8774 - val_mae: 1.3143\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8212 - mae: 1.2807 - val_loss: 0.8951 - val_mae: 1.3380\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8012 - mae: 1.2557 - val_loss: 0.8585 - val_mae: 1.3015\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8100 - mae: 1.2662 - val_loss: 0.8885 - val_mae: 1.3314\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8212 - mae: 1.2779 - val_loss: 0.8941 - val_mae: 1.3342\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8044 - mae: 1.2600 - val_loss: 0.8614 - val_mae: 1.3066\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8086 - mae: 1.2635 - val_loss: 0.8834 - val_mae: 1.3290\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8234 - mae: 1.2838 - val_loss: 0.8999 - val_mae: 1.3413\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8021 - mae: 1.2587 - val_loss: 0.8560 - val_mae: 1.2948\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8146 - mae: 1.2708 - val_loss: 0.8619 - val_mae: 1.3035\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8046 - mae: 1.2583 - val_loss: 0.9747 - val_mae: 1.4450\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8222 - mae: 1.2805 - val_loss: 0.8820 - val_mae: 1.3243\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8103 - mae: 1.2679 - val_loss: 0.8645 - val_mae: 1.2986\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8147 - mae: 1.2722 - val_loss: 0.8459 - val_mae: 1.2889\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8009 - mae: 1.2597 - val_loss: 0.8535 - val_mae: 1.2910\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8256 - mae: 1.2891 - val_loss: 0.8805 - val_mae: 1.3234\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8032 - mae: 1.2620 - val_loss: 0.8923 - val_mae: 1.3385\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8119 - mae: 1.2708 - val_loss: 0.8627 - val_mae: 1.3021\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8129 - mae: 1.2727 - val_loss: 0.8865 - val_mae: 1.3318\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.7952 - mae: 1.2506 - val_loss: 0.8679 - val_mae: 1.3157\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8072 - mae: 1.2643 - val_loss: 0.8860 - val_mae: 1.3207\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8167 - mae: 1.2758 - val_loss: 0.8866 - val_mae: 1.3251\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8039 - mae: 1.2603 - val_loss: 0.8890 - val_mae: 1.3308\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8032 - mae: 1.2579 - val_loss: 0.8779 - val_mae: 1.3222\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8178 - mae: 1.2733 - val_loss: 0.8502 - val_mae: 1.2858\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.7901 - mae: 1.2443 - val_loss: 0.8542 - val_mae: 1.2907\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8096 - mae: 1.2659 - val_loss: 0.8534 - val_mae: 1.2841\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.7998 - mae: 1.2546 - val_loss: 0.8915 - val_mae: 1.3339\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8000 - mae: 1.2536 - val_loss: 0.8530 - val_mae: 1.2913\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.7874 - mae: 1.2409 - val_loss: 0.8715 - val_mae: 1.3100\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.7802 - mae: 1.2332 - val_loss: 0.8580 - val_mae: 1.2956\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.7806 - mae: 1.2332 - val_loss: 0.8994 - val_mae: 1.3455\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8197 - mae: 1.2749 - val_loss: 0.8607 - val_mae: 1.3017\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8296 - mae: 1.2891 - val_loss: 0.8902 - val_mae: 1.3331\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.7883 - mae: 1.2404 - val_loss: 0.8477 - val_mae: 1.2836\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.7919 - mae: 1.2461 - val_loss: 0.8415 - val_mae: 1.2798\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8014 - mae: 1.2598 - val_loss: 0.8499 - val_mae: 1.2873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:12:50,591] Trial 2 finished with value: 0.8499419093132019 and parameters: {'dropout_2': 0.12660488892290706, 'dropout_3': 0.2724244164102639, 'dropout_4': 0.45940450739038996, 'dropout_5': 0.5279036824438129, 'learning_rate': 0.0005969147545719836, 'epochs': 200, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "27/27 [==============================] - 2s 17ms/step - loss: 1.9606 - mae: 2.5304 - val_loss: 1.3703 - val_mae: 1.8446\n",
            "Epoch 2/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.7590 - mae: 2.3142 - val_loss: 1.3651 - val_mae: 1.8385\n",
            "Epoch 3/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6374 - mae: 2.1748 - val_loss: 1.3361 - val_mae: 1.8003\n",
            "Epoch 4/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.5475 - mae: 2.0723 - val_loss: 1.3120 - val_mae: 1.7598\n",
            "Epoch 5/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.4391 - mae: 1.9497 - val_loss: 1.3014 - val_mae: 1.7452\n",
            "Epoch 6/50\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.3865 - mae: 1.8832 - val_loss: 1.2990 - val_mae: 1.7457\n",
            "Epoch 7/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.3325 - mae: 1.8250 - val_loss: 1.2785 - val_mae: 1.7299\n",
            "Epoch 8/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.3119 - mae: 1.7999 - val_loss: 1.2774 - val_mae: 1.7339\n",
            "Epoch 9/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.2782 - mae: 1.7614 - val_loss: 1.2592 - val_mae: 1.7195\n",
            "Epoch 10/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.2323 - mae: 1.7153 - val_loss: 1.2382 - val_mae: 1.6961\n",
            "Epoch 11/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.2265 - mae: 1.7067 - val_loss: 1.2280 - val_mae: 1.6873\n",
            "Epoch 12/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.1976 - mae: 1.6734 - val_loss: 1.2161 - val_mae: 1.6823\n",
            "Epoch 13/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.1917 - mae: 1.6716 - val_loss: 1.2145 - val_mae: 1.6792\n",
            "Epoch 14/50\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1828 - mae: 1.6633 - val_loss: 1.2076 - val_mae: 1.6728\n",
            "Epoch 15/50\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1772 - mae: 1.6586 - val_loss: 1.1942 - val_mae: 1.6574\n",
            "Epoch 16/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.1641 - mae: 1.6414 - val_loss: 1.1902 - val_mae: 1.6611\n",
            "Epoch 17/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.1487 - mae: 1.6287 - val_loss: 1.1802 - val_mae: 1.6512\n",
            "Epoch 18/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.1496 - mae: 1.6294 - val_loss: 1.1755 - val_mae: 1.6574\n",
            "Epoch 19/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.1420 - mae: 1.6240 - val_loss: 1.1669 - val_mae: 1.6477\n",
            "Epoch 20/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.1534 - mae: 1.6376 - val_loss: 1.1687 - val_mae: 1.6488\n",
            "Epoch 21/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.1432 - mae: 1.6255 - val_loss: 1.1679 - val_mae: 1.6473\n",
            "Epoch 22/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.1247 - mae: 1.6080 - val_loss: 1.1725 - val_mae: 1.6598\n",
            "Epoch 23/50\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1237 - mae: 1.6049 - val_loss: 1.1544 - val_mae: 1.6410\n",
            "Epoch 24/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.1405 - mae: 1.6226 - val_loss: 1.1441 - val_mae: 1.6270\n",
            "Epoch 25/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.1047 - mae: 1.5863 - val_loss: 1.1420 - val_mae: 1.6320\n",
            "Epoch 26/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.1090 - mae: 1.5890 - val_loss: 1.1213 - val_mae: 1.6096\n",
            "Epoch 27/50\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1090 - mae: 1.5968 - val_loss: 1.1401 - val_mae: 1.6335\n",
            "Epoch 28/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.1121 - mae: 1.6005 - val_loss: 1.1204 - val_mae: 1.6066\n",
            "Epoch 29/50\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0989 - mae: 1.5845 - val_loss: 1.1286 - val_mae: 1.6250\n",
            "Epoch 30/50\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.1018 - mae: 1.5884 - val_loss: 1.0989 - val_mae: 1.5893\n",
            "Epoch 31/50\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0957 - mae: 1.5815 - val_loss: 1.1070 - val_mae: 1.6019\n",
            "Epoch 32/50\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0987 - mae: 1.5866 - val_loss: 1.0807 - val_mae: 1.5727\n",
            "Epoch 33/50\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0994 - mae: 1.5879 - val_loss: 1.1105 - val_mae: 1.6076\n",
            "Epoch 34/50\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0741 - mae: 1.5586 - val_loss: 1.0835 - val_mae: 1.5767\n",
            "Epoch 35/50\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0822 - mae: 1.5672 - val_loss: 1.0807 - val_mae: 1.5747\n",
            "Epoch 36/50\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0891 - mae: 1.5731 - val_loss: 1.0948 - val_mae: 1.5890\n",
            "Epoch 37/50\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0853 - mae: 1.5650 - val_loss: 1.0626 - val_mae: 1.5555\n",
            "Epoch 38/50\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0827 - mae: 1.5713 - val_loss: 1.1010 - val_mae: 1.5974\n",
            "Epoch 39/50\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0780 - mae: 1.5595 - val_loss: 1.1064 - val_mae: 1.6005\n",
            "Epoch 40/50\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0988 - mae: 1.5857 - val_loss: 1.0867 - val_mae: 1.5772\n",
            "Epoch 41/50\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0715 - mae: 1.5528 - val_loss: 1.0640 - val_mae: 1.5552\n",
            "Epoch 42/50\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0763 - mae: 1.5600 - val_loss: 1.0684 - val_mae: 1.5628\n",
            "Epoch 43/50\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0654 - mae: 1.5500 - val_loss: 1.0698 - val_mae: 1.5622\n",
            "Epoch 44/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0566 - mae: 1.5409 - val_loss: 1.0552 - val_mae: 1.5520\n",
            "Epoch 45/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0587 - mae: 1.5401 - val_loss: 1.0437 - val_mae: 1.5359\n",
            "Epoch 46/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0794 - mae: 1.5641 - val_loss: 1.0662 - val_mae: 1.5617\n",
            "Epoch 47/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0675 - mae: 1.5516 - val_loss: 1.0571 - val_mae: 1.5414\n",
            "Epoch 48/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0725 - mae: 1.5535 - val_loss: 1.0677 - val_mae: 1.5546\n",
            "Epoch 49/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0793 - mae: 1.5647 - val_loss: 1.0819 - val_mae: 1.5714\n",
            "Epoch 50/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0730 - mae: 1.5541 - val_loss: 1.0695 - val_mae: 1.5585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:13:03,036] Trial 3 finished with value: 1.0695406198501587 and parameters: {'dropout_2': 0.5174071492451139, 'dropout_3': 0.5764396352717818, 'dropout_4': 0.15378803737924118, 'dropout_5': 0.7899203674675022, 'learning_rate': 0.0017533373355705887, 'epochs': 50, 'batch_size': 128}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "108/108 [==============================] - 3s 11ms/step - loss: 1.4691 - mae: 1.9850 - val_loss: 1.2708 - val_mae: 1.7324\n",
            "Epoch 2/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2396 - mae: 1.7226 - val_loss: 1.1876 - val_mae: 1.6386\n",
            "Epoch 3/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2128 - mae: 1.6948 - val_loss: 1.1641 - val_mae: 1.6420\n",
            "Epoch 4/150\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.2056 - mae: 1.6870 - val_loss: 1.1892 - val_mae: 1.6554\n",
            "Epoch 5/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1776 - mae: 1.6541 - val_loss: 1.1716 - val_mae: 1.6461\n",
            "Epoch 6/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1768 - mae: 1.6540 - val_loss: 1.2097 - val_mae: 1.6940\n",
            "Epoch 7/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1923 - mae: 1.6764 - val_loss: 1.1761 - val_mae: 1.6602\n",
            "Epoch 8/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1673 - mae: 1.6518 - val_loss: 1.1784 - val_mae: 1.6456\n",
            "Epoch 9/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1901 - mae: 1.6744 - val_loss: 1.1853 - val_mae: 1.6367\n",
            "Epoch 10/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1566 - mae: 1.6363 - val_loss: 1.1804 - val_mae: 1.6355\n",
            "Epoch 11/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1718 - mae: 1.6500 - val_loss: 1.1458 - val_mae: 1.6337\n",
            "Epoch 12/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1541 - mae: 1.6340 - val_loss: 1.2129 - val_mae: 1.6882\n",
            "Epoch 13/150\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.1683 - mae: 1.6499 - val_loss: 1.1453 - val_mae: 1.6138\n",
            "Epoch 14/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1681 - mae: 1.6504 - val_loss: 1.1844 - val_mae: 1.6728\n",
            "Epoch 15/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1698 - mae: 1.6520 - val_loss: 1.2115 - val_mae: 1.6896\n",
            "Epoch 16/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1570 - mae: 1.6371 - val_loss: 1.1639 - val_mae: 1.6423\n",
            "Epoch 17/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1673 - mae: 1.6492 - val_loss: 1.1814 - val_mae: 1.6575\n",
            "Epoch 18/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1636 - mae: 1.6440 - val_loss: 1.1656 - val_mae: 1.6547\n",
            "Epoch 19/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1575 - mae: 1.6391 - val_loss: 1.1460 - val_mae: 1.6219\n",
            "Epoch 20/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1571 - mae: 1.6402 - val_loss: 1.1514 - val_mae: 1.6164\n",
            "Epoch 21/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1409 - mae: 1.6236 - val_loss: 1.2034 - val_mae: 1.7131\n",
            "Epoch 22/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1572 - mae: 1.6409 - val_loss: 1.1663 - val_mae: 1.6594\n",
            "Epoch 23/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1306 - mae: 1.6143 - val_loss: 1.1278 - val_mae: 1.6113\n",
            "Epoch 24/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1480 - mae: 1.6326 - val_loss: 1.1441 - val_mae: 1.6224\n",
            "Epoch 25/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1427 - mae: 1.6245 - val_loss: 1.1454 - val_mae: 1.5977\n",
            "Epoch 26/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1355 - mae: 1.6160 - val_loss: 1.1679 - val_mae: 1.6412\n",
            "Epoch 27/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1383 - mae: 1.6194 - val_loss: 1.2253 - val_mae: 1.7381\n",
            "Epoch 28/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1460 - mae: 1.6279 - val_loss: 1.1468 - val_mae: 1.6339\n",
            "Epoch 29/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1354 - mae: 1.6141 - val_loss: 1.1525 - val_mae: 1.6399\n",
            "Epoch 30/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1468 - mae: 1.6252 - val_loss: 1.1474 - val_mae: 1.6459\n",
            "Epoch 31/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1571 - mae: 1.6358 - val_loss: 1.2075 - val_mae: 1.7038\n",
            "Epoch 32/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1401 - mae: 1.6194 - val_loss: 1.1770 - val_mae: 1.6547\n",
            "Epoch 33/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1280 - mae: 1.6072 - val_loss: 1.1411 - val_mae: 1.6235\n",
            "Epoch 34/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1470 - mae: 1.6288 - val_loss: 1.1361 - val_mae: 1.6090\n",
            "Epoch 35/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1388 - mae: 1.6213 - val_loss: 1.1581 - val_mae: 1.6251\n",
            "Epoch 36/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1446 - mae: 1.6263 - val_loss: 1.1226 - val_mae: 1.6014\n",
            "Epoch 37/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1267 - mae: 1.6057 - val_loss: 1.0889 - val_mae: 1.5631\n",
            "Epoch 38/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1461 - mae: 1.6324 - val_loss: 1.1419 - val_mae: 1.6275\n",
            "Epoch 39/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1329 - mae: 1.6151 - val_loss: 1.1269 - val_mae: 1.6114\n",
            "Epoch 40/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1180 - mae: 1.5989 - val_loss: 1.1461 - val_mae: 1.6505\n",
            "Epoch 41/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1392 - mae: 1.6223 - val_loss: 1.1277 - val_mae: 1.6168\n",
            "Epoch 42/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1248 - mae: 1.6107 - val_loss: 1.1361 - val_mae: 1.6007\n",
            "Epoch 43/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1388 - mae: 1.6213 - val_loss: 1.1726 - val_mae: 1.6698\n",
            "Epoch 44/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1467 - mae: 1.6259 - val_loss: 1.1418 - val_mae: 1.6380\n",
            "Epoch 45/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1357 - mae: 1.6183 - val_loss: 1.1442 - val_mae: 1.6371\n",
            "Epoch 46/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1309 - mae: 1.6146 - val_loss: 1.1289 - val_mae: 1.6132\n",
            "Epoch 47/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1533 - mae: 1.6373 - val_loss: 1.1439 - val_mae: 1.6385\n",
            "Epoch 48/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1343 - mae: 1.6197 - val_loss: 1.1035 - val_mae: 1.5889\n",
            "Epoch 49/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1344 - mae: 1.6206 - val_loss: 1.1215 - val_mae: 1.6185\n",
            "Epoch 50/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1226 - mae: 1.6081 - val_loss: 1.1335 - val_mae: 1.5994\n",
            "Epoch 51/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1396 - mae: 1.6205 - val_loss: 1.1423 - val_mae: 1.6271\n",
            "Epoch 52/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1080 - mae: 1.5886 - val_loss: 1.1370 - val_mae: 1.6343\n",
            "Epoch 53/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1315 - mae: 1.6096 - val_loss: 1.1260 - val_mae: 1.6126\n",
            "Epoch 54/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1238 - mae: 1.6023 - val_loss: 1.1427 - val_mae: 1.6268\n",
            "Epoch 55/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1197 - mae: 1.5996 - val_loss: 1.1309 - val_mae: 1.6150\n",
            "Epoch 56/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1375 - mae: 1.6150 - val_loss: 1.1647 - val_mae: 1.6600\n",
            "Epoch 57/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1419 - mae: 1.6257 - val_loss: 1.1612 - val_mae: 1.6634\n",
            "Epoch 58/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1182 - mae: 1.6032 - val_loss: 1.1160 - val_mae: 1.5980\n",
            "Epoch 59/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1245 - mae: 1.6057 - val_loss: 1.1748 - val_mae: 1.6566\n",
            "Epoch 60/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1390 - mae: 1.6200 - val_loss: 1.1427 - val_mae: 1.6297\n",
            "Epoch 61/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1141 - mae: 1.5928 - val_loss: 1.1435 - val_mae: 1.6391\n",
            "Epoch 62/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1386 - mae: 1.6204 - val_loss: 1.1749 - val_mae: 1.6578\n",
            "Epoch 63/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1214 - mae: 1.5973 - val_loss: 1.1473 - val_mae: 1.6438\n",
            "Epoch 64/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1271 - mae: 1.6082 - val_loss: 1.1084 - val_mae: 1.5993\n",
            "Epoch 65/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1225 - mae: 1.6083 - val_loss: 1.1134 - val_mae: 1.6022\n",
            "Epoch 66/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1342 - mae: 1.6197 - val_loss: 1.1613 - val_mae: 1.6465\n",
            "Epoch 67/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1265 - mae: 1.6065 - val_loss: 1.1400 - val_mae: 1.6291\n",
            "Epoch 68/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1127 - mae: 1.5944 - val_loss: 1.1134 - val_mae: 1.5982\n",
            "Epoch 69/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1191 - mae: 1.6034 - val_loss: 1.1383 - val_mae: 1.6209\n",
            "Epoch 70/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1257 - mae: 1.6101 - val_loss: 1.1279 - val_mae: 1.6082\n",
            "Epoch 71/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1337 - mae: 1.6162 - val_loss: 1.0776 - val_mae: 1.5676\n",
            "Epoch 72/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1348 - mae: 1.6215 - val_loss: 1.1150 - val_mae: 1.5959\n",
            "Epoch 73/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1389 - mae: 1.6237 - val_loss: 1.1594 - val_mae: 1.6473\n",
            "Epoch 74/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1148 - mae: 1.5952 - val_loss: 1.1050 - val_mae: 1.6033\n",
            "Epoch 75/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1087 - mae: 1.5893 - val_loss: 1.1304 - val_mae: 1.6269\n",
            "Epoch 76/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1144 - mae: 1.5963 - val_loss: 1.1584 - val_mae: 1.6516\n",
            "Epoch 77/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1101 - mae: 1.5939 - val_loss: 1.1355 - val_mae: 1.6133\n",
            "Epoch 78/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1198 - mae: 1.6035 - val_loss: 1.1346 - val_mae: 1.6286\n",
            "Epoch 79/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1229 - mae: 1.6074 - val_loss: 1.1734 - val_mae: 1.6598\n",
            "Epoch 80/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1283 - mae: 1.6134 - val_loss: 1.1721 - val_mae: 1.6614\n",
            "Epoch 81/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1152 - mae: 1.6015 - val_loss: 1.1399 - val_mae: 1.6206\n",
            "Epoch 82/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1150 - mae: 1.5965 - val_loss: 1.1747 - val_mae: 1.6752\n",
            "Epoch 83/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1347 - mae: 1.6219 - val_loss: 1.1119 - val_mae: 1.5930\n",
            "Epoch 84/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1096 - mae: 1.5924 - val_loss: 1.1095 - val_mae: 1.6082\n",
            "Epoch 85/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1059 - mae: 1.5893 - val_loss: 1.1300 - val_mae: 1.6039\n",
            "Epoch 86/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1093 - mae: 1.5929 - val_loss: 1.1867 - val_mae: 1.6863\n",
            "Epoch 87/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1083 - mae: 1.5934 - val_loss: 1.1206 - val_mae: 1.5908\n",
            "Epoch 88/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1291 - mae: 1.6090 - val_loss: 1.1551 - val_mae: 1.6316\n",
            "Epoch 89/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1169 - mae: 1.5935 - val_loss: 1.1300 - val_mae: 1.6361\n",
            "Epoch 90/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1188 - mae: 1.6058 - val_loss: 1.1434 - val_mae: 1.6266\n",
            "Epoch 91/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1139 - mae: 1.5930 - val_loss: 1.1266 - val_mae: 1.6349\n",
            "Epoch 92/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1153 - mae: 1.5994 - val_loss: 1.1302 - val_mae: 1.6230\n",
            "Epoch 93/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1166 - mae: 1.6015 - val_loss: 1.1451 - val_mae: 1.6534\n",
            "Epoch 94/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1003 - mae: 1.5829 - val_loss: 1.1106 - val_mae: 1.6165\n",
            "Epoch 95/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1065 - mae: 1.5899 - val_loss: 1.1276 - val_mae: 1.6216\n",
            "Epoch 96/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1094 - mae: 1.5934 - val_loss: 1.1253 - val_mae: 1.6245\n",
            "Epoch 97/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1223 - mae: 1.6026 - val_loss: 1.1291 - val_mae: 1.6190\n",
            "Epoch 98/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1113 - mae: 1.5932 - val_loss: 1.1343 - val_mae: 1.6343\n",
            "Epoch 99/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1104 - mae: 1.5899 - val_loss: 1.1387 - val_mae: 1.6327\n",
            "Epoch 100/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1248 - mae: 1.6044 - val_loss: 1.1206 - val_mae: 1.5985\n",
            "Epoch 101/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0993 - mae: 1.5811 - val_loss: 1.1099 - val_mae: 1.5927\n",
            "Epoch 102/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1006 - mae: 1.5833 - val_loss: 1.0825 - val_mae: 1.5619\n",
            "Epoch 103/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1205 - mae: 1.6059 - val_loss: 1.1059 - val_mae: 1.6124\n",
            "Epoch 104/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1125 - mae: 1.5963 - val_loss: 1.1530 - val_mae: 1.6481\n",
            "Epoch 105/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1100 - mae: 1.5881 - val_loss: 1.1410 - val_mae: 1.6374\n",
            "Epoch 106/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0982 - mae: 1.5819 - val_loss: 1.1027 - val_mae: 1.5963\n",
            "Epoch 107/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0891 - mae: 1.5714 - val_loss: 1.1580 - val_mae: 1.6535\n",
            "Epoch 108/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1057 - mae: 1.5906 - val_loss: 1.1105 - val_mae: 1.6068\n",
            "Epoch 109/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0969 - mae: 1.5825 - val_loss: 1.1033 - val_mae: 1.5965\n",
            "Epoch 110/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1047 - mae: 1.5872 - val_loss: 1.1363 - val_mae: 1.6251\n",
            "Epoch 111/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1026 - mae: 1.5847 - val_loss: 1.1117 - val_mae: 1.5985\n",
            "Epoch 112/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1197 - mae: 1.6024 - val_loss: 1.0999 - val_mae: 1.5761\n",
            "Epoch 113/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1075 - mae: 1.5907 - val_loss: 1.0971 - val_mae: 1.5894\n",
            "Epoch 114/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1033 - mae: 1.5855 - val_loss: 1.1310 - val_mae: 1.6294\n",
            "Epoch 115/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0993 - mae: 1.5823 - val_loss: 1.1365 - val_mae: 1.6450\n",
            "Epoch 116/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1017 - mae: 1.5840 - val_loss: 1.0938 - val_mae: 1.5660\n",
            "Epoch 117/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0944 - mae: 1.5797 - val_loss: 1.1281 - val_mae: 1.6018\n",
            "Epoch 118/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1211 - mae: 1.6065 - val_loss: 1.1287 - val_mae: 1.6356\n",
            "Epoch 119/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1090 - mae: 1.5905 - val_loss: 1.1339 - val_mae: 1.6341\n",
            "Epoch 120/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1069 - mae: 1.5873 - val_loss: 1.1030 - val_mae: 1.5948\n",
            "Epoch 121/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1034 - mae: 1.5841 - val_loss: 1.1250 - val_mae: 1.6301\n",
            "Epoch 122/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1008 - mae: 1.5825 - val_loss: 1.0943 - val_mae: 1.6002\n",
            "Epoch 123/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1048 - mae: 1.5880 - val_loss: 1.0815 - val_mae: 1.5771\n",
            "Epoch 124/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1016 - mae: 1.5832 - val_loss: 1.1131 - val_mae: 1.6156\n",
            "Epoch 125/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0850 - mae: 1.5687 - val_loss: 1.1098 - val_mae: 1.5985\n",
            "Epoch 126/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0950 - mae: 1.5776 - val_loss: 1.1083 - val_mae: 1.6082\n",
            "Epoch 127/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0964 - mae: 1.5785 - val_loss: 1.1145 - val_mae: 1.5950\n",
            "Epoch 128/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0923 - mae: 1.5712 - val_loss: 1.0870 - val_mae: 1.5581\n",
            "Epoch 129/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0949 - mae: 1.5763 - val_loss: 1.1311 - val_mae: 1.6297\n",
            "Epoch 130/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1026 - mae: 1.5843 - val_loss: 1.1247 - val_mae: 1.5984\n",
            "Epoch 131/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0930 - mae: 1.5754 - val_loss: 1.1015 - val_mae: 1.5759\n",
            "Epoch 132/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1097 - mae: 1.5938 - val_loss: 1.1411 - val_mae: 1.6485\n",
            "Epoch 133/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0920 - mae: 1.5754 - val_loss: 1.0998 - val_mae: 1.5912\n",
            "Epoch 134/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0699 - mae: 1.5528 - val_loss: 1.1314 - val_mae: 1.6287\n",
            "Epoch 135/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1150 - mae: 1.5965 - val_loss: 1.1023 - val_mae: 1.5946\n",
            "Epoch 136/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1068 - mae: 1.5900 - val_loss: 1.0894 - val_mae: 1.5852\n",
            "Epoch 137/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0824 - mae: 1.5687 - val_loss: 1.1021 - val_mae: 1.5888\n",
            "Epoch 138/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0889 - mae: 1.5702 - val_loss: 1.1181 - val_mae: 1.6085\n",
            "Epoch 139/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1093 - mae: 1.5943 - val_loss: 1.0893 - val_mae: 1.5799\n",
            "Epoch 140/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1080 - mae: 1.5896 - val_loss: 1.0965 - val_mae: 1.6008\n",
            "Epoch 141/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0893 - mae: 1.5737 - val_loss: 1.0999 - val_mae: 1.5811\n",
            "Epoch 142/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0951 - mae: 1.5721 - val_loss: 1.1216 - val_mae: 1.6174\n",
            "Epoch 143/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0878 - mae: 1.5667 - val_loss: 1.0850 - val_mae: 1.5876\n",
            "Epoch 144/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0839 - mae: 1.5626 - val_loss: 1.1107 - val_mae: 1.6117\n",
            "Epoch 145/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0873 - mae: 1.5679 - val_loss: 1.0725 - val_mae: 1.5402\n",
            "Epoch 146/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0876 - mae: 1.5647 - val_loss: 1.1125 - val_mae: 1.6141\n",
            "Epoch 147/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0915 - mae: 1.5690 - val_loss: 1.0997 - val_mae: 1.5881\n",
            "Epoch 148/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0938 - mae: 1.5791 - val_loss: 1.0923 - val_mae: 1.5790\n",
            "Epoch 149/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0925 - mae: 1.5744 - val_loss: 1.0956 - val_mae: 1.5817\n",
            "Epoch 150/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0960 - mae: 1.5816 - val_loss: 1.1322 - val_mae: 1.6119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:14:23,271] Trial 4 finished with value: 1.1321735382080078 and parameters: {'dropout_2': 0.3057080372068943, 'dropout_3': 0.4320531328329511, 'dropout_4': 0.7859195034295398, 'dropout_5': 0.8146202912492456, 'learning_rate': 0.00944365337727558, 'epochs': 150, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27/27 [==============================] - 2s 16ms/step - loss: 1.6387 - mae: 2.1931 - val_loss: 1.3462 - val_mae: 1.8091\n",
            "Epoch 2/50\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6366 - mae: 2.1913 - val_loss: 1.3410 - val_mae: 1.8153\n",
            "Epoch 3/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6128 - mae: 2.1635 - val_loss: 1.3392 - val_mae: 1.8221\n",
            "Epoch 4/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6217 - mae: 2.1740 - val_loss: 1.3393 - val_mae: 1.8252\n",
            "Epoch 5/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6114 - mae: 2.1629 - val_loss: 1.3412 - val_mae: 1.8321\n",
            "Epoch 6/50\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6174 - mae: 2.1676 - val_loss: 1.3443 - val_mae: 1.8417\n",
            "Epoch 7/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6207 - mae: 2.1740 - val_loss: 1.3454 - val_mae: 1.8455\n",
            "Epoch 8/50\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6226 - mae: 2.1774 - val_loss: 1.3482 - val_mae: 1.8519\n",
            "Epoch 9/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6045 - mae: 2.1576 - val_loss: 1.3491 - val_mae: 1.8545\n",
            "Epoch 10/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6165 - mae: 2.1721 - val_loss: 1.3487 - val_mae: 1.8550\n",
            "Epoch 11/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6107 - mae: 2.1612 - val_loss: 1.3487 - val_mae: 1.8552\n",
            "Epoch 12/50\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6377 - mae: 2.1926 - val_loss: 1.3492 - val_mae: 1.8572\n",
            "Epoch 13/50\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6408 - mae: 2.1967 - val_loss: 1.3483 - val_mae: 1.8569\n",
            "Epoch 14/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6335 - mae: 2.1834 - val_loss: 1.3475 - val_mae: 1.8564\n",
            "Epoch 15/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.5811 - mae: 2.1337 - val_loss: 1.3463 - val_mae: 1.8544\n",
            "Epoch 16/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6227 - mae: 2.1742 - val_loss: 1.3444 - val_mae: 1.8529\n",
            "Epoch 17/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6135 - mae: 2.1656 - val_loss: 1.3435 - val_mae: 1.8522\n",
            "Epoch 18/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6100 - mae: 2.1612 - val_loss: 1.3448 - val_mae: 1.8546\n",
            "Epoch 19/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6389 - mae: 2.1933 - val_loss: 1.3439 - val_mae: 1.8537\n",
            "Epoch 20/50\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6034 - mae: 2.1569 - val_loss: 1.3430 - val_mae: 1.8531\n",
            "Epoch 21/50\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 1.6341 - mae: 2.1905 - val_loss: 1.3422 - val_mae: 1.8527\n",
            "Epoch 22/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6094 - mae: 2.1615 - val_loss: 1.3416 - val_mae: 1.8528\n",
            "Epoch 23/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.5863 - mae: 2.1402 - val_loss: 1.3420 - val_mae: 1.8542\n",
            "Epoch 24/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6258 - mae: 2.1789 - val_loss: 1.3404 - val_mae: 1.8505\n",
            "Epoch 25/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.5876 - mae: 2.1377 - val_loss: 1.3392 - val_mae: 1.8494\n",
            "Epoch 26/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6285 - mae: 2.1795 - val_loss: 1.3389 - val_mae: 1.8496\n",
            "Epoch 27/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6269 - mae: 2.1796 - val_loss: 1.3384 - val_mae: 1.8503\n",
            "Epoch 28/50\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6003 - mae: 2.1522 - val_loss: 1.3368 - val_mae: 1.8471\n",
            "Epoch 29/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6325 - mae: 2.1906 - val_loss: 1.3360 - val_mae: 1.8458\n",
            "Epoch 30/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6254 - mae: 2.1771 - val_loss: 1.3363 - val_mae: 1.8470\n",
            "Epoch 31/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.5941 - mae: 2.1444 - val_loss: 1.3356 - val_mae: 1.8468\n",
            "Epoch 32/50\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6359 - mae: 2.1883 - val_loss: 1.3363 - val_mae: 1.8469\n",
            "Epoch 33/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6236 - mae: 2.1783 - val_loss: 1.3363 - val_mae: 1.8463\n",
            "Epoch 34/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6065 - mae: 2.1607 - val_loss: 1.3363 - val_mae: 1.8464\n",
            "Epoch 35/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6252 - mae: 2.1784 - val_loss: 1.3365 - val_mae: 1.8474\n",
            "Epoch 36/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6212 - mae: 2.1769 - val_loss: 1.3365 - val_mae: 1.8477\n",
            "Epoch 37/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6072 - mae: 2.1563 - val_loss: 1.3371 - val_mae: 1.8480\n",
            "Epoch 38/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6026 - mae: 2.1540 - val_loss: 1.3367 - val_mae: 1.8462\n",
            "Epoch 39/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6121 - mae: 2.1644 - val_loss: 1.3353 - val_mae: 1.8447\n",
            "Epoch 40/50\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6271 - mae: 2.1817 - val_loss: 1.3360 - val_mae: 1.8453\n",
            "Epoch 41/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6303 - mae: 2.1834 - val_loss: 1.3361 - val_mae: 1.8458\n",
            "Epoch 42/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6211 - mae: 2.1745 - val_loss: 1.3360 - val_mae: 1.8463\n",
            "Epoch 43/50\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6384 - mae: 2.1926 - val_loss: 1.3360 - val_mae: 1.8464\n",
            "Epoch 44/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6247 - mae: 2.1802 - val_loss: 1.3358 - val_mae: 1.8462\n",
            "Epoch 45/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6176 - mae: 2.1750 - val_loss: 1.3373 - val_mae: 1.8488\n",
            "Epoch 46/50\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.6054 - mae: 2.1573 - val_loss: 1.3371 - val_mae: 1.8478\n",
            "Epoch 47/50\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5749 - mae: 2.1269 - val_loss: 1.3354 - val_mae: 1.8447\n",
            "Epoch 48/50\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5829 - mae: 2.1323 - val_loss: 1.3348 - val_mae: 1.8440\n",
            "Epoch 49/50\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.6153 - mae: 2.1694 - val_loss: 1.3362 - val_mae: 1.8459\n",
            "Epoch 50/50\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.5954 - mae: 2.1491 - val_loss: 1.3368 - val_mae: 1.8467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:14:35,028] Trial 5 finished with value: 1.3367536067962646 and parameters: {'dropout_2': 0.3115901401618232, 'dropout_3': 0.8532798303853334, 'dropout_4': 0.6719186186168714, 'dropout_5': 0.14746943735601176, 'learning_rate': 2.3577244010605868e-06, 'epochs': 50, 'batch_size': 128}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54/54 [==============================] - 3s 11ms/step - loss: 1.4884 - mae: 2.0252 - val_loss: 1.3232 - val_mae: 1.7823\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.3422 - mae: 1.8496 - val_loss: 1.2932 - val_mae: 1.7655\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2447 - mae: 1.7363 - val_loss: 1.2642 - val_mae: 1.7524\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2153 - mae: 1.7033 - val_loss: 1.2320 - val_mae: 1.6974\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1774 - mae: 1.6574 - val_loss: 1.1895 - val_mae: 1.6677\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1752 - mae: 1.6538 - val_loss: 1.1816 - val_mae: 1.6596\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1503 - mae: 1.6341 - val_loss: 1.1645 - val_mae: 1.6600\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1419 - mae: 1.6294 - val_loss: 1.1694 - val_mae: 1.6553\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1247 - mae: 1.6072 - val_loss: 1.1878 - val_mae: 1.6985\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1380 - mae: 1.6276 - val_loss: 1.1728 - val_mae: 1.6656\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1376 - mae: 1.6306 - val_loss: 1.1905 - val_mae: 1.6749\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1290 - mae: 1.6150 - val_loss: 1.1832 - val_mae: 1.6858\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1286 - mae: 1.6217 - val_loss: 1.1652 - val_mae: 1.6531\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1090 - mae: 1.5996 - val_loss: 1.1621 - val_mae: 1.6722\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1262 - mae: 1.6158 - val_loss: 1.1601 - val_mae: 1.6393\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1072 - mae: 1.5914 - val_loss: 1.1726 - val_mae: 1.6565\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1080 - mae: 1.5924 - val_loss: 1.1610 - val_mae: 1.6513\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1131 - mae: 1.6045 - val_loss: 1.1791 - val_mae: 1.6695\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1093 - mae: 1.5961 - val_loss: 1.1748 - val_mae: 1.6603\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1085 - mae: 1.5954 - val_loss: 1.1510 - val_mae: 1.6408\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1018 - mae: 1.5863 - val_loss: 1.1375 - val_mae: 1.6378\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1098 - mae: 1.5977 - val_loss: 1.1589 - val_mae: 1.6613\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0963 - mae: 1.5828 - val_loss: 1.1796 - val_mae: 1.6670\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1073 - mae: 1.5912 - val_loss: 1.1667 - val_mae: 1.6531\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1133 - mae: 1.5944 - val_loss: 1.1868 - val_mae: 1.6819\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0902 - mae: 1.5732 - val_loss: 1.1590 - val_mae: 1.6499\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0914 - mae: 1.5801 - val_loss: 1.1526 - val_mae: 1.6358\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0799 - mae: 1.5654 - val_loss: 1.1559 - val_mae: 1.6475\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0987 - mae: 1.5845 - val_loss: 1.1431 - val_mae: 1.6240\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0958 - mae: 1.5797 - val_loss: 1.1349 - val_mae: 1.6240\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0944 - mae: 1.5804 - val_loss: 1.1159 - val_mae: 1.6001\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0961 - mae: 1.5807 - val_loss: 1.1361 - val_mae: 1.6299\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.1054 - mae: 1.5904 - val_loss: 1.1493 - val_mae: 1.6422\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.1035 - mae: 1.5897 - val_loss: 1.1276 - val_mae: 1.6147\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0881 - mae: 1.5747 - val_loss: 1.1463 - val_mae: 1.6295\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0898 - mae: 1.5733 - val_loss: 1.1469 - val_mae: 1.6294\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.1015 - mae: 1.5846 - val_loss: 1.1567 - val_mae: 1.6494\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0827 - mae: 1.5682 - val_loss: 1.1484 - val_mae: 1.6363\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0917 - mae: 1.5757 - val_loss: 1.1329 - val_mae: 1.6164\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0665 - mae: 1.5509 - val_loss: 1.1365 - val_mae: 1.6311\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0762 - mae: 1.5577 - val_loss: 1.1429 - val_mae: 1.6351\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0918 - mae: 1.5766 - val_loss: 1.1433 - val_mae: 1.6390\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0755 - mae: 1.5613 - val_loss: 1.1595 - val_mae: 1.6452\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0744 - mae: 1.5548 - val_loss: 1.1362 - val_mae: 1.6236\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0903 - mae: 1.5715 - val_loss: 1.1301 - val_mae: 1.6154\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0735 - mae: 1.5571 - val_loss: 1.1317 - val_mae: 1.6152\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0868 - mae: 1.5727 - val_loss: 1.1193 - val_mae: 1.6046\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0769 - mae: 1.5610 - val_loss: 1.1451 - val_mae: 1.6267\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0796 - mae: 1.5563 - val_loss: 1.1312 - val_mae: 1.6182\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0784 - mae: 1.5569 - val_loss: 1.1356 - val_mae: 1.6300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:14:54,583] Trial 6 finished with value: 1.1355676651000977 and parameters: {'dropout_2': 0.8346962059898396, 'dropout_3': 0.5655195934399094, 'dropout_4': 0.6774368333331884, 'dropout_5': 0.2909050876733682, 'learning_rate': 0.00466171077821025, 'epochs': 50, 'batch_size': 64}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "14/14 [==============================] - 2s 30ms/step - loss: 1.3241 - mae: 1.8189 - val_loss: 4.4744 - val_mae: 5.1097\n",
            "Epoch 2/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.1869 - mae: 1.6657 - val_loss: 2.9780 - val_mae: 3.5176\n",
            "Epoch 3/100\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1731 - mae: 1.6487 - val_loss: 1.5988 - val_mae: 2.1498\n",
            "Epoch 4/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.1583 - mae: 1.6352 - val_loss: 2.2425 - val_mae: 2.8042\n",
            "Epoch 5/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.1616 - mae: 1.6455 - val_loss: 1.5430 - val_mae: 2.0841\n",
            "Epoch 6/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.1376 - mae: 1.6214 - val_loss: 1.4270 - val_mae: 1.9396\n",
            "Epoch 7/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.1226 - mae: 1.6042 - val_loss: 1.1954 - val_mae: 1.6968\n",
            "Epoch 8/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1194 - mae: 1.6000 - val_loss: 1.3603 - val_mae: 1.8925\n",
            "Epoch 9/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1324 - mae: 1.6164 - val_loss: 1.1775 - val_mae: 1.6486\n",
            "Epoch 10/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1347 - mae: 1.6130 - val_loss: 1.6144 - val_mae: 2.1420\n",
            "Epoch 11/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.1365 - mae: 1.6173 - val_loss: 1.2004 - val_mae: 1.7246\n",
            "Epoch 12/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.1285 - mae: 1.6104 - val_loss: 1.1395 - val_mae: 1.6516\n",
            "Epoch 13/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1171 - mae: 1.5929 - val_loss: 1.1747 - val_mae: 1.6907\n",
            "Epoch 14/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1188 - mae: 1.6029 - val_loss: 1.1733 - val_mae: 1.7084\n",
            "Epoch 15/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1117 - mae: 1.5937 - val_loss: 1.1309 - val_mae: 1.5984\n",
            "Epoch 16/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0919 - mae: 1.5672 - val_loss: 1.1284 - val_mae: 1.6225\n",
            "Epoch 17/100\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1001 - mae: 1.5815 - val_loss: 1.1243 - val_mae: 1.6333\n",
            "Epoch 18/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.1063 - mae: 1.5878 - val_loss: 1.2177 - val_mae: 1.6984\n",
            "Epoch 19/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0915 - mae: 1.5707 - val_loss: 1.1704 - val_mae: 1.6598\n",
            "Epoch 20/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1162 - mae: 1.5993 - val_loss: 1.1648 - val_mae: 1.6632\n",
            "Epoch 21/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.1118 - mae: 1.5937 - val_loss: 1.1644 - val_mae: 1.6497\n",
            "Epoch 22/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.1127 - mae: 1.5947 - val_loss: 1.1262 - val_mae: 1.6504\n",
            "Epoch 23/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0968 - mae: 1.5823 - val_loss: 1.1283 - val_mae: 1.6426\n",
            "Epoch 24/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0933 - mae: 1.5721 - val_loss: 1.1014 - val_mae: 1.5986\n",
            "Epoch 25/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0876 - mae: 1.5693 - val_loss: 1.1472 - val_mae: 1.6671\n",
            "Epoch 26/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0910 - mae: 1.5730 - val_loss: 1.1000 - val_mae: 1.5742\n",
            "Epoch 27/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0904 - mae: 1.5739 - val_loss: 1.1219 - val_mae: 1.5779\n",
            "Epoch 28/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0893 - mae: 1.5719 - val_loss: 1.1074 - val_mae: 1.6161\n",
            "Epoch 29/100\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.1084 - mae: 1.5938 - val_loss: 1.1333 - val_mae: 1.6169\n",
            "Epoch 30/100\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.0915 - mae: 1.5771 - val_loss: 1.1331 - val_mae: 1.6544\n",
            "Epoch 31/100\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.0936 - mae: 1.5786 - val_loss: 1.1149 - val_mae: 1.6163\n",
            "Epoch 32/100\n",
            "14/14 [==============================] - 0s 21ms/step - loss: 1.0876 - mae: 1.5697 - val_loss: 1.0920 - val_mae: 1.5832\n",
            "Epoch 33/100\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.0864 - mae: 1.5713 - val_loss: 1.0914 - val_mae: 1.5798\n",
            "Epoch 34/100\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.0858 - mae: 1.5683 - val_loss: 1.0694 - val_mae: 1.5603\n",
            "Epoch 35/100\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 1.0785 - mae: 1.5642 - val_loss: 1.1011 - val_mae: 1.5994\n",
            "Epoch 36/100\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 1.0727 - mae: 1.5564 - val_loss: 1.0963 - val_mae: 1.5988\n",
            "Epoch 37/100\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.0721 - mae: 1.5516 - val_loss: 1.0970 - val_mae: 1.5563\n",
            "Epoch 38/100\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.0862 - mae: 1.5732 - val_loss: 1.1076 - val_mae: 1.5664\n",
            "Epoch 39/100\n",
            "14/14 [==============================] - 0s 20ms/step - loss: 1.0891 - mae: 1.5696 - val_loss: 1.1402 - val_mae: 1.6271\n",
            "Epoch 40/100\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 1.0768 - mae: 1.5609 - val_loss: 1.1562 - val_mae: 1.6357\n",
            "Epoch 41/100\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.0786 - mae: 1.5619 - val_loss: 1.0951 - val_mae: 1.5968\n",
            "Epoch 42/100\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.0742 - mae: 1.5576 - val_loss: 1.1388 - val_mae: 1.6305\n",
            "Epoch 43/100\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 1.0847 - mae: 1.5670 - val_loss: 1.1230 - val_mae: 1.6373\n",
            "Epoch 44/100\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.0778 - mae: 1.5633 - val_loss: 1.1306 - val_mae: 1.6468\n",
            "Epoch 45/100\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 1.0701 - mae: 1.5552 - val_loss: 1.1306 - val_mae: 1.5827\n",
            "Epoch 46/100\n",
            "14/14 [==============================] - 0s 21ms/step - loss: 1.0726 - mae: 1.5518 - val_loss: 1.1527 - val_mae: 1.6429\n",
            "Epoch 47/100\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 1.0728 - mae: 1.5525 - val_loss: 1.1275 - val_mae: 1.5970\n",
            "Epoch 48/100\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.0743 - mae: 1.5565 - val_loss: 1.0811 - val_mae: 1.5410\n",
            "Epoch 49/100\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 1.0539 - mae: 1.5324 - val_loss: 1.1480 - val_mae: 1.6486\n",
            "Epoch 50/100\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.0876 - mae: 1.5731 - val_loss: 1.1859 - val_mae: 1.6647\n",
            "Epoch 51/100\n",
            "14/14 [==============================] - 0s 22ms/step - loss: 1.0741 - mae: 1.5574 - val_loss: 1.0921 - val_mae: 1.6033\n",
            "Epoch 52/100\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 1.0720 - mae: 1.5572 - val_loss: 1.0756 - val_mae: 1.5680\n",
            "Epoch 53/100\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.0818 - mae: 1.5667 - val_loss: 1.0888 - val_mae: 1.5823\n",
            "Epoch 54/100\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.0782 - mae: 1.5564 - val_loss: 1.0782 - val_mae: 1.5689\n",
            "Epoch 55/100\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0723 - mae: 1.5550 - val_loss: 1.1172 - val_mae: 1.5808\n",
            "Epoch 56/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0703 - mae: 1.5503 - val_loss: 1.1046 - val_mae: 1.6008\n",
            "Epoch 57/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0859 - mae: 1.5669 - val_loss: 1.1282 - val_mae: 1.6073\n",
            "Epoch 58/100\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0597 - mae: 1.5396 - val_loss: 1.0886 - val_mae: 1.6014\n",
            "Epoch 59/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0872 - mae: 1.5709 - val_loss: 1.1593 - val_mae: 1.6210\n",
            "Epoch 60/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0692 - mae: 1.5516 - val_loss: 1.0981 - val_mae: 1.5801\n",
            "Epoch 61/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0633 - mae: 1.5444 - val_loss: 1.1340 - val_mae: 1.6358\n",
            "Epoch 62/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0579 - mae: 1.5382 - val_loss: 1.1460 - val_mae: 1.6632\n",
            "Epoch 63/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0641 - mae: 1.5481 - val_loss: 1.0746 - val_mae: 1.5818\n",
            "Epoch 64/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0636 - mae: 1.5482 - val_loss: 1.0743 - val_mae: 1.5601\n",
            "Epoch 65/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0587 - mae: 1.5432 - val_loss: 1.0853 - val_mae: 1.5940\n",
            "Epoch 66/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0533 - mae: 1.5407 - val_loss: 1.0724 - val_mae: 1.5641\n",
            "Epoch 67/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0337 - mae: 1.5143 - val_loss: 1.0754 - val_mae: 1.5840\n",
            "Epoch 68/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0608 - mae: 1.5455 - val_loss: 1.1051 - val_mae: 1.5722\n",
            "Epoch 69/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0591 - mae: 1.5448 - val_loss: 1.1246 - val_mae: 1.6209\n",
            "Epoch 70/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0484 - mae: 1.5339 - val_loss: 1.1120 - val_mae: 1.5713\n",
            "Epoch 71/100\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0631 - mae: 1.5443 - val_loss: 1.0358 - val_mae: 1.5271\n",
            "Epoch 72/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0580 - mae: 1.5420 - val_loss: 1.0768 - val_mae: 1.5519\n",
            "Epoch 73/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0587 - mae: 1.5419 - val_loss: 1.1290 - val_mae: 1.6181\n",
            "Epoch 74/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0796 - mae: 1.5633 - val_loss: 1.0939 - val_mae: 1.5860\n",
            "Epoch 75/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0537 - mae: 1.5395 - val_loss: 1.1117 - val_mae: 1.5958\n",
            "Epoch 76/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0507 - mae: 1.5348 - val_loss: 1.1532 - val_mae: 1.6120\n",
            "Epoch 77/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0608 - mae: 1.5413 - val_loss: 1.1470 - val_mae: 1.6597\n",
            "Epoch 78/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0575 - mae: 1.5409 - val_loss: 1.0761 - val_mae: 1.5362\n",
            "Epoch 79/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0643 - mae: 1.5419 - val_loss: 1.0766 - val_mae: 1.5879\n",
            "Epoch 80/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0670 - mae: 1.5526 - val_loss: 1.0930 - val_mae: 1.5706\n",
            "Epoch 81/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0595 - mae: 1.5465 - val_loss: 1.0492 - val_mae: 1.5402\n",
            "Epoch 82/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0649 - mae: 1.5501 - val_loss: 1.1159 - val_mae: 1.5851\n",
            "Epoch 83/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0446 - mae: 1.5282 - val_loss: 1.0875 - val_mae: 1.5947\n",
            "Epoch 84/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0554 - mae: 1.5408 - val_loss: 1.0592 - val_mae: 1.5595\n",
            "Epoch 85/100\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0557 - mae: 1.5418 - val_loss: 1.1079 - val_mae: 1.5963\n",
            "Epoch 86/100\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0467 - mae: 1.5334 - val_loss: 1.0490 - val_mae: 1.5256\n",
            "Epoch 87/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0516 - mae: 1.5361 - val_loss: 1.0639 - val_mae: 1.5366\n",
            "Epoch 88/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0446 - mae: 1.5302 - val_loss: 1.0539 - val_mae: 1.5438\n",
            "Epoch 89/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0431 - mae: 1.5268 - val_loss: 1.0388 - val_mae: 1.5337\n",
            "Epoch 90/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0609 - mae: 1.5517 - val_loss: 1.1631 - val_mae: 1.6931\n",
            "Epoch 91/100\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0556 - mae: 1.5427 - val_loss: 1.0569 - val_mae: 1.5376\n",
            "Epoch 92/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0654 - mae: 1.5537 - val_loss: 1.0978 - val_mae: 1.6050\n",
            "Epoch 93/100\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0595 - mae: 1.5453 - val_loss: 1.1033 - val_mae: 1.5928\n",
            "Epoch 94/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0565 - mae: 1.5396 - val_loss: 1.0441 - val_mae: 1.5535\n",
            "Epoch 95/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0551 - mae: 1.5425 - val_loss: 1.1189 - val_mae: 1.6384\n",
            "Epoch 96/100\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0569 - mae: 1.5411 - val_loss: 1.0559 - val_mae: 1.5700\n",
            "Epoch 97/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0464 - mae: 1.5339 - val_loss: 1.1213 - val_mae: 1.6430\n",
            "Epoch 98/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0411 - mae: 1.5290 - val_loss: 1.0676 - val_mae: 1.5588\n",
            "Epoch 99/100\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0445 - mae: 1.5282 - val_loss: 1.0442 - val_mae: 1.5376\n",
            "Epoch 100/100\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0604 - mae: 1.5449 - val_loss: 1.1049 - val_mae: 1.5981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:15:17,198] Trial 7 finished with value: 1.1048541069030762 and parameters: {'dropout_2': 0.34619264205547673, 'dropout_3': 0.45637032184773063, 'dropout_4': 0.6393921589504694, 'dropout_5': 0.6689420864253318, 'learning_rate': 0.04347149625757992, 'epochs': 100, 'batch_size': 256}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "108/108 [==============================] - 4s 10ms/step - loss: 1.3777 - mae: 1.8854 - val_loss: 1.3387 - val_mae: 1.8204\n",
            "Epoch 2/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3220 - mae: 1.8031 - val_loss: 1.3315 - val_mae: 1.8051\n",
            "Epoch 3/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3135 - mae: 1.7943 - val_loss: 1.3092 - val_mae: 1.7954\n",
            "Epoch 4/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2699 - mae: 1.7507 - val_loss: 1.2326 - val_mae: 1.7106\n",
            "Epoch 5/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2352 - mae: 1.7157 - val_loss: 1.2009 - val_mae: 1.6708\n",
            "Epoch 6/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2307 - mae: 1.7099 - val_loss: 1.2162 - val_mae: 1.6562\n",
            "Epoch 7/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2105 - mae: 1.6886 - val_loss: 1.2496 - val_mae: 1.7403\n",
            "Epoch 8/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2114 - mae: 1.6903 - val_loss: 1.2457 - val_mae: 1.7002\n",
            "Epoch 9/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2003 - mae: 1.6773 - val_loss: 1.2117 - val_mae: 1.6960\n",
            "Epoch 10/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1936 - mae: 1.6769 - val_loss: 1.2352 - val_mae: 1.7282\n",
            "Epoch 11/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2099 - mae: 1.6873 - val_loss: 1.2322 - val_mae: 1.7104\n",
            "Epoch 12/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1949 - mae: 1.6776 - val_loss: 1.2790 - val_mae: 1.7706\n",
            "Epoch 13/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1844 - mae: 1.6645 - val_loss: 1.2456 - val_mae: 1.6941\n",
            "Epoch 14/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1900 - mae: 1.6699 - val_loss: 1.2308 - val_mae: 1.7098\n",
            "Epoch 15/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1952 - mae: 1.6774 - val_loss: 1.2194 - val_mae: 1.7331\n",
            "Epoch 16/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1760 - mae: 1.6573 - val_loss: 1.2429 - val_mae: 1.7015\n",
            "Epoch 17/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1789 - mae: 1.6617 - val_loss: 1.2554 - val_mae: 1.7429\n",
            "Epoch 18/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1503 - mae: 1.6293 - val_loss: 1.2304 - val_mae: 1.6863\n",
            "Epoch 19/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1786 - mae: 1.6597 - val_loss: 1.2445 - val_mae: 1.7028\n",
            "Epoch 20/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1616 - mae: 1.6404 - val_loss: 1.2115 - val_mae: 1.6809\n",
            "Epoch 21/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1653 - mae: 1.6523 - val_loss: 1.2193 - val_mae: 1.7022\n",
            "Epoch 22/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1702 - mae: 1.6513 - val_loss: 1.1683 - val_mae: 1.6531\n",
            "Epoch 23/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1663 - mae: 1.6471 - val_loss: 1.2193 - val_mae: 1.6703\n",
            "Epoch 24/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1561 - mae: 1.6364 - val_loss: 1.1959 - val_mae: 1.6892\n",
            "Epoch 25/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1662 - mae: 1.6440 - val_loss: 1.2079 - val_mae: 1.6743\n",
            "Epoch 26/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1588 - mae: 1.6382 - val_loss: 1.1956 - val_mae: 1.6863\n",
            "Epoch 27/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1594 - mae: 1.6412 - val_loss: 1.1903 - val_mae: 1.6670\n",
            "Epoch 28/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1398 - mae: 1.6189 - val_loss: 1.2236 - val_mae: 1.6938\n",
            "Epoch 29/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1649 - mae: 1.6427 - val_loss: 1.2571 - val_mae: 1.7384\n",
            "Epoch 30/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1472 - mae: 1.6237 - val_loss: 1.2317 - val_mae: 1.7554\n",
            "Epoch 31/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1611 - mae: 1.6415 - val_loss: 1.1974 - val_mae: 1.6635\n",
            "Epoch 32/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1540 - mae: 1.6343 - val_loss: 1.1976 - val_mae: 1.6951\n",
            "Epoch 33/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1682 - mae: 1.6468 - val_loss: 1.2225 - val_mae: 1.7331\n",
            "Epoch 34/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1411 - mae: 1.6220 - val_loss: 1.1998 - val_mae: 1.6479\n",
            "Epoch 35/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1540 - mae: 1.6330 - val_loss: 1.2321 - val_mae: 1.7059\n",
            "Epoch 36/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1631 - mae: 1.6446 - val_loss: 1.2029 - val_mae: 1.6668\n",
            "Epoch 37/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1527 - mae: 1.6335 - val_loss: 1.2261 - val_mae: 1.7378\n",
            "Epoch 38/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1479 - mae: 1.6311 - val_loss: 1.1704 - val_mae: 1.6566\n",
            "Epoch 39/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1392 - mae: 1.6191 - val_loss: 1.2008 - val_mae: 1.6433\n",
            "Epoch 40/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1583 - mae: 1.6398 - val_loss: 1.1997 - val_mae: 1.6843\n",
            "Epoch 41/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1406 - mae: 1.6224 - val_loss: 1.2303 - val_mae: 1.7062\n",
            "Epoch 42/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1443 - mae: 1.6274 - val_loss: 1.2262 - val_mae: 1.6819\n",
            "Epoch 43/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1447 - mae: 1.6225 - val_loss: 1.2277 - val_mae: 1.7133\n",
            "Epoch 44/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1408 - mae: 1.6223 - val_loss: 1.2320 - val_mae: 1.6911\n",
            "Epoch 45/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1483 - mae: 1.6309 - val_loss: 1.2198 - val_mae: 1.7012\n",
            "Epoch 46/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1449 - mae: 1.6225 - val_loss: 1.2136 - val_mae: 1.7199\n",
            "Epoch 47/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1533 - mae: 1.6367 - val_loss: 1.2025 - val_mae: 1.6954\n",
            "Epoch 48/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1470 - mae: 1.6268 - val_loss: 1.2225 - val_mae: 1.6778\n",
            "Epoch 49/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1384 - mae: 1.6220 - val_loss: 1.2449 - val_mae: 1.7253\n",
            "Epoch 50/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1376 - mae: 1.6158 - val_loss: 1.2365 - val_mae: 1.7344\n",
            "Epoch 51/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1423 - mae: 1.6255 - val_loss: 1.2066 - val_mae: 1.6876\n",
            "Epoch 52/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1319 - mae: 1.6159 - val_loss: 1.2360 - val_mae: 1.7319\n",
            "Epoch 53/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1339 - mae: 1.6176 - val_loss: 1.1956 - val_mae: 1.6455\n",
            "Epoch 54/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1417 - mae: 1.6178 - val_loss: 1.2127 - val_mae: 1.6697\n",
            "Epoch 55/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1286 - mae: 1.6073 - val_loss: 1.2128 - val_mae: 1.7051\n",
            "Epoch 56/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1367 - mae: 1.6188 - val_loss: 1.2211 - val_mae: 1.7137\n",
            "Epoch 57/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1432 - mae: 1.6221 - val_loss: 1.2197 - val_mae: 1.6777\n",
            "Epoch 58/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1270 - mae: 1.6072 - val_loss: 1.2170 - val_mae: 1.6728\n",
            "Epoch 59/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1452 - mae: 1.6266 - val_loss: 1.1937 - val_mae: 1.6702\n",
            "Epoch 60/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1289 - mae: 1.6072 - val_loss: 1.2158 - val_mae: 1.6782\n",
            "Epoch 61/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1108 - mae: 1.5924 - val_loss: 1.1873 - val_mae: 1.6684\n",
            "Epoch 62/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1214 - mae: 1.6018 - val_loss: 1.2145 - val_mae: 1.6943\n",
            "Epoch 63/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1340 - mae: 1.6155 - val_loss: 1.2339 - val_mae: 1.6768\n",
            "Epoch 64/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1344 - mae: 1.6134 - val_loss: 1.1864 - val_mae: 1.6322\n",
            "Epoch 65/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1267 - mae: 1.6079 - val_loss: 1.2021 - val_mae: 1.6726\n",
            "Epoch 66/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1256 - mae: 1.6065 - val_loss: 1.1728 - val_mae: 1.6341\n",
            "Epoch 67/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1367 - mae: 1.6229 - val_loss: 1.2056 - val_mae: 1.7015\n",
            "Epoch 68/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1397 - mae: 1.6199 - val_loss: 1.2243 - val_mae: 1.7229\n",
            "Epoch 69/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1558 - mae: 1.6365 - val_loss: 1.2131 - val_mae: 1.7030\n",
            "Epoch 70/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1340 - mae: 1.6119 - val_loss: 1.2036 - val_mae: 1.6599\n",
            "Epoch 71/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1370 - mae: 1.6130 - val_loss: 1.1831 - val_mae: 1.6479\n",
            "Epoch 72/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1445 - mae: 1.6266 - val_loss: 1.1737 - val_mae: 1.6457\n",
            "Epoch 73/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1310 - mae: 1.6125 - val_loss: 1.2080 - val_mae: 1.7085\n",
            "Epoch 74/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1371 - mae: 1.6182 - val_loss: 1.2252 - val_mae: 1.6957\n",
            "Epoch 75/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1437 - mae: 1.6242 - val_loss: 1.2027 - val_mae: 1.6873\n",
            "Epoch 76/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1354 - mae: 1.6147 - val_loss: 1.1891 - val_mae: 1.6318\n",
            "Epoch 77/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1488 - mae: 1.6296 - val_loss: 1.1960 - val_mae: 1.6540\n",
            "Epoch 78/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1307 - mae: 1.6110 - val_loss: 1.2145 - val_mae: 1.6817\n",
            "Epoch 79/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1254 - mae: 1.6037 - val_loss: 1.2031 - val_mae: 1.6619\n",
            "Epoch 80/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1298 - mae: 1.6094 - val_loss: 1.2552 - val_mae: 1.7606\n",
            "Epoch 81/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1328 - mae: 1.6111 - val_loss: 1.2160 - val_mae: 1.6915\n",
            "Epoch 82/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1377 - mae: 1.6186 - val_loss: 1.2182 - val_mae: 1.6664\n",
            "Epoch 83/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1130 - mae: 1.5881 - val_loss: 1.1939 - val_mae: 1.6831\n",
            "Epoch 84/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1261 - mae: 1.6088 - val_loss: 1.2211 - val_mae: 1.6741\n",
            "Epoch 85/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1444 - mae: 1.6283 - val_loss: 1.2383 - val_mae: 1.7056\n",
            "Epoch 86/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1273 - mae: 1.6050 - val_loss: 1.2018 - val_mae: 1.6703\n",
            "Epoch 87/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1393 - mae: 1.6194 - val_loss: 1.2225 - val_mae: 1.6920\n",
            "Epoch 88/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1148 - mae: 1.5967 - val_loss: 1.1801 - val_mae: 1.6212\n",
            "Epoch 89/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1203 - mae: 1.5989 - val_loss: 1.2338 - val_mae: 1.7111\n",
            "Epoch 90/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1185 - mae: 1.5958 - val_loss: 1.1978 - val_mae: 1.6689\n",
            "Epoch 91/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1314 - mae: 1.6058 - val_loss: 1.2165 - val_mae: 1.7062\n",
            "Epoch 92/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1249 - mae: 1.6048 - val_loss: 1.2382 - val_mae: 1.6830\n",
            "Epoch 93/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1328 - mae: 1.6088 - val_loss: 1.2046 - val_mae: 1.6578\n",
            "Epoch 94/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1362 - mae: 1.6186 - val_loss: 1.1947 - val_mae: 1.6976\n",
            "Epoch 95/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1351 - mae: 1.6174 - val_loss: 1.2099 - val_mae: 1.7050\n",
            "Epoch 96/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1268 - mae: 1.6105 - val_loss: 1.1754 - val_mae: 1.6725\n",
            "Epoch 97/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1245 - mae: 1.6088 - val_loss: 1.2031 - val_mae: 1.6485\n",
            "Epoch 98/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1376 - mae: 1.6193 - val_loss: 1.2142 - val_mae: 1.6926\n",
            "Epoch 99/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1365 - mae: 1.6213 - val_loss: 1.2040 - val_mae: 1.6896\n",
            "Epoch 100/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1238 - mae: 1.6074 - val_loss: 1.1943 - val_mae: 1.6711\n",
            "Epoch 101/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1324 - mae: 1.6114 - val_loss: 1.2161 - val_mae: 1.7371\n",
            "Epoch 102/150\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.1249 - mae: 1.6081 - val_loss: 1.2027 - val_mae: 1.6551\n",
            "Epoch 103/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1244 - mae: 1.5995 - val_loss: 1.1901 - val_mae: 1.6702\n",
            "Epoch 104/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1238 - mae: 1.6005 - val_loss: 1.1987 - val_mae: 1.6789\n",
            "Epoch 105/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1292 - mae: 1.6085 - val_loss: 1.2228 - val_mae: 1.6895\n",
            "Epoch 106/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1250 - mae: 1.6033 - val_loss: 1.2225 - val_mae: 1.6919\n",
            "Epoch 107/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1211 - mae: 1.6016 - val_loss: 1.1960 - val_mae: 1.6859\n",
            "Epoch 108/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1210 - mae: 1.6034 - val_loss: 1.2012 - val_mae: 1.6587\n",
            "Epoch 109/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1207 - mae: 1.5993 - val_loss: 1.1825 - val_mae: 1.6563\n",
            "Epoch 110/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1334 - mae: 1.6137 - val_loss: 1.2068 - val_mae: 1.6854\n",
            "Epoch 111/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1305 - mae: 1.6146 - val_loss: 1.2146 - val_mae: 1.6594\n",
            "Epoch 112/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1282 - mae: 1.6057 - val_loss: 1.1954 - val_mae: 1.6584\n",
            "Epoch 113/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1384 - mae: 1.6205 - val_loss: 1.2216 - val_mae: 1.6909\n",
            "Epoch 114/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1401 - mae: 1.6223 - val_loss: 1.2176 - val_mae: 1.6820\n",
            "Epoch 115/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1305 - mae: 1.6089 - val_loss: 1.2040 - val_mae: 1.6993\n",
            "Epoch 116/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1197 - mae: 1.6015 - val_loss: 1.2008 - val_mae: 1.6467\n",
            "Epoch 117/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1312 - mae: 1.6107 - val_loss: 1.2057 - val_mae: 1.6751\n",
            "Epoch 118/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1231 - mae: 1.6022 - val_loss: 1.1915 - val_mae: 1.6645\n",
            "Epoch 119/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1286 - mae: 1.6088 - val_loss: 1.2339 - val_mae: 1.6776\n",
            "Epoch 120/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1301 - mae: 1.6054 - val_loss: 1.1851 - val_mae: 1.6574\n",
            "Epoch 121/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1245 - mae: 1.6040 - val_loss: 1.2070 - val_mae: 1.6785\n",
            "Epoch 122/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1249 - mae: 1.6021 - val_loss: 1.1884 - val_mae: 1.6774\n",
            "Epoch 123/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1261 - mae: 1.6068 - val_loss: 1.1954 - val_mae: 1.6895\n",
            "Epoch 124/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1201 - mae: 1.6027 - val_loss: 1.2136 - val_mae: 1.6768\n",
            "Epoch 125/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1258 - mae: 1.6074 - val_loss: 1.1814 - val_mae: 1.6894\n",
            "Epoch 126/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1352 - mae: 1.6165 - val_loss: 1.1871 - val_mae: 1.6643\n",
            "Epoch 127/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1516 - mae: 1.6321 - val_loss: 1.1754 - val_mae: 1.6599\n",
            "Epoch 128/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1223 - mae: 1.6040 - val_loss: 1.1722 - val_mae: 1.6500\n",
            "Epoch 129/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1276 - mae: 1.6095 - val_loss: 1.1828 - val_mae: 1.6274\n",
            "Epoch 130/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1364 - mae: 1.6147 - val_loss: 1.2357 - val_mae: 1.7081\n",
            "Epoch 131/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1289 - mae: 1.6095 - val_loss: 1.1978 - val_mae: 1.6844\n",
            "Epoch 132/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1094 - mae: 1.5895 - val_loss: 1.1650 - val_mae: 1.6346\n",
            "Epoch 133/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1245 - mae: 1.6054 - val_loss: 1.1902 - val_mae: 1.6519\n",
            "Epoch 134/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1379 - mae: 1.6160 - val_loss: 1.1971 - val_mae: 1.6438\n",
            "Epoch 135/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1095 - mae: 1.5874 - val_loss: 1.2182 - val_mae: 1.6933\n",
            "Epoch 136/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1354 - mae: 1.6107 - val_loss: 1.2016 - val_mae: 1.6599\n",
            "Epoch 137/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1211 - mae: 1.5995 - val_loss: 1.1909 - val_mae: 1.6333\n",
            "Epoch 138/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1132 - mae: 1.5954 - val_loss: 1.2056 - val_mae: 1.6580\n",
            "Epoch 139/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1477 - mae: 1.6249 - val_loss: 1.1811 - val_mae: 1.6327\n",
            "Epoch 140/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1171 - mae: 1.5944 - val_loss: 1.1919 - val_mae: 1.6828\n",
            "Epoch 141/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1486 - mae: 1.6308 - val_loss: 1.1997 - val_mae: 1.6682\n",
            "Epoch 142/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1139 - mae: 1.5929 - val_loss: 1.2047 - val_mae: 1.6496\n",
            "Epoch 143/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1235 - mae: 1.6057 - val_loss: 1.1947 - val_mae: 1.6419\n",
            "Epoch 144/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1386 - mae: 1.6163 - val_loss: 1.2051 - val_mae: 1.7143\n",
            "Epoch 145/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1334 - mae: 1.6171 - val_loss: 1.2127 - val_mae: 1.6984\n",
            "Epoch 146/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1176 - mae: 1.5977 - val_loss: 1.2015 - val_mae: 1.6969\n",
            "Epoch 147/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1174 - mae: 1.5991 - val_loss: 1.1919 - val_mae: 1.6762\n",
            "Epoch 148/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1339 - mae: 1.6144 - val_loss: 1.1968 - val_mae: 1.6437\n",
            "Epoch 149/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1304 - mae: 1.6050 - val_loss: 1.2202 - val_mae: 1.7170\n",
            "Epoch 150/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1183 - mae: 1.5972 - val_loss: 1.2140 - val_mae: 1.7026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:16:40,029] Trial 8 finished with value: 1.21396005153656 and parameters: {'dropout_2': 0.8032493320042025, 'dropout_3': 0.860697449113168, 'dropout_4': 0.8516654732930746, 'dropout_5': 0.30475098957748425, 'learning_rate': 0.01204680472341688, 'epochs': 150, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "54/54 [==============================] - 3s 15ms/step - loss: 1.5790 - mae: 2.1320 - val_loss: 1.3513 - val_mae: 1.8446\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.4486 - mae: 1.9886 - val_loss: 1.3546 - val_mae: 1.8597\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.3635 - mae: 1.8903 - val_loss: 1.3382 - val_mae: 1.8390\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2923 - mae: 1.8127 - val_loss: 1.3229 - val_mae: 1.8273\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2553 - mae: 1.7654 - val_loss: 1.2996 - val_mae: 1.8109\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2173 - mae: 1.7218 - val_loss: 1.2810 - val_mae: 1.7991\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2007 - mae: 1.7048 - val_loss: 1.2680 - val_mae: 1.7848\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 0s 4ms/step - loss: 1.1725 - mae: 1.6705 - val_loss: 1.2299 - val_mae: 1.7464\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1631 - mae: 1.6602 - val_loss: 1.2180 - val_mae: 1.7325\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1516 - mae: 1.6455 - val_loss: 1.1977 - val_mae: 1.7119\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1257 - mae: 1.6167 - val_loss: 1.1832 - val_mae: 1.6935\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1270 - mae: 1.6188 - val_loss: 1.1791 - val_mae: 1.6856\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1282 - mae: 1.6200 - val_loss: 1.1758 - val_mae: 1.6826\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1136 - mae: 1.6086 - val_loss: 1.1399 - val_mae: 1.6421\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0999 - mae: 1.5906 - val_loss: 1.1385 - val_mae: 1.6406\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1013 - mae: 1.5918 - val_loss: 1.1479 - val_mae: 1.6501\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0919 - mae: 1.5836 - val_loss: 1.1518 - val_mae: 1.6554\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0816 - mae: 1.5681 - val_loss: 1.1319 - val_mae: 1.6344\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0878 - mae: 1.5761 - val_loss: 1.1352 - val_mae: 1.6370\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0805 - mae: 1.5690 - val_loss: 1.1438 - val_mae: 1.6513\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0745 - mae: 1.5653 - val_loss: 1.1281 - val_mae: 1.6320\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0867 - mae: 1.5793 - val_loss: 1.1340 - val_mae: 1.6410\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0749 - mae: 1.5630 - val_loss: 1.1467 - val_mae: 1.6566\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0746 - mae: 1.5660 - val_loss: 1.1277 - val_mae: 1.6334\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0541 - mae: 1.5427 - val_loss: 1.1320 - val_mae: 1.6392\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0646 - mae: 1.5527 - val_loss: 1.1231 - val_mae: 1.6301\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0659 - mae: 1.5562 - val_loss: 1.1086 - val_mae: 1.6104\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0673 - mae: 1.5549 - val_loss: 1.1026 - val_mae: 1.6013\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0563 - mae: 1.5461 - val_loss: 1.1296 - val_mae: 1.6358\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0555 - mae: 1.5415 - val_loss: 1.1130 - val_mae: 1.6148\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0604 - mae: 1.5487 - val_loss: 1.1282 - val_mae: 1.6322\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0581 - mae: 1.5468 - val_loss: 1.1163 - val_mae: 1.6204\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0510 - mae: 1.5402 - val_loss: 1.1123 - val_mae: 1.6161\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0569 - mae: 1.5476 - val_loss: 1.1108 - val_mae: 1.6156\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0308 - mae: 1.5174 - val_loss: 1.0875 - val_mae: 1.5870\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0467 - mae: 1.5371 - val_loss: 1.0788 - val_mae: 1.5762\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0346 - mae: 1.5221 - val_loss: 1.0634 - val_mae: 1.5647\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0269 - mae: 1.5170 - val_loss: 1.0739 - val_mae: 1.5739\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0292 - mae: 1.5209 - val_loss: 1.0556 - val_mae: 1.5522\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0236 - mae: 1.5123 - val_loss: 1.0665 - val_mae: 1.5628\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0216 - mae: 1.5123 - val_loss: 1.0603 - val_mae: 1.5566\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0297 - mae: 1.5206 - val_loss: 1.0766 - val_mae: 1.5786\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0295 - mae: 1.5195 - val_loss: 1.0688 - val_mae: 1.5667\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0304 - mae: 1.5228 - val_loss: 1.0836 - val_mae: 1.5882\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 1s 9ms/step - loss: 1.0133 - mae: 1.4996 - val_loss: 1.0772 - val_mae: 1.5784\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 1.0056 - mae: 1.4943 - val_loss: 1.0612 - val_mae: 1.5602\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.0089 - mae: 1.4970 - val_loss: 1.0678 - val_mae: 1.5691\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0185 - mae: 1.5067 - val_loss: 1.0541 - val_mae: 1.5532\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0043 - mae: 1.4934 - val_loss: 1.0555 - val_mae: 1.5545\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0155 - mae: 1.5046 - val_loss: 1.0648 - val_mae: 1.5654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:16:59,869] Trial 9 finished with value: 1.0648491382598877 and parameters: {'dropout_2': 0.4756894474801202, 'dropout_3': 0.3827600023474532, 'dropout_4': 0.7261870957016199, 'dropout_5': 0.12334967791219108, 'learning_rate': 0.0008915990669741432, 'epochs': 50, 'batch_size': 64}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "14/14 [==============================] - 3s 31ms/step - loss: 1.7812 - mae: 2.3513 - val_loss: 1.3484 - val_mae: 1.8169\n",
            "Epoch 2/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.7387 - mae: 2.3014 - val_loss: 1.3421 - val_mae: 1.8247\n",
            "Epoch 3/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.7568 - mae: 2.3227 - val_loss: 1.3370 - val_mae: 1.8242\n",
            "Epoch 4/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.7142 - mae: 2.2796 - val_loss: 1.3317 - val_mae: 1.8233\n",
            "Epoch 5/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.6897 - mae: 2.2522 - val_loss: 1.3277 - val_mae: 1.8231\n",
            "Epoch 6/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.6888 - mae: 2.2493 - val_loss: 1.3242 - val_mae: 1.8224\n",
            "Epoch 7/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.6712 - mae: 2.2314 - val_loss: 1.3203 - val_mae: 1.8209\n",
            "Epoch 8/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.6704 - mae: 2.2344 - val_loss: 1.3163 - val_mae: 1.8182\n",
            "Epoch 9/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.6635 - mae: 2.2242 - val_loss: 1.3111 - val_mae: 1.8122\n",
            "Epoch 10/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.6654 - mae: 2.2246 - val_loss: 1.3040 - val_mae: 1.8019\n",
            "Epoch 11/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.6339 - mae: 2.1933 - val_loss: 1.2984 - val_mae: 1.7943\n",
            "Epoch 12/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.6135 - mae: 2.1718 - val_loss: 1.2913 - val_mae: 1.7839\n",
            "Epoch 13/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.6410 - mae: 2.1994 - val_loss: 1.2840 - val_mae: 1.7738\n",
            "Epoch 14/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.6015 - mae: 2.1568 - val_loss: 1.2739 - val_mae: 1.7583\n",
            "Epoch 15/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.6151 - mae: 2.1717 - val_loss: 1.2680 - val_mae: 1.7500\n",
            "Epoch 16/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.5983 - mae: 2.1577 - val_loss: 1.2627 - val_mae: 1.7442\n",
            "Epoch 17/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.6094 - mae: 2.1630 - val_loss: 1.2556 - val_mae: 1.7368\n",
            "Epoch 18/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.5708 - mae: 2.1214 - val_loss: 1.2493 - val_mae: 1.7318\n",
            "Epoch 19/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.5852 - mae: 2.1388 - val_loss: 1.2421 - val_mae: 1.7255\n",
            "Epoch 20/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.5662 - mae: 2.1192 - val_loss: 1.2340 - val_mae: 1.7195\n",
            "Epoch 21/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.5685 - mae: 2.1177 - val_loss: 1.2282 - val_mae: 1.7138\n",
            "Epoch 22/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.5140 - mae: 2.0628 - val_loss: 1.2228 - val_mae: 1.7096\n",
            "Epoch 23/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.5327 - mae: 2.0831 - val_loss: 1.2164 - val_mae: 1.7054\n",
            "Epoch 24/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.5637 - mae: 2.1166 - val_loss: 1.2112 - val_mae: 1.7022\n",
            "Epoch 25/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.5438 - mae: 2.0938 - val_loss: 1.2071 - val_mae: 1.6987\n",
            "Epoch 26/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.5214 - mae: 2.0723 - val_loss: 1.2030 - val_mae: 1.6965\n",
            "Epoch 27/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.5318 - mae: 2.0807 - val_loss: 1.1990 - val_mae: 1.6928\n",
            "Epoch 28/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.5284 - mae: 2.0738 - val_loss: 1.1952 - val_mae: 1.6902\n",
            "Epoch 29/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.5184 - mae: 2.0653 - val_loss: 1.1911 - val_mae: 1.6875\n",
            "Epoch 30/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.4919 - mae: 2.0387 - val_loss: 1.1892 - val_mae: 1.6858\n",
            "Epoch 31/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.5152 - mae: 2.0632 - val_loss: 1.1845 - val_mae: 1.6816\n",
            "Epoch 32/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.4880 - mae: 2.0340 - val_loss: 1.1810 - val_mae: 1.6776\n",
            "Epoch 33/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.4433 - mae: 1.9873 - val_loss: 1.1780 - val_mae: 1.6747\n",
            "Epoch 34/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.4927 - mae: 2.0386 - val_loss: 1.1748 - val_mae: 1.6707\n",
            "Epoch 35/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.4640 - mae: 2.0081 - val_loss: 1.1731 - val_mae: 1.6691\n",
            "Epoch 36/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.4407 - mae: 1.9847 - val_loss: 1.1697 - val_mae: 1.6661\n",
            "Epoch 37/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4760 - mae: 2.0174 - val_loss: 1.1675 - val_mae: 1.6633\n",
            "Epoch 38/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.4420 - mae: 1.9844 - val_loss: 1.1654 - val_mae: 1.6609\n",
            "Epoch 39/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.4684 - mae: 2.0129 - val_loss: 1.1618 - val_mae: 1.6563\n",
            "Epoch 40/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.4595 - mae: 2.0013 - val_loss: 1.1608 - val_mae: 1.6545\n",
            "Epoch 41/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4519 - mae: 1.9924 - val_loss: 1.1566 - val_mae: 1.6499\n",
            "Epoch 42/200\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.4338 - mae: 1.9753 - val_loss: 1.1548 - val_mae: 1.6482\n",
            "Epoch 43/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.4423 - mae: 1.9800 - val_loss: 1.1526 - val_mae: 1.6454\n",
            "Epoch 44/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.4394 - mae: 1.9771 - val_loss: 1.1517 - val_mae: 1.6444\n",
            "Epoch 45/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.4460 - mae: 1.9858 - val_loss: 1.1498 - val_mae: 1.6422\n",
            "Epoch 46/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.4334 - mae: 1.9758 - val_loss: 1.1492 - val_mae: 1.6410\n",
            "Epoch 47/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.4259 - mae: 1.9658 - val_loss: 1.1490 - val_mae: 1.6410\n",
            "Epoch 48/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.4153 - mae: 1.9537 - val_loss: 1.1471 - val_mae: 1.6384\n",
            "Epoch 49/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.4279 - mae: 1.9653 - val_loss: 1.1467 - val_mae: 1.6378\n",
            "Epoch 50/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.4272 - mae: 1.9679 - val_loss: 1.1437 - val_mae: 1.6341\n",
            "Epoch 51/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3989 - mae: 1.9341 - val_loss: 1.1437 - val_mae: 1.6343\n",
            "Epoch 52/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.4061 - mae: 1.9437 - val_loss: 1.1433 - val_mae: 1.6336\n",
            "Epoch 53/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3988 - mae: 1.9316 - val_loss: 1.1414 - val_mae: 1.6309\n",
            "Epoch 54/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3998 - mae: 1.9381 - val_loss: 1.1401 - val_mae: 1.6292\n",
            "Epoch 55/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.4067 - mae: 1.9424 - val_loss: 1.1384 - val_mae: 1.6274\n",
            "Epoch 56/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.4097 - mae: 1.9454 - val_loss: 1.1360 - val_mae: 1.6245\n",
            "Epoch 57/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.4094 - mae: 1.9477 - val_loss: 1.1351 - val_mae: 1.6227\n",
            "Epoch 58/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3606 - mae: 1.8911 - val_loss: 1.1339 - val_mae: 1.6215\n",
            "Epoch 59/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3780 - mae: 1.9129 - val_loss: 1.1321 - val_mae: 1.6194\n",
            "Epoch 60/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3580 - mae: 1.8894 - val_loss: 1.1303 - val_mae: 1.6173\n",
            "Epoch 61/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.3737 - mae: 1.9066 - val_loss: 1.1284 - val_mae: 1.6144\n",
            "Epoch 62/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3460 - mae: 1.8792 - val_loss: 1.1278 - val_mae: 1.6140\n",
            "Epoch 63/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3881 - mae: 1.9213 - val_loss: 1.1255 - val_mae: 1.6113\n",
            "Epoch 64/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3894 - mae: 1.9239 - val_loss: 1.1265 - val_mae: 1.6124\n",
            "Epoch 65/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3598 - mae: 1.8939 - val_loss: 1.1271 - val_mae: 1.6133\n",
            "Epoch 66/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3688 - mae: 1.9022 - val_loss: 1.1263 - val_mae: 1.6123\n",
            "Epoch 67/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3512 - mae: 1.8798 - val_loss: 1.1260 - val_mae: 1.6116\n",
            "Epoch 68/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3749 - mae: 1.9058 - val_loss: 1.1247 - val_mae: 1.6103\n",
            "Epoch 69/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3684 - mae: 1.8957 - val_loss: 1.1218 - val_mae: 1.6067\n",
            "Epoch 70/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3356 - mae: 1.8635 - val_loss: 1.1224 - val_mae: 1.6073\n",
            "Epoch 71/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3620 - mae: 1.8916 - val_loss: 1.1208 - val_mae: 1.6058\n",
            "Epoch 72/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3565 - mae: 1.8888 - val_loss: 1.1201 - val_mae: 1.6050\n",
            "Epoch 73/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3505 - mae: 1.8824 - val_loss: 1.1186 - val_mae: 1.6032\n",
            "Epoch 74/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3605 - mae: 1.8917 - val_loss: 1.1185 - val_mae: 1.6031\n",
            "Epoch 75/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3561 - mae: 1.8829 - val_loss: 1.1166 - val_mae: 1.6010\n",
            "Epoch 76/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3397 - mae: 1.8691 - val_loss: 1.1187 - val_mae: 1.6030\n",
            "Epoch 77/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.3334 - mae: 1.8580 - val_loss: 1.1181 - val_mae: 1.6020\n",
            "Epoch 78/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.3326 - mae: 1.8614 - val_loss: 1.1168 - val_mae: 1.6009\n",
            "Epoch 79/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3497 - mae: 1.8820 - val_loss: 1.1163 - val_mae: 1.6002\n",
            "Epoch 80/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.3432 - mae: 1.8706 - val_loss: 1.1162 - val_mae: 1.6002\n",
            "Epoch 81/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3164 - mae: 1.8424 - val_loss: 1.1148 - val_mae: 1.5985\n",
            "Epoch 82/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3122 - mae: 1.8359 - val_loss: 1.1163 - val_mae: 1.6001\n",
            "Epoch 83/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3352 - mae: 1.8644 - val_loss: 1.1163 - val_mae: 1.6001\n",
            "Epoch 84/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.3328 - mae: 1.8598 - val_loss: 1.1144 - val_mae: 1.5980\n",
            "Epoch 85/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3309 - mae: 1.8585 - val_loss: 1.1150 - val_mae: 1.5985\n",
            "Epoch 86/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3454 - mae: 1.8685 - val_loss: 1.1137 - val_mae: 1.5972\n",
            "Epoch 87/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3160 - mae: 1.8393 - val_loss: 1.1119 - val_mae: 1.5949\n",
            "Epoch 88/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3226 - mae: 1.8510 - val_loss: 1.1114 - val_mae: 1.5941\n",
            "Epoch 89/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.3186 - mae: 1.8424 - val_loss: 1.1123 - val_mae: 1.5953\n",
            "Epoch 90/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.3290 - mae: 1.8553 - val_loss: 1.1103 - val_mae: 1.5935\n",
            "Epoch 91/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2943 - mae: 1.8174 - val_loss: 1.1101 - val_mae: 1.5935\n",
            "Epoch 92/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.3121 - mae: 1.8391 - val_loss: 1.1073 - val_mae: 1.5903\n",
            "Epoch 93/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3149 - mae: 1.8401 - val_loss: 1.1079 - val_mae: 1.5902\n",
            "Epoch 94/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2963 - mae: 1.8196 - val_loss: 1.1073 - val_mae: 1.5899\n",
            "Epoch 95/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3137 - mae: 1.8408 - val_loss: 1.1063 - val_mae: 1.5890\n",
            "Epoch 96/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2899 - mae: 1.8123 - val_loss: 1.1043 - val_mae: 1.5868\n",
            "Epoch 97/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.3090 - mae: 1.8323 - val_loss: 1.1043 - val_mae: 1.5861\n",
            "Epoch 98/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2803 - mae: 1.8051 - val_loss: 1.1044 - val_mae: 1.5865\n",
            "Epoch 99/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2850 - mae: 1.8083 - val_loss: 1.1029 - val_mae: 1.5850\n",
            "Epoch 100/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2967 - mae: 1.8222 - val_loss: 1.1027 - val_mae: 1.5848\n",
            "Epoch 101/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2885 - mae: 1.8103 - val_loss: 1.1044 - val_mae: 1.5868\n",
            "Epoch 102/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2843 - mae: 1.8046 - val_loss: 1.1035 - val_mae: 1.5861\n",
            "Epoch 103/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2856 - mae: 1.8069 - val_loss: 1.1030 - val_mae: 1.5849\n",
            "Epoch 104/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.2953 - mae: 1.8217 - val_loss: 1.1025 - val_mae: 1.5848\n",
            "Epoch 105/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2875 - mae: 1.8112 - val_loss: 1.1023 - val_mae: 1.5843\n",
            "Epoch 106/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2961 - mae: 1.8195 - val_loss: 1.1016 - val_mae: 1.5843\n",
            "Epoch 107/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.2746 - mae: 1.7931 - val_loss: 1.1018 - val_mae: 1.5849\n",
            "Epoch 108/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2842 - mae: 1.8045 - val_loss: 1.1017 - val_mae: 1.5849\n",
            "Epoch 109/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2986 - mae: 1.8225 - val_loss: 1.0990 - val_mae: 1.5819\n",
            "Epoch 110/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.2781 - mae: 1.8007 - val_loss: 1.1003 - val_mae: 1.5833\n",
            "Epoch 111/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2812 - mae: 1.8017 - val_loss: 1.0992 - val_mae: 1.5820\n",
            "Epoch 112/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2686 - mae: 1.7864 - val_loss: 1.0986 - val_mae: 1.5811\n",
            "Epoch 113/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2713 - mae: 1.7896 - val_loss: 1.0973 - val_mae: 1.5801\n",
            "Epoch 114/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2814 - mae: 1.8066 - val_loss: 1.0961 - val_mae: 1.5792\n",
            "Epoch 115/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.2795 - mae: 1.7998 - val_loss: 1.0959 - val_mae: 1.5790\n",
            "Epoch 116/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.2554 - mae: 1.7732 - val_loss: 1.0961 - val_mae: 1.5792\n",
            "Epoch 117/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2770 - mae: 1.7998 - val_loss: 1.0967 - val_mae: 1.5790\n",
            "Epoch 118/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2446 - mae: 1.7639 - val_loss: 1.0956 - val_mae: 1.5779\n",
            "Epoch 119/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2463 - mae: 1.7637 - val_loss: 1.0955 - val_mae: 1.5779\n",
            "Epoch 120/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2393 - mae: 1.7526 - val_loss: 1.0951 - val_mae: 1.5775\n",
            "Epoch 121/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.2604 - mae: 1.7802 - val_loss: 1.0945 - val_mae: 1.5773\n",
            "Epoch 122/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.2562 - mae: 1.7740 - val_loss: 1.0956 - val_mae: 1.5790\n",
            "Epoch 123/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2631 - mae: 1.7825 - val_loss: 1.0944 - val_mae: 1.5779\n",
            "Epoch 124/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2348 - mae: 1.7489 - val_loss: 1.0929 - val_mae: 1.5756\n",
            "Epoch 125/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2792 - mae: 1.7988 - val_loss: 1.0925 - val_mae: 1.5748\n",
            "Epoch 126/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.2737 - mae: 1.7945 - val_loss: 1.0933 - val_mae: 1.5761\n",
            "Epoch 127/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2489 - mae: 1.7660 - val_loss: 1.0933 - val_mae: 1.5765\n",
            "Epoch 128/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2419 - mae: 1.7552 - val_loss: 1.0926 - val_mae: 1.5751\n",
            "Epoch 129/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2493 - mae: 1.7672 - val_loss: 1.0914 - val_mae: 1.5740\n",
            "Epoch 130/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2640 - mae: 1.7851 - val_loss: 1.0905 - val_mae: 1.5731\n",
            "Epoch 131/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2390 - mae: 1.7541 - val_loss: 1.0909 - val_mae: 1.5740\n",
            "Epoch 132/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2266 - mae: 1.7399 - val_loss: 1.0906 - val_mae: 1.5737\n",
            "Epoch 133/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2528 - mae: 1.7698 - val_loss: 1.0900 - val_mae: 1.5729\n",
            "Epoch 134/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2542 - mae: 1.7726 - val_loss: 1.0898 - val_mae: 1.5729\n",
            "Epoch 135/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2483 - mae: 1.7642 - val_loss: 1.0874 - val_mae: 1.5708\n",
            "Epoch 136/200\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2436 - mae: 1.7594 - val_loss: 1.0879 - val_mae: 1.5711\n",
            "Epoch 137/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2427 - mae: 1.7619 - val_loss: 1.0866 - val_mae: 1.5696\n",
            "Epoch 138/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2370 - mae: 1.7513 - val_loss: 1.0861 - val_mae: 1.5688\n",
            "Epoch 139/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2342 - mae: 1.7484 - val_loss: 1.0863 - val_mae: 1.5693\n",
            "Epoch 140/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2378 - mae: 1.7512 - val_loss: 1.0890 - val_mae: 1.5725\n",
            "Epoch 141/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2354 - mae: 1.7497 - val_loss: 1.0882 - val_mae: 1.5718\n",
            "Epoch 142/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.2469 - mae: 1.7672 - val_loss: 1.0871 - val_mae: 1.5709\n",
            "Epoch 143/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2355 - mae: 1.7510 - val_loss: 1.0863 - val_mae: 1.5703\n",
            "Epoch 144/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2482 - mae: 1.7625 - val_loss: 1.0863 - val_mae: 1.5705\n",
            "Epoch 145/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2192 - mae: 1.7360 - val_loss: 1.0865 - val_mae: 1.5705\n",
            "Epoch 146/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2109 - mae: 1.7246 - val_loss: 1.0862 - val_mae: 1.5701\n",
            "Epoch 147/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2226 - mae: 1.7348 - val_loss: 1.0876 - val_mae: 1.5715\n",
            "Epoch 148/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.2342 - mae: 1.7455 - val_loss: 1.0880 - val_mae: 1.5718\n",
            "Epoch 149/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2348 - mae: 1.7469 - val_loss: 1.0894 - val_mae: 1.5735\n",
            "Epoch 150/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2159 - mae: 1.7297 - val_loss: 1.0862 - val_mae: 1.5693\n",
            "Epoch 151/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.2024 - mae: 1.7148 - val_loss: 1.0843 - val_mae: 1.5673\n",
            "Epoch 152/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2223 - mae: 1.7344 - val_loss: 1.0832 - val_mae: 1.5662\n",
            "Epoch 153/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2045 - mae: 1.7162 - val_loss: 1.0847 - val_mae: 1.5685\n",
            "Epoch 154/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2203 - mae: 1.7365 - val_loss: 1.0839 - val_mae: 1.5678\n",
            "Epoch 155/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2038 - mae: 1.7119 - val_loss: 1.0811 - val_mae: 1.5648\n",
            "Epoch 156/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2164 - mae: 1.7292 - val_loss: 1.0817 - val_mae: 1.5655\n",
            "Epoch 157/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2290 - mae: 1.7411 - val_loss: 1.0814 - val_mae: 1.5655\n",
            "Epoch 158/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.1990 - mae: 1.7071 - val_loss: 1.0816 - val_mae: 1.5662\n",
            "Epoch 159/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2095 - mae: 1.7206 - val_loss: 1.0818 - val_mae: 1.5665\n",
            "Epoch 160/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2309 - mae: 1.7427 - val_loss: 1.0810 - val_mae: 1.5653\n",
            "Epoch 161/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2132 - mae: 1.7273 - val_loss: 1.0818 - val_mae: 1.5660\n",
            "Epoch 162/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2027 - mae: 1.7110 - val_loss: 1.0816 - val_mae: 1.5660\n",
            "Epoch 163/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2179 - mae: 1.7307 - val_loss: 1.0807 - val_mae: 1.5650\n",
            "Epoch 164/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.2337 - mae: 1.7469 - val_loss: 1.0815 - val_mae: 1.5664\n",
            "Epoch 165/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2121 - mae: 1.7224 - val_loss: 1.0843 - val_mae: 1.5701\n",
            "Epoch 166/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2107 - mae: 1.7251 - val_loss: 1.0832 - val_mae: 1.5693\n",
            "Epoch 167/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2243 - mae: 1.7415 - val_loss: 1.0840 - val_mae: 1.5710\n",
            "Epoch 168/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2093 - mae: 1.7181 - val_loss: 1.0810 - val_mae: 1.5678\n",
            "Epoch 169/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1933 - mae: 1.6998 - val_loss: 1.0807 - val_mae: 1.5675\n",
            "Epoch 170/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1897 - mae: 1.6992 - val_loss: 1.0803 - val_mae: 1.5667\n",
            "Epoch 171/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.1932 - mae: 1.7038 - val_loss: 1.0781 - val_mae: 1.5645\n",
            "Epoch 172/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2051 - mae: 1.7181 - val_loss: 1.0770 - val_mae: 1.5632\n",
            "Epoch 173/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.1890 - mae: 1.6989 - val_loss: 1.0761 - val_mae: 1.5619\n",
            "Epoch 174/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2000 - mae: 1.7129 - val_loss: 1.0759 - val_mae: 1.5614\n",
            "Epoch 175/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1820 - mae: 1.6902 - val_loss: 1.0761 - val_mae: 1.5613\n",
            "Epoch 176/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2076 - mae: 1.7187 - val_loss: 1.0762 - val_mae: 1.5616\n",
            "Epoch 177/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2080 - mae: 1.7214 - val_loss: 1.0757 - val_mae: 1.5611\n",
            "Epoch 178/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.1980 - mae: 1.7063 - val_loss: 1.0752 - val_mae: 1.5606\n",
            "Epoch 179/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.1948 - mae: 1.7019 - val_loss: 1.0755 - val_mae: 1.5609\n",
            "Epoch 180/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1975 - mae: 1.7090 - val_loss: 1.0755 - val_mae: 1.5608\n",
            "Epoch 181/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1812 - mae: 1.6900 - val_loss: 1.0756 - val_mae: 1.5611\n",
            "Epoch 182/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2066 - mae: 1.7187 - val_loss: 1.0743 - val_mae: 1.5595\n",
            "Epoch 183/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1835 - mae: 1.6903 - val_loss: 1.0745 - val_mae: 1.5601\n",
            "Epoch 184/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.1793 - mae: 1.6833 - val_loss: 1.0739 - val_mae: 1.5596\n",
            "Epoch 185/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1950 - mae: 1.7022 - val_loss: 1.0737 - val_mae: 1.5589\n",
            "Epoch 186/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1903 - mae: 1.6993 - val_loss: 1.0750 - val_mae: 1.5606\n",
            "Epoch 187/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.1688 - mae: 1.6741 - val_loss: 1.0753 - val_mae: 1.5607\n",
            "Epoch 188/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1700 - mae: 1.6762 - val_loss: 1.0766 - val_mae: 1.5622\n",
            "Epoch 189/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1708 - mae: 1.6759 - val_loss: 1.0741 - val_mae: 1.5587\n",
            "Epoch 190/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1739 - mae: 1.6797 - val_loss: 1.0753 - val_mae: 1.5595\n",
            "Epoch 191/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1800 - mae: 1.6888 - val_loss: 1.0736 - val_mae: 1.5573\n",
            "Epoch 192/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1686 - mae: 1.6712 - val_loss: 1.0743 - val_mae: 1.5582\n",
            "Epoch 193/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1828 - mae: 1.6903 - val_loss: 1.0744 - val_mae: 1.5585\n",
            "Epoch 194/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.1662 - mae: 1.6712 - val_loss: 1.0718 - val_mae: 1.5554\n",
            "Epoch 195/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1959 - mae: 1.7056 - val_loss: 1.0705 - val_mae: 1.5537\n",
            "Epoch 196/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1993 - mae: 1.7101 - val_loss: 1.0704 - val_mae: 1.5537\n",
            "Epoch 197/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.1735 - mae: 1.6798 - val_loss: 1.0695 - val_mae: 1.5527\n",
            "Epoch 198/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1671 - mae: 1.6720 - val_loss: 1.0706 - val_mae: 1.5543\n",
            "Epoch 199/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.1492 - mae: 1.6559 - val_loss: 1.0713 - val_mae: 1.5551\n",
            "Epoch 200/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1660 - mae: 1.6732 - val_loss: 1.0688 - val_mae: 1.5521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:17:35,767] Trial 10 finished with value: 1.0687990188598633 and parameters: {'dropout_2': 0.1281802419617952, 'dropout_3': 0.10673656780014812, 'dropout_4': 0.414915502027238, 'dropout_5': 0.5846709157830872, 'learning_rate': 5.8375355062184674e-05, 'epochs': 200, 'batch_size': 256}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "54/54 [==============================] - 3s 15ms/step - loss: 1.7305 - mae: 2.3041 - val_loss: 1.3220 - val_mae: 1.8267\n",
            "Epoch 2/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.6045 - mae: 2.1755 - val_loss: 1.3408 - val_mae: 1.8737\n",
            "Epoch 3/200\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 1.4869 - mae: 2.0452 - val_loss: 1.3264 - val_mae: 1.8594\n",
            "Epoch 4/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.4656 - mae: 2.0186 - val_loss: 1.2957 - val_mae: 1.8253\n",
            "Epoch 5/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.4368 - mae: 1.9914 - val_loss: 1.2597 - val_mae: 1.7849\n",
            "Epoch 6/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.3714 - mae: 1.9183 - val_loss: 1.2204 - val_mae: 1.7354\n",
            "Epoch 7/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.3633 - mae: 1.9122 - val_loss: 1.1880 - val_mae: 1.6889\n",
            "Epoch 8/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.3358 - mae: 1.8795 - val_loss: 1.1604 - val_mae: 1.6571\n",
            "Epoch 9/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.3085 - mae: 1.8473 - val_loss: 1.1476 - val_mae: 1.6432\n",
            "Epoch 10/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2926 - mae: 1.8322 - val_loss: 1.1324 - val_mae: 1.6223\n",
            "Epoch 11/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2646 - mae: 1.7956 - val_loss: 1.1267 - val_mae: 1.6192\n",
            "Epoch 12/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.2407 - mae: 1.7722 - val_loss: 1.1201 - val_mae: 1.6119\n",
            "Epoch 13/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.2350 - mae: 1.7649 - val_loss: 1.1146 - val_mae: 1.6030\n",
            "Epoch 14/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.2066 - mae: 1.7315 - val_loss: 1.1055 - val_mae: 1.5917\n",
            "Epoch 15/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2179 - mae: 1.7448 - val_loss: 1.1100 - val_mae: 1.5986\n",
            "Epoch 16/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2159 - mae: 1.7437 - val_loss: 1.1083 - val_mae: 1.5980\n",
            "Epoch 17/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1868 - mae: 1.7097 - val_loss: 1.1016 - val_mae: 1.5875\n",
            "Epoch 18/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1893 - mae: 1.7156 - val_loss: 1.0982 - val_mae: 1.5821\n",
            "Epoch 19/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1631 - mae: 1.6795 - val_loss: 1.0930 - val_mae: 1.5759\n",
            "Epoch 20/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1716 - mae: 1.6882 - val_loss: 1.0973 - val_mae: 1.5796\n",
            "Epoch 21/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1539 - mae: 1.6717 - val_loss: 1.0948 - val_mae: 1.5801\n",
            "Epoch 22/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1302 - mae: 1.6445 - val_loss: 1.0835 - val_mae: 1.5668\n",
            "Epoch 23/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1334 - mae: 1.6453 - val_loss: 1.0857 - val_mae: 1.5656\n",
            "Epoch 24/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1412 - mae: 1.6529 - val_loss: 1.0811 - val_mae: 1.5611\n",
            "Epoch 25/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1138 - mae: 1.6196 - val_loss: 1.0825 - val_mae: 1.5669\n",
            "Epoch 26/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1141 - mae: 1.6201 - val_loss: 1.0739 - val_mae: 1.5520\n",
            "Epoch 27/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0859 - mae: 1.5880 - val_loss: 1.0746 - val_mae: 1.5518\n",
            "Epoch 28/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1160 - mae: 1.6218 - val_loss: 1.0640 - val_mae: 1.5446\n",
            "Epoch 29/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0930 - mae: 1.5945 - val_loss: 1.0641 - val_mae: 1.5457\n",
            "Epoch 30/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0923 - mae: 1.5931 - val_loss: 1.0727 - val_mae: 1.5511\n",
            "Epoch 31/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0775 - mae: 1.5800 - val_loss: 1.0673 - val_mae: 1.5457\n",
            "Epoch 32/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0923 - mae: 1.5919 - val_loss: 1.0652 - val_mae: 1.5427\n",
            "Epoch 33/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0673 - mae: 1.5664 - val_loss: 1.0627 - val_mae: 1.5427\n",
            "Epoch 34/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0761 - mae: 1.5733 - val_loss: 1.0592 - val_mae: 1.5384\n",
            "Epoch 35/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0692 - mae: 1.5652 - val_loss: 1.0562 - val_mae: 1.5354\n",
            "Epoch 36/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0639 - mae: 1.5579 - val_loss: 1.0550 - val_mae: 1.5336\n",
            "Epoch 37/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0609 - mae: 1.5562 - val_loss: 1.0524 - val_mae: 1.5269\n",
            "Epoch 38/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.0650 - mae: 1.5587 - val_loss: 1.0582 - val_mae: 1.5381\n",
            "Epoch 39/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0578 - mae: 1.5547 - val_loss: 1.0580 - val_mae: 1.5401\n",
            "Epoch 40/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0463 - mae: 1.5423 - val_loss: 1.0392 - val_mae: 1.5159\n",
            "Epoch 41/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0446 - mae: 1.5408 - val_loss: 1.0310 - val_mae: 1.5087\n",
            "Epoch 42/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.0495 - mae: 1.5443 - val_loss: 1.0376 - val_mae: 1.5206\n",
            "Epoch 43/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0294 - mae: 1.5199 - val_loss: 1.0356 - val_mae: 1.5178\n",
            "Epoch 44/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0513 - mae: 1.5474 - val_loss: 1.0357 - val_mae: 1.5152\n",
            "Epoch 45/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0372 - mae: 1.5292 - val_loss: 1.0411 - val_mae: 1.5226\n",
            "Epoch 46/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0250 - mae: 1.5133 - val_loss: 1.0359 - val_mae: 1.5167\n",
            "Epoch 47/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0246 - mae: 1.5180 - val_loss: 1.0289 - val_mae: 1.5086\n",
            "Epoch 48/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0125 - mae: 1.5024 - val_loss: 1.0197 - val_mae: 1.4974\n",
            "Epoch 49/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0334 - mae: 1.5237 - val_loss: 1.0256 - val_mae: 1.5012\n",
            "Epoch 50/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0148 - mae: 1.5024 - val_loss: 1.0252 - val_mae: 1.5016\n",
            "Epoch 51/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0112 - mae: 1.5033 - val_loss: 1.0257 - val_mae: 1.5035\n",
            "Epoch 52/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0179 - mae: 1.5073 - val_loss: 1.0264 - val_mae: 1.5076\n",
            "Epoch 53/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0202 - mae: 1.5104 - val_loss: 1.0255 - val_mae: 1.5076\n",
            "Epoch 54/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0037 - mae: 1.4929 - val_loss: 1.0176 - val_mae: 1.4988\n",
            "Epoch 55/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9988 - mae: 1.4894 - val_loss: 1.0297 - val_mae: 1.5141\n",
            "Epoch 56/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0134 - mae: 1.5051 - val_loss: 1.0129 - val_mae: 1.4947\n",
            "Epoch 57/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9859 - mae: 1.4745 - val_loss: 1.0071 - val_mae: 1.4867\n",
            "Epoch 58/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9937 - mae: 1.4836 - val_loss: 1.0076 - val_mae: 1.4864\n",
            "Epoch 59/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9938 - mae: 1.4792 - val_loss: 1.0052 - val_mae: 1.4829\n",
            "Epoch 60/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9909 - mae: 1.4793 - val_loss: 0.9978 - val_mae: 1.4721\n",
            "Epoch 61/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9801 - mae: 1.4678 - val_loss: 0.9901 - val_mae: 1.4666\n",
            "Epoch 62/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9978 - mae: 1.4835 - val_loss: 0.9895 - val_mae: 1.4646\n",
            "Epoch 63/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.9816 - mae: 1.4696 - val_loss: 0.9965 - val_mae: 1.4755\n",
            "Epoch 64/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9628 - mae: 1.4503 - val_loss: 0.9786 - val_mae: 1.4519\n",
            "Epoch 65/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9880 - mae: 1.4768 - val_loss: 0.9757 - val_mae: 1.4509\n",
            "Epoch 66/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9779 - mae: 1.4634 - val_loss: 0.9749 - val_mae: 1.4531\n",
            "Epoch 67/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9783 - mae: 1.4628 - val_loss: 0.9808 - val_mae: 1.4580\n",
            "Epoch 68/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9748 - mae: 1.4606 - val_loss: 0.9808 - val_mae: 1.4579\n",
            "Epoch 69/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9690 - mae: 1.4517 - val_loss: 0.9736 - val_mae: 1.4465\n",
            "Epoch 70/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9694 - mae: 1.4542 - val_loss: 0.9742 - val_mae: 1.4463\n",
            "Epoch 71/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9853 - mae: 1.4728 - val_loss: 0.9746 - val_mae: 1.4466\n",
            "Epoch 72/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.9595 - mae: 1.4410 - val_loss: 0.9883 - val_mae: 1.4651\n",
            "Epoch 73/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9613 - mae: 1.4474 - val_loss: 0.9828 - val_mae: 1.4613\n",
            "Epoch 74/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9758 - mae: 1.4601 - val_loss: 0.9780 - val_mae: 1.4557\n",
            "Epoch 75/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9671 - mae: 1.4523 - val_loss: 0.9686 - val_mae: 1.4417\n",
            "Epoch 76/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9647 - mae: 1.4510 - val_loss: 0.9619 - val_mae: 1.4358\n",
            "Epoch 77/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9457 - mae: 1.4307 - val_loss: 0.9647 - val_mae: 1.4407\n",
            "Epoch 78/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.9603 - mae: 1.4454 - val_loss: 0.9599 - val_mae: 1.4321\n",
            "Epoch 79/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9562 - mae: 1.4397 - val_loss: 0.9630 - val_mae: 1.4353\n",
            "Epoch 80/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9526 - mae: 1.4353 - val_loss: 0.9665 - val_mae: 1.4406\n",
            "Epoch 81/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9655 - mae: 1.4502 - val_loss: 0.9609 - val_mae: 1.4342\n",
            "Epoch 82/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9439 - mae: 1.4220 - val_loss: 0.9593 - val_mae: 1.4322\n",
            "Epoch 83/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9408 - mae: 1.4250 - val_loss: 0.9562 - val_mae: 1.4256\n",
            "Epoch 84/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9426 - mae: 1.4290 - val_loss: 0.9662 - val_mae: 1.4403\n",
            "Epoch 85/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9569 - mae: 1.4431 - val_loss: 0.9630 - val_mae: 1.4346\n",
            "Epoch 86/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9431 - mae: 1.4243 - val_loss: 0.9600 - val_mae: 1.4293\n",
            "Epoch 87/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.9474 - mae: 1.4259 - val_loss: 0.9668 - val_mae: 1.4367\n",
            "Epoch 88/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.9322 - mae: 1.4172 - val_loss: 0.9721 - val_mae: 1.4419\n",
            "Epoch 89/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9403 - mae: 1.4223 - val_loss: 0.9659 - val_mae: 1.4375\n",
            "Epoch 90/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9390 - mae: 1.4243 - val_loss: 0.9598 - val_mae: 1.4340\n",
            "Epoch 91/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9218 - mae: 1.4036 - val_loss: 0.9638 - val_mae: 1.4365\n",
            "Epoch 92/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9274 - mae: 1.4082 - val_loss: 0.9833 - val_mae: 1.4574\n",
            "Epoch 93/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9334 - mae: 1.4150 - val_loss: 0.9667 - val_mae: 1.4394\n",
            "Epoch 94/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9200 - mae: 1.4018 - val_loss: 0.9741 - val_mae: 1.4444\n",
            "Epoch 95/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9159 - mae: 1.3973 - val_loss: 0.9670 - val_mae: 1.4391\n",
            "Epoch 96/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9210 - mae: 1.3990 - val_loss: 0.9632 - val_mae: 1.4359\n",
            "Epoch 97/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9327 - mae: 1.4128 - val_loss: 0.9626 - val_mae: 1.4338\n",
            "Epoch 98/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.9254 - mae: 1.4060 - val_loss: 0.9510 - val_mae: 1.4201\n",
            "Epoch 99/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9287 - mae: 1.4092 - val_loss: 0.9720 - val_mae: 1.4440\n",
            "Epoch 100/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9250 - mae: 1.4080 - val_loss: 0.9918 - val_mae: 1.4642\n",
            "Epoch 101/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9218 - mae: 1.3992 - val_loss: 0.9681 - val_mae: 1.4395\n",
            "Epoch 102/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9236 - mae: 1.4067 - val_loss: 0.9717 - val_mae: 1.4403\n",
            "Epoch 103/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9366 - mae: 1.4196 - val_loss: 0.9642 - val_mae: 1.4347\n",
            "Epoch 104/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9204 - mae: 1.3996 - val_loss: 0.9636 - val_mae: 1.4349\n",
            "Epoch 105/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9347 - mae: 1.4187 - val_loss: 0.9720 - val_mae: 1.4454\n",
            "Epoch 106/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9026 - mae: 1.3854 - val_loss: 0.9733 - val_mae: 1.4430\n",
            "Epoch 107/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8994 - mae: 1.3788 - val_loss: 0.9730 - val_mae: 1.4425\n",
            "Epoch 108/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8985 - mae: 1.3765 - val_loss: 0.9540 - val_mae: 1.4217\n",
            "Epoch 109/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9112 - mae: 1.3913 - val_loss: 0.9554 - val_mae: 1.4248\n",
            "Epoch 110/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9140 - mae: 1.3957 - val_loss: 0.9490 - val_mae: 1.4168\n",
            "Epoch 111/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9120 - mae: 1.3917 - val_loss: 0.9618 - val_mae: 1.4317\n",
            "Epoch 112/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9208 - mae: 1.4012 - val_loss: 0.9550 - val_mae: 1.4224\n",
            "Epoch 113/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8906 - mae: 1.3630 - val_loss: 0.9536 - val_mae: 1.4206\n",
            "Epoch 114/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9203 - mae: 1.3969 - val_loss: 0.9618 - val_mae: 1.4301\n",
            "Epoch 115/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9039 - mae: 1.3795 - val_loss: 0.9549 - val_mae: 1.4236\n",
            "Epoch 116/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9053 - mae: 1.3865 - val_loss: 0.9639 - val_mae: 1.4313\n",
            "Epoch 117/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9039 - mae: 1.3840 - val_loss: 0.9647 - val_mae: 1.4309\n",
            "Epoch 118/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8928 - mae: 1.3676 - val_loss: 0.9572 - val_mae: 1.4215\n",
            "Epoch 119/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.8889 - mae: 1.3645 - val_loss: 0.9554 - val_mae: 1.4189\n",
            "Epoch 120/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.8914 - mae: 1.3668 - val_loss: 0.9577 - val_mae: 1.4247\n",
            "Epoch 121/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9017 - mae: 1.3790 - val_loss: 0.9534 - val_mae: 1.4179\n",
            "Epoch 122/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.8969 - mae: 1.3735 - val_loss: 0.9592 - val_mae: 1.4204\n",
            "Epoch 123/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.8862 - mae: 1.3608 - val_loss: 0.9582 - val_mae: 1.4229\n",
            "Epoch 124/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.8962 - mae: 1.3668 - val_loss: 0.9608 - val_mae: 1.4260\n",
            "Epoch 125/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.8903 - mae: 1.3672 - val_loss: 0.9541 - val_mae: 1.4161\n",
            "Epoch 126/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.8896 - mae: 1.3615 - val_loss: 0.9572 - val_mae: 1.4188\n",
            "Epoch 127/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.8983 - mae: 1.3742 - val_loss: 0.9507 - val_mae: 1.4099\n",
            "Epoch 128/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9013 - mae: 1.3801 - val_loss: 0.9538 - val_mae: 1.4177\n",
            "Epoch 129/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.8852 - mae: 1.3593 - val_loss: 0.9382 - val_mae: 1.4006\n",
            "Epoch 130/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8730 - mae: 1.3441 - val_loss: 0.9358 - val_mae: 1.3935\n",
            "Epoch 131/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8905 - mae: 1.3679 - val_loss: 0.9271 - val_mae: 1.3856\n",
            "Epoch 132/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8776 - mae: 1.3485 - val_loss: 0.9406 - val_mae: 1.4009\n",
            "Epoch 133/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8784 - mae: 1.3530 - val_loss: 0.9412 - val_mae: 1.3970\n",
            "Epoch 134/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8769 - mae: 1.3462 - val_loss: 0.9426 - val_mae: 1.4009\n",
            "Epoch 135/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8796 - mae: 1.3533 - val_loss: 0.9298 - val_mae: 1.3885\n",
            "Epoch 136/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8922 - mae: 1.3691 - val_loss: 0.9318 - val_mae: 1.3901\n",
            "Epoch 137/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8792 - mae: 1.3544 - val_loss: 0.9334 - val_mae: 1.3919\n",
            "Epoch 138/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8615 - mae: 1.3367 - val_loss: 0.9196 - val_mae: 1.3788\n",
            "Epoch 139/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8864 - mae: 1.3618 - val_loss: 0.9406 - val_mae: 1.4016\n",
            "Epoch 140/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8611 - mae: 1.3291 - val_loss: 0.9329 - val_mae: 1.3903\n",
            "Epoch 141/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.8726 - mae: 1.3473 - val_loss: 0.9418 - val_mae: 1.3999\n",
            "Epoch 142/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8711 - mae: 1.3445 - val_loss: 0.9336 - val_mae: 1.3897\n",
            "Epoch 143/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.8606 - mae: 1.3328 - val_loss: 0.9402 - val_mae: 1.3969\n",
            "Epoch 144/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8615 - mae: 1.3288 - val_loss: 0.9550 - val_mae: 1.4139\n",
            "Epoch 145/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8787 - mae: 1.3506 - val_loss: 0.9407 - val_mae: 1.3999\n",
            "Epoch 146/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8793 - mae: 1.3521 - val_loss: 0.9304 - val_mae: 1.3876\n",
            "Epoch 147/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8715 - mae: 1.3454 - val_loss: 0.9357 - val_mae: 1.3908\n",
            "Epoch 148/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8759 - mae: 1.3466 - val_loss: 0.9630 - val_mae: 1.4252\n",
            "Epoch 149/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8579 - mae: 1.3279 - val_loss: 0.9522 - val_mae: 1.4105\n",
            "Epoch 150/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8597 - mae: 1.3306 - val_loss: 0.9356 - val_mae: 1.3882\n",
            "Epoch 151/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8616 - mae: 1.3335 - val_loss: 0.9493 - val_mae: 1.4046\n",
            "Epoch 152/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.8570 - mae: 1.3239 - val_loss: 0.9255 - val_mae: 1.3817\n",
            "Epoch 153/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8743 - mae: 1.3474 - val_loss: 0.9433 - val_mae: 1.3977\n",
            "Epoch 154/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8700 - mae: 1.3386 - val_loss: 0.9431 - val_mae: 1.3972\n",
            "Epoch 155/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8669 - mae: 1.3418 - val_loss: 0.9364 - val_mae: 1.3935\n",
            "Epoch 156/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8574 - mae: 1.3267 - val_loss: 0.9271 - val_mae: 1.3813\n",
            "Epoch 157/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8535 - mae: 1.3241 - val_loss: 0.9169 - val_mae: 1.3703\n",
            "Epoch 158/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8597 - mae: 1.3323 - val_loss: 0.9313 - val_mae: 1.3837\n",
            "Epoch 159/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8663 - mae: 1.3384 - val_loss: 0.9226 - val_mae: 1.3762\n",
            "Epoch 160/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.8523 - mae: 1.3235 - val_loss: 0.9241 - val_mae: 1.3775\n",
            "Epoch 161/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.8655 - mae: 1.3381 - val_loss: 0.9298 - val_mae: 1.3876\n",
            "Epoch 162/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.8468 - mae: 1.3162 - val_loss: 0.9135 - val_mae: 1.3676\n",
            "Epoch 163/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.8521 - mae: 1.3215 - val_loss: 0.9261 - val_mae: 1.3783\n",
            "Epoch 164/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.8614 - mae: 1.3308 - val_loss: 0.9610 - val_mae: 1.4143\n",
            "Epoch 165/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.8546 - mae: 1.3221 - val_loss: 0.9413 - val_mae: 1.3940\n",
            "Epoch 166/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.8457 - mae: 1.3120 - val_loss: 0.9219 - val_mae: 1.3779\n",
            "Epoch 167/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.8399 - mae: 1.3083 - val_loss: 0.9185 - val_mae: 1.3735\n",
            "Epoch 168/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.8509 - mae: 1.3223 - val_loss: 0.9348 - val_mae: 1.3888\n",
            "Epoch 169/200\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 0.8490 - mae: 1.3162 - val_loss: 0.9514 - val_mae: 1.4065\n",
            "Epoch 170/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.8401 - mae: 1.3081 - val_loss: 0.9212 - val_mae: 1.3715\n",
            "Epoch 171/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8334 - mae: 1.2988 - val_loss: 0.9329 - val_mae: 1.3851\n",
            "Epoch 172/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8661 - mae: 1.3376 - val_loss: 0.9390 - val_mae: 1.3914\n",
            "Epoch 173/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8129 - mae: 1.2761 - val_loss: 0.9157 - val_mae: 1.3631\n",
            "Epoch 174/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8350 - mae: 1.3002 - val_loss: 0.9301 - val_mae: 1.3822\n",
            "Epoch 175/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8346 - mae: 1.2988 - val_loss: 0.9517 - val_mae: 1.4063\n",
            "Epoch 176/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8500 - mae: 1.3179 - val_loss: 0.9350 - val_mae: 1.3843\n",
            "Epoch 177/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8491 - mae: 1.3136 - val_loss: 0.9481 - val_mae: 1.3999\n",
            "Epoch 178/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8365 - mae: 1.3058 - val_loss: 0.9432 - val_mae: 1.3939\n",
            "Epoch 179/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8321 - mae: 1.2967 - val_loss: 0.9198 - val_mae: 1.3701\n",
            "Epoch 180/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8250 - mae: 1.2878 - val_loss: 0.9202 - val_mae: 1.3708\n",
            "Epoch 181/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8307 - mae: 1.2931 - val_loss: 0.9398 - val_mae: 1.3947\n",
            "Epoch 182/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8339 - mae: 1.3025 - val_loss: 0.9366 - val_mae: 1.3824\n",
            "Epoch 183/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8434 - mae: 1.3124 - val_loss: 0.9547 - val_mae: 1.4035\n",
            "Epoch 184/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8324 - mae: 1.2984 - val_loss: 0.9285 - val_mae: 1.3773\n",
            "Epoch 185/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8155 - mae: 1.2762 - val_loss: 0.9303 - val_mae: 1.3816\n",
            "Epoch 186/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8278 - mae: 1.2917 - val_loss: 0.9477 - val_mae: 1.3990\n",
            "Epoch 187/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8414 - mae: 1.3053 - val_loss: 0.9503 - val_mae: 1.4017\n",
            "Epoch 188/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8309 - mae: 1.2947 - val_loss: 0.9596 - val_mae: 1.4108\n",
            "Epoch 189/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8483 - mae: 1.3161 - val_loss: 0.9421 - val_mae: 1.3939\n",
            "Epoch 190/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8230 - mae: 1.2864 - val_loss: 0.9445 - val_mae: 1.3920\n",
            "Epoch 191/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8201 - mae: 1.2817 - val_loss: 0.9337 - val_mae: 1.3842\n",
            "Epoch 192/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8191 - mae: 1.2832 - val_loss: 0.9493 - val_mae: 1.4015\n",
            "Epoch 193/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8153 - mae: 1.2734 - val_loss: 0.9517 - val_mae: 1.4041\n",
            "Epoch 194/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8170 - mae: 1.2798 - val_loss: 0.9544 - val_mae: 1.4069\n",
            "Epoch 195/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8108 - mae: 1.2723 - val_loss: 0.9698 - val_mae: 1.4224\n",
            "Epoch 196/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8332 - mae: 1.3011 - val_loss: 0.9448 - val_mae: 1.3950\n",
            "Epoch 197/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8241 - mae: 1.2895 - val_loss: 0.9510 - val_mae: 1.4011\n",
            "Epoch 198/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8231 - mae: 1.2856 - val_loss: 0.9592 - val_mae: 1.4081\n",
            "Epoch 199/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.8310 - mae: 1.2941 - val_loss: 0.9744 - val_mae: 1.4269\n",
            "Epoch 200/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.8072 - mae: 1.2669 - val_loss: 0.9592 - val_mae: 1.4115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:19:00,107] Trial 11 finished with value: 0.9592189192771912 and parameters: {'dropout_2': 0.11014742697755343, 'dropout_3': 0.24713879189854454, 'dropout_4': 0.35810433352540416, 'dropout_5': 0.4787699883763758, 'learning_rate': 0.00023560288481931803, 'epochs': 200, 'batch_size': 64}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "54/54 [==============================] - 3s 16ms/step - loss: 1.7837 - mae: 2.3561 - val_loss: 1.3600 - val_mae: 1.8531\n",
            "Epoch 2/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.7002 - mae: 2.2675 - val_loss: 1.3763 - val_mae: 1.8987\n",
            "Epoch 3/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6481 - mae: 2.2136 - val_loss: 1.3830 - val_mae: 1.9176\n",
            "Epoch 4/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5907 - mae: 2.1521 - val_loss: 1.3802 - val_mae: 1.9207\n",
            "Epoch 5/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.5768 - mae: 2.1350 - val_loss: 1.3598 - val_mae: 1.9004\n",
            "Epoch 6/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5215 - mae: 2.0777 - val_loss: 1.3378 - val_mae: 1.8751\n",
            "Epoch 7/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.5170 - mae: 2.0707 - val_loss: 1.3065 - val_mae: 1.8289\n",
            "Epoch 8/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.4767 - mae: 2.0290 - val_loss: 1.2845 - val_mae: 1.7951\n",
            "Epoch 9/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.4779 - mae: 2.0290 - val_loss: 1.2616 - val_mae: 1.7680\n",
            "Epoch 10/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.4609 - mae: 2.0113 - val_loss: 1.2383 - val_mae: 1.7432\n",
            "Epoch 11/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.4147 - mae: 1.9616 - val_loss: 1.2175 - val_mae: 1.7208\n",
            "Epoch 12/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.4258 - mae: 1.9730 - val_loss: 1.2044 - val_mae: 1.7086\n",
            "Epoch 13/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.4052 - mae: 1.9479 - val_loss: 1.1983 - val_mae: 1.7008\n",
            "Epoch 14/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.3868 - mae: 1.9266 - val_loss: 1.1933 - val_mae: 1.6969\n",
            "Epoch 15/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.3543 - mae: 1.8924 - val_loss: 1.1821 - val_mae: 1.6827\n",
            "Epoch 16/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.3788 - mae: 1.9225 - val_loss: 1.1763 - val_mae: 1.6761\n",
            "Epoch 17/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.3546 - mae: 1.8929 - val_loss: 1.1751 - val_mae: 1.6749\n",
            "Epoch 18/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.3442 - mae: 1.8815 - val_loss: 1.1672 - val_mae: 1.6671\n",
            "Epoch 19/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.3346 - mae: 1.8699 - val_loss: 1.1625 - val_mae: 1.6625\n",
            "Epoch 20/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.3272 - mae: 1.8606 - val_loss: 1.1645 - val_mae: 1.6631\n",
            "Epoch 21/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.3177 - mae: 1.8506 - val_loss: 1.1571 - val_mae: 1.6548\n",
            "Epoch 22/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2941 - mae: 1.8248 - val_loss: 1.1544 - val_mae: 1.6515\n",
            "Epoch 23/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.3153 - mae: 1.8470 - val_loss: 1.1521 - val_mae: 1.6486\n",
            "Epoch 24/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2849 - mae: 1.8155 - val_loss: 1.1477 - val_mae: 1.6460\n",
            "Epoch 25/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.2796 - mae: 1.8056 - val_loss: 1.1490 - val_mae: 1.6471\n",
            "Epoch 26/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.2872 - mae: 1.8113 - val_loss: 1.1433 - val_mae: 1.6410\n",
            "Epoch 27/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.2513 - mae: 1.7737 - val_loss: 1.1395 - val_mae: 1.6358\n",
            "Epoch 28/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.2472 - mae: 1.7704 - val_loss: 1.1363 - val_mae: 1.6316\n",
            "Epoch 29/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.2392 - mae: 1.7615 - val_loss: 1.1377 - val_mae: 1.6323\n",
            "Epoch 30/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.2381 - mae: 1.7636 - val_loss: 1.1346 - val_mae: 1.6303\n",
            "Epoch 31/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.2260 - mae: 1.7472 - val_loss: 1.1331 - val_mae: 1.6279\n",
            "Epoch 32/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2359 - mae: 1.7556 - val_loss: 1.1306 - val_mae: 1.6257\n",
            "Epoch 33/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.2323 - mae: 1.7544 - val_loss: 1.1338 - val_mae: 1.6303\n",
            "Epoch 34/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.2274 - mae: 1.7484 - val_loss: 1.1301 - val_mae: 1.6254\n",
            "Epoch 35/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.2145 - mae: 1.7282 - val_loss: 1.1251 - val_mae: 1.6201\n",
            "Epoch 36/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.2150 - mae: 1.7311 - val_loss: 1.1261 - val_mae: 1.6211\n",
            "Epoch 37/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.2048 - mae: 1.7219 - val_loss: 1.1290 - val_mae: 1.6249\n",
            "Epoch 38/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.2176 - mae: 1.7356 - val_loss: 1.1257 - val_mae: 1.6224\n",
            "Epoch 39/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.1913 - mae: 1.7049 - val_loss: 1.1206 - val_mae: 1.6150\n",
            "Epoch 40/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.1887 - mae: 1.7017 - val_loss: 1.1165 - val_mae: 1.6109\n",
            "Epoch 41/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.1814 - mae: 1.6920 - val_loss: 1.1188 - val_mae: 1.6136\n",
            "Epoch 42/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.1741 - mae: 1.6863 - val_loss: 1.1219 - val_mae: 1.6171\n",
            "Epoch 43/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1737 - mae: 1.6842 - val_loss: 1.1199 - val_mae: 1.6136\n",
            "Epoch 44/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.1779 - mae: 1.6874 - val_loss: 1.1155 - val_mae: 1.6083\n",
            "Epoch 45/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1565 - mae: 1.6661 - val_loss: 1.1151 - val_mae: 1.6075\n",
            "Epoch 46/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1715 - mae: 1.6820 - val_loss: 1.1213 - val_mae: 1.6147\n",
            "Epoch 47/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1540 - mae: 1.6624 - val_loss: 1.1155 - val_mae: 1.6087\n",
            "Epoch 48/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1500 - mae: 1.6590 - val_loss: 1.1112 - val_mae: 1.6032\n",
            "Epoch 49/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1348 - mae: 1.6407 - val_loss: 1.1142 - val_mae: 1.6065\n",
            "Epoch 50/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1413 - mae: 1.6478 - val_loss: 1.1142 - val_mae: 1.6072\n",
            "Epoch 51/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1407 - mae: 1.6503 - val_loss: 1.1156 - val_mae: 1.6088\n",
            "Epoch 52/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1437 - mae: 1.6481 - val_loss: 1.1143 - val_mae: 1.6083\n",
            "Epoch 53/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1348 - mae: 1.6425 - val_loss: 1.1107 - val_mae: 1.6038\n",
            "Epoch 54/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1246 - mae: 1.6306 - val_loss: 1.1089 - val_mae: 1.6021\n",
            "Epoch 55/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1328 - mae: 1.6380 - val_loss: 1.1038 - val_mae: 1.5971\n",
            "Epoch 56/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1152 - mae: 1.6163 - val_loss: 1.1070 - val_mae: 1.6009\n",
            "Epoch 57/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1205 - mae: 1.6216 - val_loss: 1.1040 - val_mae: 1.5981\n",
            "Epoch 58/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1193 - mae: 1.6232 - val_loss: 1.1122 - val_mae: 1.6070\n",
            "Epoch 59/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1150 - mae: 1.6178 - val_loss: 1.1087 - val_mae: 1.6024\n",
            "Epoch 60/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1121 - mae: 1.6142 - val_loss: 1.1032 - val_mae: 1.5956\n",
            "Epoch 61/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1144 - mae: 1.6126 - val_loss: 1.1011 - val_mae: 1.5924\n",
            "Epoch 62/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1046 - mae: 1.6042 - val_loss: 1.1017 - val_mae: 1.5938\n",
            "Epoch 63/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1034 - mae: 1.6032 - val_loss: 1.1070 - val_mae: 1.6010\n",
            "Epoch 64/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1055 - mae: 1.6044 - val_loss: 1.0968 - val_mae: 1.5872\n",
            "Epoch 65/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0980 - mae: 1.5957 - val_loss: 1.0956 - val_mae: 1.5858\n",
            "Epoch 66/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0989 - mae: 1.5985 - val_loss: 1.0967 - val_mae: 1.5879\n",
            "Epoch 67/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1057 - mae: 1.6067 - val_loss: 1.0989 - val_mae: 1.5894\n",
            "Epoch 68/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0910 - mae: 1.5888 - val_loss: 1.1026 - val_mae: 1.5934\n",
            "Epoch 69/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0888 - mae: 1.5857 - val_loss: 1.0982 - val_mae: 1.5880\n",
            "Epoch 70/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0686 - mae: 1.5638 - val_loss: 1.0946 - val_mae: 1.5843\n",
            "Epoch 71/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0868 - mae: 1.5839 - val_loss: 1.0963 - val_mae: 1.5864\n",
            "Epoch 72/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0824 - mae: 1.5808 - val_loss: 1.0984 - val_mae: 1.5891\n",
            "Epoch 73/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0754 - mae: 1.5703 - val_loss: 1.0915 - val_mae: 1.5829\n",
            "Epoch 74/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0939 - mae: 1.5914 - val_loss: 1.0946 - val_mae: 1.5864\n",
            "Epoch 75/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0706 - mae: 1.5695 - val_loss: 1.0873 - val_mae: 1.5778\n",
            "Epoch 76/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0882 - mae: 1.5880 - val_loss: 1.0859 - val_mae: 1.5769\n",
            "Epoch 77/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0774 - mae: 1.5772 - val_loss: 1.0860 - val_mae: 1.5784\n",
            "Epoch 78/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0699 - mae: 1.5684 - val_loss: 1.0853 - val_mae: 1.5766\n",
            "Epoch 79/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0735 - mae: 1.5715 - val_loss: 1.0838 - val_mae: 1.5741\n",
            "Epoch 80/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0711 - mae: 1.5675 - val_loss: 1.0843 - val_mae: 1.5735\n",
            "Epoch 81/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0803 - mae: 1.5778 - val_loss: 1.0846 - val_mae: 1.5732\n",
            "Epoch 82/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0655 - mae: 1.5641 - val_loss: 1.0845 - val_mae: 1.5723\n",
            "Epoch 83/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.0810 - mae: 1.5799 - val_loss: 1.0845 - val_mae: 1.5721\n",
            "Epoch 84/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0470 - mae: 1.5411 - val_loss: 1.0862 - val_mae: 1.5730\n",
            "Epoch 85/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0647 - mae: 1.5586 - val_loss: 1.0863 - val_mae: 1.5735\n",
            "Epoch 86/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0875 - mae: 1.5840 - val_loss: 1.0823 - val_mae: 1.5690\n",
            "Epoch 87/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0689 - mae: 1.5670 - val_loss: 1.0804 - val_mae: 1.5662\n",
            "Epoch 88/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0564 - mae: 1.5496 - val_loss: 1.0827 - val_mae: 1.5678\n",
            "Epoch 89/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0557 - mae: 1.5484 - val_loss: 1.0801 - val_mae: 1.5650\n",
            "Epoch 90/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0489 - mae: 1.5416 - val_loss: 1.0772 - val_mae: 1.5614\n",
            "Epoch 91/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0560 - mae: 1.5512 - val_loss: 1.0748 - val_mae: 1.5594\n",
            "Epoch 92/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0437 - mae: 1.5356 - val_loss: 1.0783 - val_mae: 1.5624\n",
            "Epoch 93/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0446 - mae: 1.5371 - val_loss: 1.0737 - val_mae: 1.5579\n",
            "Epoch 94/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0518 - mae: 1.5430 - val_loss: 1.0729 - val_mae: 1.5564\n",
            "Epoch 95/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0529 - mae: 1.5477 - val_loss: 1.0694 - val_mae: 1.5521\n",
            "Epoch 96/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0353 - mae: 1.5268 - val_loss: 1.0667 - val_mae: 1.5491\n",
            "Epoch 97/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0445 - mae: 1.5376 - val_loss: 1.0648 - val_mae: 1.5480\n",
            "Epoch 98/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0485 - mae: 1.5398 - val_loss: 1.0638 - val_mae: 1.5467\n",
            "Epoch 99/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0277 - mae: 1.5224 - val_loss: 1.0630 - val_mae: 1.5457\n",
            "Epoch 100/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0412 - mae: 1.5310 - val_loss: 1.0602 - val_mae: 1.5427\n",
            "Epoch 101/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0313 - mae: 1.5225 - val_loss: 1.0597 - val_mae: 1.5427\n",
            "Epoch 102/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0256 - mae: 1.5114 - val_loss: 1.0626 - val_mae: 1.5453\n",
            "Epoch 103/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0268 - mae: 1.5178 - val_loss: 1.0644 - val_mae: 1.5465\n",
            "Epoch 104/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0279 - mae: 1.5174 - val_loss: 1.0605 - val_mae: 1.5421\n",
            "Epoch 105/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0248 - mae: 1.5152 - val_loss: 1.0578 - val_mae: 1.5391\n",
            "Epoch 106/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0272 - mae: 1.5184 - val_loss: 1.0596 - val_mae: 1.5419\n",
            "Epoch 107/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0237 - mae: 1.5157 - val_loss: 1.0604 - val_mae: 1.5430\n",
            "Epoch 108/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0233 - mae: 1.5158 - val_loss: 1.0583 - val_mae: 1.5404\n",
            "Epoch 109/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0354 - mae: 1.5238 - val_loss: 1.0533 - val_mae: 1.5352\n",
            "Epoch 110/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0105 - mae: 1.4972 - val_loss: 1.0538 - val_mae: 1.5350\n",
            "Epoch 111/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0159 - mae: 1.5105 - val_loss: 1.0502 - val_mae: 1.5309\n",
            "Epoch 112/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0243 - mae: 1.5143 - val_loss: 1.0545 - val_mae: 1.5351\n",
            "Epoch 113/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0212 - mae: 1.5118 - val_loss: 1.0502 - val_mae: 1.5311\n",
            "Epoch 114/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0218 - mae: 1.5148 - val_loss: 1.0466 - val_mae: 1.5284\n",
            "Epoch 115/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0297 - mae: 1.5234 - val_loss: 1.0456 - val_mae: 1.5281\n",
            "Epoch 116/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0267 - mae: 1.5181 - val_loss: 1.0460 - val_mae: 1.5288\n",
            "Epoch 117/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0191 - mae: 1.5078 - val_loss: 1.0467 - val_mae: 1.5284\n",
            "Epoch 118/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0114 - mae: 1.4992 - val_loss: 1.0508 - val_mae: 1.5345\n",
            "Epoch 119/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0087 - mae: 1.4963 - val_loss: 1.0512 - val_mae: 1.5337\n",
            "Epoch 120/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0139 - mae: 1.5019 - val_loss: 1.0429 - val_mae: 1.5244\n",
            "Epoch 121/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0257 - mae: 1.5184 - val_loss: 1.0424 - val_mae: 1.5233\n",
            "Epoch 122/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0129 - mae: 1.5047 - val_loss: 1.0410 - val_mae: 1.5224\n",
            "Epoch 123/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0078 - mae: 1.4980 - val_loss: 1.0420 - val_mae: 1.5232\n",
            "Epoch 124/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0095 - mae: 1.4959 - val_loss: 1.0409 - val_mae: 1.5230\n",
            "Epoch 125/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0008 - mae: 1.4884 - val_loss: 1.0392 - val_mae: 1.5214\n",
            "Epoch 126/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0049 - mae: 1.4930 - val_loss: 1.0410 - val_mae: 1.5227\n",
            "Epoch 127/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9910 - mae: 1.4772 - val_loss: 1.0371 - val_mae: 1.5179\n",
            "Epoch 128/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9991 - mae: 1.4893 - val_loss: 1.0372 - val_mae: 1.5164\n",
            "Epoch 129/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0079 - mae: 1.4973 - val_loss: 1.0322 - val_mae: 1.5095\n",
            "Epoch 130/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0080 - mae: 1.4952 - val_loss: 1.0334 - val_mae: 1.5119\n",
            "Epoch 131/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9946 - mae: 1.4807 - val_loss: 1.0317 - val_mae: 1.5097\n",
            "Epoch 132/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0030 - mae: 1.4929 - val_loss: 1.0322 - val_mae: 1.5114\n",
            "Epoch 133/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0086 - mae: 1.4983 - val_loss: 1.0299 - val_mae: 1.5083\n",
            "Epoch 134/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9969 - mae: 1.4823 - val_loss: 1.0318 - val_mae: 1.5120\n",
            "Epoch 135/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9921 - mae: 1.4805 - val_loss: 1.0291 - val_mae: 1.5089\n",
            "Epoch 136/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9944 - mae: 1.4835 - val_loss: 1.0304 - val_mae: 1.5110\n",
            "Epoch 137/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9983 - mae: 1.4875 - val_loss: 1.0325 - val_mae: 1.5137\n",
            "Epoch 138/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9838 - mae: 1.4690 - val_loss: 1.0298 - val_mae: 1.5105\n",
            "Epoch 139/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9900 - mae: 1.4791 - val_loss: 1.0297 - val_mae: 1.5104\n",
            "Epoch 140/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9816 - mae: 1.4650 - val_loss: 1.0302 - val_mae: 1.5102\n",
            "Epoch 141/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9860 - mae: 1.4709 - val_loss: 1.0298 - val_mae: 1.5097\n",
            "Epoch 142/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9796 - mae: 1.4665 - val_loss: 1.0267 - val_mae: 1.5054\n",
            "Epoch 143/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9877 - mae: 1.4727 - val_loss: 1.0232 - val_mae: 1.5009\n",
            "Epoch 144/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9898 - mae: 1.4764 - val_loss: 1.0205 - val_mae: 1.4972\n",
            "Epoch 145/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9746 - mae: 1.4578 - val_loss: 1.0218 - val_mae: 1.5001\n",
            "Epoch 146/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9948 - mae: 1.4801 - val_loss: 1.0198 - val_mae: 1.4975\n",
            "Epoch 147/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9861 - mae: 1.4729 - val_loss: 1.0232 - val_mae: 1.5012\n",
            "Epoch 148/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9787 - mae: 1.4640 - val_loss: 1.0215 - val_mae: 1.4978\n",
            "Epoch 149/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9736 - mae: 1.4585 - val_loss: 1.0212 - val_mae: 1.4977\n",
            "Epoch 150/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9705 - mae: 1.4512 - val_loss: 1.0202 - val_mae: 1.4968\n",
            "Epoch 151/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9842 - mae: 1.4688 - val_loss: 1.0187 - val_mae: 1.4958\n",
            "Epoch 152/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9725 - mae: 1.4604 - val_loss: 1.0165 - val_mae: 1.4928\n",
            "Epoch 153/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9823 - mae: 1.4637 - val_loss: 1.0137 - val_mae: 1.4902\n",
            "Epoch 154/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9731 - mae: 1.4582 - val_loss: 1.0179 - val_mae: 1.4962\n",
            "Epoch 155/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9730 - mae: 1.4570 - val_loss: 1.0128 - val_mae: 1.4909\n",
            "Epoch 156/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9641 - mae: 1.4477 - val_loss: 1.0144 - val_mae: 1.4922\n",
            "Epoch 157/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9698 - mae: 1.4566 - val_loss: 1.0132 - val_mae: 1.4901\n",
            "Epoch 158/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9721 - mae: 1.4586 - val_loss: 1.0125 - val_mae: 1.4902\n",
            "Epoch 159/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9854 - mae: 1.4720 - val_loss: 1.0125 - val_mae: 1.4905\n",
            "Epoch 160/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9650 - mae: 1.4492 - val_loss: 1.0141 - val_mae: 1.4926\n",
            "Epoch 161/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9707 - mae: 1.4540 - val_loss: 1.0105 - val_mae: 1.4894\n",
            "Epoch 162/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9645 - mae: 1.4486 - val_loss: 1.0115 - val_mae: 1.4897\n",
            "Epoch 163/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.9647 - mae: 1.4459 - val_loss: 1.0076 - val_mae: 1.4849\n",
            "Epoch 164/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9619 - mae: 1.4438 - val_loss: 1.0095 - val_mae: 1.4868\n",
            "Epoch 165/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9649 - mae: 1.4513 - val_loss: 1.0079 - val_mae: 1.4841\n",
            "Epoch 166/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.9836 - mae: 1.4704 - val_loss: 1.0087 - val_mae: 1.4860\n",
            "Epoch 167/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.9779 - mae: 1.4621 - val_loss: 1.0054 - val_mae: 1.4813\n",
            "Epoch 168/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.9717 - mae: 1.4537 - val_loss: 1.0027 - val_mae: 1.4775\n",
            "Epoch 169/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9609 - mae: 1.4426 - val_loss: 1.0038 - val_mae: 1.4805\n",
            "Epoch 170/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9685 - mae: 1.4584 - val_loss: 1.0069 - val_mae: 1.4832\n",
            "Epoch 171/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9646 - mae: 1.4496 - val_loss: 1.0084 - val_mae: 1.4846\n",
            "Epoch 172/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9590 - mae: 1.4443 - val_loss: 1.0083 - val_mae: 1.4848\n",
            "Epoch 173/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9535 - mae: 1.4337 - val_loss: 1.0042 - val_mae: 1.4813\n",
            "Epoch 174/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9607 - mae: 1.4445 - val_loss: 0.9996 - val_mae: 1.4762\n",
            "Epoch 175/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9917 - mae: 1.4773 - val_loss: 0.9997 - val_mae: 1.4762\n",
            "Epoch 176/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9678 - mae: 1.4496 - val_loss: 0.9989 - val_mae: 1.4779\n",
            "Epoch 177/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9726 - mae: 1.4544 - val_loss: 1.0006 - val_mae: 1.4792\n",
            "Epoch 178/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.9576 - mae: 1.4402 - val_loss: 0.9981 - val_mae: 1.4737\n",
            "Epoch 179/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9501 - mae: 1.4318 - val_loss: 0.9959 - val_mae: 1.4697\n",
            "Epoch 180/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9525 - mae: 1.4329 - val_loss: 0.9989 - val_mae: 1.4748\n",
            "Epoch 181/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9628 - mae: 1.4455 - val_loss: 0.9948 - val_mae: 1.4685\n",
            "Epoch 182/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9598 - mae: 1.4425 - val_loss: 0.9948 - val_mae: 1.4670\n",
            "Epoch 183/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9599 - mae: 1.4419 - val_loss: 0.9926 - val_mae: 1.4644\n",
            "Epoch 184/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9712 - mae: 1.4578 - val_loss: 0.9913 - val_mae: 1.4641\n",
            "Epoch 185/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9621 - mae: 1.4424 - val_loss: 0.9945 - val_mae: 1.4673\n",
            "Epoch 186/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9548 - mae: 1.4379 - val_loss: 0.9950 - val_mae: 1.4690\n",
            "Epoch 187/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9560 - mae: 1.4360 - val_loss: 0.9927 - val_mae: 1.4673\n",
            "Epoch 188/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9760 - mae: 1.4582 - val_loss: 0.9906 - val_mae: 1.4657\n",
            "Epoch 189/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9567 - mae: 1.4386 - val_loss: 0.9913 - val_mae: 1.4652\n",
            "Epoch 190/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9449 - mae: 1.4245 - val_loss: 0.9914 - val_mae: 1.4649\n",
            "Epoch 191/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9537 - mae: 1.4359 - val_loss: 0.9898 - val_mae: 1.4632\n",
            "Epoch 192/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9479 - mae: 1.4268 - val_loss: 0.9902 - val_mae: 1.4647\n",
            "Epoch 193/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9536 - mae: 1.4358 - val_loss: 0.9902 - val_mae: 1.4659\n",
            "Epoch 194/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9540 - mae: 1.4357 - val_loss: 0.9898 - val_mae: 1.4655\n",
            "Epoch 195/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9530 - mae: 1.4351 - val_loss: 0.9876 - val_mae: 1.4619\n",
            "Epoch 196/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9472 - mae: 1.4272 - val_loss: 0.9868 - val_mae: 1.4596\n",
            "Epoch 197/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9556 - mae: 1.4358 - val_loss: 0.9890 - val_mae: 1.4633\n",
            "Epoch 198/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9671 - mae: 1.4499 - val_loss: 0.9889 - val_mae: 1.4617\n",
            "Epoch 199/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9470 - mae: 1.4312 - val_loss: 0.9877 - val_mae: 1.4618\n",
            "Epoch 200/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9341 - mae: 1.4138 - val_loss: 0.9850 - val_mae: 1.4582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:20:24,979] Trial 12 finished with value: 0.9849500060081482 and parameters: {'dropout_2': 0.1236419058583895, 'dropout_3': 0.1889347508191036, 'dropout_4': 0.3277335382204417, 'dropout_5': 0.4690397739911043, 'learning_rate': 9.119174236931316e-05, 'epochs': 200, 'batch_size': 64}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "54/54 [==============================] - 3s 11ms/step - loss: 1.7755 - mae: 2.3421 - val_loss: 1.3661 - val_mae: 1.8406\n",
            "Epoch 2/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.7467 - mae: 2.3105 - val_loss: 1.3599 - val_mae: 1.8535\n",
            "Epoch 3/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.6546 - mae: 2.2147 - val_loss: 1.3521 - val_mae: 1.8571\n",
            "Epoch 4/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.6227 - mae: 2.1766 - val_loss: 1.3430 - val_mae: 1.8499\n",
            "Epoch 5/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5445 - mae: 2.0921 - val_loss: 1.3202 - val_mae: 1.8209\n",
            "Epoch 6/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5447 - mae: 2.0948 - val_loss: 1.3062 - val_mae: 1.8039\n",
            "Epoch 7/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.5039 - mae: 2.0520 - val_loss: 1.2851 - val_mae: 1.7766\n",
            "Epoch 8/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.4875 - mae: 2.0360 - val_loss: 1.2650 - val_mae: 1.7505\n",
            "Epoch 9/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.4537 - mae: 1.9920 - val_loss: 1.2476 - val_mae: 1.7284\n",
            "Epoch 10/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.4382 - mae: 1.9789 - val_loss: 1.2312 - val_mae: 1.7104\n",
            "Epoch 11/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.4315 - mae: 1.9671 - val_loss: 1.2174 - val_mae: 1.7012\n",
            "Epoch 12/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.3787 - mae: 1.9112 - val_loss: 1.2106 - val_mae: 1.6969\n",
            "Epoch 13/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.3774 - mae: 1.9089 - val_loss: 1.2072 - val_mae: 1.6928\n",
            "Epoch 14/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.3582 - mae: 1.8867 - val_loss: 1.1964 - val_mae: 1.6804\n",
            "Epoch 15/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.3462 - mae: 1.8744 - val_loss: 1.1924 - val_mae: 1.6757\n",
            "Epoch 16/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.3474 - mae: 1.8749 - val_loss: 1.1937 - val_mae: 1.6762\n",
            "Epoch 17/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.3112 - mae: 1.8324 - val_loss: 1.1862 - val_mae: 1.6664\n",
            "Epoch 18/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.3044 - mae: 1.8246 - val_loss: 1.1818 - val_mae: 1.6630\n",
            "Epoch 19/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.2791 - mae: 1.7950 - val_loss: 1.1805 - val_mae: 1.6602\n",
            "Epoch 20/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2851 - mae: 1.8049 - val_loss: 1.1751 - val_mae: 1.6547\n",
            "Epoch 21/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2769 - mae: 1.7947 - val_loss: 1.1718 - val_mae: 1.6521\n",
            "Epoch 22/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.2541 - mae: 1.7689 - val_loss: 1.1651 - val_mae: 1.6454\n",
            "Epoch 23/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2598 - mae: 1.7732 - val_loss: 1.1568 - val_mae: 1.6368\n",
            "Epoch 24/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2449 - mae: 1.7565 - val_loss: 1.1551 - val_mae: 1.6351\n",
            "Epoch 25/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.2475 - mae: 1.7598 - val_loss: 1.1512 - val_mae: 1.6318\n",
            "Epoch 26/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.2326 - mae: 1.7409 - val_loss: 1.1514 - val_mae: 1.6316\n",
            "Epoch 27/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.2286 - mae: 1.7367 - val_loss: 1.1497 - val_mae: 1.6302\n",
            "Epoch 28/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.2326 - mae: 1.7425 - val_loss: 1.1467 - val_mae: 1.6268\n",
            "Epoch 29/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.2184 - mae: 1.7259 - val_loss: 1.1399 - val_mae: 1.6197\n",
            "Epoch 30/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.2071 - mae: 1.7109 - val_loss: 1.1375 - val_mae: 1.6145\n",
            "Epoch 31/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.1911 - mae: 1.6936 - val_loss: 1.1344 - val_mae: 1.6123\n",
            "Epoch 32/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.1930 - mae: 1.6944 - val_loss: 1.1345 - val_mae: 1.6117\n",
            "Epoch 33/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.1927 - mae: 1.6950 - val_loss: 1.1320 - val_mae: 1.6108\n",
            "Epoch 34/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.1913 - mae: 1.6937 - val_loss: 1.1293 - val_mae: 1.6066\n",
            "Epoch 35/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.1683 - mae: 1.6679 - val_loss: 1.1296 - val_mae: 1.6095\n",
            "Epoch 36/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.1841 - mae: 1.6870 - val_loss: 1.1239 - val_mae: 1.6021\n",
            "Epoch 37/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1614 - mae: 1.6585 - val_loss: 1.1211 - val_mae: 1.5989\n",
            "Epoch 38/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1667 - mae: 1.6640 - val_loss: 1.1187 - val_mae: 1.5949\n",
            "Epoch 39/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1649 - mae: 1.6638 - val_loss: 1.1232 - val_mae: 1.6012\n",
            "Epoch 40/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1596 - mae: 1.6566 - val_loss: 1.1190 - val_mae: 1.5970\n",
            "Epoch 41/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1526 - mae: 1.6496 - val_loss: 1.1174 - val_mae: 1.5945\n",
            "Epoch 42/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1427 - mae: 1.6382 - val_loss: 1.1162 - val_mae: 1.5945\n",
            "Epoch 43/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1405 - mae: 1.6373 - val_loss: 1.1174 - val_mae: 1.5974\n",
            "Epoch 44/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1348 - mae: 1.6353 - val_loss: 1.1141 - val_mae: 1.5927\n",
            "Epoch 45/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1335 - mae: 1.6310 - val_loss: 1.1102 - val_mae: 1.5871\n",
            "Epoch 46/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1218 - mae: 1.6175 - val_loss: 1.1140 - val_mae: 1.5925\n",
            "Epoch 47/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1295 - mae: 1.6235 - val_loss: 1.1105 - val_mae: 1.5878\n",
            "Epoch 48/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1279 - mae: 1.6200 - val_loss: 1.1109 - val_mae: 1.5897\n",
            "Epoch 49/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1270 - mae: 1.6230 - val_loss: 1.1100 - val_mae: 1.5887\n",
            "Epoch 50/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1304 - mae: 1.6229 - val_loss: 1.1046 - val_mae: 1.5832\n",
            "Epoch 51/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1208 - mae: 1.6157 - val_loss: 1.0993 - val_mae: 1.5771\n",
            "Epoch 52/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0944 - mae: 1.5861 - val_loss: 1.1004 - val_mae: 1.5796\n",
            "Epoch 53/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1263 - mae: 1.6219 - val_loss: 1.0968 - val_mae: 1.5718\n",
            "Epoch 54/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1070 - mae: 1.6007 - val_loss: 1.0972 - val_mae: 1.5744\n",
            "Epoch 55/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0961 - mae: 1.5876 - val_loss: 1.0936 - val_mae: 1.5686\n",
            "Epoch 56/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0935 - mae: 1.5828 - val_loss: 1.0870 - val_mae: 1.5628\n",
            "Epoch 57/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0872 - mae: 1.5797 - val_loss: 1.0874 - val_mae: 1.5642\n",
            "Epoch 58/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1068 - mae: 1.5973 - val_loss: 1.0865 - val_mae: 1.5622\n",
            "Epoch 59/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1114 - mae: 1.6058 - val_loss: 1.0890 - val_mae: 1.5649\n",
            "Epoch 60/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0772 - mae: 1.5667 - val_loss: 1.0845 - val_mae: 1.5586\n",
            "Epoch 61/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0869 - mae: 1.5777 - val_loss: 1.0842 - val_mae: 1.5617\n",
            "Epoch 62/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0846 - mae: 1.5756 - val_loss: 1.0868 - val_mae: 1.5642\n",
            "Epoch 63/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0837 - mae: 1.5737 - val_loss: 1.0940 - val_mae: 1.5738\n",
            "Epoch 64/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0751 - mae: 1.5660 - val_loss: 1.0830 - val_mae: 1.5599\n",
            "Epoch 65/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0679 - mae: 1.5576 - val_loss: 1.0799 - val_mae: 1.5571\n",
            "Epoch 66/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0723 - mae: 1.5601 - val_loss: 1.0736 - val_mae: 1.5496\n",
            "Epoch 67/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0801 - mae: 1.5706 - val_loss: 1.0709 - val_mae: 1.5472\n",
            "Epoch 68/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0606 - mae: 1.5468 - val_loss: 1.0669 - val_mae: 1.5436\n",
            "Epoch 69/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0848 - mae: 1.5762 - val_loss: 1.0662 - val_mae: 1.5415\n",
            "Epoch 70/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0615 - mae: 1.5522 - val_loss: 1.0681 - val_mae: 1.5456\n",
            "Epoch 71/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0749 - mae: 1.5659 - val_loss: 1.0672 - val_mae: 1.5444\n",
            "Epoch 72/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0640 - mae: 1.5554 - val_loss: 1.0631 - val_mae: 1.5401\n",
            "Epoch 73/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.0689 - mae: 1.5574 - val_loss: 1.0644 - val_mae: 1.5417\n",
            "Epoch 74/200\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 1.0642 - mae: 1.5551 - val_loss: 1.0652 - val_mae: 1.5405\n",
            "Epoch 75/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0808 - mae: 1.5722 - val_loss: 1.0651 - val_mae: 1.5427\n",
            "Epoch 76/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0515 - mae: 1.5395 - val_loss: 1.0673 - val_mae: 1.5444\n",
            "Epoch 77/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0748 - mae: 1.5686 - val_loss: 1.0619 - val_mae: 1.5382\n",
            "Epoch 78/200\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 1.0677 - mae: 1.5601 - val_loss: 1.0631 - val_mae: 1.5389\n",
            "Epoch 79/200\n",
            "54/54 [==============================] - 1s 9ms/step - loss: 1.0455 - mae: 1.5326 - val_loss: 1.0602 - val_mae: 1.5364\n",
            "Epoch 80/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0573 - mae: 1.5451 - val_loss: 1.0565 - val_mae: 1.5327\n",
            "Epoch 81/200\n",
            "54/54 [==============================] - 1s 10ms/step - loss: 1.0343 - mae: 1.5203 - val_loss: 1.0550 - val_mae: 1.5329\n",
            "Epoch 82/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.0548 - mae: 1.5444 - val_loss: 1.0557 - val_mae: 1.5346\n",
            "Epoch 83/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0405 - mae: 1.5278 - val_loss: 1.0556 - val_mae: 1.5339\n",
            "Epoch 84/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.0488 - mae: 1.5386 - val_loss: 1.0536 - val_mae: 1.5301\n",
            "Epoch 85/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.0445 - mae: 1.5335 - val_loss: 1.0503 - val_mae: 1.5268\n",
            "Epoch 86/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0490 - mae: 1.5407 - val_loss: 1.0511 - val_mae: 1.5274\n",
            "Epoch 87/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0406 - mae: 1.5279 - val_loss: 1.0504 - val_mae: 1.5263\n",
            "Epoch 88/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0328 - mae: 1.5241 - val_loss: 1.0507 - val_mae: 1.5285\n",
            "Epoch 89/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0337 - mae: 1.5217 - val_loss: 1.0481 - val_mae: 1.5244\n",
            "Epoch 90/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0651 - mae: 1.5548 - val_loss: 1.0474 - val_mae: 1.5243\n",
            "Epoch 91/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0218 - mae: 1.5091 - val_loss: 1.0457 - val_mae: 1.5224\n",
            "Epoch 92/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0191 - mae: 1.5044 - val_loss: 1.0407 - val_mae: 1.5170\n",
            "Epoch 93/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0221 - mae: 1.5079 - val_loss: 1.0438 - val_mae: 1.5216\n",
            "Epoch 94/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0194 - mae: 1.5067 - val_loss: 1.0377 - val_mae: 1.5149\n",
            "Epoch 95/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0425 - mae: 1.5305 - val_loss: 1.0353 - val_mae: 1.5120\n",
            "Epoch 96/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0334 - mae: 1.5204 - val_loss: 1.0391 - val_mae: 1.5181\n",
            "Epoch 97/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0338 - mae: 1.5239 - val_loss: 1.0435 - val_mae: 1.5219\n",
            "Epoch 98/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0200 - mae: 1.5087 - val_loss: 1.0369 - val_mae: 1.5129\n",
            "Epoch 99/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0166 - mae: 1.5033 - val_loss: 1.0427 - val_mae: 1.5204\n",
            "Epoch 100/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0269 - mae: 1.5142 - val_loss: 1.0379 - val_mae: 1.5153\n",
            "Epoch 101/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0314 - mae: 1.5232 - val_loss: 1.0343 - val_mae: 1.5127\n",
            "Epoch 102/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0397 - mae: 1.5293 - val_loss: 1.0346 - val_mae: 1.5129\n",
            "Epoch 103/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0149 - mae: 1.4997 - val_loss: 1.0327 - val_mae: 1.5091\n",
            "Epoch 104/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0360 - mae: 1.5266 - val_loss: 1.0319 - val_mae: 1.5081\n",
            "Epoch 105/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0269 - mae: 1.5189 - val_loss: 1.0303 - val_mae: 1.5058\n",
            "Epoch 106/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0301 - mae: 1.5199 - val_loss: 1.0343 - val_mae: 1.5112\n",
            "Epoch 107/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0129 - mae: 1.5001 - val_loss: 1.0395 - val_mae: 1.5177\n",
            "Epoch 108/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0261 - mae: 1.5183 - val_loss: 1.0309 - val_mae: 1.5079\n",
            "Epoch 109/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0184 - mae: 1.5061 - val_loss: 1.0309 - val_mae: 1.5087\n",
            "Epoch 110/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0064 - mae: 1.4930 - val_loss: 1.0313 - val_mae: 1.5093\n",
            "Epoch 111/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0255 - mae: 1.5154 - val_loss: 1.0245 - val_mae: 1.4994\n",
            "Epoch 112/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0216 - mae: 1.5099 - val_loss: 1.0266 - val_mae: 1.5040\n",
            "Epoch 113/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0137 - mae: 1.5021 - val_loss: 1.0263 - val_mae: 1.5042\n",
            "Epoch 114/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0107 - mae: 1.5003 - val_loss: 1.0174 - val_mae: 1.4933\n",
            "Epoch 115/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0153 - mae: 1.5008 - val_loss: 1.0253 - val_mae: 1.5011\n",
            "Epoch 116/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0217 - mae: 1.5111 - val_loss: 1.0182 - val_mae: 1.4948\n",
            "Epoch 117/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9958 - mae: 1.4834 - val_loss: 1.0168 - val_mae: 1.4933\n",
            "Epoch 118/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0083 - mae: 1.4950 - val_loss: 1.0201 - val_mae: 1.4977\n",
            "Epoch 119/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0298 - mae: 1.5213 - val_loss: 1.0140 - val_mae: 1.4894\n",
            "Epoch 120/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0063 - mae: 1.4925 - val_loss: 1.0147 - val_mae: 1.4901\n",
            "Epoch 121/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9951 - mae: 1.4820 - val_loss: 1.0143 - val_mae: 1.4921\n",
            "Epoch 122/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9999 - mae: 1.4864 - val_loss: 1.0148 - val_mae: 1.4913\n",
            "Epoch 123/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0037 - mae: 1.4920 - val_loss: 1.0136 - val_mae: 1.4877\n",
            "Epoch 124/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0058 - mae: 1.4945 - val_loss: 1.0107 - val_mae: 1.4863\n",
            "Epoch 125/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.9955 - mae: 1.4799 - val_loss: 1.0053 - val_mae: 1.4828\n",
            "Epoch 126/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0174 - mae: 1.5076 - val_loss: 1.0097 - val_mae: 1.4868\n",
            "Epoch 127/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9955 - mae: 1.4793 - val_loss: 1.0120 - val_mae: 1.4894\n",
            "Epoch 128/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0036 - mae: 1.4900 - val_loss: 1.0092 - val_mae: 1.4867\n",
            "Epoch 129/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9898 - mae: 1.4724 - val_loss: 1.0103 - val_mae: 1.4862\n",
            "Epoch 130/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0029 - mae: 1.4865 - val_loss: 1.0089 - val_mae: 1.4848\n",
            "Epoch 131/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9982 - mae: 1.4871 - val_loss: 1.0049 - val_mae: 1.4791\n",
            "Epoch 132/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9942 - mae: 1.4818 - val_loss: 1.0040 - val_mae: 1.4787\n",
            "Epoch 133/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9924 - mae: 1.4772 - val_loss: 1.0026 - val_mae: 1.4776\n",
            "Epoch 134/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9940 - mae: 1.4781 - val_loss: 0.9981 - val_mae: 1.4735\n",
            "Epoch 135/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9984 - mae: 1.4857 - val_loss: 0.9986 - val_mae: 1.4725\n",
            "Epoch 136/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9848 - mae: 1.4701 - val_loss: 0.9981 - val_mae: 1.4711\n",
            "Epoch 137/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9822 - mae: 1.4674 - val_loss: 0.9998 - val_mae: 1.4735\n",
            "Epoch 138/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9931 - mae: 1.4777 - val_loss: 0.9927 - val_mae: 1.4655\n",
            "Epoch 139/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9889 - mae: 1.4746 - val_loss: 0.9998 - val_mae: 1.4745\n",
            "Epoch 140/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9861 - mae: 1.4739 - val_loss: 0.9916 - val_mae: 1.4646\n",
            "Epoch 141/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9854 - mae: 1.4696 - val_loss: 0.9960 - val_mae: 1.4697\n",
            "Epoch 142/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9786 - mae: 1.4619 - val_loss: 0.9894 - val_mae: 1.4658\n",
            "Epoch 143/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9785 - mae: 1.4642 - val_loss: 0.9903 - val_mae: 1.4653\n",
            "Epoch 144/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9718 - mae: 1.4552 - val_loss: 0.9876 - val_mae: 1.4606\n",
            "Epoch 145/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9863 - mae: 1.4736 - val_loss: 0.9881 - val_mae: 1.4628\n",
            "Epoch 146/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9839 - mae: 1.4725 - val_loss: 0.9916 - val_mae: 1.4668\n",
            "Epoch 147/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0056 - mae: 1.4943 - val_loss: 0.9968 - val_mae: 1.4715\n",
            "Epoch 148/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9762 - mae: 1.4585 - val_loss: 0.9875 - val_mae: 1.4634\n",
            "Epoch 149/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9786 - mae: 1.4663 - val_loss: 0.9886 - val_mae: 1.4635\n",
            "Epoch 150/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9966 - mae: 1.4845 - val_loss: 0.9883 - val_mae: 1.4643\n",
            "Epoch 151/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9808 - mae: 1.4660 - val_loss: 0.9840 - val_mae: 1.4564\n",
            "Epoch 152/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9841 - mae: 1.4715 - val_loss: 0.9885 - val_mae: 1.4631\n",
            "Epoch 153/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9797 - mae: 1.4650 - val_loss: 0.9819 - val_mae: 1.4588\n",
            "Epoch 154/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.9911 - mae: 1.4794 - val_loss: 0.9867 - val_mae: 1.4617\n",
            "Epoch 155/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9635 - mae: 1.4500 - val_loss: 0.9914 - val_mae: 1.4662\n",
            "Epoch 156/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9798 - mae: 1.4642 - val_loss: 0.9906 - val_mae: 1.4665\n",
            "Epoch 157/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9668 - mae: 1.4489 - val_loss: 0.9845 - val_mae: 1.4570\n",
            "Epoch 158/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9713 - mae: 1.4568 - val_loss: 0.9889 - val_mae: 1.4635\n",
            "Epoch 159/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9819 - mae: 1.4663 - val_loss: 0.9806 - val_mae: 1.4530\n",
            "Epoch 160/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9643 - mae: 1.4481 - val_loss: 0.9778 - val_mae: 1.4496\n",
            "Epoch 161/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9870 - mae: 1.4721 - val_loss: 0.9755 - val_mae: 1.4475\n",
            "Epoch 162/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.9736 - mae: 1.4582 - val_loss: 0.9828 - val_mae: 1.4567\n",
            "Epoch 163/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.9663 - mae: 1.4484 - val_loss: 0.9753 - val_mae: 1.4469\n",
            "Epoch 164/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9874 - mae: 1.4731 - val_loss: 0.9729 - val_mae: 1.4440\n",
            "Epoch 165/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9580 - mae: 1.4404 - val_loss: 0.9773 - val_mae: 1.4478\n",
            "Epoch 166/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9956 - mae: 1.4807 - val_loss: 0.9766 - val_mae: 1.4478\n",
            "Epoch 167/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9593 - mae: 1.4409 - val_loss: 0.9792 - val_mae: 1.4513\n",
            "Epoch 168/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.9711 - mae: 1.4572 - val_loss: 0.9784 - val_mae: 1.4493\n",
            "Epoch 169/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9691 - mae: 1.4555 - val_loss: 0.9736 - val_mae: 1.4429\n",
            "Epoch 170/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9487 - mae: 1.4288 - val_loss: 0.9724 - val_mae: 1.4418\n",
            "Epoch 171/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9698 - mae: 1.4539 - val_loss: 0.9693 - val_mae: 1.4393\n",
            "Epoch 172/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9640 - mae: 1.4487 - val_loss: 0.9664 - val_mae: 1.4379\n",
            "Epoch 173/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9715 - mae: 1.4529 - val_loss: 0.9732 - val_mae: 1.4436\n",
            "Epoch 174/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9507 - mae: 1.4328 - val_loss: 0.9727 - val_mae: 1.4419\n",
            "Epoch 175/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9653 - mae: 1.4465 - val_loss: 0.9723 - val_mae: 1.4407\n",
            "Epoch 176/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9556 - mae: 1.4365 - val_loss: 0.9780 - val_mae: 1.4476\n",
            "Epoch 177/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9641 - mae: 1.4480 - val_loss: 0.9778 - val_mae: 1.4481\n",
            "Epoch 178/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9599 - mae: 1.4411 - val_loss: 0.9754 - val_mae: 1.4436\n",
            "Epoch 179/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9612 - mae: 1.4475 - val_loss: 0.9757 - val_mae: 1.4462\n",
            "Epoch 180/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9621 - mae: 1.4462 - val_loss: 0.9670 - val_mae: 1.4340\n",
            "Epoch 181/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9563 - mae: 1.4383 - val_loss: 0.9592 - val_mae: 1.4266\n",
            "Epoch 182/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9769 - mae: 1.4614 - val_loss: 0.9676 - val_mae: 1.4367\n",
            "Epoch 183/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9652 - mae: 1.4484 - val_loss: 0.9692 - val_mae: 1.4397\n",
            "Epoch 184/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9563 - mae: 1.4388 - val_loss: 0.9688 - val_mae: 1.4405\n",
            "Epoch 185/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9560 - mae: 1.4422 - val_loss: 0.9617 - val_mae: 1.4308\n",
            "Epoch 186/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9505 - mae: 1.4354 - val_loss: 0.9676 - val_mae: 1.4355\n",
            "Epoch 187/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9564 - mae: 1.4375 - val_loss: 0.9639 - val_mae: 1.4314\n",
            "Epoch 188/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9475 - mae: 1.4302 - val_loss: 0.9628 - val_mae: 1.4312\n",
            "Epoch 189/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9582 - mae: 1.4435 - val_loss: 0.9645 - val_mae: 1.4323\n",
            "Epoch 190/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.9529 - mae: 1.4306 - val_loss: 0.9646 - val_mae: 1.4329\n",
            "Epoch 191/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.9633 - mae: 1.4460 - val_loss: 0.9609 - val_mae: 1.4283\n",
            "Epoch 192/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9482 - mae: 1.4282 - val_loss: 0.9638 - val_mae: 1.4327\n",
            "Epoch 193/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9698 - mae: 1.4513 - val_loss: 0.9642 - val_mae: 1.4332\n",
            "Epoch 194/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9576 - mae: 1.4393 - val_loss: 0.9642 - val_mae: 1.4323\n",
            "Epoch 195/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9599 - mae: 1.4439 - val_loss: 0.9570 - val_mae: 1.4239\n",
            "Epoch 196/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9401 - mae: 1.4171 - val_loss: 0.9574 - val_mae: 1.4238\n",
            "Epoch 197/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9213 - mae: 1.3926 - val_loss: 0.9519 - val_mae: 1.4184\n",
            "Epoch 198/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9337 - mae: 1.4103 - val_loss: 0.9482 - val_mae: 1.4161\n",
            "Epoch 199/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9441 - mae: 1.4217 - val_loss: 0.9550 - val_mae: 1.4235\n",
            "Epoch 200/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.9511 - mae: 1.4335 - val_loss: 0.9535 - val_mae: 1.4210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:21:35,947] Trial 13 finished with value: 0.9534856677055359 and parameters: {'dropout_2': 0.20717086705066462, 'dropout_3': 0.2596628276804146, 'dropout_4': 0.5427626585632102, 'dropout_5': 0.584379380518163, 'learning_rate': 0.00017186991217796964, 'epochs': 200, 'batch_size': 64}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 3s 7ms/step - loss: 1.9151 - mae: 2.4911 - val_loss: 1.3886 - val_mae: 1.8893\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.8764 - mae: 2.4460 - val_loss: 1.4212 - val_mae: 1.9337\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.8878 - mae: 2.4640 - val_loss: 1.4293 - val_mae: 1.9455\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.8501 - mae: 2.4247 - val_loss: 1.4282 - val_mae: 1.9442\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.8535 - mae: 2.4266 - val_loss: 1.4107 - val_mae: 1.9264\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.8546 - mae: 2.4276 - val_loss: 1.4058 - val_mae: 1.9208\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.8012 - mae: 2.3699 - val_loss: 1.3989 - val_mae: 1.9127\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.7941 - mae: 2.3624 - val_loss: 1.3850 - val_mae: 1.8976\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.7792 - mae: 2.3475 - val_loss: 1.3836 - val_mae: 1.8963\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.7669 - mae: 2.3311 - val_loss: 1.3752 - val_mae: 1.8868\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7809 - mae: 2.3494 - val_loss: 1.3745 - val_mae: 1.8861\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.7478 - mae: 2.3158 - val_loss: 1.3694 - val_mae: 1.8798\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.7362 - mae: 2.3031 - val_loss: 1.3601 - val_mae: 1.8689\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.7478 - mae: 2.3139 - val_loss: 1.3606 - val_mae: 1.8696\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7193 - mae: 2.2850 - val_loss: 1.3550 - val_mae: 1.8640\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.7368 - mae: 2.3040 - val_loss: 1.3516 - val_mae: 1.8597\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.6974 - mae: 2.2637 - val_loss: 1.3473 - val_mae: 1.8550\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.7169 - mae: 2.2812 - val_loss: 1.3412 - val_mae: 1.8483\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.6820 - mae: 2.2421 - val_loss: 1.3371 - val_mae: 1.8436\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.6929 - mae: 2.2570 - val_loss: 1.3289 - val_mae: 1.8352\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.6966 - mae: 2.2581 - val_loss: 1.3288 - val_mae: 1.8341\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.6600 - mae: 2.2214 - val_loss: 1.3302 - val_mae: 1.8360\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.6656 - mae: 2.2284 - val_loss: 1.3289 - val_mae: 1.8348\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.6546 - mae: 2.2139 - val_loss: 1.3207 - val_mae: 1.8260\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.6633 - mae: 2.2220 - val_loss: 1.3206 - val_mae: 1.8250\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.6324 - mae: 2.1904 - val_loss: 1.3155 - val_mae: 1.8198\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.6133 - mae: 2.1681 - val_loss: 1.3179 - val_mae: 1.8224\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.6296 - mae: 2.1873 - val_loss: 1.3145 - val_mae: 1.8184\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.6044 - mae: 2.1577 - val_loss: 1.3109 - val_mae: 1.8142\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.6033 - mae: 2.1617 - val_loss: 1.3107 - val_mae: 1.8127\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.6149 - mae: 2.1738 - val_loss: 1.3046 - val_mae: 1.8061\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.5957 - mae: 2.1535 - val_loss: 1.3065 - val_mae: 1.8078\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.6033 - mae: 2.1604 - val_loss: 1.3021 - val_mae: 1.8027\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.5782 - mae: 2.1327 - val_loss: 1.3015 - val_mae: 1.8016\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5631 - mae: 2.1171 - val_loss: 1.2965 - val_mae: 1.7956\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5675 - mae: 2.1211 - val_loss: 1.2920 - val_mae: 1.7923\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.5657 - mae: 2.1175 - val_loss: 1.2914 - val_mae: 1.7913\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5618 - mae: 2.1112 - val_loss: 1.2884 - val_mae: 1.7881\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5392 - mae: 2.0866 - val_loss: 1.2855 - val_mae: 1.7843\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.5465 - mae: 2.0975 - val_loss: 1.2834 - val_mae: 1.7826\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5701 - mae: 2.1237 - val_loss: 1.2814 - val_mae: 1.7813\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5159 - mae: 2.0624 - val_loss: 1.2807 - val_mae: 1.7812\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.5272 - mae: 2.0759 - val_loss: 1.2800 - val_mae: 1.7818\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5123 - mae: 2.0615 - val_loss: 1.2757 - val_mae: 1.7768\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.5506 - mae: 2.1032 - val_loss: 1.2745 - val_mae: 1.7754\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.5267 - mae: 2.0740 - val_loss: 1.2718 - val_mae: 1.7722\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.5020 - mae: 2.0473 - val_loss: 1.2685 - val_mae: 1.7677\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.5164 - mae: 2.0616 - val_loss: 1.2673 - val_mae: 1.7661\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.4973 - mae: 2.0437 - val_loss: 1.2643 - val_mae: 1.7627\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.5135 - mae: 2.0613 - val_loss: 1.2616 - val_mae: 1.7582\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4789 - mae: 2.0229 - val_loss: 1.2620 - val_mae: 1.7601\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.5232 - mae: 2.0696 - val_loss: 1.2599 - val_mae: 1.7584\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.4840 - mae: 2.0301 - val_loss: 1.2583 - val_mae: 1.7566\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4753 - mae: 2.0185 - val_loss: 1.2542 - val_mae: 1.7537\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4953 - mae: 2.0396 - val_loss: 1.2547 - val_mae: 1.7525\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.4808 - mae: 2.0254 - val_loss: 1.2513 - val_mae: 1.7486\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4566 - mae: 1.9986 - val_loss: 1.2529 - val_mae: 1.7495\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4555 - mae: 1.9958 - val_loss: 1.2488 - val_mae: 1.7439\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4777 - mae: 2.0201 - val_loss: 1.2454 - val_mae: 1.7405\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4260 - mae: 1.9650 - val_loss: 1.2450 - val_mae: 1.7390\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4608 - mae: 2.0019 - val_loss: 1.2462 - val_mae: 1.7416\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4466 - mae: 1.9838 - val_loss: 1.2420 - val_mae: 1.7359\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4365 - mae: 1.9756 - val_loss: 1.2449 - val_mae: 1.7409\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4529 - mae: 1.9933 - val_loss: 1.2393 - val_mae: 1.7345\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4445 - mae: 1.9870 - val_loss: 1.2373 - val_mae: 1.7313\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4490 - mae: 1.9890 - val_loss: 1.2348 - val_mae: 1.7283\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4269 - mae: 1.9648 - val_loss: 1.2338 - val_mae: 1.7266\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4392 - mae: 1.9786 - val_loss: 1.2319 - val_mae: 1.7248\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4360 - mae: 1.9740 - val_loss: 1.2339 - val_mae: 1.7272\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.4184 - mae: 1.9555 - val_loss: 1.2316 - val_mae: 1.7256\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4405 - mae: 1.9801 - val_loss: 1.2296 - val_mae: 1.7219\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.4323 - mae: 1.9684 - val_loss: 1.2291 - val_mae: 1.7217\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.3954 - mae: 1.9310 - val_loss: 1.2287 - val_mae: 1.7210\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.4248 - mae: 1.9616 - val_loss: 1.2243 - val_mae: 1.7164\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.4059 - mae: 1.9409 - val_loss: 1.2258 - val_mae: 1.7175\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.4013 - mae: 1.9347 - val_loss: 1.2232 - val_mae: 1.7127\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.4191 - mae: 1.9539 - val_loss: 1.2202 - val_mae: 1.7092\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.4054 - mae: 1.9381 - val_loss: 1.2236 - val_mae: 1.7133\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3821 - mae: 1.9131 - val_loss: 1.2230 - val_mae: 1.7123\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4057 - mae: 1.9417 - val_loss: 1.2208 - val_mae: 1.7105\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4004 - mae: 1.9312 - val_loss: 1.2190 - val_mae: 1.7084\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3827 - mae: 1.9148 - val_loss: 1.2150 - val_mae: 1.7040\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3793 - mae: 1.9108 - val_loss: 1.2143 - val_mae: 1.7024\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3939 - mae: 1.9237 - val_loss: 1.2130 - val_mae: 1.7020\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3721 - mae: 1.9012 - val_loss: 1.2123 - val_mae: 1.7004\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3735 - mae: 1.9041 - val_loss: 1.2108 - val_mae: 1.6994\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3753 - mae: 1.9088 - val_loss: 1.2125 - val_mae: 1.6996\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3495 - mae: 1.8796 - val_loss: 1.2165 - val_mae: 1.7043\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3785 - mae: 1.9073 - val_loss: 1.2154 - val_mae: 1.7030\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.3665 - mae: 1.8919 - val_loss: 1.2107 - val_mae: 1.6980\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3577 - mae: 1.8873 - val_loss: 1.2101 - val_mae: 1.6966\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3702 - mae: 1.8965 - val_loss: 1.2080 - val_mae: 1.6942\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3451 - mae: 1.8737 - val_loss: 1.2048 - val_mae: 1.6910\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.3466 - mae: 1.8723 - val_loss: 1.2033 - val_mae: 1.6891\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3644 - mae: 1.8899 - val_loss: 1.2051 - val_mae: 1.6909\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.3660 - mae: 1.8934 - val_loss: 1.2087 - val_mae: 1.6954\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3678 - mae: 1.8967 - val_loss: 1.2088 - val_mae: 1.6951\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3397 - mae: 1.8660 - val_loss: 1.2041 - val_mae: 1.6897\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.3471 - mae: 1.8744 - val_loss: 1.2019 - val_mae: 1.6874\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.3444 - mae: 1.8705 - val_loss: 1.2030 - val_mae: 1.6882\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3410 - mae: 1.8668 - val_loss: 1.2063 - val_mae: 1.6919\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3278 - mae: 1.8517 - val_loss: 1.2032 - val_mae: 1.6875\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3471 - mae: 1.8711 - val_loss: 1.2027 - val_mae: 1.6873\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3515 - mae: 1.8788 - val_loss: 1.1969 - val_mae: 1.6811\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3127 - mae: 1.8349 - val_loss: 1.2012 - val_mae: 1.6869\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3377 - mae: 1.8648 - val_loss: 1.1990 - val_mae: 1.6845\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3472 - mae: 1.8700 - val_loss: 1.2006 - val_mae: 1.6846\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3396 - mae: 1.8613 - val_loss: 1.2019 - val_mae: 1.6878\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3098 - mae: 1.8293 - val_loss: 1.2035 - val_mae: 1.6889\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.3137 - mae: 1.8336 - val_loss: 1.1998 - val_mae: 1.6842\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3194 - mae: 1.8387 - val_loss: 1.2013 - val_mae: 1.6874\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3127 - mae: 1.8336 - val_loss: 1.2008 - val_mae: 1.6850\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3287 - mae: 1.8504 - val_loss: 1.1961 - val_mae: 1.6801\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3176 - mae: 1.8371 - val_loss: 1.1988 - val_mae: 1.6825\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3320 - mae: 1.8542 - val_loss: 1.1991 - val_mae: 1.6844\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3110 - mae: 1.8294 - val_loss: 1.2014 - val_mae: 1.6865\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2890 - mae: 1.8053 - val_loss: 1.1972 - val_mae: 1.6806\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3170 - mae: 1.8402 - val_loss: 1.1952 - val_mae: 1.6786\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3090 - mae: 1.8281 - val_loss: 1.1956 - val_mae: 1.6795\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3006 - mae: 1.8188 - val_loss: 1.1945 - val_mae: 1.6781\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2931 - mae: 1.8062 - val_loss: 1.1931 - val_mae: 1.6751\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3083 - mae: 1.8253 - val_loss: 1.1954 - val_mae: 1.6781\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3078 - mae: 1.8261 - val_loss: 1.1943 - val_mae: 1.6759\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3027 - mae: 1.8216 - val_loss: 1.1942 - val_mae: 1.6757\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2951 - mae: 1.8133 - val_loss: 1.1928 - val_mae: 1.6740\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2968 - mae: 1.8147 - val_loss: 1.1927 - val_mae: 1.6749\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.2870 - mae: 1.8033 - val_loss: 1.1961 - val_mae: 1.6797\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.3064 - mae: 1.8250 - val_loss: 1.1920 - val_mae: 1.6742\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.2934 - mae: 1.8092 - val_loss: 1.1875 - val_mae: 1.6693\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.3098 - mae: 1.8262 - val_loss: 1.1875 - val_mae: 1.6677\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2799 - mae: 1.7941 - val_loss: 1.1903 - val_mae: 1.6706\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3066 - mae: 1.8216 - val_loss: 1.1917 - val_mae: 1.6712\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2713 - mae: 1.7838 - val_loss: 1.1946 - val_mae: 1.6766\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2770 - mae: 1.7911 - val_loss: 1.1916 - val_mae: 1.6739\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2659 - mae: 1.7785 - val_loss: 1.1918 - val_mae: 1.6714\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2827 - mae: 1.7987 - val_loss: 1.1943 - val_mae: 1.6745\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2918 - mae: 1.8052 - val_loss: 1.1884 - val_mae: 1.6680\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.2891 - mae: 1.8024 - val_loss: 1.1891 - val_mae: 1.6691\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2667 - mae: 1.7786 - val_loss: 1.1881 - val_mae: 1.6688\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2652 - mae: 1.7766 - val_loss: 1.1890 - val_mae: 1.6689\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2787 - mae: 1.7925 - val_loss: 1.1891 - val_mae: 1.6689\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2675 - mae: 1.7783 - val_loss: 1.1885 - val_mae: 1.6679\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2655 - mae: 1.7770 - val_loss: 1.1857 - val_mae: 1.6660\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2770 - mae: 1.7903 - val_loss: 1.1881 - val_mae: 1.6680\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2715 - mae: 1.7803 - val_loss: 1.1868 - val_mae: 1.6666\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2795 - mae: 1.7950 - val_loss: 1.1888 - val_mae: 1.6693\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2605 - mae: 1.7714 - val_loss: 1.1854 - val_mae: 1.6653\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2915 - mae: 1.8048 - val_loss: 1.1871 - val_mae: 1.6683\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2593 - mae: 1.7679 - val_loss: 1.1874 - val_mae: 1.6685\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2640 - mae: 1.7735 - val_loss: 1.1867 - val_mae: 1.6669\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2620 - mae: 1.7720 - val_loss: 1.1881 - val_mae: 1.6686\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2667 - mae: 1.7787 - val_loss: 1.1870 - val_mae: 1.6679\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2660 - mae: 1.7777 - val_loss: 1.1857 - val_mae: 1.6657\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.2715 - mae: 1.7854 - val_loss: 1.1845 - val_mae: 1.6638\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2535 - mae: 1.7647 - val_loss: 1.1839 - val_mae: 1.6631\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2451 - mae: 1.7528 - val_loss: 1.1861 - val_mae: 1.6658\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2637 - mae: 1.7752 - val_loss: 1.1814 - val_mae: 1.6602\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2537 - mae: 1.7635 - val_loss: 1.1793 - val_mae: 1.6583\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2477 - mae: 1.7571 - val_loss: 1.1849 - val_mae: 1.6636\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2570 - mae: 1.7678 - val_loss: 1.1809 - val_mae: 1.6599\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2466 - mae: 1.7541 - val_loss: 1.1786 - val_mae: 1.6582\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2523 - mae: 1.7622 - val_loss: 1.1792 - val_mae: 1.6584\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2476 - mae: 1.7530 - val_loss: 1.1788 - val_mae: 1.6583\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2433 - mae: 1.7518 - val_loss: 1.1828 - val_mae: 1.6627\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2489 - mae: 1.7592 - val_loss: 1.1822 - val_mae: 1.6619\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2299 - mae: 1.7352 - val_loss: 1.1833 - val_mae: 1.6624\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2598 - mae: 1.7663 - val_loss: 1.1806 - val_mae: 1.6602\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2403 - mae: 1.7445 - val_loss: 1.1777 - val_mae: 1.6567\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2457 - mae: 1.7507 - val_loss: 1.1803 - val_mae: 1.6598\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2445 - mae: 1.7505 - val_loss: 1.1791 - val_mae: 1.6594\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.2392 - mae: 1.7463 - val_loss: 1.1802 - val_mae: 1.6596\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2337 - mae: 1.7351 - val_loss: 1.1797 - val_mae: 1.6585\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.2415 - mae: 1.7467 - val_loss: 1.1781 - val_mae: 1.6573\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2366 - mae: 1.7425 - val_loss: 1.1789 - val_mae: 1.6597\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2320 - mae: 1.7382 - val_loss: 1.1821 - val_mae: 1.6635\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2181 - mae: 1.7199 - val_loss: 1.1849 - val_mae: 1.6647\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2253 - mae: 1.7297 - val_loss: 1.1814 - val_mae: 1.6610\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2386 - mae: 1.7471 - val_loss: 1.1791 - val_mae: 1.6582\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2194 - mae: 1.7236 - val_loss: 1.1825 - val_mae: 1.6624\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2316 - mae: 1.7355 - val_loss: 1.1812 - val_mae: 1.6608\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2145 - mae: 1.7160 - val_loss: 1.1780 - val_mae: 1.6575\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2214 - mae: 1.7241 - val_loss: 1.1776 - val_mae: 1.6580\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.2314 - mae: 1.7365 - val_loss: 1.1771 - val_mae: 1.6580\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2194 - mae: 1.7228 - val_loss: 1.1792 - val_mae: 1.6591\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.2330 - mae: 1.7380 - val_loss: 1.1774 - val_mae: 1.6563\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.2201 - mae: 1.7232 - val_loss: 1.1765 - val_mae: 1.6567\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2122 - mae: 1.7120 - val_loss: 1.1784 - val_mae: 1.6585\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2271 - mae: 1.7308 - val_loss: 1.1779 - val_mae: 1.6581\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1987 - mae: 1.6993 - val_loss: 1.1755 - val_mae: 1.6561\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2347 - mae: 1.7370 - val_loss: 1.1760 - val_mae: 1.6558\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2198 - mae: 1.7184 - val_loss: 1.1728 - val_mae: 1.6525\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2166 - mae: 1.7186 - val_loss: 1.1725 - val_mae: 1.6521\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.2261 - mae: 1.7287 - val_loss: 1.1729 - val_mae: 1.6533\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.2067 - mae: 1.7066 - val_loss: 1.1755 - val_mae: 1.6550\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2176 - mae: 1.7173 - val_loss: 1.1744 - val_mae: 1.6545\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2097 - mae: 1.7118 - val_loss: 1.1792 - val_mae: 1.6593\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.2071 - mae: 1.7069 - val_loss: 1.1772 - val_mae: 1.6573\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.2277 - mae: 1.7292 - val_loss: 1.1754 - val_mae: 1.6545\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2201 - mae: 1.7167 - val_loss: 1.1742 - val_mae: 1.6539\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2180 - mae: 1.7189 - val_loss: 1.1711 - val_mae: 1.6509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:23:29,212] Trial 14 finished with value: 1.1710659265518188 and parameters: {'dropout_2': 0.21866602106396144, 'dropout_3': 0.2751334087044054, 'dropout_4': 0.5495784438563095, 'dropout_5': 0.6633477062279449, 'learning_rate': 2.1549945386078795e-05, 'epochs': 200, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "54/54 [==============================] - 3s 15ms/step - loss: 1.8561 - mae: 2.4331 - val_loss: 1.3658 - val_mae: 1.8879\n",
            "Epoch 2/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.8633 - mae: 2.4409 - val_loss: 1.4096 - val_mae: 1.9588\n",
            "Epoch 3/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.9192 - mae: 2.4958 - val_loss: 1.4394 - val_mae: 1.9974\n",
            "Epoch 4/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.8870 - mae: 2.4640 - val_loss: 1.4645 - val_mae: 2.0288\n",
            "Epoch 5/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.8684 - mae: 2.4452 - val_loss: 1.4790 - val_mae: 2.0455\n",
            "Epoch 6/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.8469 - mae: 2.4233 - val_loss: 1.4852 - val_mae: 2.0506\n",
            "Epoch 7/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.8634 - mae: 2.4392 - val_loss: 1.4897 - val_mae: 2.0550\n",
            "Epoch 8/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.8566 - mae: 2.4314 - val_loss: 1.4945 - val_mae: 2.0594\n",
            "Epoch 9/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.8505 - mae: 2.4262 - val_loss: 1.4947 - val_mae: 2.0588\n",
            "Epoch 10/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.8810 - mae: 2.4598 - val_loss: 1.4917 - val_mae: 2.0541\n",
            "Epoch 11/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.8371 - mae: 2.4066 - val_loss: 1.4927 - val_mae: 2.0545\n",
            "Epoch 12/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.8550 - mae: 2.4322 - val_loss: 1.4906 - val_mae: 2.0515\n",
            "Epoch 13/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.8814 - mae: 2.4598 - val_loss: 1.4863 - val_mae: 2.0457\n",
            "Epoch 14/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.8504 - mae: 2.4243 - val_loss: 1.4828 - val_mae: 2.0413\n",
            "Epoch 15/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.8357 - mae: 2.4104 - val_loss: 1.4807 - val_mae: 2.0402\n",
            "Epoch 16/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.8604 - mae: 2.4392 - val_loss: 1.4781 - val_mae: 2.0370\n",
            "Epoch 17/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.8423 - mae: 2.4184 - val_loss: 1.4749 - val_mae: 2.0330\n",
            "Epoch 18/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.8507 - mae: 2.4269 - val_loss: 1.4771 - val_mae: 2.0358\n",
            "Epoch 19/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.8348 - mae: 2.4067 - val_loss: 1.4738 - val_mae: 2.0315\n",
            "Epoch 20/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.8356 - mae: 2.4104 - val_loss: 1.4687 - val_mae: 2.0253\n",
            "Epoch 21/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.8428 - mae: 2.4165 - val_loss: 1.4682 - val_mae: 2.0244\n",
            "Epoch 22/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.8380 - mae: 2.4134 - val_loss: 1.4645 - val_mae: 2.0204\n",
            "Epoch 23/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.8566 - mae: 2.4286 - val_loss: 1.4638 - val_mae: 2.0197\n",
            "Epoch 24/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.8203 - mae: 2.3958 - val_loss: 1.4658 - val_mae: 2.0223\n",
            "Epoch 25/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.8051 - mae: 2.3752 - val_loss: 1.4622 - val_mae: 2.0180\n",
            "Epoch 26/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.8005 - mae: 2.3735 - val_loss: 1.4599 - val_mae: 2.0156\n",
            "Epoch 27/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.8197 - mae: 2.3926 - val_loss: 1.4573 - val_mae: 2.0128\n",
            "Epoch 28/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.8552 - mae: 2.4314 - val_loss: 1.4562 - val_mae: 2.0113\n",
            "Epoch 29/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.8106 - mae: 2.3846 - val_loss: 1.4539 - val_mae: 2.0087\n",
            "Epoch 30/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.8338 - mae: 2.4072 - val_loss: 1.4545 - val_mae: 2.0090\n",
            "Epoch 31/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.8279 - mae: 2.4029 - val_loss: 1.4483 - val_mae: 2.0019\n",
            "Epoch 32/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.8000 - mae: 2.3716 - val_loss: 1.4420 - val_mae: 1.9943\n",
            "Epoch 33/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.8074 - mae: 2.3782 - val_loss: 1.4417 - val_mae: 1.9940\n",
            "Epoch 34/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.8039 - mae: 2.3760 - val_loss: 1.4438 - val_mae: 1.9963\n",
            "Epoch 35/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.7808 - mae: 2.3502 - val_loss: 1.4410 - val_mae: 1.9931\n",
            "Epoch 36/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.8023 - mae: 2.3730 - val_loss: 1.4397 - val_mae: 1.9914\n",
            "Epoch 37/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.8238 - mae: 2.3996 - val_loss: 1.4379 - val_mae: 1.9889\n",
            "Epoch 38/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7989 - mae: 2.3738 - val_loss: 1.4366 - val_mae: 1.9878\n",
            "Epoch 39/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7898 - mae: 2.3618 - val_loss: 1.4370 - val_mae: 1.9882\n",
            "Epoch 40/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7917 - mae: 2.3626 - val_loss: 1.4313 - val_mae: 1.9816\n",
            "Epoch 41/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.8079 - mae: 2.3803 - val_loss: 1.4283 - val_mae: 1.9781\n",
            "Epoch 42/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.8068 - mae: 2.3832 - val_loss: 1.4298 - val_mae: 1.9792\n",
            "Epoch 43/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.7695 - mae: 2.3415 - val_loss: 1.4279 - val_mae: 1.9766\n",
            "Epoch 44/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.7929 - mae: 2.3661 - val_loss: 1.4250 - val_mae: 1.9733\n",
            "Epoch 45/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.8158 - mae: 2.3905 - val_loss: 1.4217 - val_mae: 1.9699\n",
            "Epoch 46/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.7795 - mae: 2.3501 - val_loss: 1.4179 - val_mae: 1.9651\n",
            "Epoch 47/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.8104 - mae: 2.3838 - val_loss: 1.4162 - val_mae: 1.9630\n",
            "Epoch 48/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.7784 - mae: 2.3485 - val_loss: 1.4169 - val_mae: 1.9639\n",
            "Epoch 49/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.7639 - mae: 2.3334 - val_loss: 1.4185 - val_mae: 1.9652\n",
            "Epoch 50/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7728 - mae: 2.3461 - val_loss: 1.4171 - val_mae: 1.9641\n",
            "Epoch 51/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7782 - mae: 2.3472 - val_loss: 1.4134 - val_mae: 1.9596\n",
            "Epoch 52/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7830 - mae: 2.3563 - val_loss: 1.4108 - val_mae: 1.9561\n",
            "Epoch 53/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.8042 - mae: 2.3737 - val_loss: 1.4107 - val_mae: 1.9562\n",
            "Epoch 54/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7853 - mae: 2.3557 - val_loss: 1.4043 - val_mae: 1.9483\n",
            "Epoch 55/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7482 - mae: 2.3139 - val_loss: 1.4032 - val_mae: 1.9472\n",
            "Epoch 56/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7466 - mae: 2.3149 - val_loss: 1.4035 - val_mae: 1.9472\n",
            "Epoch 57/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7339 - mae: 2.3004 - val_loss: 1.4028 - val_mae: 1.9467\n",
            "Epoch 58/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.7505 - mae: 2.3201 - val_loss: 1.4028 - val_mae: 1.9467\n",
            "Epoch 59/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7644 - mae: 2.3341 - val_loss: 1.4035 - val_mae: 1.9478\n",
            "Epoch 60/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7689 - mae: 2.3398 - val_loss: 1.4019 - val_mae: 1.9453\n",
            "Epoch 61/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.7485 - mae: 2.3187 - val_loss: 1.4018 - val_mae: 1.9452\n",
            "Epoch 62/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.7586 - mae: 2.3325 - val_loss: 1.4015 - val_mae: 1.9454\n",
            "Epoch 63/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7804 - mae: 2.3498 - val_loss: 1.3984 - val_mae: 1.9417\n",
            "Epoch 64/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7321 - mae: 2.3018 - val_loss: 1.3921 - val_mae: 1.9336\n",
            "Epoch 65/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.7383 - mae: 2.3080 - val_loss: 1.3896 - val_mae: 1.9305\n",
            "Epoch 66/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7605 - mae: 2.3305 - val_loss: 1.3926 - val_mae: 1.9339\n",
            "Epoch 67/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.7521 - mae: 2.3206 - val_loss: 1.3873 - val_mae: 1.9277\n",
            "Epoch 68/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7379 - mae: 2.3049 - val_loss: 1.3852 - val_mae: 1.9252\n",
            "Epoch 69/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.7233 - mae: 2.2909 - val_loss: 1.3858 - val_mae: 1.9262\n",
            "Epoch 70/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7598 - mae: 2.3299 - val_loss: 1.3825 - val_mae: 1.9222\n",
            "Epoch 71/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7444 - mae: 2.3135 - val_loss: 1.3777 - val_mae: 1.9159\n",
            "Epoch 72/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7755 - mae: 2.3463 - val_loss: 1.3808 - val_mae: 1.9199\n",
            "Epoch 73/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7285 - mae: 2.2958 - val_loss: 1.3774 - val_mae: 1.9156\n",
            "Epoch 74/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7261 - mae: 2.2919 - val_loss: 1.3761 - val_mae: 1.9135\n",
            "Epoch 75/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.7221 - mae: 2.2899 - val_loss: 1.3747 - val_mae: 1.9117\n",
            "Epoch 76/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7507 - mae: 2.3203 - val_loss: 1.3739 - val_mae: 1.9104\n",
            "Epoch 77/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7477 - mae: 2.3179 - val_loss: 1.3756 - val_mae: 1.9131\n",
            "Epoch 78/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7312 - mae: 2.2953 - val_loss: 1.3712 - val_mae: 1.9075\n",
            "Epoch 79/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7222 - mae: 2.2887 - val_loss: 1.3716 - val_mae: 1.9080\n",
            "Epoch 80/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.7361 - mae: 2.3036 - val_loss: 1.3668 - val_mae: 1.9016\n",
            "Epoch 81/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.7144 - mae: 2.2809 - val_loss: 1.3683 - val_mae: 1.9030\n",
            "Epoch 82/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.7089 - mae: 2.2726 - val_loss: 1.3688 - val_mae: 1.9039\n",
            "Epoch 83/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.7299 - mae: 2.2975 - val_loss: 1.3676 - val_mae: 1.9019\n",
            "Epoch 84/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.7326 - mae: 2.2999 - val_loss: 1.3630 - val_mae: 1.8960\n",
            "Epoch 85/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.7272 - mae: 2.2955 - val_loss: 1.3619 - val_mae: 1.8952\n",
            "Epoch 86/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.7175 - mae: 2.2836 - val_loss: 1.3604 - val_mae: 1.8934\n",
            "Epoch 87/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.7459 - mae: 2.3166 - val_loss: 1.3607 - val_mae: 1.8935\n",
            "Epoch 88/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.6579 - mae: 2.2198 - val_loss: 1.3598 - val_mae: 1.8924\n",
            "Epoch 89/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.7148 - mae: 2.2812 - val_loss: 1.3552 - val_mae: 1.8863\n",
            "Epoch 90/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.7101 - mae: 2.2768 - val_loss: 1.3530 - val_mae: 1.8838\n",
            "Epoch 91/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6855 - mae: 2.2505 - val_loss: 1.3527 - val_mae: 1.8843\n",
            "Epoch 92/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6814 - mae: 2.2468 - val_loss: 1.3535 - val_mae: 1.8853\n",
            "Epoch 93/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7014 - mae: 2.2660 - val_loss: 1.3528 - val_mae: 1.8839\n",
            "Epoch 94/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.6998 - mae: 2.2687 - val_loss: 1.3515 - val_mae: 1.8821\n",
            "Epoch 95/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7001 - mae: 2.2663 - val_loss: 1.3533 - val_mae: 1.8848\n",
            "Epoch 96/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6869 - mae: 2.2517 - val_loss: 1.3493 - val_mae: 1.8791\n",
            "Epoch 97/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7072 - mae: 2.2733 - val_loss: 1.3459 - val_mae: 1.8743\n",
            "Epoch 98/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.7084 - mae: 2.2730 - val_loss: 1.3473 - val_mae: 1.8765\n",
            "Epoch 99/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6746 - mae: 2.2400 - val_loss: 1.3475 - val_mae: 1.8773\n",
            "Epoch 100/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7162 - mae: 2.2825 - val_loss: 1.3459 - val_mae: 1.8741\n",
            "Epoch 101/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6920 - mae: 2.2560 - val_loss: 1.3437 - val_mae: 1.8715\n",
            "Epoch 102/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6935 - mae: 2.2592 - val_loss: 1.3438 - val_mae: 1.8728\n",
            "Epoch 103/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6939 - mae: 2.2585 - val_loss: 1.3429 - val_mae: 1.8716\n",
            "Epoch 104/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.6896 - mae: 2.2541 - val_loss: 1.3415 - val_mae: 1.8693\n",
            "Epoch 105/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.6998 - mae: 2.2623 - val_loss: 1.3395 - val_mae: 1.8663\n",
            "Epoch 106/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6829 - mae: 2.2462 - val_loss: 1.3407 - val_mae: 1.8677\n",
            "Epoch 107/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6836 - mae: 2.2483 - val_loss: 1.3399 - val_mae: 1.8672\n",
            "Epoch 108/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6963 - mae: 2.2629 - val_loss: 1.3394 - val_mae: 1.8664\n",
            "Epoch 109/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.6551 - mae: 2.2154 - val_loss: 1.3359 - val_mae: 1.8619\n",
            "Epoch 110/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6874 - mae: 2.2479 - val_loss: 1.3357 - val_mae: 1.8613\n",
            "Epoch 111/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.6638 - mae: 2.2253 - val_loss: 1.3359 - val_mae: 1.8612\n",
            "Epoch 112/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6688 - mae: 2.2309 - val_loss: 1.3351 - val_mae: 1.8602\n",
            "Epoch 113/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.6645 - mae: 2.2272 - val_loss: 1.3357 - val_mae: 1.8608\n",
            "Epoch 114/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6615 - mae: 2.2227 - val_loss: 1.3349 - val_mae: 1.8600\n",
            "Epoch 115/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.6812 - mae: 2.2453 - val_loss: 1.3323 - val_mae: 1.8562\n",
            "Epoch 116/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6798 - mae: 2.2421 - val_loss: 1.3321 - val_mae: 1.8562\n",
            "Epoch 117/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6811 - mae: 2.2437 - val_loss: 1.3340 - val_mae: 1.8588\n",
            "Epoch 118/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6654 - mae: 2.2271 - val_loss: 1.3317 - val_mae: 1.8556\n",
            "Epoch 119/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6696 - mae: 2.2313 - val_loss: 1.3301 - val_mae: 1.8532\n",
            "Epoch 120/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6530 - mae: 2.2141 - val_loss: 1.3273 - val_mae: 1.8493\n",
            "Epoch 121/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.6808 - mae: 2.2474 - val_loss: 1.3260 - val_mae: 1.8473\n",
            "Epoch 122/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.6592 - mae: 2.2241 - val_loss: 1.3256 - val_mae: 1.8472\n",
            "Epoch 123/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.6551 - mae: 2.2190 - val_loss: 1.3249 - val_mae: 1.8463\n",
            "Epoch 124/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.6671 - mae: 2.2317 - val_loss: 1.3247 - val_mae: 1.8467\n",
            "Epoch 125/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.6614 - mae: 2.2230 - val_loss: 1.3228 - val_mae: 1.8437\n",
            "Epoch 126/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.6578 - mae: 2.2177 - val_loss: 1.3207 - val_mae: 1.8413\n",
            "Epoch 127/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.6442 - mae: 2.2060 - val_loss: 1.3190 - val_mae: 1.8390\n",
            "Epoch 128/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.6736 - mae: 2.2393 - val_loss: 1.3217 - val_mae: 1.8421\n",
            "Epoch 129/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.6456 - mae: 2.2088 - val_loss: 1.3199 - val_mae: 1.8392\n",
            "Epoch 130/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.6393 - mae: 2.1991 - val_loss: 1.3191 - val_mae: 1.8374\n",
            "Epoch 131/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6636 - mae: 2.2281 - val_loss: 1.3184 - val_mae: 1.8374\n",
            "Epoch 132/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6468 - mae: 2.2093 - val_loss: 1.3165 - val_mae: 1.8346\n",
            "Epoch 133/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6425 - mae: 2.2027 - val_loss: 1.3156 - val_mae: 1.8326\n",
            "Epoch 134/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6425 - mae: 2.2027 - val_loss: 1.3131 - val_mae: 1.8288\n",
            "Epoch 135/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6564 - mae: 2.2199 - val_loss: 1.3109 - val_mae: 1.8258\n",
            "Epoch 136/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6649 - mae: 2.2272 - val_loss: 1.3113 - val_mae: 1.8267\n",
            "Epoch 137/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6416 - mae: 2.2003 - val_loss: 1.3115 - val_mae: 1.8273\n",
            "Epoch 138/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6439 - mae: 2.2040 - val_loss: 1.3111 - val_mae: 1.8264\n",
            "Epoch 139/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.6393 - mae: 2.2007 - val_loss: 1.3110 - val_mae: 1.8258\n",
            "Epoch 140/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.6365 - mae: 2.1967 - val_loss: 1.3101 - val_mae: 1.8244\n",
            "Epoch 141/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6443 - mae: 2.2041 - val_loss: 1.3098 - val_mae: 1.8240\n",
            "Epoch 142/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6305 - mae: 2.1881 - val_loss: 1.3111 - val_mae: 1.8266\n",
            "Epoch 143/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6226 - mae: 2.1827 - val_loss: 1.3081 - val_mae: 1.8223\n",
            "Epoch 144/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6290 - mae: 2.1895 - val_loss: 1.3090 - val_mae: 1.8240\n",
            "Epoch 145/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6442 - mae: 2.2053 - val_loss: 1.3056 - val_mae: 1.8191\n",
            "Epoch 146/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.6339 - mae: 2.1925 - val_loss: 1.3063 - val_mae: 1.8194\n",
            "Epoch 147/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6285 - mae: 2.1899 - val_loss: 1.3067 - val_mae: 1.8201\n",
            "Epoch 148/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6339 - mae: 2.1923 - val_loss: 1.3059 - val_mae: 1.8189\n",
            "Epoch 149/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.6133 - mae: 2.1719 - val_loss: 1.3047 - val_mae: 1.8174\n",
            "Epoch 150/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6012 - mae: 2.1590 - val_loss: 1.3024 - val_mae: 1.8143\n",
            "Epoch 151/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.6441 - mae: 2.2066 - val_loss: 1.3026 - val_mae: 1.8148\n",
            "Epoch 152/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6200 - mae: 2.1791 - val_loss: 1.3025 - val_mae: 1.8140\n",
            "Epoch 153/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6117 - mae: 2.1669 - val_loss: 1.3017 - val_mae: 1.8126\n",
            "Epoch 154/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6163 - mae: 2.1764 - val_loss: 1.2989 - val_mae: 1.8083\n",
            "Epoch 155/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6105 - mae: 2.1660 - val_loss: 1.2984 - val_mae: 1.8071\n",
            "Epoch 156/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6087 - mae: 2.1684 - val_loss: 1.2973 - val_mae: 1.8051\n",
            "Epoch 157/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5979 - mae: 2.1561 - val_loss: 1.2979 - val_mae: 1.8058\n",
            "Epoch 158/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6287 - mae: 2.1858 - val_loss: 1.2965 - val_mae: 1.8041\n",
            "Epoch 159/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6007 - mae: 2.1571 - val_loss: 1.2974 - val_mae: 1.8064\n",
            "Epoch 160/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6077 - mae: 2.1652 - val_loss: 1.2966 - val_mae: 1.8054\n",
            "Epoch 161/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.6127 - mae: 2.1691 - val_loss: 1.2962 - val_mae: 1.8051\n",
            "Epoch 162/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.5912 - mae: 2.1466 - val_loss: 1.2951 - val_mae: 1.8034\n",
            "Epoch 163/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.6108 - mae: 2.1709 - val_loss: 1.2952 - val_mae: 1.8041\n",
            "Epoch 164/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.6012 - mae: 2.1602 - val_loss: 1.2956 - val_mae: 1.8037\n",
            "Epoch 165/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.6004 - mae: 2.1570 - val_loss: 1.2943 - val_mae: 1.8018\n",
            "Epoch 166/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.5856 - mae: 2.1418 - val_loss: 1.2934 - val_mae: 1.8007\n",
            "Epoch 167/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.6153 - mae: 2.1731 - val_loss: 1.2921 - val_mae: 1.7992\n",
            "Epoch 168/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.5881 - mae: 2.1455 - val_loss: 1.2913 - val_mae: 1.7977\n",
            "Epoch 169/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.5900 - mae: 2.1441 - val_loss: 1.2901 - val_mae: 1.7964\n",
            "Epoch 170/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.6104 - mae: 2.1692 - val_loss: 1.2891 - val_mae: 1.7946\n",
            "Epoch 171/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5664 - mae: 2.1196 - val_loss: 1.2897 - val_mae: 1.7956\n",
            "Epoch 172/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5917 - mae: 2.1463 - val_loss: 1.2888 - val_mae: 1.7942\n",
            "Epoch 173/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6029 - mae: 2.1577 - val_loss: 1.2891 - val_mae: 1.7940\n",
            "Epoch 174/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6065 - mae: 2.1659 - val_loss: 1.2879 - val_mae: 1.7918\n",
            "Epoch 175/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.6282 - mae: 2.1875 - val_loss: 1.2886 - val_mae: 1.7934\n",
            "Epoch 176/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5879 - mae: 2.1415 - val_loss: 1.2879 - val_mae: 1.7932\n",
            "Epoch 177/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5943 - mae: 2.1465 - val_loss: 1.2879 - val_mae: 1.7928\n",
            "Epoch 178/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5872 - mae: 2.1419 - val_loss: 1.2867 - val_mae: 1.7911\n",
            "Epoch 179/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5803 - mae: 2.1361 - val_loss: 1.2865 - val_mae: 1.7905\n",
            "Epoch 180/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.5884 - mae: 2.1466 - val_loss: 1.2861 - val_mae: 1.7896\n",
            "Epoch 181/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5679 - mae: 2.1245 - val_loss: 1.2856 - val_mae: 1.7888\n",
            "Epoch 182/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5727 - mae: 2.1298 - val_loss: 1.2841 - val_mae: 1.7867\n",
            "Epoch 183/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.5758 - mae: 2.1288 - val_loss: 1.2835 - val_mae: 1.7858\n",
            "Epoch 184/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5514 - mae: 2.1034 - val_loss: 1.2839 - val_mae: 1.7863\n",
            "Epoch 185/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5641 - mae: 2.1188 - val_loss: 1.2829 - val_mae: 1.7842\n",
            "Epoch 186/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5641 - mae: 2.1178 - val_loss: 1.2842 - val_mae: 1.7853\n",
            "Epoch 187/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5829 - mae: 2.1381 - val_loss: 1.2831 - val_mae: 1.7840\n",
            "Epoch 188/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5701 - mae: 2.1259 - val_loss: 1.2818 - val_mae: 1.7814\n",
            "Epoch 189/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.5848 - mae: 2.1406 - val_loss: 1.2822 - val_mae: 1.7820\n",
            "Epoch 190/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5620 - mae: 2.1180 - val_loss: 1.2827 - val_mae: 1.7840\n",
            "Epoch 191/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5397 - mae: 2.0908 - val_loss: 1.2813 - val_mae: 1.7809\n",
            "Epoch 192/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5797 - mae: 2.1361 - val_loss: 1.2795 - val_mae: 1.7788\n",
            "Epoch 193/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5767 - mae: 2.1302 - val_loss: 1.2792 - val_mae: 1.7782\n",
            "Epoch 194/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5647 - mae: 2.1181 - val_loss: 1.2797 - val_mae: 1.7792\n",
            "Epoch 195/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.5651 - mae: 2.1180 - val_loss: 1.2785 - val_mae: 1.7772\n",
            "Epoch 196/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.5590 - mae: 2.1116 - val_loss: 1.2768 - val_mae: 1.7757\n",
            "Epoch 197/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.5781 - mae: 2.1315 - val_loss: 1.2766 - val_mae: 1.7754\n",
            "Epoch 198/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.5632 - mae: 2.1175 - val_loss: 1.2740 - val_mae: 1.7704\n",
            "Epoch 199/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.5776 - mae: 2.1365 - val_loss: 1.2742 - val_mae: 1.7711\n",
            "Epoch 200/200\n",
            "54/54 [==============================] - 1s 9ms/step - loss: 1.5700 - mae: 2.1244 - val_loss: 1.2750 - val_mae: 1.7729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:24:44,086] Trial 15 finished with value: 1.2750385999679565 and parameters: {'dropout_2': 0.44670049657790517, 'dropout_3': 0.125712763911788, 'dropout_4': 0.5575263527584581, 'dropout_5': 0.5927140471792786, 'learning_rate': 8.11719510636233e-06, 'epochs': 200, 'batch_size': 64}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 4s 11ms/step - loss: 1.6017 - mae: 2.1586 - val_loss: 1.3588 - val_mae: 1.8739\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.4908 - mae: 2.0380 - val_loss: 1.3539 - val_mae: 1.8674\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4008 - mae: 1.9340 - val_loss: 1.3228 - val_mae: 1.8206\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3599 - mae: 1.8909 - val_loss: 1.2774 - val_mae: 1.7675\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2933 - mae: 1.8131 - val_loss: 1.2528 - val_mae: 1.7362\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.2715 - mae: 1.7885 - val_loss: 1.2307 - val_mae: 1.7147\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2362 - mae: 1.7473 - val_loss: 1.2048 - val_mae: 1.6860\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2278 - mae: 1.7324 - val_loss: 1.2077 - val_mae: 1.6891\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1933 - mae: 1.6963 - val_loss: 1.1884 - val_mae: 1.6695\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1893 - mae: 1.6874 - val_loss: 1.1775 - val_mae: 1.6560\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1714 - mae: 1.6677 - val_loss: 1.1760 - val_mae: 1.6580\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1488 - mae: 1.6428 - val_loss: 1.1841 - val_mae: 1.6681\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1513 - mae: 1.6454 - val_loss: 1.1593 - val_mae: 1.6410\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1469 - mae: 1.6365 - val_loss: 1.1648 - val_mae: 1.6447\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1422 - mae: 1.6333 - val_loss: 1.1434 - val_mae: 1.6217\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1252 - mae: 1.6144 - val_loss: 1.1402 - val_mae: 1.6199\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1146 - mae: 1.6047 - val_loss: 1.1399 - val_mae: 1.6190\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1128 - mae: 1.6030 - val_loss: 1.1415 - val_mae: 1.6235\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1057 - mae: 1.5979 - val_loss: 1.1402 - val_mae: 1.6220\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1021 - mae: 1.5927 - val_loss: 1.1159 - val_mae: 1.5937\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0884 - mae: 1.5764 - val_loss: 1.1196 - val_mae: 1.5993\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0888 - mae: 1.5752 - val_loss: 1.1234 - val_mae: 1.6065\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0911 - mae: 1.5818 - val_loss: 1.1237 - val_mae: 1.6063\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0932 - mae: 1.5829 - val_loss: 1.1284 - val_mae: 1.6131\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0848 - mae: 1.5749 - val_loss: 1.1080 - val_mae: 1.5934\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0630 - mae: 1.5526 - val_loss: 1.0967 - val_mae: 1.5840\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0708 - mae: 1.5649 - val_loss: 1.0984 - val_mae: 1.5824\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0630 - mae: 1.5495 - val_loss: 1.1146 - val_mae: 1.6035\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0587 - mae: 1.5460 - val_loss: 1.0980 - val_mae: 1.5836\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0681 - mae: 1.5618 - val_loss: 1.0867 - val_mae: 1.5751\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0641 - mae: 1.5547 - val_loss: 1.0968 - val_mae: 1.5841\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0444 - mae: 1.5320 - val_loss: 1.0906 - val_mae: 1.5822\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0631 - mae: 1.5562 - val_loss: 1.0822 - val_mae: 1.5682\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0724 - mae: 1.5658 - val_loss: 1.0839 - val_mae: 1.5685\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0572 - mae: 1.5499 - val_loss: 1.0705 - val_mae: 1.5501\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0445 - mae: 1.5334 - val_loss: 1.0842 - val_mae: 1.5718\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0359 - mae: 1.5258 - val_loss: 1.0683 - val_mae: 1.5527\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0456 - mae: 1.5370 - val_loss: 1.0786 - val_mae: 1.5625\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0448 - mae: 1.5332 - val_loss: 1.0684 - val_mae: 1.5525\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0382 - mae: 1.5302 - val_loss: 1.0614 - val_mae: 1.5447\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0358 - mae: 1.5213 - val_loss: 1.0648 - val_mae: 1.5506\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0304 - mae: 1.5215 - val_loss: 1.0770 - val_mae: 1.5653\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0173 - mae: 1.5032 - val_loss: 1.0570 - val_mae: 1.5409\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0229 - mae: 1.5140 - val_loss: 1.0517 - val_mae: 1.5318\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0435 - mae: 1.5353 - val_loss: 1.0649 - val_mae: 1.5477\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0026 - mae: 1.4893 - val_loss: 1.0545 - val_mae: 1.5326\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0114 - mae: 1.5003 - val_loss: 1.0666 - val_mae: 1.5519\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0234 - mae: 1.5138 - val_loss: 1.0556 - val_mae: 1.5369\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0137 - mae: 1.5005 - val_loss: 1.0459 - val_mae: 1.5317\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0130 - mae: 1.5016 - val_loss: 1.0478 - val_mae: 1.5341\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0133 - mae: 1.5022 - val_loss: 1.0316 - val_mae: 1.5083\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0165 - mae: 1.5066 - val_loss: 1.0472 - val_mae: 1.5226\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0151 - mae: 1.5017 - val_loss: 1.0422 - val_mae: 1.5189\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0098 - mae: 1.4987 - val_loss: 1.0470 - val_mae: 1.5317\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0211 - mae: 1.5093 - val_loss: 1.0544 - val_mae: 1.5377\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0086 - mae: 1.4968 - val_loss: 1.0570 - val_mae: 1.5403\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0136 - mae: 1.5050 - val_loss: 1.0604 - val_mae: 1.5455\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0069 - mae: 1.4940 - val_loss: 1.0399 - val_mae: 1.5228\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9951 - mae: 1.4814 - val_loss: 1.0266 - val_mae: 1.5068\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9950 - mae: 1.4821 - val_loss: 1.0554 - val_mae: 1.5488\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0123 - mae: 1.5018 - val_loss: 1.0488 - val_mae: 1.5350\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0027 - mae: 1.4880 - val_loss: 1.0304 - val_mae: 1.5112\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0017 - mae: 1.4898 - val_loss: 1.0653 - val_mae: 1.5604\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9929 - mae: 1.4759 - val_loss: 1.0380 - val_mae: 1.5231\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0044 - mae: 1.4959 - val_loss: 1.0518 - val_mae: 1.5359\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9968 - mae: 1.4832 - val_loss: 1.0543 - val_mae: 1.5388\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0034 - mae: 1.4932 - val_loss: 1.0447 - val_mae: 1.5294\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9993 - mae: 1.4864 - val_loss: 1.0218 - val_mae: 1.5085\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9829 - mae: 1.4713 - val_loss: 1.0256 - val_mae: 1.5060\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9814 - mae: 1.4668 - val_loss: 1.0127 - val_mae: 1.4898\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9673 - mae: 1.4545 - val_loss: 1.0224 - val_mae: 1.5024\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9884 - mae: 1.4756 - val_loss: 1.0515 - val_mae: 1.5417\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9841 - mae: 1.4686 - val_loss: 1.0302 - val_mae: 1.5110\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9800 - mae: 1.4673 - val_loss: 1.0286 - val_mae: 1.5102\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9788 - mae: 1.4624 - val_loss: 1.0201 - val_mae: 1.5030\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9791 - mae: 1.4662 - val_loss: 1.0220 - val_mae: 1.4956\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9894 - mae: 1.4779 - val_loss: 1.0318 - val_mae: 1.5151\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9872 - mae: 1.4776 - val_loss: 1.0297 - val_mae: 1.5120\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9834 - mae: 1.4672 - val_loss: 1.0193 - val_mae: 1.4956\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9607 - mae: 1.4448 - val_loss: 1.0148 - val_mae: 1.4934\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9762 - mae: 1.4623 - val_loss: 1.0305 - val_mae: 1.5079\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9676 - mae: 1.4505 - val_loss: 1.0307 - val_mae: 1.4990\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9635 - mae: 1.4484 - val_loss: 1.0148 - val_mae: 1.4916\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9701 - mae: 1.4596 - val_loss: 1.0276 - val_mae: 1.5024\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9782 - mae: 1.4640 - val_loss: 1.0218 - val_mae: 1.4962\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9704 - mae: 1.4540 - val_loss: 1.0247 - val_mae: 1.4928\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9788 - mae: 1.4648 - val_loss: 1.0098 - val_mae: 1.4784\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9568 - mae: 1.4371 - val_loss: 1.0177 - val_mae: 1.4942\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9681 - mae: 1.4503 - val_loss: 1.0223 - val_mae: 1.4897\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9621 - mae: 1.4441 - val_loss: 1.0104 - val_mae: 1.4765\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9848 - mae: 1.4726 - val_loss: 1.0274 - val_mae: 1.4963\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9799 - mae: 1.4616 - val_loss: 1.0194 - val_mae: 1.4944\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9631 - mae: 1.4465 - val_loss: 1.0181 - val_mae: 1.4895\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9636 - mae: 1.4479 - val_loss: 1.0237 - val_mae: 1.4889\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9462 - mae: 1.4242 - val_loss: 1.0092 - val_mae: 1.4840\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9497 - mae: 1.4315 - val_loss: 1.0034 - val_mae: 1.4712\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9589 - mae: 1.4400 - val_loss: 1.0061 - val_mae: 1.4788\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9671 - mae: 1.4528 - val_loss: 1.0109 - val_mae: 1.4808\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9307 - mae: 1.4075 - val_loss: 1.0082 - val_mae: 1.4797\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9515 - mae: 1.4359 - val_loss: 1.0084 - val_mae: 1.4793\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9569 - mae: 1.4379 - val_loss: 1.0183 - val_mae: 1.4861\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9588 - mae: 1.4416 - val_loss: 1.0009 - val_mae: 1.4679\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9484 - mae: 1.4275 - val_loss: 1.0073 - val_mae: 1.4826\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9614 - mae: 1.4453 - val_loss: 1.0170 - val_mae: 1.4849\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9415 - mae: 1.4205 - val_loss: 1.0076 - val_mae: 1.4722\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9471 - mae: 1.4252 - val_loss: 1.0003 - val_mae: 1.4705\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9509 - mae: 1.4311 - val_loss: 1.0083 - val_mae: 1.4775\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9436 - mae: 1.4217 - val_loss: 0.9907 - val_mae: 1.4558\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9428 - mae: 1.4218 - val_loss: 1.0001 - val_mae: 1.4635\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9412 - mae: 1.4199 - val_loss: 0.9935 - val_mae: 1.4649\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9421 - mae: 1.4242 - val_loss: 1.0189 - val_mae: 1.4869\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9408 - mae: 1.4192 - val_loss: 0.9973 - val_mae: 1.4664\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9576 - mae: 1.4398 - val_loss: 0.9901 - val_mae: 1.4571\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9336 - mae: 1.4133 - val_loss: 1.0134 - val_mae: 1.4892\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9588 - mae: 1.4400 - val_loss: 1.0130 - val_mae: 1.4835\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9477 - mae: 1.4329 - val_loss: 0.9966 - val_mae: 1.4639\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9435 - mae: 1.4245 - val_loss: 1.0249 - val_mae: 1.4898\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9408 - mae: 1.4201 - val_loss: 0.9765 - val_mae: 1.4376\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9438 - mae: 1.4214 - val_loss: 0.9846 - val_mae: 1.4450\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9361 - mae: 1.4153 - val_loss: 0.9827 - val_mae: 1.4521\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9425 - mae: 1.4235 - val_loss: 0.9931 - val_mae: 1.4618\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9376 - mae: 1.4174 - val_loss: 0.9924 - val_mae: 1.4570\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9398 - mae: 1.4204 - val_loss: 0.9863 - val_mae: 1.4474\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9329 - mae: 1.4092 - val_loss: 0.9965 - val_mae: 1.4627\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9379 - mae: 1.4161 - val_loss: 0.9957 - val_mae: 1.4532\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9126 - mae: 1.3880 - val_loss: 0.9961 - val_mae: 1.4632\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9185 - mae: 1.3926 - val_loss: 0.9955 - val_mae: 1.4545\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9384 - mae: 1.4156 - val_loss: 0.9727 - val_mae: 1.4349\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9329 - mae: 1.4110 - val_loss: 0.9980 - val_mae: 1.4623\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9448 - mae: 1.4242 - val_loss: 0.9854 - val_mae: 1.4513\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9399 - mae: 1.4187 - val_loss: 0.9910 - val_mae: 1.4511\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9444 - mae: 1.4264 - val_loss: 0.9660 - val_mae: 1.4283\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9132 - mae: 1.3872 - val_loss: 0.9825 - val_mae: 1.4389\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9233 - mae: 1.4011 - val_loss: 0.9768 - val_mae: 1.4352\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9208 - mae: 1.3973 - val_loss: 0.9882 - val_mae: 1.4464\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9269 - mae: 1.4043 - val_loss: 0.9925 - val_mae: 1.4533\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9214 - mae: 1.4003 - val_loss: 1.0164 - val_mae: 1.4791\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9305 - mae: 1.4109 - val_loss: 0.9810 - val_mae: 1.4376\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9263 - mae: 1.4054 - val_loss: 1.0078 - val_mae: 1.4580\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9224 - mae: 1.3998 - val_loss: 0.9876 - val_mae: 1.4408\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9267 - mae: 1.4023 - val_loss: 0.9641 - val_mae: 1.4219\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9190 - mae: 1.3945 - val_loss: 1.0014 - val_mae: 1.4655\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9313 - mae: 1.4111 - val_loss: 0.9976 - val_mae: 1.4527\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9191 - mae: 1.3967 - val_loss: 0.9808 - val_mae: 1.4387\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9247 - mae: 1.4021 - val_loss: 0.9901 - val_mae: 1.4457\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9124 - mae: 1.3864 - val_loss: 0.9724 - val_mae: 1.4293\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9190 - mae: 1.3918 - val_loss: 0.9910 - val_mae: 1.4375\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9168 - mae: 1.3858 - val_loss: 0.9792 - val_mae: 1.4403\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9081 - mae: 1.3815 - val_loss: 0.9898 - val_mae: 1.4400\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9415 - mae: 1.4212 - val_loss: 0.9637 - val_mae: 1.4174\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9198 - mae: 1.3951 - val_loss: 0.9748 - val_mae: 1.4241\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9135 - mae: 1.3866 - val_loss: 0.9530 - val_mae: 1.4117\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9115 - mae: 1.3862 - val_loss: 0.9523 - val_mae: 1.4050\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9213 - mae: 1.4010 - val_loss: 0.9769 - val_mae: 1.4293\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9095 - mae: 1.3852 - val_loss: 0.9761 - val_mae: 1.4265\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9010 - mae: 1.3741 - val_loss: 0.9630 - val_mae: 1.4091\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9055 - mae: 1.3806 - val_loss: 0.9419 - val_mae: 1.3968\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9154 - mae: 1.3894 - val_loss: 0.9608 - val_mae: 1.4080\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9144 - mae: 1.3907 - val_loss: 0.9451 - val_mae: 1.3964\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9059 - mae: 1.3782 - val_loss: 0.9835 - val_mae: 1.4420\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9242 - mae: 1.4025 - val_loss: 0.9814 - val_mae: 1.4409\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8896 - mae: 1.3637 - val_loss: 0.9593 - val_mae: 1.4166\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9106 - mae: 1.3784 - val_loss: 0.9522 - val_mae: 1.3986\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9101 - mae: 1.3805 - val_loss: 0.9851 - val_mae: 1.4363\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9058 - mae: 1.3766 - val_loss: 0.9585 - val_mae: 1.4098\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9107 - mae: 1.3902 - val_loss: 0.9725 - val_mae: 1.4358\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9114 - mae: 1.3853 - val_loss: 0.9461 - val_mae: 1.4037\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8908 - mae: 1.3612 - val_loss: 0.9459 - val_mae: 1.3974\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9036 - mae: 1.3808 - val_loss: 0.9700 - val_mae: 1.4240\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9140 - mae: 1.3881 - val_loss: 0.9407 - val_mae: 1.3969\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9038 - mae: 1.3780 - val_loss: 0.9553 - val_mae: 1.4126\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9121 - mae: 1.3849 - val_loss: 0.9463 - val_mae: 1.4021\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9080 - mae: 1.3837 - val_loss: 0.9378 - val_mae: 1.3974\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8967 - mae: 1.3704 - val_loss: 0.9508 - val_mae: 1.4043\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8884 - mae: 1.3587 - val_loss: 0.9504 - val_mae: 1.4053\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8793 - mae: 1.3499 - val_loss: 0.9524 - val_mae: 1.4063\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9124 - mae: 1.3866 - val_loss: 0.9965 - val_mae: 1.4559\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8882 - mae: 1.3586 - val_loss: 0.9570 - val_mae: 1.4066\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9020 - mae: 1.3784 - val_loss: 0.9142 - val_mae: 1.3676\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8938 - mae: 1.3636 - val_loss: 0.9208 - val_mae: 1.3693\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8873 - mae: 1.3576 - val_loss: 0.9493 - val_mae: 1.4036\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8940 - mae: 1.3664 - val_loss: 0.9486 - val_mae: 1.4039\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9103 - mae: 1.3867 - val_loss: 0.9109 - val_mae: 1.3681\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8861 - mae: 1.3570 - val_loss: 0.9701 - val_mae: 1.4300\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8876 - mae: 1.3616 - val_loss: 0.9470 - val_mae: 1.3981\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8768 - mae: 1.3473 - val_loss: 0.9343 - val_mae: 1.3806\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8802 - mae: 1.3462 - val_loss: 0.9515 - val_mae: 1.4042\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8807 - mae: 1.3478 - val_loss: 0.9231 - val_mae: 1.3756\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8845 - mae: 1.3574 - val_loss: 0.9466 - val_mae: 1.4007\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8827 - mae: 1.3549 - val_loss: 0.9548 - val_mae: 1.4106\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8903 - mae: 1.3607 - val_loss: 0.9568 - val_mae: 1.4144\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8892 - mae: 1.3625 - val_loss: 0.9340 - val_mae: 1.3949\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8690 - mae: 1.3367 - val_loss: 0.9089 - val_mae: 1.3611\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8842 - mae: 1.3603 - val_loss: 0.9087 - val_mae: 1.3617\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8784 - mae: 1.3531 - val_loss: 0.9080 - val_mae: 1.3597\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8714 - mae: 1.3396 - val_loss: 0.9097 - val_mae: 1.3656\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8893 - mae: 1.3585 - val_loss: 0.9162 - val_mae: 1.3663\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.8728 - mae: 1.3414 - val_loss: 0.9149 - val_mae: 1.3656\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8930 - mae: 1.3641 - val_loss: 0.8993 - val_mae: 1.3507\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8875 - mae: 1.3612 - val_loss: 0.9000 - val_mae: 1.3537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:26:41,878] Trial 16 finished with value: 0.8999512195587158 and parameters: {'dropout_2': 0.20066090852524504, 'dropout_3': 0.7220192883992254, 'dropout_4': 0.48533831869090854, 'dropout_5': 0.3708552411229495, 'learning_rate': 0.000480607611248244, 'epochs': 200, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 4s 7ms/step - loss: 1.6093 - mae: 2.1687 - val_loss: 1.3532 - val_mae: 1.8697\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4293 - mae: 1.9757 - val_loss: 1.3366 - val_mae: 1.8679\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3595 - mae: 1.8934 - val_loss: 1.3001 - val_mae: 1.8301\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2807 - mae: 1.8024 - val_loss: 1.2469 - val_mae: 1.7697\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2365 - mae: 1.7491 - val_loss: 1.2190 - val_mae: 1.7317\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2146 - mae: 1.7213 - val_loss: 1.2147 - val_mae: 1.7260\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1925 - mae: 1.6944 - val_loss: 1.2008 - val_mae: 1.7070\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1932 - mae: 1.6933 - val_loss: 1.1884 - val_mae: 1.6902\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1491 - mae: 1.6392 - val_loss: 1.1798 - val_mae: 1.6769\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1460 - mae: 1.6363 - val_loss: 1.1652 - val_mae: 1.6577\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1376 - mae: 1.6266 - val_loss: 1.1751 - val_mae: 1.6685\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1201 - mae: 1.6078 - val_loss: 1.1668 - val_mae: 1.6585\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1108 - mae: 1.5986 - val_loss: 1.1558 - val_mae: 1.6508\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1168 - mae: 1.6060 - val_loss: 1.1515 - val_mae: 1.6446\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1026 - mae: 1.5931 - val_loss: 1.1311 - val_mae: 1.6217\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1029 - mae: 1.5931 - val_loss: 1.1247 - val_mae: 1.6163\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1147 - mae: 1.6063 - val_loss: 1.1326 - val_mae: 1.6281\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0842 - mae: 1.5742 - val_loss: 1.1375 - val_mae: 1.6373\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0889 - mae: 1.5807 - val_loss: 1.1342 - val_mae: 1.6311\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0819 - mae: 1.5729 - val_loss: 1.1246 - val_mae: 1.6208\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0843 - mae: 1.5731 - val_loss: 1.1175 - val_mae: 1.6155\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0774 - mae: 1.5672 - val_loss: 1.1228 - val_mae: 1.6179\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0747 - mae: 1.5660 - val_loss: 1.1020 - val_mae: 1.5979\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0651 - mae: 1.5582 - val_loss: 1.0884 - val_mae: 1.5808\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0603 - mae: 1.5533 - val_loss: 1.0983 - val_mae: 1.5928\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0513 - mae: 1.5433 - val_loss: 1.0963 - val_mae: 1.5913\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0721 - mae: 1.5638 - val_loss: 1.1137 - val_mae: 1.6088\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0703 - mae: 1.5637 - val_loss: 1.1095 - val_mae: 1.6051\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0555 - mae: 1.5445 - val_loss: 1.0951 - val_mae: 1.5887\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0551 - mae: 1.5430 - val_loss: 1.1120 - val_mae: 1.6100\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0644 - mae: 1.5565 - val_loss: 1.1150 - val_mae: 1.6112\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0621 - mae: 1.5551 - val_loss: 1.1089 - val_mae: 1.6058\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0549 - mae: 1.5489 - val_loss: 1.1311 - val_mae: 1.6285\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0498 - mae: 1.5407 - val_loss: 1.1254 - val_mae: 1.6276\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0552 - mae: 1.5438 - val_loss: 1.1141 - val_mae: 1.6117\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0454 - mae: 1.5338 - val_loss: 1.0852 - val_mae: 1.5761\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0399 - mae: 1.5286 - val_loss: 1.0809 - val_mae: 1.5713\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0423 - mae: 1.5348 - val_loss: 1.0759 - val_mae: 1.5670\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0532 - mae: 1.5443 - val_loss: 1.1302 - val_mae: 1.6281\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0543 - mae: 1.5479 - val_loss: 1.0886 - val_mae: 1.5805\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0387 - mae: 1.5279 - val_loss: 1.0894 - val_mae: 1.5793\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0455 - mae: 1.5350 - val_loss: 1.1246 - val_mae: 1.6237\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0234 - mae: 1.5126 - val_loss: 1.0925 - val_mae: 1.5904\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0403 - mae: 1.5291 - val_loss: 1.1181 - val_mae: 1.6100\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0189 - mae: 1.5057 - val_loss: 1.0878 - val_mae: 1.5878\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0534 - mae: 1.5469 - val_loss: 1.1163 - val_mae: 1.6115\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0376 - mae: 1.5252 - val_loss: 1.0998 - val_mae: 1.5892\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0192 - mae: 1.5046 - val_loss: 1.0909 - val_mae: 1.5818\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0244 - mae: 1.5147 - val_loss: 1.0930 - val_mae: 1.5866\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0337 - mae: 1.5228 - val_loss: 1.1088 - val_mae: 1.5997\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0215 - mae: 1.5092 - val_loss: 1.0841 - val_mae: 1.5727\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0377 - mae: 1.5261 - val_loss: 1.1140 - val_mae: 1.6017\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0380 - mae: 1.5269 - val_loss: 1.1216 - val_mae: 1.6126\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0164 - mae: 1.5068 - val_loss: 1.1041 - val_mae: 1.5993\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0190 - mae: 1.5091 - val_loss: 1.1011 - val_mae: 1.5900\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0235 - mae: 1.5160 - val_loss: 1.0952 - val_mae: 1.5833\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0094 - mae: 1.4956 - val_loss: 1.0895 - val_mae: 1.5846\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0234 - mae: 1.5149 - val_loss: 1.0892 - val_mae: 1.5804\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0166 - mae: 1.5056 - val_loss: 1.1175 - val_mae: 1.6108\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0255 - mae: 1.5140 - val_loss: 1.0965 - val_mae: 1.5848\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0203 - mae: 1.5053 - val_loss: 1.0636 - val_mae: 1.5557\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0193 - mae: 1.5095 - val_loss: 1.1195 - val_mae: 1.6119\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0400 - mae: 1.5306 - val_loss: 1.1016 - val_mae: 1.5962\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0061 - mae: 1.4921 - val_loss: 1.0933 - val_mae: 1.5848\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0009 - mae: 1.4892 - val_loss: 1.0835 - val_mae: 1.5772\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0117 - mae: 1.5027 - val_loss: 1.1219 - val_mae: 1.6152\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9926 - mae: 1.4762 - val_loss: 1.0793 - val_mae: 1.5651\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0163 - mae: 1.5036 - val_loss: 1.0901 - val_mae: 1.5830\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9989 - mae: 1.4849 - val_loss: 1.0848 - val_mae: 1.5735\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0155 - mae: 1.5034 - val_loss: 1.0856 - val_mae: 1.5731\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0307 - mae: 1.5208 - val_loss: 1.0992 - val_mae: 1.5884\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0057 - mae: 1.4937 - val_loss: 1.1327 - val_mae: 1.6215\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0096 - mae: 1.4973 - val_loss: 1.1014 - val_mae: 1.5964\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0081 - mae: 1.4956 - val_loss: 1.0931 - val_mae: 1.5757\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0075 - mae: 1.4995 - val_loss: 1.1059 - val_mae: 1.5982\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9944 - mae: 1.4776 - val_loss: 1.1000 - val_mae: 1.5937\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0010 - mae: 1.4875 - val_loss: 1.1073 - val_mae: 1.5976\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0059 - mae: 1.4938 - val_loss: 1.0859 - val_mae: 1.5741\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0022 - mae: 1.4933 - val_loss: 1.0670 - val_mae: 1.5557\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0094 - mae: 1.5018 - val_loss: 1.0806 - val_mae: 1.5681\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0023 - mae: 1.4913 - val_loss: 1.0889 - val_mae: 1.5726\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0003 - mae: 1.4890 - val_loss: 1.0996 - val_mae: 1.5922\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0021 - mae: 1.4866 - val_loss: 1.0710 - val_mae: 1.5566\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0160 - mae: 1.5054 - val_loss: 1.0823 - val_mae: 1.5683\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9941 - mae: 1.4783 - val_loss: 1.0731 - val_mae: 1.5616\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9887 - mae: 1.4760 - val_loss: 1.0855 - val_mae: 1.5691\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9824 - mae: 1.4665 - val_loss: 1.0874 - val_mae: 1.5714\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9933 - mae: 1.4785 - val_loss: 1.0705 - val_mae: 1.5495\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9900 - mae: 1.4748 - val_loss: 1.0981 - val_mae: 1.5858\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9606 - mae: 1.4436 - val_loss: 1.0523 - val_mae: 1.5422\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0090 - mae: 1.4966 - val_loss: 1.0899 - val_mae: 1.5722\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9790 - mae: 1.4614 - val_loss: 1.0960 - val_mae: 1.5815\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9955 - mae: 1.4832 - val_loss: 1.0848 - val_mae: 1.5646\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9918 - mae: 1.4788 - val_loss: 1.1057 - val_mae: 1.5898\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9826 - mae: 1.4630 - val_loss: 1.0819 - val_mae: 1.5664\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9911 - mae: 1.4757 - val_loss: 1.0856 - val_mae: 1.5698\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9876 - mae: 1.4712 - val_loss: 1.0775 - val_mae: 1.5615\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9976 - mae: 1.4865 - val_loss: 1.0765 - val_mae: 1.5626\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9969 - mae: 1.4828 - val_loss: 1.0779 - val_mae: 1.5667\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9952 - mae: 1.4825 - val_loss: 1.0747 - val_mae: 1.5543\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9977 - mae: 1.4840 - val_loss: 1.0645 - val_mae: 1.5394\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9927 - mae: 1.4761 - val_loss: 1.0641 - val_mae: 1.5425\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0012 - mae: 1.4859 - val_loss: 1.0667 - val_mae: 1.5482\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9880 - mae: 1.4739 - val_loss: 1.0688 - val_mae: 1.5569\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9741 - mae: 1.4574 - val_loss: 1.0493 - val_mae: 1.5296\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9800 - mae: 1.4633 - val_loss: 1.0396 - val_mae: 1.5215\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9718 - mae: 1.4552 - val_loss: 1.0447 - val_mae: 1.5312\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9813 - mae: 1.4687 - val_loss: 1.0693 - val_mae: 1.5518\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9821 - mae: 1.4639 - val_loss: 1.0698 - val_mae: 1.5533\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9992 - mae: 1.4839 - val_loss: 1.0523 - val_mae: 1.5229\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9819 - mae: 1.4672 - val_loss: 1.0649 - val_mae: 1.5394\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9771 - mae: 1.4581 - val_loss: 1.0706 - val_mae: 1.5437\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9742 - mae: 1.4577 - val_loss: 1.0829 - val_mae: 1.5625\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9748 - mae: 1.4535 - val_loss: 1.0463 - val_mae: 1.5265\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9785 - mae: 1.4626 - val_loss: 1.0430 - val_mae: 1.5168\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9607 - mae: 1.4410 - val_loss: 1.0469 - val_mae: 1.5162\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9830 - mae: 1.4665 - val_loss: 1.0374 - val_mae: 1.5132\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9667 - mae: 1.4498 - val_loss: 1.0624 - val_mae: 1.5368\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9641 - mae: 1.4466 - val_loss: 1.0414 - val_mae: 1.5170\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9847 - mae: 1.4715 - val_loss: 1.0473 - val_mae: 1.5240\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9844 - mae: 1.4689 - val_loss: 1.0466 - val_mae: 1.5260\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9726 - mae: 1.4575 - val_loss: 1.0553 - val_mae: 1.5354\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9789 - mae: 1.4628 - val_loss: 1.0436 - val_mae: 1.5254\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9731 - mae: 1.4577 - val_loss: 1.0410 - val_mae: 1.5227\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9862 - mae: 1.4695 - val_loss: 1.0451 - val_mae: 1.5197\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9797 - mae: 1.4601 - val_loss: 1.0404 - val_mae: 1.5130\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9642 - mae: 1.4458 - val_loss: 1.0295 - val_mae: 1.5077\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9653 - mae: 1.4468 - val_loss: 1.0361 - val_mae: 1.5107\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9545 - mae: 1.4308 - val_loss: 1.0431 - val_mae: 1.5146\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9543 - mae: 1.4365 - val_loss: 1.0537 - val_mae: 1.5278\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9680 - mae: 1.4532 - val_loss: 1.0479 - val_mae: 1.5201\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9644 - mae: 1.4464 - val_loss: 1.0578 - val_mae: 1.5281\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9660 - mae: 1.4469 - val_loss: 1.0553 - val_mae: 1.5310\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9741 - mae: 1.4566 - val_loss: 1.0447 - val_mae: 1.5152\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9741 - mae: 1.4516 - val_loss: 1.0349 - val_mae: 1.5110\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9763 - mae: 1.4594 - val_loss: 1.0468 - val_mae: 1.5096\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9843 - mae: 1.4672 - val_loss: 1.0396 - val_mae: 1.5098\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9690 - mae: 1.4479 - val_loss: 1.0427 - val_mae: 1.5106\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9546 - mae: 1.4356 - val_loss: 1.0196 - val_mae: 1.4927\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9606 - mae: 1.4420 - val_loss: 1.0362 - val_mae: 1.5164\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9459 - mae: 1.4234 - val_loss: 1.0529 - val_mae: 1.5273\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9564 - mae: 1.4342 - val_loss: 1.0656 - val_mae: 1.5355\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9565 - mae: 1.4386 - val_loss: 1.0422 - val_mae: 1.5134\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9566 - mae: 1.4374 - val_loss: 1.0394 - val_mae: 1.5081\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9553 - mae: 1.4347 - val_loss: 1.0636 - val_mae: 1.5419\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9618 - mae: 1.4449 - val_loss: 1.0532 - val_mae: 1.5283\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9583 - mae: 1.4386 - val_loss: 1.0437 - val_mae: 1.5190\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9408 - mae: 1.4208 - val_loss: 1.0371 - val_mae: 1.5080\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9601 - mae: 1.4421 - val_loss: 1.0638 - val_mae: 1.5385\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9529 - mae: 1.4281 - val_loss: 1.0445 - val_mae: 1.5220\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9428 - mae: 1.4225 - val_loss: 1.0293 - val_mae: 1.5088\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9397 - mae: 1.4160 - val_loss: 1.0478 - val_mae: 1.5268\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9481 - mae: 1.4280 - val_loss: 1.0408 - val_mae: 1.5179\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9615 - mae: 1.4398 - val_loss: 1.0281 - val_mae: 1.5005\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9419 - mae: 1.4178 - val_loss: 1.0318 - val_mae: 1.5111\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9707 - mae: 1.4540 - val_loss: 1.0168 - val_mae: 1.4922\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9698 - mae: 1.4504 - val_loss: 1.0485 - val_mae: 1.5186\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9588 - mae: 1.4411 - val_loss: 1.0213 - val_mae: 1.4987\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9642 - mae: 1.4444 - val_loss: 1.0198 - val_mae: 1.4937\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9590 - mae: 1.4359 - val_loss: 1.0274 - val_mae: 1.5065\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9579 - mae: 1.4380 - val_loss: 1.0295 - val_mae: 1.5013\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9399 - mae: 1.4176 - val_loss: 1.0497 - val_mae: 1.5229\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9574 - mae: 1.4379 - val_loss: 1.0736 - val_mae: 1.5493\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9698 - mae: 1.4531 - val_loss: 1.0528 - val_mae: 1.5273\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9597 - mae: 1.4385 - val_loss: 1.0608 - val_mae: 1.5360\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9581 - mae: 1.4371 - val_loss: 1.0319 - val_mae: 1.5094\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9624 - mae: 1.4428 - val_loss: 1.0336 - val_mae: 1.5106\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9626 - mae: 1.4394 - val_loss: 1.0542 - val_mae: 1.5231\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9570 - mae: 1.4358 - val_loss: 1.0642 - val_mae: 1.5452\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9612 - mae: 1.4404 - val_loss: 1.0636 - val_mae: 1.5395\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9444 - mae: 1.4191 - val_loss: 1.0321 - val_mae: 1.5006\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9434 - mae: 1.4201 - val_loss: 1.0336 - val_mae: 1.5063\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9309 - mae: 1.4072 - val_loss: 1.0245 - val_mae: 1.4967\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9631 - mae: 1.4461 - val_loss: 1.0454 - val_mae: 1.5162\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9435 - mae: 1.4221 - val_loss: 1.0378 - val_mae: 1.5058\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9320 - mae: 1.4070 - val_loss: 1.0480 - val_mae: 1.5265\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9610 - mae: 1.4434 - val_loss: 1.0397 - val_mae: 1.5096\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9556 - mae: 1.4389 - val_loss: 1.0400 - val_mae: 1.5119\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9743 - mae: 1.4539 - val_loss: 1.0230 - val_mae: 1.4957\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9687 - mae: 1.4500 - val_loss: 1.0218 - val_mae: 1.4941\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9594 - mae: 1.4387 - val_loss: 1.0312 - val_mae: 1.5016\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9517 - mae: 1.4277 - val_loss: 1.0257 - val_mae: 1.4983\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9422 - mae: 1.4196 - val_loss: 1.0386 - val_mae: 1.5104\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9463 - mae: 1.4266 - val_loss: 1.0488 - val_mae: 1.5216\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9565 - mae: 1.4375 - val_loss: 1.0447 - val_mae: 1.5227\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9434 - mae: 1.4204 - val_loss: 1.0164 - val_mae: 1.4930\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9477 - mae: 1.4267 - val_loss: 1.0185 - val_mae: 1.4973\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9599 - mae: 1.4421 - val_loss: 1.0349 - val_mae: 1.5151\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9508 - mae: 1.4285 - val_loss: 1.0257 - val_mae: 1.4962\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9540 - mae: 1.4373 - val_loss: 1.0017 - val_mae: 1.4755\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9624 - mae: 1.4452 - val_loss: 1.0232 - val_mae: 1.4980\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9495 - mae: 1.4263 - val_loss: 1.0220 - val_mae: 1.4945\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9421 - mae: 1.4171 - val_loss: 1.0019 - val_mae: 1.4628\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9566 - mae: 1.4356 - val_loss: 1.0075 - val_mae: 1.4795\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9403 - mae: 1.4187 - val_loss: 1.0126 - val_mae: 1.4844\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9446 - mae: 1.4251 - val_loss: 1.0154 - val_mae: 1.4897\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9484 - mae: 1.4265 - val_loss: 0.9869 - val_mae: 1.4597\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9502 - mae: 1.4296 - val_loss: 1.0047 - val_mae: 1.4765\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9433 - mae: 1.4262 - val_loss: 1.0262 - val_mae: 1.5022\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9428 - mae: 1.4202 - val_loss: 0.9955 - val_mae: 1.4621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:29:07,632] Trial 17 finished with value: 0.9954807758331299 and parameters: {'dropout_2': 0.5555804100793633, 'dropout_3': 0.6680507277102373, 'dropout_4': 0.27318282340646655, 'dropout_5': 0.3492397551948169, 'learning_rate': 0.0006896181467932462, 'epochs': 200, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 3s 8ms/step - loss: 1.4594 - mae: 2.0006 - val_loss: 1.3222 - val_mae: 1.7815\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2679 - mae: 1.7825 - val_loss: 1.2650 - val_mae: 1.7289\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1967 - mae: 1.6957 - val_loss: 1.2455 - val_mae: 1.7331\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1752 - mae: 1.6670 - val_loss: 1.1776 - val_mae: 1.6551\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1333 - mae: 1.6222 - val_loss: 1.1764 - val_mae: 1.6712\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1149 - mae: 1.6021 - val_loss: 1.1320 - val_mae: 1.6280\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1169 - mae: 1.6070 - val_loss: 1.1356 - val_mae: 1.6319\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0988 - mae: 1.5880 - val_loss: 1.1463 - val_mae: 1.6414\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0892 - mae: 1.5780 - val_loss: 1.1165 - val_mae: 1.6160\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1014 - mae: 1.5907 - val_loss: 1.1505 - val_mae: 1.6542\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0898 - mae: 1.5813 - val_loss: 1.1158 - val_mae: 1.5964\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0661 - mae: 1.5540 - val_loss: 1.1302 - val_mae: 1.6260\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0738 - mae: 1.5643 - val_loss: 1.1242 - val_mae: 1.6253\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0770 - mae: 1.5655 - val_loss: 1.1509 - val_mae: 1.6535\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0791 - mae: 1.5626 - val_loss: 1.1206 - val_mae: 1.6266\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0531 - mae: 1.5418 - val_loss: 1.0783 - val_mae: 1.5670\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0557 - mae: 1.5395 - val_loss: 1.0712 - val_mae: 1.5626\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0675 - mae: 1.5575 - val_loss: 1.1096 - val_mae: 1.5974\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0612 - mae: 1.5474 - val_loss: 1.0897 - val_mae: 1.5778\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0753 - mae: 1.5643 - val_loss: 1.1047 - val_mae: 1.6058\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0490 - mae: 1.5387 - val_loss: 1.0795 - val_mae: 1.5689\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0488 - mae: 1.5369 - val_loss: 1.1242 - val_mae: 1.6191\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0507 - mae: 1.5398 - val_loss: 1.0904 - val_mae: 1.5822\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0507 - mae: 1.5378 - val_loss: 1.0850 - val_mae: 1.5785\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0523 - mae: 1.5434 - val_loss: 1.1068 - val_mae: 1.5969\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0440 - mae: 1.5340 - val_loss: 1.1229 - val_mae: 1.6208\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0401 - mae: 1.5272 - val_loss: 1.1036 - val_mae: 1.5930\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0266 - mae: 1.5142 - val_loss: 1.1098 - val_mae: 1.6103\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0300 - mae: 1.5176 - val_loss: 1.1097 - val_mae: 1.6102\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0277 - mae: 1.5168 - val_loss: 1.1209 - val_mae: 1.6225\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0218 - mae: 1.5120 - val_loss: 1.0975 - val_mae: 1.6003\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0055 - mae: 1.4931 - val_loss: 1.0784 - val_mae: 1.5811\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0265 - mae: 1.5134 - val_loss: 1.0565 - val_mae: 1.5435\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0146 - mae: 1.5028 - val_loss: 1.0890 - val_mae: 1.5882\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0202 - mae: 1.5039 - val_loss: 1.0874 - val_mae: 1.5856\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0288 - mae: 1.5154 - val_loss: 1.0745 - val_mae: 1.5717\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0243 - mae: 1.5064 - val_loss: 1.0416 - val_mae: 1.5346\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0148 - mae: 1.5042 - val_loss: 1.0655 - val_mae: 1.5568\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0208 - mae: 1.5092 - val_loss: 1.0933 - val_mae: 1.5938\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0290 - mae: 1.5164 - val_loss: 1.0811 - val_mae: 1.5694\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0113 - mae: 1.4967 - val_loss: 1.0681 - val_mae: 1.5654\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0127 - mae: 1.4949 - val_loss: 1.0412 - val_mae: 1.5247\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9969 - mae: 1.4796 - val_loss: 1.0226 - val_mae: 1.5146\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0198 - mae: 1.5086 - val_loss: 1.0715 - val_mae: 1.5649\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0149 - mae: 1.5012 - val_loss: 1.0523 - val_mae: 1.5415\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0142 - mae: 1.4989 - val_loss: 1.0776 - val_mae: 1.5734\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0033 - mae: 1.4886 - val_loss: 1.0810 - val_mae: 1.5655\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9903 - mae: 1.4732 - val_loss: 1.0531 - val_mae: 1.5455\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0046 - mae: 1.4885 - val_loss: 1.0476 - val_mae: 1.5368\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9920 - mae: 1.4757 - val_loss: 1.1003 - val_mae: 1.5980\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9982 - mae: 1.4812 - val_loss: 1.0656 - val_mae: 1.5526\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9982 - mae: 1.4812 - val_loss: 1.0875 - val_mae: 1.5749\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9979 - mae: 1.4796 - val_loss: 1.0967 - val_mae: 1.5928\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9832 - mae: 1.4651 - val_loss: 1.0527 - val_mae: 1.5445\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0008 - mae: 1.4830 - val_loss: 1.0655 - val_mae: 1.5601\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9836 - mae: 1.4641 - val_loss: 1.0574 - val_mae: 1.5449\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0012 - mae: 1.4820 - val_loss: 1.0436 - val_mae: 1.5365\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9844 - mae: 1.4652 - val_loss: 1.0369 - val_mae: 1.5235\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9905 - mae: 1.4735 - val_loss: 1.0845 - val_mae: 1.5731\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9945 - mae: 1.4771 - val_loss: 1.0555 - val_mae: 1.5377\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9916 - mae: 1.4729 - val_loss: 1.0914 - val_mae: 1.5813\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9908 - mae: 1.4692 - val_loss: 1.0515 - val_mae: 1.5320\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9846 - mae: 1.4675 - val_loss: 1.0468 - val_mae: 1.5347\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9852 - mae: 1.4686 - val_loss: 1.0743 - val_mae: 1.5596\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9979 - mae: 1.4768 - val_loss: 1.0675 - val_mae: 1.5560\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0019 - mae: 1.4823 - val_loss: 1.0558 - val_mae: 1.5428\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9839 - mae: 1.4643 - val_loss: 1.0526 - val_mae: 1.5352\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9859 - mae: 1.4685 - val_loss: 1.0604 - val_mae: 1.5307\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9841 - mae: 1.4654 - val_loss: 1.0867 - val_mae: 1.5730\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9900 - mae: 1.4767 - val_loss: 1.0673 - val_mae: 1.5433\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9875 - mae: 1.4709 - val_loss: 1.0522 - val_mae: 1.5260\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9931 - mae: 1.4741 - val_loss: 1.0781 - val_mae: 1.5628\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9720 - mae: 1.4554 - val_loss: 1.0883 - val_mae: 1.5804\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9964 - mae: 1.4814 - val_loss: 1.0265 - val_mae: 1.5102\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9728 - mae: 1.4563 - val_loss: 1.0429 - val_mae: 1.5206\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9572 - mae: 1.4359 - val_loss: 1.0106 - val_mae: 1.4850\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9740 - mae: 1.4521 - val_loss: 1.0537 - val_mae: 1.5405\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9670 - mae: 1.4459 - val_loss: 1.0144 - val_mae: 1.4969\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9902 - mae: 1.4767 - val_loss: 1.0337 - val_mae: 1.5202\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9834 - mae: 1.4670 - val_loss: 1.0206 - val_mae: 1.5001\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9687 - mae: 1.4486 - val_loss: 1.0019 - val_mae: 1.4671\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9795 - mae: 1.4631 - val_loss: 1.0067 - val_mae: 1.4846\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9722 - mae: 1.4517 - val_loss: 1.0088 - val_mae: 1.4852\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9748 - mae: 1.4520 - val_loss: 1.0284 - val_mae: 1.5096\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9587 - mae: 1.4369 - val_loss: 1.0127 - val_mae: 1.4763\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9895 - mae: 1.4736 - val_loss: 1.0066 - val_mae: 1.4831\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9648 - mae: 1.4438 - val_loss: 1.0129 - val_mae: 1.4955\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9639 - mae: 1.4448 - val_loss: 1.0174 - val_mae: 1.5007\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9688 - mae: 1.4496 - val_loss: 1.0015 - val_mae: 1.4778\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9811 - mae: 1.4628 - val_loss: 1.0153 - val_mae: 1.4848\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9533 - mae: 1.4295 - val_loss: 1.0250 - val_mae: 1.5072\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9669 - mae: 1.4479 - val_loss: 1.0228 - val_mae: 1.5060\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9584 - mae: 1.4338 - val_loss: 1.0302 - val_mae: 1.5027\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9667 - mae: 1.4458 - val_loss: 1.0042 - val_mae: 1.4765\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9602 - mae: 1.4369 - val_loss: 1.0308 - val_mae: 1.5047\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9522 - mae: 1.4346 - val_loss: 1.0203 - val_mae: 1.5089\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9575 - mae: 1.4392 - val_loss: 0.9858 - val_mae: 1.4638\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9358 - mae: 1.4118 - val_loss: 1.0057 - val_mae: 1.4771\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9544 - mae: 1.4318 - val_loss: 0.9960 - val_mae: 1.4722\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9520 - mae: 1.4339 - val_loss: 1.0138 - val_mae: 1.4923\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9557 - mae: 1.4311 - val_loss: 0.9849 - val_mae: 1.4510\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9441 - mae: 1.4206 - val_loss: 0.9964 - val_mae: 1.4667\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9590 - mae: 1.4405 - val_loss: 1.0181 - val_mae: 1.4974\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9468 - mae: 1.4281 - val_loss: 1.0113 - val_mae: 1.4675\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9575 - mae: 1.4360 - val_loss: 1.0291 - val_mae: 1.5016\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9626 - mae: 1.4393 - val_loss: 1.0228 - val_mae: 1.4943\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9469 - mae: 1.4220 - val_loss: 1.0695 - val_mae: 1.5566\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9575 - mae: 1.4350 - val_loss: 1.0251 - val_mae: 1.4957\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9590 - mae: 1.4385 - val_loss: 1.0054 - val_mae: 1.4703\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9516 - mae: 1.4306 - val_loss: 0.9992 - val_mae: 1.4781\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9527 - mae: 1.4322 - val_loss: 1.0114 - val_mae: 1.4837\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9659 - mae: 1.4415 - val_loss: 0.9849 - val_mae: 1.4499\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9417 - mae: 1.4172 - val_loss: 1.0068 - val_mae: 1.4848\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9464 - mae: 1.4218 - val_loss: 1.0149 - val_mae: 1.4868\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9486 - mae: 1.4255 - val_loss: 1.0104 - val_mae: 1.4814\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9434 - mae: 1.4192 - val_loss: 1.0059 - val_mae: 1.4753\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9585 - mae: 1.4323 - val_loss: 1.0109 - val_mae: 1.4726\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9329 - mae: 1.4082 - val_loss: 1.0036 - val_mae: 1.4764\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9393 - mae: 1.4145 - val_loss: 0.9471 - val_mae: 1.4085\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9499 - mae: 1.4274 - val_loss: 0.9946 - val_mae: 1.4614\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9339 - mae: 1.4079 - val_loss: 1.0124 - val_mae: 1.4672\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9519 - mae: 1.4277 - val_loss: 1.0052 - val_mae: 1.4740\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9378 - mae: 1.4179 - val_loss: 0.9892 - val_mae: 1.4597\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9403 - mae: 1.4172 - val_loss: 0.9789 - val_mae: 1.4452\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9307 - mae: 1.4036 - val_loss: 0.9844 - val_mae: 1.4480\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9576 - mae: 1.4375 - val_loss: 0.9963 - val_mae: 1.4566\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9571 - mae: 1.4352 - val_loss: 1.0429 - val_mae: 1.5279\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9582 - mae: 1.4324 - val_loss: 1.0021 - val_mae: 1.4635\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9563 - mae: 1.4290 - val_loss: 1.0191 - val_mae: 1.4801\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9450 - mae: 1.4189 - val_loss: 1.0117 - val_mae: 1.4692\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9265 - mae: 1.3985 - val_loss: 0.9945 - val_mae: 1.4552\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9426 - mae: 1.4168 - val_loss: 1.0153 - val_mae: 1.4773\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9519 - mae: 1.4322 - val_loss: 1.0089 - val_mae: 1.4788\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9358 - mae: 1.4075 - val_loss: 0.9983 - val_mae: 1.4532\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9373 - mae: 1.4106 - val_loss: 0.9656 - val_mae: 1.4284\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9506 - mae: 1.4242 - val_loss: 0.9971 - val_mae: 1.4632\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9201 - mae: 1.3963 - val_loss: 1.0331 - val_mae: 1.5096\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9394 - mae: 1.4144 - val_loss: 0.9693 - val_mae: 1.4308\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9285 - mae: 1.4050 - val_loss: 0.9886 - val_mae: 1.4556\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9257 - mae: 1.4023 - val_loss: 0.9949 - val_mae: 1.4677\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9442 - mae: 1.4214 - val_loss: 0.9925 - val_mae: 1.4530\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9336 - mae: 1.4050 - val_loss: 0.9863 - val_mae: 1.4451\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9486 - mae: 1.4228 - val_loss: 1.0083 - val_mae: 1.4779\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9469 - mae: 1.4219 - val_loss: 0.9937 - val_mae: 1.4623\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9498 - mae: 1.4236 - val_loss: 1.0077 - val_mae: 1.4752\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9442 - mae: 1.4174 - val_loss: 0.9704 - val_mae: 1.4340\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9386 - mae: 1.4120 - val_loss: 1.0079 - val_mae: 1.4836\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9337 - mae: 1.4066 - val_loss: 0.9854 - val_mae: 1.4522\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9390 - mae: 1.4113 - val_loss: 0.9654 - val_mae: 1.4386\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9341 - mae: 1.4117 - val_loss: 1.0082 - val_mae: 1.4735\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9437 - mae: 1.4179 - val_loss: 0.9691 - val_mae: 1.4235\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9281 - mae: 1.4010 - val_loss: 0.9896 - val_mae: 1.4591\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9392 - mae: 1.4092 - val_loss: 0.9948 - val_mae: 1.4587\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9406 - mae: 1.4163 - val_loss: 1.0015 - val_mae: 1.4611\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9564 - mae: 1.4333 - val_loss: 1.0152 - val_mae: 1.4727\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9392 - mae: 1.4142 - val_loss: 0.9920 - val_mae: 1.4474\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9221 - mae: 1.3996 - val_loss: 0.9951 - val_mae: 1.4644\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9283 - mae: 1.4017 - val_loss: 0.9871 - val_mae: 1.4603\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9377 - mae: 1.4142 - val_loss: 0.9865 - val_mae: 1.4420\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9369 - mae: 1.4112 - val_loss: 0.9843 - val_mae: 1.4435\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9289 - mae: 1.4028 - val_loss: 0.9935 - val_mae: 1.4574\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9135 - mae: 1.3842 - val_loss: 1.0048 - val_mae: 1.4706\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9259 - mae: 1.4010 - val_loss: 1.0099 - val_mae: 1.4682\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9396 - mae: 1.4138 - val_loss: 0.9988 - val_mae: 1.4723\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9462 - mae: 1.4211 - val_loss: 1.0341 - val_mae: 1.5103\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9164 - mae: 1.3885 - val_loss: 1.0143 - val_mae: 1.4699\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9355 - mae: 1.4094 - val_loss: 0.9980 - val_mae: 1.4719\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9345 - mae: 1.4104 - val_loss: 0.9794 - val_mae: 1.4423\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9374 - mae: 1.4072 - val_loss: 1.0075 - val_mae: 1.4756\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9217 - mae: 1.3926 - val_loss: 0.9917 - val_mae: 1.4606\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9183 - mae: 1.3903 - val_loss: 1.0155 - val_mae: 1.4887\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9506 - mae: 1.4250 - val_loss: 0.9853 - val_mae: 1.4421\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9337 - mae: 1.4099 - val_loss: 1.0200 - val_mae: 1.4806\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9285 - mae: 1.4012 - val_loss: 0.9740 - val_mae: 1.4288\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9069 - mae: 1.3799 - val_loss: 0.9990 - val_mae: 1.4556\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9320 - mae: 1.4058 - val_loss: 0.9898 - val_mae: 1.4532\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9177 - mae: 1.3895 - val_loss: 0.9754 - val_mae: 1.4396\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9436 - mae: 1.4172 - val_loss: 0.9866 - val_mae: 1.4496\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9084 - mae: 1.3739 - val_loss: 0.9844 - val_mae: 1.4397\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9368 - mae: 1.4097 - val_loss: 0.9972 - val_mae: 1.4574\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9315 - mae: 1.4041 - val_loss: 0.9981 - val_mae: 1.4624\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9260 - mae: 1.4034 - val_loss: 0.9807 - val_mae: 1.4475\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9353 - mae: 1.4100 - val_loss: 1.0677 - val_mae: 1.5515\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9348 - mae: 1.4088 - val_loss: 0.9987 - val_mae: 1.4632\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9307 - mae: 1.4050 - val_loss: 1.0060 - val_mae: 1.4748\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9359 - mae: 1.4080 - val_loss: 0.9998 - val_mae: 1.4689\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9072 - mae: 1.3787 - val_loss: 0.9906 - val_mae: 1.4647\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9367 - mae: 1.4159 - val_loss: 1.0110 - val_mae: 1.4864\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9192 - mae: 1.3935 - val_loss: 1.0317 - val_mae: 1.4922\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9060 - mae: 1.3798 - val_loss: 0.9858 - val_mae: 1.4583\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9346 - mae: 1.4083 - val_loss: 0.9958 - val_mae: 1.4700\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9228 - mae: 1.3934 - val_loss: 0.9871 - val_mae: 1.4496\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9245 - mae: 1.3965 - val_loss: 1.0284 - val_mae: 1.4793\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9252 - mae: 1.3987 - val_loss: 1.0068 - val_mae: 1.4689\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9222 - mae: 1.3935 - val_loss: 1.0004 - val_mae: 1.4533\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9191 - mae: 1.3900 - val_loss: 0.9932 - val_mae: 1.4547\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9124 - mae: 1.3830 - val_loss: 0.9929 - val_mae: 1.4631\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9092 - mae: 1.3755 - val_loss: 0.9588 - val_mae: 1.4208\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9180 - mae: 1.3890 - val_loss: 0.9529 - val_mae: 1.4069\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.8931 - mae: 1.3620 - val_loss: 0.9861 - val_mae: 1.4444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:31:31,762] Trial 18 finished with value: 0.9860584735870361 and parameters: {'dropout_2': 0.2312311473321915, 'dropout_3': 0.7512350851577765, 'dropout_4': 0.47775743957013633, 'dropout_5': 0.3888922448221563, 'learning_rate': 0.0018866776453587183, 'epochs': 200, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "108/108 [==============================] - 3s 11ms/step - loss: 1.6904 - mae: 2.2566 - val_loss: 1.3651 - val_mae: 1.8761\n",
            "Epoch 2/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.6732 - mae: 2.2394 - val_loss: 1.3780 - val_mae: 1.8963\n",
            "Epoch 3/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.6406 - mae: 2.2021 - val_loss: 1.3768 - val_mae: 1.8946\n",
            "Epoch 4/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.6317 - mae: 2.1926 - val_loss: 1.3793 - val_mae: 1.8984\n",
            "Epoch 5/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.6217 - mae: 2.1806 - val_loss: 1.3762 - val_mae: 1.8940\n",
            "Epoch 6/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.6060 - mae: 2.1682 - val_loss: 1.3842 - val_mae: 1.9052\n",
            "Epoch 7/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.6028 - mae: 2.1626 - val_loss: 1.3803 - val_mae: 1.8988\n",
            "Epoch 8/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.6024 - mae: 2.1605 - val_loss: 1.3755 - val_mae: 1.8917\n",
            "Epoch 9/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5974 - mae: 2.1572 - val_loss: 1.3725 - val_mae: 1.8872\n",
            "Epoch 10/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.5980 - mae: 2.1564 - val_loss: 1.3724 - val_mae: 1.8878\n",
            "Epoch 11/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.5895 - mae: 2.1484 - val_loss: 1.3671 - val_mae: 1.8806\n",
            "Epoch 12/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5781 - mae: 2.1361 - val_loss: 1.3629 - val_mae: 1.8743\n",
            "Epoch 13/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5809 - mae: 2.1371 - val_loss: 1.3616 - val_mae: 1.8725\n",
            "Epoch 14/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5526 - mae: 2.1074 - val_loss: 1.3629 - val_mae: 1.8726\n",
            "Epoch 15/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.5575 - mae: 2.1143 - val_loss: 1.3604 - val_mae: 1.8708\n",
            "Epoch 16/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5415 - mae: 2.0946 - val_loss: 1.3586 - val_mae: 1.8681\n",
            "Epoch 17/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.5176 - mae: 2.0683 - val_loss: 1.3558 - val_mae: 1.8637\n",
            "Epoch 18/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5512 - mae: 2.1087 - val_loss: 1.3540 - val_mae: 1.8612\n",
            "Epoch 19/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.4969 - mae: 2.0457 - val_loss: 1.3505 - val_mae: 1.8541\n",
            "Epoch 20/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.5181 - mae: 2.0684 - val_loss: 1.3491 - val_mae: 1.8523\n",
            "Epoch 21/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4997 - mae: 2.0529 - val_loss: 1.3469 - val_mae: 1.8467\n",
            "Epoch 22/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.5100 - mae: 2.0622 - val_loss: 1.3466 - val_mae: 1.8472\n",
            "Epoch 23/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.5154 - mae: 2.0655 - val_loss: 1.3439 - val_mae: 1.8432\n",
            "Epoch 24/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.4926 - mae: 2.0429 - val_loss: 1.3442 - val_mae: 1.8425\n",
            "Epoch 25/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.4937 - mae: 2.0415 - val_loss: 1.3426 - val_mae: 1.8417\n",
            "Epoch 26/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.5128 - mae: 2.0642 - val_loss: 1.3406 - val_mae: 1.8364\n",
            "Epoch 27/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.4945 - mae: 2.0447 - val_loss: 1.3412 - val_mae: 1.8369\n",
            "Epoch 28/100\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.4654 - mae: 2.0109 - val_loss: 1.3403 - val_mae: 1.8354\n",
            "Epoch 29/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.4902 - mae: 2.0380 - val_loss: 1.3411 - val_mae: 1.8361\n",
            "Epoch 30/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.4683 - mae: 2.0176 - val_loss: 1.3388 - val_mae: 1.8331\n",
            "Epoch 31/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.4640 - mae: 2.0100 - val_loss: 1.3377 - val_mae: 1.8307\n",
            "Epoch 32/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.4586 - mae: 2.0010 - val_loss: 1.3388 - val_mae: 1.8319\n",
            "Epoch 33/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4477 - mae: 1.9941 - val_loss: 1.3348 - val_mae: 1.8260\n",
            "Epoch 34/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.4539 - mae: 1.9976 - val_loss: 1.3322 - val_mae: 1.8222\n",
            "Epoch 35/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4447 - mae: 1.9896 - val_loss: 1.3331 - val_mae: 1.8218\n",
            "Epoch 36/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.4683 - mae: 2.0128 - val_loss: 1.3319 - val_mae: 1.8209\n",
            "Epoch 37/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.4496 - mae: 1.9957 - val_loss: 1.3310 - val_mae: 1.8198\n",
            "Epoch 38/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.4323 - mae: 1.9737 - val_loss: 1.3306 - val_mae: 1.8181\n",
            "Epoch 39/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4525 - mae: 1.9981 - val_loss: 1.3298 - val_mae: 1.8157\n",
            "Epoch 40/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.4324 - mae: 1.9752 - val_loss: 1.3298 - val_mae: 1.8164\n",
            "Epoch 41/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.4219 - mae: 1.9648 - val_loss: 1.3283 - val_mae: 1.8120\n",
            "Epoch 42/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.4086 - mae: 1.9521 - val_loss: 1.3265 - val_mae: 1.8097\n",
            "Epoch 43/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.4214 - mae: 1.9621 - val_loss: 1.3247 - val_mae: 1.8071\n",
            "Epoch 44/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.4125 - mae: 1.9507 - val_loss: 1.3231 - val_mae: 1.8036\n",
            "Epoch 45/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.4081 - mae: 1.9486 - val_loss: 1.3231 - val_mae: 1.8053\n",
            "Epoch 46/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.4284 - mae: 1.9696 - val_loss: 1.3240 - val_mae: 1.8038\n",
            "Epoch 47/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4187 - mae: 1.9574 - val_loss: 1.3230 - val_mae: 1.8045\n",
            "Epoch 48/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.4053 - mae: 1.9438 - val_loss: 1.3237 - val_mae: 1.8055\n",
            "Epoch 49/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.4063 - mae: 1.9425 - val_loss: 1.3220 - val_mae: 1.8050\n",
            "Epoch 50/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3964 - mae: 1.9362 - val_loss: 1.3216 - val_mae: 1.8031\n",
            "Epoch 51/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3923 - mae: 1.9296 - val_loss: 1.3200 - val_mae: 1.8020\n",
            "Epoch 52/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3953 - mae: 1.9328 - val_loss: 1.3182 - val_mae: 1.8020\n",
            "Epoch 53/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.4106 - mae: 1.9470 - val_loss: 1.3187 - val_mae: 1.8018\n",
            "Epoch 54/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3912 - mae: 1.9264 - val_loss: 1.3160 - val_mae: 1.7981\n",
            "Epoch 55/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3851 - mae: 1.9198 - val_loss: 1.3135 - val_mae: 1.7972\n",
            "Epoch 56/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3758 - mae: 1.9074 - val_loss: 1.3157 - val_mae: 1.8030\n",
            "Epoch 57/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3868 - mae: 1.9207 - val_loss: 1.3159 - val_mae: 1.8070\n",
            "Epoch 58/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3869 - mae: 1.9208 - val_loss: 1.3134 - val_mae: 1.8031\n",
            "Epoch 59/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3812 - mae: 1.9149 - val_loss: 1.3134 - val_mae: 1.7972\n",
            "Epoch 60/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3767 - mae: 1.9102 - val_loss: 1.3096 - val_mae: 1.7932\n",
            "Epoch 61/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3613 - mae: 1.8940 - val_loss: 1.3106 - val_mae: 1.7948\n",
            "Epoch 62/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3531 - mae: 1.8853 - val_loss: 1.3118 - val_mae: 1.8002\n",
            "Epoch 63/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3653 - mae: 1.8969 - val_loss: 1.3087 - val_mae: 1.7979\n",
            "Epoch 64/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3664 - mae: 1.8977 - val_loss: 1.3066 - val_mae: 1.7928\n",
            "Epoch 65/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3429 - mae: 1.8695 - val_loss: 1.3076 - val_mae: 1.7991\n",
            "Epoch 66/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3605 - mae: 1.8902 - val_loss: 1.3047 - val_mae: 1.7962\n",
            "Epoch 67/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3753 - mae: 1.9081 - val_loss: 1.3045 - val_mae: 1.7944\n",
            "Epoch 68/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3493 - mae: 1.8815 - val_loss: 1.3032 - val_mae: 1.7945\n",
            "Epoch 69/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3495 - mae: 1.8780 - val_loss: 1.3023 - val_mae: 1.7909\n",
            "Epoch 70/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3591 - mae: 1.8915 - val_loss: 1.3017 - val_mae: 1.7911\n",
            "Epoch 71/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3503 - mae: 1.8790 - val_loss: 1.3009 - val_mae: 1.7915\n",
            "Epoch 72/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3380 - mae: 1.8677 - val_loss: 1.2990 - val_mae: 1.7885\n",
            "Epoch 73/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3362 - mae: 1.8644 - val_loss: 1.2991 - val_mae: 1.7917\n",
            "Epoch 74/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3575 - mae: 1.8885 - val_loss: 1.2966 - val_mae: 1.7893\n",
            "Epoch 75/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3301 - mae: 1.8554 - val_loss: 1.2958 - val_mae: 1.7854\n",
            "Epoch 76/100\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.3495 - mae: 1.8787 - val_loss: 1.2942 - val_mae: 1.7864\n",
            "Epoch 77/100\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.3183 - mae: 1.8470 - val_loss: 1.2927 - val_mae: 1.7877\n",
            "Epoch 78/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3251 - mae: 1.8512 - val_loss: 1.2926 - val_mae: 1.7856\n",
            "Epoch 79/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.3282 - mae: 1.8574 - val_loss: 1.2929 - val_mae: 1.7861\n",
            "Epoch 80/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3366 - mae: 1.8631 - val_loss: 1.2916 - val_mae: 1.7860\n",
            "Epoch 81/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.3268 - mae: 1.8518 - val_loss: 1.2886 - val_mae: 1.7841\n",
            "Epoch 82/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3255 - mae: 1.8504 - val_loss: 1.2880 - val_mae: 1.7829\n",
            "Epoch 83/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3366 - mae: 1.8623 - val_loss: 1.2883 - val_mae: 1.7840\n",
            "Epoch 84/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3208 - mae: 1.8494 - val_loss: 1.2873 - val_mae: 1.7824\n",
            "Epoch 85/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3245 - mae: 1.8460 - val_loss: 1.2847 - val_mae: 1.7784\n",
            "Epoch 86/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2989 - mae: 1.8194 - val_loss: 1.2868 - val_mae: 1.7807\n",
            "Epoch 87/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.3041 - mae: 1.8257 - val_loss: 1.2870 - val_mae: 1.7807\n",
            "Epoch 88/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3200 - mae: 1.8432 - val_loss: 1.2846 - val_mae: 1.7816\n",
            "Epoch 89/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2961 - mae: 1.8180 - val_loss: 1.2827 - val_mae: 1.7811\n",
            "Epoch 90/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2951 - mae: 1.8164 - val_loss: 1.2826 - val_mae: 1.7797\n",
            "Epoch 91/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3036 - mae: 1.8254 - val_loss: 1.2811 - val_mae: 1.7772\n",
            "Epoch 92/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3024 - mae: 1.8243 - val_loss: 1.2827 - val_mae: 1.7788\n",
            "Epoch 93/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2998 - mae: 1.8219 - val_loss: 1.2818 - val_mae: 1.7775\n",
            "Epoch 94/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3069 - mae: 1.8300 - val_loss: 1.2779 - val_mae: 1.7734\n",
            "Epoch 95/100\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.2829 - mae: 1.8030 - val_loss: 1.2779 - val_mae: 1.7733\n",
            "Epoch 96/100\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.2830 - mae: 1.8013 - val_loss: 1.2791 - val_mae: 1.7756\n",
            "Epoch 97/100\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.2880 - mae: 1.8080 - val_loss: 1.2775 - val_mae: 1.7743\n",
            "Epoch 98/100\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.2916 - mae: 1.8142 - val_loss: 1.2773 - val_mae: 1.7766\n",
            "Epoch 99/100\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.2843 - mae: 1.8040 - val_loss: 1.2744 - val_mae: 1.7747\n",
            "Epoch 100/100\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.2880 - mae: 1.8100 - val_loss: 1.2725 - val_mae: 1.7715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:32:56,302] Trial 19 finished with value: 1.2724874019622803 and parameters: {'dropout_2': 0.615805386117916, 'dropout_3': 0.7300730800417815, 'dropout_4': 0.3801429870775192, 'dropout_5': 0.23892484828987148, 'learning_rate': 3.069609666672695e-05, 'epochs': 100, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 4s 11ms/step - loss: 2.6058 - mae: 3.1842 - val_loss: 1.3489 - val_mae: 1.8665\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 2.4598 - mae: 3.0335 - val_loss: 1.3354 - val_mae: 1.8497\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 2.2047 - mae: 2.7676 - val_loss: 1.3241 - val_mae: 1.8292\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.9954 - mae: 2.5506 - val_loss: 1.2947 - val_mae: 1.7697\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.8493 - mae: 2.3942 - val_loss: 1.2906 - val_mae: 1.7688\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7261 - mae: 2.2666 - val_loss: 1.2691 - val_mae: 1.7355\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.5795 - mae: 2.1096 - val_loss: 1.2621 - val_mae: 1.7221\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5115 - mae: 2.0339 - val_loss: 1.2535 - val_mae: 1.7138\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.4712 - mae: 1.9862 - val_loss: 1.2636 - val_mae: 1.7216\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.4171 - mae: 1.9244 - val_loss: 1.2653 - val_mae: 1.7223\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3805 - mae: 1.8797 - val_loss: 1.2561 - val_mae: 1.7088\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3443 - mae: 1.8407 - val_loss: 1.2564 - val_mae: 1.7118\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.3290 - mae: 1.8186 - val_loss: 1.2434 - val_mae: 1.7005\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2864 - mae: 1.7725 - val_loss: 1.2375 - val_mae: 1.6957\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2797 - mae: 1.7647 - val_loss: 1.2271 - val_mae: 1.6869\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2552 - mae: 1.7367 - val_loss: 1.2235 - val_mae: 1.6857\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2676 - mae: 1.7448 - val_loss: 1.2181 - val_mae: 1.6792\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2546 - mae: 1.7328 - val_loss: 1.2164 - val_mae: 1.6784\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2381 - mae: 1.7145 - val_loss: 1.2104 - val_mae: 1.6724\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2363 - mae: 1.7105 - val_loss: 1.2071 - val_mae: 1.6696\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2247 - mae: 1.6990 - val_loss: 1.2075 - val_mae: 1.6704\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2179 - mae: 1.6946 - val_loss: 1.2017 - val_mae: 1.6662\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2154 - mae: 1.6917 - val_loss: 1.2029 - val_mae: 1.6685\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2156 - mae: 1.6912 - val_loss: 1.2057 - val_mae: 1.6716\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1951 - mae: 1.6686 - val_loss: 1.1988 - val_mae: 1.6673\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2099 - mae: 1.6915 - val_loss: 1.1915 - val_mae: 1.6612\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1879 - mae: 1.6649 - val_loss: 1.1920 - val_mae: 1.6626\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1925 - mae: 1.6746 - val_loss: 1.1910 - val_mae: 1.6654\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1819 - mae: 1.6613 - val_loss: 1.1823 - val_mae: 1.6566\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1777 - mae: 1.6561 - val_loss: 1.1782 - val_mae: 1.6516\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1673 - mae: 1.6482 - val_loss: 1.1603 - val_mae: 1.6307\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1743 - mae: 1.6539 - val_loss: 1.1749 - val_mae: 1.6484\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1595 - mae: 1.6411 - val_loss: 1.1812 - val_mae: 1.6634\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1707 - mae: 1.6517 - val_loss: 1.1595 - val_mae: 1.6370\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1670 - mae: 1.6474 - val_loss: 1.1461 - val_mae: 1.6234\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1750 - mae: 1.6618 - val_loss: 1.1423 - val_mae: 1.6172\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1695 - mae: 1.6522 - val_loss: 1.1512 - val_mae: 1.6285\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1626 - mae: 1.6431 - val_loss: 1.1689 - val_mae: 1.6518\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1738 - mae: 1.6573 - val_loss: 1.1591 - val_mae: 1.6416\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1674 - mae: 1.6514 - val_loss: 1.1410 - val_mae: 1.6249\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1552 - mae: 1.6363 - val_loss: 1.1526 - val_mae: 1.6378\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1554 - mae: 1.6386 - val_loss: 1.1456 - val_mae: 1.6343\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1465 - mae: 1.6315 - val_loss: 1.1443 - val_mae: 1.6338\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1654 - mae: 1.6522 - val_loss: 1.1349 - val_mae: 1.6223\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1450 - mae: 1.6257 - val_loss: 1.1438 - val_mae: 1.6306\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1502 - mae: 1.6383 - val_loss: 1.1492 - val_mae: 1.6378\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1416 - mae: 1.6271 - val_loss: 1.1356 - val_mae: 1.6231\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1379 - mae: 1.6214 - val_loss: 1.1536 - val_mae: 1.6441\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1647 - mae: 1.6536 - val_loss: 1.1309 - val_mae: 1.6201\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1516 - mae: 1.6391 - val_loss: 1.1371 - val_mae: 1.6288\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1354 - mae: 1.6197 - val_loss: 1.1222 - val_mae: 1.6077\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1314 - mae: 1.6181 - val_loss: 1.1239 - val_mae: 1.6102\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1574 - mae: 1.6461 - val_loss: 1.1258 - val_mae: 1.6148\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1456 - mae: 1.6317 - val_loss: 1.1402 - val_mae: 1.6266\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1411 - mae: 1.6269 - val_loss: 1.1314 - val_mae: 1.6192\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1541 - mae: 1.6412 - val_loss: 1.1508 - val_mae: 1.6386\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1300 - mae: 1.6114 - val_loss: 1.1352 - val_mae: 1.6170\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1434 - mae: 1.6317 - val_loss: 1.1280 - val_mae: 1.6145\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1346 - mae: 1.6166 - val_loss: 1.1241 - val_mae: 1.6098\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1309 - mae: 1.6178 - val_loss: 1.1335 - val_mae: 1.6250\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1363 - mae: 1.6246 - val_loss: 1.1140 - val_mae: 1.5973\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1241 - mae: 1.6138 - val_loss: 1.1261 - val_mae: 1.6179\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1189 - mae: 1.6030 - val_loss: 1.1276 - val_mae: 1.6229\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1315 - mae: 1.6190 - val_loss: 1.1117 - val_mae: 1.6048\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1367 - mae: 1.6240 - val_loss: 1.1303 - val_mae: 1.6207\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1179 - mae: 1.6020 - val_loss: 1.1012 - val_mae: 1.5900\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1218 - mae: 1.6079 - val_loss: 1.1175 - val_mae: 1.6100\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1274 - mae: 1.6150 - val_loss: 1.1037 - val_mae: 1.5922\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1190 - mae: 1.6030 - val_loss: 1.1145 - val_mae: 1.6054\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1183 - mae: 1.6064 - val_loss: 1.1100 - val_mae: 1.6017\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1239 - mae: 1.6113 - val_loss: 1.1080 - val_mae: 1.5947\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1278 - mae: 1.6147 - val_loss: 1.1091 - val_mae: 1.5980\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1424 - mae: 1.6306 - val_loss: 1.1135 - val_mae: 1.5998\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1223 - mae: 1.6102 - val_loss: 1.1140 - val_mae: 1.6026\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1232 - mae: 1.6072 - val_loss: 1.1110 - val_mae: 1.5975\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1188 - mae: 1.6041 - val_loss: 1.1084 - val_mae: 1.5930\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1284 - mae: 1.6124 - val_loss: 1.1163 - val_mae: 1.6081\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1381 - mae: 1.6284 - val_loss: 1.1123 - val_mae: 1.6019\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1274 - mae: 1.6155 - val_loss: 1.1163 - val_mae: 1.6097\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1139 - mae: 1.5986 - val_loss: 1.0978 - val_mae: 1.5843\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1096 - mae: 1.5956 - val_loss: 1.1043 - val_mae: 1.5948\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1321 - mae: 1.6223 - val_loss: 1.1183 - val_mae: 1.6126\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1121 - mae: 1.5961 - val_loss: 1.0941 - val_mae: 1.5813\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1176 - mae: 1.6023 - val_loss: 1.1024 - val_mae: 1.5964\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1120 - mae: 1.5968 - val_loss: 1.1078 - val_mae: 1.6029\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1277 - mae: 1.6148 - val_loss: 1.0930 - val_mae: 1.5833\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1219 - mae: 1.6056 - val_loss: 1.1089 - val_mae: 1.5969\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1090 - mae: 1.5931 - val_loss: 1.1020 - val_mae: 1.5895\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1117 - mae: 1.5973 - val_loss: 1.0994 - val_mae: 1.5848\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1287 - mae: 1.6125 - val_loss: 1.1355 - val_mae: 1.6253\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1032 - mae: 1.5878 - val_loss: 1.1097 - val_mae: 1.6043\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1281 - mae: 1.6157 - val_loss: 1.1169 - val_mae: 1.6103\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1153 - mae: 1.6029 - val_loss: 1.1177 - val_mae: 1.6153\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1123 - mae: 1.5952 - val_loss: 1.1064 - val_mae: 1.5999\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1176 - mae: 1.6045 - val_loss: 1.1013 - val_mae: 1.5960\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1039 - mae: 1.5913 - val_loss: 1.0966 - val_mae: 1.5931\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1030 - mae: 1.5882 - val_loss: 1.0933 - val_mae: 1.5824\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1002 - mae: 1.5868 - val_loss: 1.0926 - val_mae: 1.5812\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1145 - mae: 1.5976 - val_loss: 1.1001 - val_mae: 1.5893\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1137 - mae: 1.6000 - val_loss: 1.1023 - val_mae: 1.5910\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1296 - mae: 1.6167 - val_loss: 1.1093 - val_mae: 1.5961\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0988 - mae: 1.5839 - val_loss: 1.1047 - val_mae: 1.5921\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1081 - mae: 1.5915 - val_loss: 1.0920 - val_mae: 1.5826\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1145 - mae: 1.6005 - val_loss: 1.0922 - val_mae: 1.5819\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0986 - mae: 1.5788 - val_loss: 1.0859 - val_mae: 1.5748\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1061 - mae: 1.5924 - val_loss: 1.0967 - val_mae: 1.5904\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1132 - mae: 1.5978 - val_loss: 1.0720 - val_mae: 1.5582\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1054 - mae: 1.5929 - val_loss: 1.0820 - val_mae: 1.5753\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1088 - mae: 1.5940 - val_loss: 1.0910 - val_mae: 1.5815\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1018 - mae: 1.5884 - val_loss: 1.0789 - val_mae: 1.5655\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1057 - mae: 1.5907 - val_loss: 1.0907 - val_mae: 1.5827\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1053 - mae: 1.5930 - val_loss: 1.0879 - val_mae: 1.5759\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0977 - mae: 1.5845 - val_loss: 1.0951 - val_mae: 1.5856\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0952 - mae: 1.5827 - val_loss: 1.0957 - val_mae: 1.5906\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0917 - mae: 1.5775 - val_loss: 1.1138 - val_mae: 1.6160\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0958 - mae: 1.5775 - val_loss: 1.0736 - val_mae: 1.5619\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1062 - mae: 1.5950 - val_loss: 1.0909 - val_mae: 1.5837\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1065 - mae: 1.5940 - val_loss: 1.1050 - val_mae: 1.6056\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1029 - mae: 1.5892 - val_loss: 1.1034 - val_mae: 1.6031\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0849 - mae: 1.5660 - val_loss: 1.0903 - val_mae: 1.5888\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0854 - mae: 1.5718 - val_loss: 1.0877 - val_mae: 1.5848\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0856 - mae: 1.5716 - val_loss: 1.0875 - val_mae: 1.5831\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0839 - mae: 1.5658 - val_loss: 1.0788 - val_mae: 1.5691\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1022 - mae: 1.5861 - val_loss: 1.0835 - val_mae: 1.5738\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0899 - mae: 1.5795 - val_loss: 1.0763 - val_mae: 1.5672\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1055 - mae: 1.5960 - val_loss: 1.0868 - val_mae: 1.5756\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0937 - mae: 1.5810 - val_loss: 1.0751 - val_mae: 1.5622\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0889 - mae: 1.5732 - val_loss: 1.0671 - val_mae: 1.5565\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0872 - mae: 1.5736 - val_loss: 1.0935 - val_mae: 1.5885\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0843 - mae: 1.5679 - val_loss: 1.0696 - val_mae: 1.5621\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0731 - mae: 1.5538 - val_loss: 1.0799 - val_mae: 1.5733\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0834 - mae: 1.5689 - val_loss: 1.0726 - val_mae: 1.5602\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1020 - mae: 1.5883 - val_loss: 1.0807 - val_mae: 1.5701\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0955 - mae: 1.5830 - val_loss: 1.0833 - val_mae: 1.5728\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0972 - mae: 1.5858 - val_loss: 1.0746 - val_mae: 1.5643\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1025 - mae: 1.5870 - val_loss: 1.0749 - val_mae: 1.5637\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0781 - mae: 1.5619 - val_loss: 1.0673 - val_mae: 1.5547\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1124 - mae: 1.6024 - val_loss: 1.0728 - val_mae: 1.5623\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0922 - mae: 1.5733 - val_loss: 1.0882 - val_mae: 1.5826\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0875 - mae: 1.5709 - val_loss: 1.0730 - val_mae: 1.5638\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1050 - mae: 1.5924 - val_loss: 1.0840 - val_mae: 1.5729\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0760 - mae: 1.5587 - val_loss: 1.0725 - val_mae: 1.5599\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0857 - mae: 1.5684 - val_loss: 1.0692 - val_mae: 1.5608\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0920 - mae: 1.5799 - val_loss: 1.0710 - val_mae: 1.5619\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0797 - mae: 1.5632 - val_loss: 1.0825 - val_mae: 1.5775\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0968 - mae: 1.5857 - val_loss: 1.0809 - val_mae: 1.5752\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0964 - mae: 1.5831 - val_loss: 1.0721 - val_mae: 1.5608\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0856 - mae: 1.5690 - val_loss: 1.0602 - val_mae: 1.5483\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0911 - mae: 1.5762 - val_loss: 1.0710 - val_mae: 1.5635\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0798 - mae: 1.5660 - val_loss: 1.0660 - val_mae: 1.5585\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0948 - mae: 1.5823 - val_loss: 1.0610 - val_mae: 1.5514\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0693 - mae: 1.5493 - val_loss: 1.0628 - val_mae: 1.5542\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0905 - mae: 1.5781 - val_loss: 1.0589 - val_mae: 1.5423\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1109 - mae: 1.5944 - val_loss: 1.0758 - val_mae: 1.5603\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0794 - mae: 1.5609 - val_loss: 1.0647 - val_mae: 1.5518\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0949 - mae: 1.5832 - val_loss: 1.0734 - val_mae: 1.5631\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 1s 9ms/step - loss: 1.0940 - mae: 1.5821 - val_loss: 1.0759 - val_mae: 1.5656\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0855 - mae: 1.5706 - val_loss: 1.0567 - val_mae: 1.5419\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0885 - mae: 1.5740 - val_loss: 1.0752 - val_mae: 1.5647\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0921 - mae: 1.5804 - val_loss: 1.0649 - val_mae: 1.5532\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0828 - mae: 1.5673 - val_loss: 1.0588 - val_mae: 1.5445\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0756 - mae: 1.5611 - val_loss: 1.0604 - val_mae: 1.5464\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0894 - mae: 1.5777 - val_loss: 1.0704 - val_mae: 1.5604\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0786 - mae: 1.5639 - val_loss: 1.0671 - val_mae: 1.5519\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0818 - mae: 1.5650 - val_loss: 1.0729 - val_mae: 1.5629\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0908 - mae: 1.5784 - val_loss: 1.0644 - val_mae: 1.5484\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0784 - mae: 1.5637 - val_loss: 1.0614 - val_mae: 1.5480\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0860 - mae: 1.5689 - val_loss: 1.0775 - val_mae: 1.5672\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0842 - mae: 1.5706 - val_loss: 1.0773 - val_mae: 1.5633\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0828 - mae: 1.5672 - val_loss: 1.0496 - val_mae: 1.5310\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0712 - mae: 1.5527 - val_loss: 1.0480 - val_mae: 1.5296\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0690 - mae: 1.5523 - val_loss: 1.0463 - val_mae: 1.5308\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1013 - mae: 1.5874 - val_loss: 1.0531 - val_mae: 1.5363\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0682 - mae: 1.5488 - val_loss: 1.0545 - val_mae: 1.5321\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0777 - mae: 1.5600 - val_loss: 1.0535 - val_mae: 1.5398\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0571 - mae: 1.5362 - val_loss: 1.0394 - val_mae: 1.5218\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0863 - mae: 1.5707 - val_loss: 1.0566 - val_mae: 1.5410\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0683 - mae: 1.5516 - val_loss: 1.0535 - val_mae: 1.5396\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0687 - mae: 1.5521 - val_loss: 1.0571 - val_mae: 1.5390\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0802 - mae: 1.5638 - val_loss: 1.0621 - val_mae: 1.5462\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0682 - mae: 1.5507 - val_loss: 1.0584 - val_mae: 1.5461\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0993 - mae: 1.5879 - val_loss: 1.0594 - val_mae: 1.5428\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0890 - mae: 1.5774 - val_loss: 1.0686 - val_mae: 1.5542\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0897 - mae: 1.5738 - val_loss: 1.0573 - val_mae: 1.5359\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0698 - mae: 1.5526 - val_loss: 1.0467 - val_mae: 1.5285\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0848 - mae: 1.5675 - val_loss: 1.0517 - val_mae: 1.5266\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0699 - mae: 1.5531 - val_loss: 1.0625 - val_mae: 1.5454\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0850 - mae: 1.5677 - val_loss: 1.0544 - val_mae: 1.5320\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0557 - mae: 1.5333 - val_loss: 1.0379 - val_mae: 1.5148\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0841 - mae: 1.5724 - val_loss: 1.0491 - val_mae: 1.5271\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0593 - mae: 1.5429 - val_loss: 1.0610 - val_mae: 1.5436\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0922 - mae: 1.5784 - val_loss: 1.0629 - val_mae: 1.5447\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0734 - mae: 1.5497 - val_loss: 1.0454 - val_mae: 1.5268\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0692 - mae: 1.5466 - val_loss: 1.0544 - val_mae: 1.5438\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0772 - mae: 1.5604 - val_loss: 1.0531 - val_mae: 1.5341\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0840 - mae: 1.5689 - val_loss: 1.0486 - val_mae: 1.5298\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0715 - mae: 1.5540 - val_loss: 1.0507 - val_mae: 1.5345\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0824 - mae: 1.5648 - val_loss: 1.0425 - val_mae: 1.5199\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0937 - mae: 1.5806 - val_loss: 1.0505 - val_mae: 1.5343\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0798 - mae: 1.5640 - val_loss: 1.0528 - val_mae: 1.5304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:35:21,198] Trial 20 finished with value: 1.0527660846710205 and parameters: {'dropout_2': 0.39419011660076336, 'dropout_3': 0.49852040339526027, 'dropout_4': 0.11627712044595445, 'dropout_5': 0.8945102750686341, 'learning_rate': 0.00045730747090875904, 'epochs': 200, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "14/14 [==============================] - 2s 30ms/step - loss: 1.7386 - mae: 2.3058 - val_loss: 1.3322 - val_mae: 1.7919\n",
            "Epoch 2/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.7125 - mae: 2.2733 - val_loss: 1.3179 - val_mae: 1.7750\n",
            "Epoch 3/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.6631 - mae: 2.2225 - val_loss: 1.3060 - val_mae: 1.7608\n",
            "Epoch 4/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.6193 - mae: 2.1753 - val_loss: 1.3001 - val_mae: 1.7546\n",
            "Epoch 5/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.5909 - mae: 2.1457 - val_loss: 1.2929 - val_mae: 1.7464\n",
            "Epoch 6/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.5883 - mae: 2.1435 - val_loss: 1.2890 - val_mae: 1.7429\n",
            "Epoch 7/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.5529 - mae: 2.1041 - val_loss: 1.2853 - val_mae: 1.7424\n",
            "Epoch 8/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.5381 - mae: 2.0871 - val_loss: 1.2829 - val_mae: 1.7441\n",
            "Epoch 9/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.5182 - mae: 2.0629 - val_loss: 1.2857 - val_mae: 1.7493\n",
            "Epoch 10/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.4687 - mae: 2.0096 - val_loss: 1.2812 - val_mae: 1.7465\n",
            "Epoch 11/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.4862 - mae: 2.0280 - val_loss: 1.2777 - val_mae: 1.7461\n",
            "Epoch 12/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.4681 - mae: 2.0083 - val_loss: 1.2728 - val_mae: 1.7424\n",
            "Epoch 13/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.4722 - mae: 2.0108 - val_loss: 1.2692 - val_mae: 1.7427\n",
            "Epoch 14/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4558 - mae: 1.9984 - val_loss: 1.2632 - val_mae: 1.7379\n",
            "Epoch 15/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.4232 - mae: 1.9617 - val_loss: 1.2608 - val_mae: 1.7373\n",
            "Epoch 16/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4149 - mae: 1.9521 - val_loss: 1.2502 - val_mae: 1.7279\n",
            "Epoch 17/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3968 - mae: 1.9353 - val_loss: 1.2438 - val_mae: 1.7231\n",
            "Epoch 18/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.4044 - mae: 1.9349 - val_loss: 1.2366 - val_mae: 1.7176\n",
            "Epoch 19/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.3924 - mae: 1.9240 - val_loss: 1.2315 - val_mae: 1.7119\n",
            "Epoch 20/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3678 - mae: 1.8973 - val_loss: 1.2239 - val_mae: 1.7039\n",
            "Epoch 21/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3633 - mae: 1.8881 - val_loss: 1.2183 - val_mae: 1.6992\n",
            "Epoch 22/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3681 - mae: 1.8973 - val_loss: 1.2154 - val_mae: 1.6971\n",
            "Epoch 23/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.3592 - mae: 1.8880 - val_loss: 1.2089 - val_mae: 1.6906\n",
            "Epoch 24/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.3200 - mae: 1.8425 - val_loss: 1.2010 - val_mae: 1.6819\n",
            "Epoch 25/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.3100 - mae: 1.8319 - val_loss: 1.1960 - val_mae: 1.6773\n",
            "Epoch 26/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.3271 - mae: 1.8520 - val_loss: 1.1877 - val_mae: 1.6678\n",
            "Epoch 27/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.2898 - mae: 1.8086 - val_loss: 1.1816 - val_mae: 1.6603\n",
            "Epoch 28/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3231 - mae: 1.8455 - val_loss: 1.1827 - val_mae: 1.6622\n",
            "Epoch 29/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2923 - mae: 1.8108 - val_loss: 1.1768 - val_mae: 1.6562\n",
            "Epoch 30/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3085 - mae: 1.8284 - val_loss: 1.1686 - val_mae: 1.6481\n",
            "Epoch 31/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3051 - mae: 1.8217 - val_loss: 1.1651 - val_mae: 1.6439\n",
            "Epoch 32/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2962 - mae: 1.8113 - val_loss: 1.1643 - val_mae: 1.6425\n",
            "Epoch 33/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2767 - mae: 1.7953 - val_loss: 1.1599 - val_mae: 1.6372\n",
            "Epoch 34/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2848 - mae: 1.8017 - val_loss: 1.1564 - val_mae: 1.6320\n",
            "Epoch 35/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2609 - mae: 1.7724 - val_loss: 1.1560 - val_mae: 1.6314\n",
            "Epoch 36/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2507 - mae: 1.7614 - val_loss: 1.1536 - val_mae: 1.6277\n",
            "Epoch 37/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2559 - mae: 1.7697 - val_loss: 1.1463 - val_mae: 1.6188\n",
            "Epoch 38/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.2669 - mae: 1.7792 - val_loss: 1.1442 - val_mae: 1.6157\n",
            "Epoch 39/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2932 - mae: 1.8030 - val_loss: 1.1428 - val_mae: 1.6158\n",
            "Epoch 40/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2513 - mae: 1.7592 - val_loss: 1.1384 - val_mae: 1.6103\n",
            "Epoch 41/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2571 - mae: 1.7684 - val_loss: 1.1335 - val_mae: 1.6053\n",
            "Epoch 42/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.2088 - mae: 1.7145 - val_loss: 1.1319 - val_mae: 1.6054\n",
            "Epoch 43/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.2485 - mae: 1.7592 - val_loss: 1.1249 - val_mae: 1.5979\n",
            "Epoch 44/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.2473 - mae: 1.7524 - val_loss: 1.1259 - val_mae: 1.6002\n",
            "Epoch 45/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2235 - mae: 1.7318 - val_loss: 1.1291 - val_mae: 1.6039\n",
            "Epoch 46/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2400 - mae: 1.7498 - val_loss: 1.1308 - val_mae: 1.6064\n",
            "Epoch 47/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.2160 - mae: 1.7215 - val_loss: 1.1212 - val_mae: 1.5951\n",
            "Epoch 48/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2226 - mae: 1.7327 - val_loss: 1.1150 - val_mae: 1.5881\n",
            "Epoch 49/200\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.2062 - mae: 1.7132 - val_loss: 1.1094 - val_mae: 1.5806\n",
            "Epoch 50/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2067 - mae: 1.7104 - val_loss: 1.1076 - val_mae: 1.5777\n",
            "Epoch 51/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.1998 - mae: 1.7070 - val_loss: 1.1073 - val_mae: 1.5764\n",
            "Epoch 52/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.1884 - mae: 1.6895 - val_loss: 1.1024 - val_mae: 1.5709\n",
            "Epoch 53/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.2050 - mae: 1.7059 - val_loss: 1.1021 - val_mae: 1.5712\n",
            "Epoch 54/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.1995 - mae: 1.7033 - val_loss: 1.1004 - val_mae: 1.5699\n",
            "Epoch 55/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.1965 - mae: 1.6969 - val_loss: 1.0986 - val_mae: 1.5683\n",
            "Epoch 56/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2016 - mae: 1.7043 - val_loss: 1.1039 - val_mae: 1.5749\n",
            "Epoch 57/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.1996 - mae: 1.7016 - val_loss: 1.0958 - val_mae: 1.5657\n",
            "Epoch 58/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.1881 - mae: 1.6861 - val_loss: 1.0948 - val_mae: 1.5647\n",
            "Epoch 59/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.1924 - mae: 1.6934 - val_loss: 1.0956 - val_mae: 1.5655\n",
            "Epoch 60/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.1974 - mae: 1.6993 - val_loss: 1.0915 - val_mae: 1.5613\n",
            "Epoch 61/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.1821 - mae: 1.6802 - val_loss: 1.0894 - val_mae: 1.5594\n",
            "Epoch 62/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.1606 - mae: 1.6580 - val_loss: 1.0849 - val_mae: 1.5541\n",
            "Epoch 63/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.1995 - mae: 1.6991 - val_loss: 1.0852 - val_mae: 1.5535\n",
            "Epoch 64/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.1834 - mae: 1.6838 - val_loss: 1.0865 - val_mae: 1.5551\n",
            "Epoch 65/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.1788 - mae: 1.6789 - val_loss: 1.0935 - val_mae: 1.5629\n",
            "Epoch 66/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.1734 - mae: 1.6717 - val_loss: 1.0963 - val_mae: 1.5660\n",
            "Epoch 67/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.1685 - mae: 1.6675 - val_loss: 1.0933 - val_mae: 1.5625\n",
            "Epoch 68/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.1757 - mae: 1.6735 - val_loss: 1.1025 - val_mae: 1.5737\n",
            "Epoch 69/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.1573 - mae: 1.6525 - val_loss: 1.0985 - val_mae: 1.5691\n",
            "Epoch 70/200\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.1508 - mae: 1.6457 - val_loss: 1.0976 - val_mae: 1.5674\n",
            "Epoch 71/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1647 - mae: 1.6570 - val_loss: 1.0947 - val_mae: 1.5641\n",
            "Epoch 72/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1499 - mae: 1.6459 - val_loss: 1.0891 - val_mae: 1.5553\n",
            "Epoch 73/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1380 - mae: 1.6336 - val_loss: 1.0907 - val_mae: 1.5579\n",
            "Epoch 74/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1469 - mae: 1.6406 - val_loss: 1.0987 - val_mae: 1.5680\n",
            "Epoch 75/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1132 - mae: 1.6049 - val_loss: 1.0948 - val_mae: 1.5639\n",
            "Epoch 76/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1439 - mae: 1.6387 - val_loss: 1.0902 - val_mae: 1.5593\n",
            "Epoch 77/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.1391 - mae: 1.6297 - val_loss: 1.0861 - val_mae: 1.5544\n",
            "Epoch 78/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1414 - mae: 1.6385 - val_loss: 1.0833 - val_mae: 1.5510\n",
            "Epoch 79/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1421 - mae: 1.6347 - val_loss: 1.0831 - val_mae: 1.5491\n",
            "Epoch 80/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1322 - mae: 1.6242 - val_loss: 1.0796 - val_mae: 1.5456\n",
            "Epoch 81/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1264 - mae: 1.6161 - val_loss: 1.0763 - val_mae: 1.5420\n",
            "Epoch 82/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1208 - mae: 1.6132 - val_loss: 1.0797 - val_mae: 1.5464\n",
            "Epoch 83/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1288 - mae: 1.6187 - val_loss: 1.0802 - val_mae: 1.5475\n",
            "Epoch 84/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1226 - mae: 1.6143 - val_loss: 1.0818 - val_mae: 1.5491\n",
            "Epoch 85/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1316 - mae: 1.6213 - val_loss: 1.0791 - val_mae: 1.5453\n",
            "Epoch 86/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1251 - mae: 1.6152 - val_loss: 1.0842 - val_mae: 1.5529\n",
            "Epoch 87/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1246 - mae: 1.6162 - val_loss: 1.0824 - val_mae: 1.5503\n",
            "Epoch 88/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1079 - mae: 1.5951 - val_loss: 1.0844 - val_mae: 1.5528\n",
            "Epoch 89/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1176 - mae: 1.6070 - val_loss: 1.0820 - val_mae: 1.5509\n",
            "Epoch 90/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1146 - mae: 1.6069 - val_loss: 1.0791 - val_mae: 1.5472\n",
            "Epoch 91/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1145 - mae: 1.6068 - val_loss: 1.0815 - val_mae: 1.5505\n",
            "Epoch 92/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1146 - mae: 1.6041 - val_loss: 1.0843 - val_mae: 1.5545\n",
            "Epoch 93/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1031 - mae: 1.5927 - val_loss: 1.0833 - val_mae: 1.5538\n",
            "Epoch 94/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1040 - mae: 1.5934 - val_loss: 1.0754 - val_mae: 1.5436\n",
            "Epoch 95/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1022 - mae: 1.5932 - val_loss: 1.0719 - val_mae: 1.5392\n",
            "Epoch 96/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.1027 - mae: 1.5936 - val_loss: 1.0739 - val_mae: 1.5418\n",
            "Epoch 97/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.1063 - mae: 1.5928 - val_loss: 1.0751 - val_mae: 1.5433\n",
            "Epoch 98/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1143 - mae: 1.6064 - val_loss: 1.0723 - val_mae: 1.5388\n",
            "Epoch 99/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0945 - mae: 1.5831 - val_loss: 1.0737 - val_mae: 1.5425\n",
            "Epoch 100/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0991 - mae: 1.5874 - val_loss: 1.0706 - val_mae: 1.5374\n",
            "Epoch 101/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0957 - mae: 1.5810 - val_loss: 1.0720 - val_mae: 1.5383\n",
            "Epoch 102/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0960 - mae: 1.5847 - val_loss: 1.0691 - val_mae: 1.5347\n",
            "Epoch 103/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.0991 - mae: 1.5864 - val_loss: 1.0627 - val_mae: 1.5274\n",
            "Epoch 104/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0980 - mae: 1.5852 - val_loss: 1.0659 - val_mae: 1.5318\n",
            "Epoch 105/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0701 - mae: 1.5568 - val_loss: 1.0647 - val_mae: 1.5293\n",
            "Epoch 106/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0935 - mae: 1.5826 - val_loss: 1.0651 - val_mae: 1.5303\n",
            "Epoch 107/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0805 - mae: 1.5654 - val_loss: 1.0676 - val_mae: 1.5338\n",
            "Epoch 108/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0795 - mae: 1.5656 - val_loss: 1.0681 - val_mae: 1.5348\n",
            "Epoch 109/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0760 - mae: 1.5621 - val_loss: 1.0642 - val_mae: 1.5300\n",
            "Epoch 110/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.0794 - mae: 1.5630 - val_loss: 1.0671 - val_mae: 1.5357\n",
            "Epoch 111/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0759 - mae: 1.5625 - val_loss: 1.0637 - val_mae: 1.5306\n",
            "Epoch 112/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0744 - mae: 1.5629 - val_loss: 1.0604 - val_mae: 1.5262\n",
            "Epoch 113/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0806 - mae: 1.5673 - val_loss: 1.0618 - val_mae: 1.5272\n",
            "Epoch 114/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0757 - mae: 1.5655 - val_loss: 1.0646 - val_mae: 1.5298\n",
            "Epoch 115/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0862 - mae: 1.5750 - val_loss: 1.0634 - val_mae: 1.5283\n",
            "Epoch 116/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0784 - mae: 1.5673 - val_loss: 1.0634 - val_mae: 1.5302\n",
            "Epoch 117/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0851 - mae: 1.5751 - val_loss: 1.0638 - val_mae: 1.5318\n",
            "Epoch 118/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0777 - mae: 1.5616 - val_loss: 1.0576 - val_mae: 1.5235\n",
            "Epoch 119/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0670 - mae: 1.5520 - val_loss: 1.0630 - val_mae: 1.5303\n",
            "Epoch 120/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0902 - mae: 1.5779 - val_loss: 1.0550 - val_mae: 1.5214\n",
            "Epoch 121/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0626 - mae: 1.5463 - val_loss: 1.0502 - val_mae: 1.5161\n",
            "Epoch 122/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0711 - mae: 1.5592 - val_loss: 1.0500 - val_mae: 1.5141\n",
            "Epoch 123/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0701 - mae: 1.5571 - val_loss: 1.0481 - val_mae: 1.5116\n",
            "Epoch 124/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0756 - mae: 1.5639 - val_loss: 1.0432 - val_mae: 1.5058\n",
            "Epoch 125/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0709 - mae: 1.5562 - val_loss: 1.0501 - val_mae: 1.5147\n",
            "Epoch 126/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0529 - mae: 1.5381 - val_loss: 1.0491 - val_mae: 1.5140\n",
            "Epoch 127/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0767 - mae: 1.5632 - val_loss: 1.0527 - val_mae: 1.5177\n",
            "Epoch 128/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0614 - mae: 1.5467 - val_loss: 1.0481 - val_mae: 1.5124\n",
            "Epoch 129/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0461 - mae: 1.5310 - val_loss: 1.0467 - val_mae: 1.5108\n",
            "Epoch 130/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0567 - mae: 1.5409 - val_loss: 1.0423 - val_mae: 1.5070\n",
            "Epoch 131/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0586 - mae: 1.5431 - val_loss: 1.0463 - val_mae: 1.5118\n",
            "Epoch 132/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0465 - mae: 1.5258 - val_loss: 1.0449 - val_mae: 1.5098\n",
            "Epoch 133/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0638 - mae: 1.5491 - val_loss: 1.0466 - val_mae: 1.5125\n",
            "Epoch 134/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0471 - mae: 1.5295 - val_loss: 1.0430 - val_mae: 1.5087\n",
            "Epoch 135/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.0434 - mae: 1.5302 - val_loss: 1.0381 - val_mae: 1.5028\n",
            "Epoch 136/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.0431 - mae: 1.5270 - val_loss: 1.0377 - val_mae: 1.5021\n",
            "Epoch 137/200\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 1.0659 - mae: 1.5556 - val_loss: 1.0347 - val_mae: 1.4992\n",
            "Epoch 138/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.0438 - mae: 1.5276 - val_loss: 1.0383 - val_mae: 1.5043\n",
            "Epoch 139/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.0428 - mae: 1.5268 - val_loss: 1.0398 - val_mae: 1.5067\n",
            "Epoch 140/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.0348 - mae: 1.5211 - val_loss: 1.0343 - val_mae: 1.5007\n",
            "Epoch 141/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.0430 - mae: 1.5286 - val_loss: 1.0335 - val_mae: 1.4984\n",
            "Epoch 142/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.0583 - mae: 1.5469 - val_loss: 1.0311 - val_mae: 1.4942\n",
            "Epoch 143/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.0572 - mae: 1.5403 - val_loss: 1.0317 - val_mae: 1.4964\n",
            "Epoch 144/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.0263 - mae: 1.5114 - val_loss: 1.0303 - val_mae: 1.4957\n",
            "Epoch 145/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.0412 - mae: 1.5236 - val_loss: 1.0270 - val_mae: 1.4929\n",
            "Epoch 146/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.0337 - mae: 1.5156 - val_loss: 1.0275 - val_mae: 1.4942\n",
            "Epoch 147/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.0563 - mae: 1.5459 - val_loss: 1.0269 - val_mae: 1.4939\n",
            "Epoch 148/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.0350 - mae: 1.5213 - val_loss: 1.0267 - val_mae: 1.4929\n",
            "Epoch 149/200\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.0557 - mae: 1.5420 - val_loss: 1.0329 - val_mae: 1.5006\n",
            "Epoch 150/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.0237 - mae: 1.5113 - val_loss: 1.0240 - val_mae: 1.4888\n",
            "Epoch 151/200\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.0317 - mae: 1.5138 - val_loss: 1.0232 - val_mae: 1.4881\n",
            "Epoch 152/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.0488 - mae: 1.5335 - val_loss: 1.0240 - val_mae: 1.4915\n",
            "Epoch 153/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.0247 - mae: 1.5085 - val_loss: 1.0196 - val_mae: 1.4856\n",
            "Epoch 154/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.0364 - mae: 1.5237 - val_loss: 1.0187 - val_mae: 1.4856\n",
            "Epoch 155/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.0319 - mae: 1.5138 - val_loss: 1.0209 - val_mae: 1.4894\n",
            "Epoch 156/200\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.0318 - mae: 1.5184 - val_loss: 1.0190 - val_mae: 1.4865\n",
            "Epoch 157/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.0083 - mae: 1.4917 - val_loss: 1.0153 - val_mae: 1.4810\n",
            "Epoch 158/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0235 - mae: 1.5091 - val_loss: 1.0110 - val_mae: 1.4778\n",
            "Epoch 159/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0313 - mae: 1.5164 - val_loss: 1.0141 - val_mae: 1.4824\n",
            "Epoch 160/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0287 - mae: 1.5140 - val_loss: 1.0143 - val_mae: 1.4827\n",
            "Epoch 161/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0169 - mae: 1.4999 - val_loss: 1.0082 - val_mae: 1.4744\n",
            "Epoch 162/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.0404 - mae: 1.5286 - val_loss: 1.0084 - val_mae: 1.4750\n",
            "Epoch 163/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.0281 - mae: 1.5114 - val_loss: 1.0077 - val_mae: 1.4747\n",
            "Epoch 164/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.0313 - mae: 1.5171 - val_loss: 1.0104 - val_mae: 1.4779\n",
            "Epoch 165/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0405 - mae: 1.5281 - val_loss: 1.0104 - val_mae: 1.4766\n",
            "Epoch 166/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0432 - mae: 1.5324 - val_loss: 1.0056 - val_mae: 1.4727\n",
            "Epoch 167/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0287 - mae: 1.5143 - val_loss: 1.0069 - val_mae: 1.4748\n",
            "Epoch 168/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.0175 - mae: 1.5053 - val_loss: 1.0046 - val_mae: 1.4714\n",
            "Epoch 169/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0256 - mae: 1.5109 - val_loss: 1.0054 - val_mae: 1.4714\n",
            "Epoch 170/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0216 - mae: 1.5062 - val_loss: 1.0090 - val_mae: 1.4766\n",
            "Epoch 171/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0309 - mae: 1.5131 - val_loss: 1.0044 - val_mae: 1.4718\n",
            "Epoch 172/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0246 - mae: 1.5123 - val_loss: 1.0034 - val_mae: 1.4709\n",
            "Epoch 173/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0156 - mae: 1.4993 - val_loss: 1.0026 - val_mae: 1.4702\n",
            "Epoch 174/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0095 - mae: 1.4945 - val_loss: 1.0037 - val_mae: 1.4720\n",
            "Epoch 175/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.0140 - mae: 1.5012 - val_loss: 1.0023 - val_mae: 1.4711\n",
            "Epoch 176/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0126 - mae: 1.4964 - val_loss: 1.0037 - val_mae: 1.4723\n",
            "Epoch 177/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.9939 - mae: 1.4751 - val_loss: 1.0056 - val_mae: 1.4754\n",
            "Epoch 178/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0036 - mae: 1.4854 - val_loss: 1.0003 - val_mae: 1.4699\n",
            "Epoch 179/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0135 - mae: 1.4991 - val_loss: 0.9981 - val_mae: 1.4664\n",
            "Epoch 180/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0145 - mae: 1.4954 - val_loss: 1.0003 - val_mae: 1.4694\n",
            "Epoch 181/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0124 - mae: 1.4958 - val_loss: 0.9971 - val_mae: 1.4653\n",
            "Epoch 182/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0204 - mae: 1.5016 - val_loss: 0.9980 - val_mae: 1.4649\n",
            "Epoch 183/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.9950 - mae: 1.4771 - val_loss: 1.0034 - val_mae: 1.4749\n",
            "Epoch 184/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0073 - mae: 1.4897 - val_loss: 0.9982 - val_mae: 1.4696\n",
            "Epoch 185/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.9988 - mae: 1.4825 - val_loss: 0.9892 - val_mae: 1.4610\n",
            "Epoch 186/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.0185 - mae: 1.5082 - val_loss: 0.9898 - val_mae: 1.4602\n",
            "Epoch 187/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.9990 - mae: 1.4829 - val_loss: 0.9908 - val_mae: 1.4626\n",
            "Epoch 188/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0131 - mae: 1.4976 - val_loss: 0.9868 - val_mae: 1.4550\n",
            "Epoch 189/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0150 - mae: 1.5002 - val_loss: 0.9855 - val_mae: 1.4523\n",
            "Epoch 190/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0171 - mae: 1.4994 - val_loss: 0.9849 - val_mae: 1.4529\n",
            "Epoch 191/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0005 - mae: 1.4847 - val_loss: 0.9855 - val_mae: 1.4557\n",
            "Epoch 192/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0105 - mae: 1.4960 - val_loss: 0.9820 - val_mae: 1.4512\n",
            "Epoch 193/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.0141 - mae: 1.5022 - val_loss: 0.9825 - val_mae: 1.4510\n",
            "Epoch 194/200\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.0018 - mae: 1.4840 - val_loss: 0.9826 - val_mae: 1.4535\n",
            "Epoch 195/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.9881 - mae: 1.4715 - val_loss: 0.9841 - val_mae: 1.4518\n",
            "Epoch 196/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.9989 - mae: 1.4851 - val_loss: 0.9870 - val_mae: 1.4553\n",
            "Epoch 197/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.9979 - mae: 1.4822 - val_loss: 0.9843 - val_mae: 1.4532\n",
            "Epoch 198/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.9984 - mae: 1.4841 - val_loss: 0.9824 - val_mae: 1.4533\n",
            "Epoch 199/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.9896 - mae: 1.4693 - val_loss: 0.9865 - val_mae: 1.4553\n",
            "Epoch 200/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.9995 - mae: 1.4832 - val_loss: 0.9822 - val_mae: 1.4492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:35:57,683] Trial 21 finished with value: 0.9822193384170532 and parameters: {'dropout_2': 0.2213838518630918, 'dropout_3': 0.3190279581991111, 'dropout_4': 0.5514819483714698, 'dropout_5': 0.5599047118424052, 'learning_rate': 0.0002334875890764978, 'epochs': 200, 'batch_size': 256}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "54/54 [==============================] - 2s 11ms/step - loss: 1.8039 - mae: 2.3713 - val_loss: 1.3487 - val_mae: 1.8249\n",
            "Epoch 2/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.7455 - mae: 2.3119 - val_loss: 1.3564 - val_mae: 1.8401\n",
            "Epoch 3/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.7069 - mae: 2.2674 - val_loss: 1.3598 - val_mae: 1.8474\n",
            "Epoch 4/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.6811 - mae: 2.2391 - val_loss: 1.3590 - val_mae: 1.8414\n",
            "Epoch 5/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.6526 - mae: 2.2094 - val_loss: 1.3499 - val_mae: 1.8312\n",
            "Epoch 6/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.6043 - mae: 2.1566 - val_loss: 1.3336 - val_mae: 1.8105\n",
            "Epoch 7/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.5465 - mae: 2.0965 - val_loss: 1.3207 - val_mae: 1.7946\n",
            "Epoch 8/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.5429 - mae: 2.0879 - val_loss: 1.3077 - val_mae: 1.7789\n",
            "Epoch 9/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.5032 - mae: 2.0425 - val_loss: 1.2914 - val_mae: 1.7617\n",
            "Epoch 10/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.4965 - mae: 2.0388 - val_loss: 1.2767 - val_mae: 1.7453\n",
            "Epoch 11/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.4472 - mae: 1.9845 - val_loss: 1.2619 - val_mae: 1.7299\n",
            "Epoch 12/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.4387 - mae: 1.9724 - val_loss: 1.2522 - val_mae: 1.7195\n",
            "Epoch 13/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.4254 - mae: 1.9602 - val_loss: 1.2501 - val_mae: 1.7182\n",
            "Epoch 14/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.3882 - mae: 1.9158 - val_loss: 1.2355 - val_mae: 1.7036\n",
            "Epoch 15/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.3942 - mae: 1.9230 - val_loss: 1.2297 - val_mae: 1.6962\n",
            "Epoch 16/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.3629 - mae: 1.8888 - val_loss: 1.2169 - val_mae: 1.6830\n",
            "Epoch 17/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.3771 - mae: 1.9017 - val_loss: 1.2192 - val_mae: 1.6819\n",
            "Epoch 18/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.3369 - mae: 1.8535 - val_loss: 1.2125 - val_mae: 1.6779\n",
            "Epoch 19/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.3285 - mae: 1.8463 - val_loss: 1.2054 - val_mae: 1.6712\n",
            "Epoch 20/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.3135 - mae: 1.8294 - val_loss: 1.2020 - val_mae: 1.6678\n",
            "Epoch 21/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.3065 - mae: 1.8179 - val_loss: 1.1997 - val_mae: 1.6640\n",
            "Epoch 22/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.3065 - mae: 1.8185 - val_loss: 1.2013 - val_mae: 1.6663\n",
            "Epoch 23/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2873 - mae: 1.7999 - val_loss: 1.1985 - val_mae: 1.6631\n",
            "Epoch 24/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2595 - mae: 1.7685 - val_loss: 1.1920 - val_mae: 1.6568\n",
            "Epoch 25/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2770 - mae: 1.7824 - val_loss: 1.1935 - val_mae: 1.6559\n",
            "Epoch 26/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2534 - mae: 1.7596 - val_loss: 1.1918 - val_mae: 1.6528\n",
            "Epoch 27/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.2610 - mae: 1.7634 - val_loss: 1.1951 - val_mae: 1.6599\n",
            "Epoch 28/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2391 - mae: 1.7390 - val_loss: 1.1875 - val_mae: 1.6514\n",
            "Epoch 29/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2389 - mae: 1.7389 - val_loss: 1.1824 - val_mae: 1.6462\n",
            "Epoch 30/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2431 - mae: 1.7450 - val_loss: 1.1828 - val_mae: 1.6475\n",
            "Epoch 31/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.2125 - mae: 1.7118 - val_loss: 1.1823 - val_mae: 1.6493\n",
            "Epoch 32/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.2260 - mae: 1.7233 - val_loss: 1.1775 - val_mae: 1.6425\n",
            "Epoch 33/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2061 - mae: 1.7032 - val_loss: 1.1844 - val_mae: 1.6519\n",
            "Epoch 34/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2100 - mae: 1.7044 - val_loss: 1.1794 - val_mae: 1.6436\n",
            "Epoch 35/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1815 - mae: 1.6778 - val_loss: 1.1786 - val_mae: 1.6438\n",
            "Epoch 36/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.2077 - mae: 1.7044 - val_loss: 1.1744 - val_mae: 1.6392\n",
            "Epoch 37/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1952 - mae: 1.6891 - val_loss: 1.1664 - val_mae: 1.6310\n",
            "Epoch 38/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1882 - mae: 1.6835 - val_loss: 1.1664 - val_mae: 1.6341\n",
            "Epoch 39/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1761 - mae: 1.6676 - val_loss: 1.1588 - val_mae: 1.6253\n",
            "Epoch 40/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1792 - mae: 1.6702 - val_loss: 1.1614 - val_mae: 1.6299\n",
            "Epoch 41/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1543 - mae: 1.6453 - val_loss: 1.1695 - val_mae: 1.6407\n",
            "Epoch 42/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1728 - mae: 1.6657 - val_loss: 1.1583 - val_mae: 1.6264\n",
            "Epoch 43/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1767 - mae: 1.6660 - val_loss: 1.1646 - val_mae: 1.6369\n",
            "Epoch 44/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1731 - mae: 1.6657 - val_loss: 1.1615 - val_mae: 1.6338\n",
            "Epoch 45/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1513 - mae: 1.6398 - val_loss: 1.1679 - val_mae: 1.6412\n",
            "Epoch 46/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1532 - mae: 1.6460 - val_loss: 1.1605 - val_mae: 1.6349\n",
            "Epoch 47/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1424 - mae: 1.6317 - val_loss: 1.1564 - val_mae: 1.6295\n",
            "Epoch 48/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1617 - mae: 1.6565 - val_loss: 1.1651 - val_mae: 1.6396\n",
            "Epoch 49/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.1284 - mae: 1.6174 - val_loss: 1.1635 - val_mae: 1.6391\n",
            "Epoch 50/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.1452 - mae: 1.6334 - val_loss: 1.1515 - val_mae: 1.6260\n",
            "Epoch 51/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.1180 - mae: 1.6027 - val_loss: 1.1465 - val_mae: 1.6234\n",
            "Epoch 52/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.1299 - mae: 1.6169 - val_loss: 1.1508 - val_mae: 1.6276\n",
            "Epoch 53/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.1287 - mae: 1.6173 - val_loss: 1.1363 - val_mae: 1.6116\n",
            "Epoch 54/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.1334 - mae: 1.6223 - val_loss: 1.1421 - val_mae: 1.6218\n",
            "Epoch 55/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.1371 - mae: 1.6287 - val_loss: 1.1327 - val_mae: 1.6107\n",
            "Epoch 56/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.1107 - mae: 1.5989 - val_loss: 1.1282 - val_mae: 1.6066\n",
            "Epoch 57/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.1302 - mae: 1.6212 - val_loss: 1.1492 - val_mae: 1.6338\n",
            "Epoch 58/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.1192 - mae: 1.6102 - val_loss: 1.1433 - val_mae: 1.6275\n",
            "Epoch 59/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.1147 - mae: 1.6026 - val_loss: 1.1433 - val_mae: 1.6281\n",
            "Epoch 60/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.1096 - mae: 1.5980 - val_loss: 1.1389 - val_mae: 1.6224\n",
            "Epoch 61/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0965 - mae: 1.5849 - val_loss: 1.1272 - val_mae: 1.6091\n",
            "Epoch 62/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0989 - mae: 1.5865 - val_loss: 1.1215 - val_mae: 1.6039\n",
            "Epoch 63/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.1039 - mae: 1.5925 - val_loss: 1.1191 - val_mae: 1.6019\n",
            "Epoch 64/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0881 - mae: 1.5747 - val_loss: 1.1099 - val_mae: 1.5886\n",
            "Epoch 65/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.1040 - mae: 1.5893 - val_loss: 1.1239 - val_mae: 1.6087\n",
            "Epoch 66/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.1043 - mae: 1.5935 - val_loss: 1.1171 - val_mae: 1.5987\n",
            "Epoch 67/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0946 - mae: 1.5869 - val_loss: 1.1081 - val_mae: 1.5867\n",
            "Epoch 68/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0932 - mae: 1.5796 - val_loss: 1.1103 - val_mae: 1.5902\n",
            "Epoch 69/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.1016 - mae: 1.5878 - val_loss: 1.1212 - val_mae: 1.6074\n",
            "Epoch 70/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0850 - mae: 1.5722 - val_loss: 1.1000 - val_mae: 1.5804\n",
            "Epoch 71/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0800 - mae: 1.5694 - val_loss: 1.1158 - val_mae: 1.6025\n",
            "Epoch 72/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.0877 - mae: 1.5790 - val_loss: 1.0994 - val_mae: 1.5804\n",
            "Epoch 73/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0970 - mae: 1.5837 - val_loss: 1.1012 - val_mae: 1.5831\n",
            "Epoch 74/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0895 - mae: 1.5758 - val_loss: 1.0996 - val_mae: 1.5794\n",
            "Epoch 75/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0757 - mae: 1.5614 - val_loss: 1.1101 - val_mae: 1.5934\n",
            "Epoch 76/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0865 - mae: 1.5744 - val_loss: 1.1116 - val_mae: 1.5951\n",
            "Epoch 77/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0911 - mae: 1.5795 - val_loss: 1.1094 - val_mae: 1.5933\n",
            "Epoch 78/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0808 - mae: 1.5720 - val_loss: 1.0963 - val_mae: 1.5748\n",
            "Epoch 79/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0851 - mae: 1.5721 - val_loss: 1.1016 - val_mae: 1.5837\n",
            "Epoch 80/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0903 - mae: 1.5794 - val_loss: 1.0995 - val_mae: 1.5793\n",
            "Epoch 81/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0634 - mae: 1.5513 - val_loss: 1.0947 - val_mae: 1.5736\n",
            "Epoch 82/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0785 - mae: 1.5675 - val_loss: 1.1012 - val_mae: 1.5828\n",
            "Epoch 83/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0821 - mae: 1.5737 - val_loss: 1.0971 - val_mae: 1.5777\n",
            "Epoch 84/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0697 - mae: 1.5588 - val_loss: 1.1052 - val_mae: 1.5896\n",
            "Epoch 85/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0575 - mae: 1.5409 - val_loss: 1.1014 - val_mae: 1.5854\n",
            "Epoch 86/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0830 - mae: 1.5753 - val_loss: 1.1076 - val_mae: 1.5936\n",
            "Epoch 87/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0643 - mae: 1.5492 - val_loss: 1.0883 - val_mae: 1.5686\n",
            "Epoch 88/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0530 - mae: 1.5368 - val_loss: 1.0859 - val_mae: 1.5652\n",
            "Epoch 89/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0609 - mae: 1.5482 - val_loss: 1.0919 - val_mae: 1.5718\n",
            "Epoch 90/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0483 - mae: 1.5344 - val_loss: 1.0851 - val_mae: 1.5658\n",
            "Epoch 91/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.0659 - mae: 1.5564 - val_loss: 1.0807 - val_mae: 1.5578\n",
            "Epoch 92/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0676 - mae: 1.5579 - val_loss: 1.0860 - val_mae: 1.5659\n",
            "Epoch 93/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0570 - mae: 1.5428 - val_loss: 1.0805 - val_mae: 1.5603\n",
            "Epoch 94/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0689 - mae: 1.5572 - val_loss: 1.0742 - val_mae: 1.5520\n",
            "Epoch 95/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0677 - mae: 1.5561 - val_loss: 1.0797 - val_mae: 1.5596\n",
            "Epoch 96/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0736 - mae: 1.5653 - val_loss: 1.0758 - val_mae: 1.5523\n",
            "Epoch 97/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0488 - mae: 1.5338 - val_loss: 1.0819 - val_mae: 1.5624\n",
            "Epoch 98/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0442 - mae: 1.5297 - val_loss: 1.0855 - val_mae: 1.5666\n",
            "Epoch 99/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0402 - mae: 1.5282 - val_loss: 1.0692 - val_mae: 1.5455\n",
            "Epoch 100/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0529 - mae: 1.5404 - val_loss: 1.0661 - val_mae: 1.5428\n",
            "Epoch 101/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0526 - mae: 1.5400 - val_loss: 1.0785 - val_mae: 1.5610\n",
            "Epoch 102/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0459 - mae: 1.5300 - val_loss: 1.0788 - val_mae: 1.5614\n",
            "Epoch 103/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0486 - mae: 1.5384 - val_loss: 1.0776 - val_mae: 1.5605\n",
            "Epoch 104/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0401 - mae: 1.5284 - val_loss: 1.0617 - val_mae: 1.5413\n",
            "Epoch 105/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0367 - mae: 1.5224 - val_loss: 1.0764 - val_mae: 1.5598\n",
            "Epoch 106/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0502 - mae: 1.5386 - val_loss: 1.0598 - val_mae: 1.5372\n",
            "Epoch 107/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0404 - mae: 1.5281 - val_loss: 1.0623 - val_mae: 1.5419\n",
            "Epoch 108/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0595 - mae: 1.5487 - val_loss: 1.0665 - val_mae: 1.5465\n",
            "Epoch 109/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0343 - mae: 1.5193 - val_loss: 1.0755 - val_mae: 1.5584\n",
            "Epoch 110/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0427 - mae: 1.5282 - val_loss: 1.0709 - val_mae: 1.5526\n",
            "Epoch 111/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0291 - mae: 1.5170 - val_loss: 1.0685 - val_mae: 1.5495\n",
            "Epoch 112/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0512 - mae: 1.5407 - val_loss: 1.0650 - val_mae: 1.5452\n",
            "Epoch 113/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0225 - mae: 1.5092 - val_loss: 1.0641 - val_mae: 1.5449\n",
            "Epoch 114/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0143 - mae: 1.4988 - val_loss: 1.0595 - val_mae: 1.5420\n",
            "Epoch 115/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0153 - mae: 1.5018 - val_loss: 1.0591 - val_mae: 1.5431\n",
            "Epoch 116/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0408 - mae: 1.5284 - val_loss: 1.0501 - val_mae: 1.5310\n",
            "Epoch 117/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0164 - mae: 1.5022 - val_loss: 1.0543 - val_mae: 1.5377\n",
            "Epoch 118/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0166 - mae: 1.5019 - val_loss: 1.0410 - val_mae: 1.5193\n",
            "Epoch 119/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0344 - mae: 1.5219 - val_loss: 1.0376 - val_mae: 1.5144\n",
            "Epoch 120/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0100 - mae: 1.4932 - val_loss: 1.0479 - val_mae: 1.5278\n",
            "Epoch 121/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0323 - mae: 1.5190 - val_loss: 1.0304 - val_mae: 1.5065\n",
            "Epoch 122/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0303 - mae: 1.5201 - val_loss: 1.0375 - val_mae: 1.5159\n",
            "Epoch 123/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0365 - mae: 1.5255 - val_loss: 1.0482 - val_mae: 1.5292\n",
            "Epoch 124/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0193 - mae: 1.5040 - val_loss: 1.0350 - val_mae: 1.5129\n",
            "Epoch 125/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0382 - mae: 1.5270 - val_loss: 1.0321 - val_mae: 1.5100\n",
            "Epoch 126/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0195 - mae: 1.5039 - val_loss: 1.0360 - val_mae: 1.5149\n",
            "Epoch 127/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0196 - mae: 1.5074 - val_loss: 1.0383 - val_mae: 1.5164\n",
            "Epoch 128/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0303 - mae: 1.5131 - val_loss: 1.0408 - val_mae: 1.5185\n",
            "Epoch 129/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0213 - mae: 1.5081 - val_loss: 1.0331 - val_mae: 1.5123\n",
            "Epoch 130/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0136 - mae: 1.4983 - val_loss: 1.0461 - val_mae: 1.5275\n",
            "Epoch 131/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0111 - mae: 1.4973 - val_loss: 1.0336 - val_mae: 1.5127\n",
            "Epoch 132/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0227 - mae: 1.5122 - val_loss: 1.0294 - val_mae: 1.5078\n",
            "Epoch 133/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0117 - mae: 1.4967 - val_loss: 1.0291 - val_mae: 1.5098\n",
            "Epoch 134/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0077 - mae: 1.4943 - val_loss: 1.0281 - val_mae: 1.5088\n",
            "Epoch 135/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 1.0154 - mae: 1.5047 - val_loss: 1.0359 - val_mae: 1.5207\n",
            "Epoch 136/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0194 - mae: 1.5048 - val_loss: 1.0334 - val_mae: 1.5168\n",
            "Epoch 137/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9966 - mae: 1.4830 - val_loss: 1.0332 - val_mae: 1.5165\n",
            "Epoch 138/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0125 - mae: 1.4986 - val_loss: 1.0304 - val_mae: 1.5113\n",
            "Epoch 139/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0248 - mae: 1.5133 - val_loss: 1.0285 - val_mae: 1.5079\n",
            "Epoch 140/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0002 - mae: 1.4862 - val_loss: 1.0270 - val_mae: 1.5072\n",
            "Epoch 141/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0122 - mae: 1.4955 - val_loss: 1.0277 - val_mae: 1.5083\n",
            "Epoch 142/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 1.0146 - mae: 1.5020 - val_loss: 1.0220 - val_mae: 1.5024\n",
            "Epoch 143/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 1.0005 - mae: 1.4821 - val_loss: 1.0249 - val_mae: 1.5058\n",
            "Epoch 144/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0042 - mae: 1.4911 - val_loss: 1.0254 - val_mae: 1.5046\n",
            "Epoch 145/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9910 - mae: 1.4724 - val_loss: 1.0152 - val_mae: 1.4918\n",
            "Epoch 146/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0141 - mae: 1.5012 - val_loss: 1.0173 - val_mae: 1.4950\n",
            "Epoch 147/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9958 - mae: 1.4807 - val_loss: 1.0198 - val_mae: 1.4998\n",
            "Epoch 148/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0069 - mae: 1.4933 - val_loss: 1.0245 - val_mae: 1.5085\n",
            "Epoch 149/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0084 - mae: 1.4956 - val_loss: 1.0149 - val_mae: 1.4955\n",
            "Epoch 150/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0058 - mae: 1.4902 - val_loss: 1.0226 - val_mae: 1.5022\n",
            "Epoch 151/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9906 - mae: 1.4734 - val_loss: 1.0257 - val_mae: 1.5048\n",
            "Epoch 152/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9946 - mae: 1.4787 - val_loss: 1.0207 - val_mae: 1.4979\n",
            "Epoch 153/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0222 - mae: 1.5133 - val_loss: 1.0160 - val_mae: 1.4932\n",
            "Epoch 154/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9870 - mae: 1.4712 - val_loss: 1.0100 - val_mae: 1.4871\n",
            "Epoch 155/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0165 - mae: 1.5047 - val_loss: 1.0203 - val_mae: 1.4983\n",
            "Epoch 156/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9916 - mae: 1.4745 - val_loss: 1.0147 - val_mae: 1.4941\n",
            "Epoch 157/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9948 - mae: 1.4768 - val_loss: 1.0122 - val_mae: 1.4916\n",
            "Epoch 158/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9831 - mae: 1.4658 - val_loss: 1.0140 - val_mae: 1.4926\n",
            "Epoch 159/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9853 - mae: 1.4676 - val_loss: 1.0097 - val_mae: 1.4863\n",
            "Epoch 160/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9869 - mae: 1.4670 - val_loss: 1.0115 - val_mae: 1.4897\n",
            "Epoch 161/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9859 - mae: 1.4716 - val_loss: 1.0187 - val_mae: 1.4972\n",
            "Epoch 162/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9771 - mae: 1.4589 - val_loss: 1.0032 - val_mae: 1.4802\n",
            "Epoch 163/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0014 - mae: 1.4888 - val_loss: 0.9971 - val_mae: 1.4743\n",
            "Epoch 164/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9842 - mae: 1.4694 - val_loss: 1.0003 - val_mae: 1.4773\n",
            "Epoch 165/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9986 - mae: 1.4834 - val_loss: 1.0092 - val_mae: 1.4873\n",
            "Epoch 166/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0038 - mae: 1.4838 - val_loss: 0.9994 - val_mae: 1.4753\n",
            "Epoch 167/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 1.0043 - mae: 1.4888 - val_loss: 1.0056 - val_mae: 1.4817\n",
            "Epoch 168/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9903 - mae: 1.4768 - val_loss: 1.0041 - val_mae: 1.4815\n",
            "Epoch 169/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9711 - mae: 1.4538 - val_loss: 1.0049 - val_mae: 1.4818\n",
            "Epoch 170/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9913 - mae: 1.4781 - val_loss: 1.0035 - val_mae: 1.4798\n",
            "Epoch 171/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9963 - mae: 1.4839 - val_loss: 1.0003 - val_mae: 1.4790\n",
            "Epoch 172/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9869 - mae: 1.4694 - val_loss: 0.9949 - val_mae: 1.4741\n",
            "Epoch 173/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9856 - mae: 1.4690 - val_loss: 1.0081 - val_mae: 1.4868\n",
            "Epoch 174/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9963 - mae: 1.4787 - val_loss: 1.0066 - val_mae: 1.4857\n",
            "Epoch 175/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 1.0042 - mae: 1.4905 - val_loss: 1.0043 - val_mae: 1.4813\n",
            "Epoch 176/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.9965 - mae: 1.4807 - val_loss: 1.0009 - val_mae: 1.4790\n",
            "Epoch 177/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.9810 - mae: 1.4642 - val_loss: 0.9987 - val_mae: 1.4772\n",
            "Epoch 178/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9850 - mae: 1.4740 - val_loss: 1.0040 - val_mae: 1.4845\n",
            "Epoch 179/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.9878 - mae: 1.4717 - val_loss: 0.9950 - val_mae: 1.4752\n",
            "Epoch 180/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.9886 - mae: 1.4728 - val_loss: 0.9905 - val_mae: 1.4701\n",
            "Epoch 181/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9897 - mae: 1.4732 - val_loss: 0.9884 - val_mae: 1.4661\n",
            "Epoch 182/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.9816 - mae: 1.4638 - val_loss: 0.9908 - val_mae: 1.4704\n",
            "Epoch 183/200\n",
            "54/54 [==============================] - 0s 8ms/step - loss: 0.9888 - mae: 1.4718 - val_loss: 1.0066 - val_mae: 1.4906\n",
            "Epoch 184/200\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.9774 - mae: 1.4622 - val_loss: 0.9921 - val_mae: 1.4731\n",
            "Epoch 185/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.9745 - mae: 1.4609 - val_loss: 0.9828 - val_mae: 1.4602\n",
            "Epoch 186/200\n",
            "54/54 [==============================] - 0s 9ms/step - loss: 0.9844 - mae: 1.4673 - val_loss: 0.9889 - val_mae: 1.4693\n",
            "Epoch 187/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9723 - mae: 1.4528 - val_loss: 0.9952 - val_mae: 1.4739\n",
            "Epoch 188/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9590 - mae: 1.4403 - val_loss: 0.9931 - val_mae: 1.4722\n",
            "Epoch 189/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9844 - mae: 1.4698 - val_loss: 0.9926 - val_mae: 1.4713\n",
            "Epoch 190/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9794 - mae: 1.4625 - val_loss: 0.9873 - val_mae: 1.4653\n",
            "Epoch 191/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9763 - mae: 1.4586 - val_loss: 0.9894 - val_mae: 1.4676\n",
            "Epoch 192/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9869 - mae: 1.4710 - val_loss: 0.9906 - val_mae: 1.4690\n",
            "Epoch 193/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9699 - mae: 1.4481 - val_loss: 0.9923 - val_mae: 1.4734\n",
            "Epoch 194/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9716 - mae: 1.4547 - val_loss: 0.9896 - val_mae: 1.4711\n",
            "Epoch 195/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9710 - mae: 1.4562 - val_loss: 0.9907 - val_mae: 1.4710\n",
            "Epoch 196/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9653 - mae: 1.4476 - val_loss: 0.9850 - val_mae: 1.4634\n",
            "Epoch 197/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9855 - mae: 1.4695 - val_loss: 0.9800 - val_mae: 1.4589\n",
            "Epoch 198/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9881 - mae: 1.4728 - val_loss: 0.9757 - val_mae: 1.4547\n",
            "Epoch 199/200\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.9652 - mae: 1.4459 - val_loss: 0.9756 - val_mae: 1.4544\n",
            "Epoch 200/200\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.9748 - mae: 1.4603 - val_loss: 0.9849 - val_mae: 1.4642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:37:08,812] Trial 22 finished with value: 0.9848942160606384 and parameters: {'dropout_2': 0.21513540745746934, 'dropout_3': 0.23276970893508417, 'dropout_4': 0.6181982692420354, 'dropout_5': 0.6885820304467996, 'learning_rate': 0.00018520107815146882, 'epochs': 200, 'batch_size': 64}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 3s 7ms/step - loss: 1.6378 - mae: 2.1997 - val_loss: 1.4232 - val_mae: 1.9347\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.5867 - mae: 2.1443 - val_loss: 1.4475 - val_mae: 1.9713\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5286 - mae: 2.0820 - val_loss: 1.4118 - val_mae: 1.9383\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4825 - mae: 2.0314 - val_loss: 1.3568 - val_mae: 1.8768\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4215 - mae: 1.9637 - val_loss: 1.3083 - val_mae: 1.8246\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4248 - mae: 1.9690 - val_loss: 1.2832 - val_mae: 1.7976\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3908 - mae: 1.9272 - val_loss: 1.2604 - val_mae: 1.7734\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3815 - mae: 1.9176 - val_loss: 1.2441 - val_mae: 1.7554\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.3342 - mae: 1.8642 - val_loss: 1.2301 - val_mae: 1.7388\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.3384 - mae: 1.8693 - val_loss: 1.2219 - val_mae: 1.7281\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.3416 - mae: 1.8731 - val_loss: 1.2085 - val_mae: 1.7116\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.3008 - mae: 1.8295 - val_loss: 1.2034 - val_mae: 1.7056\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2881 - mae: 1.8109 - val_loss: 1.2069 - val_mae: 1.7096\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2898 - mae: 1.8136 - val_loss: 1.1979 - val_mae: 1.6983\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3001 - mae: 1.8241 - val_loss: 1.1985 - val_mae: 1.6989\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2696 - mae: 1.7916 - val_loss: 1.1819 - val_mae: 1.6792\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2645 - mae: 1.7846 - val_loss: 1.1837 - val_mae: 1.6814\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2570 - mae: 1.7754 - val_loss: 1.1850 - val_mae: 1.6832\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2189 - mae: 1.7329 - val_loss: 1.1851 - val_mae: 1.6827\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2243 - mae: 1.7362 - val_loss: 1.1790 - val_mae: 1.6757\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2105 - mae: 1.7208 - val_loss: 1.1749 - val_mae: 1.6724\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.2068 - mae: 1.7153 - val_loss: 1.1771 - val_mae: 1.6745\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2235 - mae: 1.7350 - val_loss: 1.1719 - val_mae: 1.6668\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1905 - mae: 1.6992 - val_loss: 1.1663 - val_mae: 1.6598\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1971 - mae: 1.7062 - val_loss: 1.1711 - val_mae: 1.6645\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1889 - mae: 1.6966 - val_loss: 1.1631 - val_mae: 1.6566\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1929 - mae: 1.6992 - val_loss: 1.1534 - val_mae: 1.6433\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1676 - mae: 1.6703 - val_loss: 1.1609 - val_mae: 1.6530\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1720 - mae: 1.6750 - val_loss: 1.1538 - val_mae: 1.6447\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1609 - mae: 1.6634 - val_loss: 1.1469 - val_mae: 1.6340\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1691 - mae: 1.6749 - val_loss: 1.1431 - val_mae: 1.6297\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1658 - mae: 1.6689 - val_loss: 1.1453 - val_mae: 1.6341\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1589 - mae: 1.6619 - val_loss: 1.1471 - val_mae: 1.6349\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1402 - mae: 1.6397 - val_loss: 1.1428 - val_mae: 1.6280\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1479 - mae: 1.6491 - val_loss: 1.1409 - val_mae: 1.6264\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1415 - mae: 1.6455 - val_loss: 1.1382 - val_mae: 1.6245\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1512 - mae: 1.6496 - val_loss: 1.1392 - val_mae: 1.6271\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1309 - mae: 1.6312 - val_loss: 1.1443 - val_mae: 1.6312\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1414 - mae: 1.6438 - val_loss: 1.1368 - val_mae: 1.6217\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1311 - mae: 1.6308 - val_loss: 1.1354 - val_mae: 1.6210\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1458 - mae: 1.6444 - val_loss: 1.1281 - val_mae: 1.6134\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1271 - mae: 1.6234 - val_loss: 1.1310 - val_mae: 1.6167\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1183 - mae: 1.6163 - val_loss: 1.1291 - val_mae: 1.6148\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1152 - mae: 1.6131 - val_loss: 1.1352 - val_mae: 1.6210\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1214 - mae: 1.6195 - val_loss: 1.1255 - val_mae: 1.6092\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1096 - mae: 1.6033 - val_loss: 1.1273 - val_mae: 1.6127\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1070 - mae: 1.6021 - val_loss: 1.1194 - val_mae: 1.6042\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0968 - mae: 1.5900 - val_loss: 1.1129 - val_mae: 1.5946\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0983 - mae: 1.5940 - val_loss: 1.1090 - val_mae: 1.5914\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0940 - mae: 1.5865 - val_loss: 1.1195 - val_mae: 1.6034\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0934 - mae: 1.5888 - val_loss: 1.1140 - val_mae: 1.5951\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0996 - mae: 1.5932 - val_loss: 1.1213 - val_mae: 1.6046\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0929 - mae: 1.5864 - val_loss: 1.1147 - val_mae: 1.5989\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0961 - mae: 1.5898 - val_loss: 1.1071 - val_mae: 1.5910\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0940 - mae: 1.5869 - val_loss: 1.1093 - val_mae: 1.5925\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0902 - mae: 1.5834 - val_loss: 1.1082 - val_mae: 1.5906\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0806 - mae: 1.5738 - val_loss: 1.1000 - val_mae: 1.5807\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0842 - mae: 1.5795 - val_loss: 1.1088 - val_mae: 1.5931\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0752 - mae: 1.5666 - val_loss: 1.0950 - val_mae: 1.5746\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0881 - mae: 1.5811 - val_loss: 1.0978 - val_mae: 1.5818\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0770 - mae: 1.5718 - val_loss: 1.0958 - val_mae: 1.5779\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0640 - mae: 1.5536 - val_loss: 1.0930 - val_mae: 1.5736\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0690 - mae: 1.5607 - val_loss: 1.0872 - val_mae: 1.5681\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0701 - mae: 1.5632 - val_loss: 1.0821 - val_mae: 1.5607\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0708 - mae: 1.5629 - val_loss: 1.0880 - val_mae: 1.5689\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0667 - mae: 1.5601 - val_loss: 1.0817 - val_mae: 1.5598\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0705 - mae: 1.5599 - val_loss: 1.0874 - val_mae: 1.5681\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0596 - mae: 1.5512 - val_loss: 1.0845 - val_mae: 1.5653\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0583 - mae: 1.5479 - val_loss: 1.0807 - val_mae: 1.5608\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0574 - mae: 1.5483 - val_loss: 1.0837 - val_mae: 1.5624\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0555 - mae: 1.5482 - val_loss: 1.0765 - val_mae: 1.5544\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0671 - mae: 1.5612 - val_loss: 1.0843 - val_mae: 1.5664\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0569 - mae: 1.5489 - val_loss: 1.0688 - val_mae: 1.5479\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0494 - mae: 1.5434 - val_loss: 1.0663 - val_mae: 1.5445\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0496 - mae: 1.5406 - val_loss: 1.0687 - val_mae: 1.5472\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0491 - mae: 1.5430 - val_loss: 1.0675 - val_mae: 1.5465\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0611 - mae: 1.5516 - val_loss: 1.0730 - val_mae: 1.5542\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0460 - mae: 1.5406 - val_loss: 1.0694 - val_mae: 1.5499\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0370 - mae: 1.5259 - val_loss: 1.0674 - val_mae: 1.5472\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0475 - mae: 1.5395 - val_loss: 1.0675 - val_mae: 1.5479\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0421 - mae: 1.5351 - val_loss: 1.0687 - val_mae: 1.5500\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0331 - mae: 1.5239 - val_loss: 1.0621 - val_mae: 1.5423\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0377 - mae: 1.5286 - val_loss: 1.0588 - val_mae: 1.5385\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0302 - mae: 1.5197 - val_loss: 1.0588 - val_mae: 1.5382\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0365 - mae: 1.5279 - val_loss: 1.0600 - val_mae: 1.5414\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0450 - mae: 1.5362 - val_loss: 1.0558 - val_mae: 1.5359\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0369 - mae: 1.5274 - val_loss: 1.0532 - val_mae: 1.5326\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0369 - mae: 1.5272 - val_loss: 1.0615 - val_mae: 1.5429\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0360 - mae: 1.5269 - val_loss: 1.0595 - val_mae: 1.5407\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0295 - mae: 1.5224 - val_loss: 1.0551 - val_mae: 1.5358\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0368 - mae: 1.5311 - val_loss: 1.0450 - val_mae: 1.5261\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0221 - mae: 1.5123 - val_loss: 1.0516 - val_mae: 1.5312\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0314 - mae: 1.5194 - val_loss: 1.0441 - val_mae: 1.5228\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0208 - mae: 1.5080 - val_loss: 1.0367 - val_mae: 1.5136\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0279 - mae: 1.5193 - val_loss: 1.0413 - val_mae: 1.5200\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0197 - mae: 1.5085 - val_loss: 1.0533 - val_mae: 1.5329\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0209 - mae: 1.5109 - val_loss: 1.0469 - val_mae: 1.5261\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0205 - mae: 1.5125 - val_loss: 1.0402 - val_mae: 1.5192\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0018 - mae: 1.4930 - val_loss: 1.0504 - val_mae: 1.5315\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0341 - mae: 1.5236 - val_loss: 1.0460 - val_mae: 1.5270\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0048 - mae: 1.4944 - val_loss: 1.0399 - val_mae: 1.5193\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0046 - mae: 1.4936 - val_loss: 1.0364 - val_mae: 1.5168\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9991 - mae: 1.4872 - val_loss: 1.0416 - val_mae: 1.5216\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0074 - mae: 1.4990 - val_loss: 1.0324 - val_mae: 1.5133\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0033 - mae: 1.4908 - val_loss: 1.0266 - val_mae: 1.5075\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0084 - mae: 1.4967 - val_loss: 1.0230 - val_mae: 1.5023\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0063 - mae: 1.4945 - val_loss: 1.0250 - val_mae: 1.5056\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0054 - mae: 1.4967 - val_loss: 1.0202 - val_mae: 1.5011\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0083 - mae: 1.4999 - val_loss: 1.0236 - val_mae: 1.5039\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9980 - mae: 1.4845 - val_loss: 1.0161 - val_mae: 1.4980\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0072 - mae: 1.4967 - val_loss: 1.0105 - val_mae: 1.4907\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9956 - mae: 1.4855 - val_loss: 1.0088 - val_mae: 1.4846\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0163 - mae: 1.5082 - val_loss: 1.0067 - val_mae: 1.4864\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0180 - mae: 1.5085 - val_loss: 1.0121 - val_mae: 1.4915\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0092 - mae: 1.4968 - val_loss: 1.0094 - val_mae: 1.4878\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9955 - mae: 1.4818 - val_loss: 1.0124 - val_mae: 1.4905\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0169 - mae: 1.5080 - val_loss: 1.0072 - val_mae: 1.4844\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0149 - mae: 1.5067 - val_loss: 1.0013 - val_mae: 1.4770\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0024 - mae: 1.4891 - val_loss: 1.0081 - val_mae: 1.4852\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9835 - mae: 1.4736 - val_loss: 0.9991 - val_mae: 1.4752\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9896 - mae: 1.4735 - val_loss: 0.9998 - val_mae: 1.4774\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9969 - mae: 1.4873 - val_loss: 1.0061 - val_mae: 1.4845\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9811 - mae: 1.4680 - val_loss: 1.0016 - val_mae: 1.4794\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9814 - mae: 1.4691 - val_loss: 1.0060 - val_mae: 1.4831\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9749 - mae: 1.4644 - val_loss: 0.9943 - val_mae: 1.4737\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9850 - mae: 1.4725 - val_loss: 0.9950 - val_mae: 1.4749\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9916 - mae: 1.4830 - val_loss: 0.9870 - val_mae: 1.4647\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9818 - mae: 1.4689 - val_loss: 0.9926 - val_mae: 1.4711\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0055 - mae: 1.4918 - val_loss: 1.0012 - val_mae: 1.4790\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9806 - mae: 1.4659 - val_loss: 0.9994 - val_mae: 1.4756\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9689 - mae: 1.4557 - val_loss: 0.9982 - val_mae: 1.4742\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9885 - mae: 1.4748 - val_loss: 0.9963 - val_mae: 1.4744\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0038 - mae: 1.4930 - val_loss: 0.9981 - val_mae: 1.4747\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9673 - mae: 1.4533 - val_loss: 0.9959 - val_mae: 1.4735\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9713 - mae: 1.4574 - val_loss: 0.9935 - val_mae: 1.4703\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9753 - mae: 1.4628 - val_loss: 0.9842 - val_mae: 1.4594\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9684 - mae: 1.4524 - val_loss: 0.9815 - val_mae: 1.4581\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9762 - mae: 1.4605 - val_loss: 0.9800 - val_mae: 1.4565\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9691 - mae: 1.4561 - val_loss: 0.9833 - val_mae: 1.4598\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9692 - mae: 1.4554 - val_loss: 0.9860 - val_mae: 1.4622\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9699 - mae: 1.4557 - val_loss: 0.9845 - val_mae: 1.4615\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9882 - mae: 1.4756 - val_loss: 0.9875 - val_mae: 1.4644\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9660 - mae: 1.4517 - val_loss: 0.9794 - val_mae: 1.4558\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9593 - mae: 1.4444 - val_loss: 0.9772 - val_mae: 1.4523\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9767 - mae: 1.4623 - val_loss: 0.9848 - val_mae: 1.4601\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9747 - mae: 1.4648 - val_loss: 0.9845 - val_mae: 1.4589\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9661 - mae: 1.4468 - val_loss: 0.9858 - val_mae: 1.4608\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9764 - mae: 1.4637 - val_loss: 0.9808 - val_mae: 1.4563\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9697 - mae: 1.4550 - val_loss: 0.9792 - val_mae: 1.4529\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9866 - mae: 1.4772 - val_loss: 0.9867 - val_mae: 1.4626\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9716 - mae: 1.4583 - val_loss: 0.9852 - val_mae: 1.4609\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9751 - mae: 1.4601 - val_loss: 0.9858 - val_mae: 1.4591\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9716 - mae: 1.4583 - val_loss: 0.9857 - val_mae: 1.4568\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9607 - mae: 1.4460 - val_loss: 0.9818 - val_mae: 1.4553\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9583 - mae: 1.4426 - val_loss: 0.9951 - val_mae: 1.4699\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9633 - mae: 1.4500 - val_loss: 0.9784 - val_mae: 1.4530\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9496 - mae: 1.4340 - val_loss: 0.9879 - val_mae: 1.4595\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9633 - mae: 1.4494 - val_loss: 1.0117 - val_mae: 1.4875\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9573 - mae: 1.4446 - val_loss: 0.9879 - val_mae: 1.4636\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9532 - mae: 1.4393 - val_loss: 0.9976 - val_mae: 1.4736\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9621 - mae: 1.4456 - val_loss: 0.9943 - val_mae: 1.4681\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9481 - mae: 1.4275 - val_loss: 0.9842 - val_mae: 1.4557\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9562 - mae: 1.4412 - val_loss: 0.9956 - val_mae: 1.4671\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9669 - mae: 1.4533 - val_loss: 0.9917 - val_mae: 1.4617\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9417 - mae: 1.4210 - val_loss: 0.9972 - val_mae: 1.4708\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9428 - mae: 1.4247 - val_loss: 0.9889 - val_mae: 1.4633\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9450 - mae: 1.4285 - val_loss: 0.9802 - val_mae: 1.4538\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9500 - mae: 1.4330 - val_loss: 0.9918 - val_mae: 1.4653\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9605 - mae: 1.4441 - val_loss: 1.0088 - val_mae: 1.4853\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9520 - mae: 1.4331 - val_loss: 1.0027 - val_mae: 1.4801\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9430 - mae: 1.4264 - val_loss: 0.9994 - val_mae: 1.4742\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9567 - mae: 1.4402 - val_loss: 0.9959 - val_mae: 1.4702\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9472 - mae: 1.4313 - val_loss: 0.9965 - val_mae: 1.4705\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9472 - mae: 1.4346 - val_loss: 0.9921 - val_mae: 1.4667\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9497 - mae: 1.4341 - val_loss: 0.9971 - val_mae: 1.4735\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9538 - mae: 1.4354 - val_loss: 0.9920 - val_mae: 1.4641\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9434 - mae: 1.4253 - val_loss: 0.9915 - val_mae: 1.4657\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9492 - mae: 1.4340 - val_loss: 0.9850 - val_mae: 1.4577\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9659 - mae: 1.4464 - val_loss: 0.9816 - val_mae: 1.4524\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9346 - mae: 1.4166 - val_loss: 0.9716 - val_mae: 1.4430\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9376 - mae: 1.4154 - val_loss: 0.9869 - val_mae: 1.4606\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9520 - mae: 1.4335 - val_loss: 0.9697 - val_mae: 1.4400\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9465 - mae: 1.4301 - val_loss: 0.9819 - val_mae: 1.4534\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9399 - mae: 1.4270 - val_loss: 0.9681 - val_mae: 1.4391\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9403 - mae: 1.4210 - val_loss: 0.9800 - val_mae: 1.4516\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9457 - mae: 1.4284 - val_loss: 0.9866 - val_mae: 1.4573\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9447 - mae: 1.4269 - val_loss: 0.9869 - val_mae: 1.4576\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9397 - mae: 1.4177 - val_loss: 0.9777 - val_mae: 1.4488\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9446 - mae: 1.4268 - val_loss: 0.9811 - val_mae: 1.4509\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9520 - mae: 1.4345 - val_loss: 1.0002 - val_mae: 1.4700\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9458 - mae: 1.4297 - val_loss: 0.9919 - val_mae: 1.4633\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9334 - mae: 1.4147 - val_loss: 0.9913 - val_mae: 1.4624\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9454 - mae: 1.4249 - val_loss: 0.9904 - val_mae: 1.4607\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9382 - mae: 1.4193 - val_loss: 0.9978 - val_mae: 1.4695\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9521 - mae: 1.4368 - val_loss: 0.9824 - val_mae: 1.4545\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9371 - mae: 1.4178 - val_loss: 0.9879 - val_mae: 1.4582\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9460 - mae: 1.4262 - val_loss: 0.9989 - val_mae: 1.4705\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9464 - mae: 1.4279 - val_loss: 0.9949 - val_mae: 1.4660\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9432 - mae: 1.4279 - val_loss: 0.9993 - val_mae: 1.4724\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9221 - mae: 1.4000 - val_loss: 0.9901 - val_mae: 1.4624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:39:01,868] Trial 23 finished with value: 0.9901328086853027 and parameters: {'dropout_2': 0.17156117164046653, 'dropout_3': 0.1972586829270968, 'dropout_4': 0.45036337824812583, 'dropout_5': 0.3979668639892267, 'learning_rate': 0.00010916585202556442, 'epochs': 200, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 3s 7ms/step - loss: 1.4853 - mae: 2.0345 - val_loss: 1.3133 - val_mae: 1.8169\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.2802 - mae: 1.7982 - val_loss: 1.2295 - val_mae: 1.7204\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2099 - mae: 1.7145 - val_loss: 1.1951 - val_mae: 1.6766\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1490 - mae: 1.6388 - val_loss: 1.1682 - val_mae: 1.6571\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1290 - mae: 1.6181 - val_loss: 1.1426 - val_mae: 1.6243\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1193 - mae: 1.6051 - val_loss: 1.1265 - val_mae: 1.6058\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1102 - mae: 1.5953 - val_loss: 1.1031 - val_mae: 1.5765\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0982 - mae: 1.5838 - val_loss: 1.1373 - val_mae: 1.6173\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0820 - mae: 1.5676 - val_loss: 1.1011 - val_mae: 1.5826\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0821 - mae: 1.5719 - val_loss: 1.1031 - val_mae: 1.5847\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0898 - mae: 1.5749 - val_loss: 1.0948 - val_mae: 1.5841\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0732 - mae: 1.5617 - val_loss: 1.0737 - val_mae: 1.5646\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0697 - mae: 1.5558 - val_loss: 1.1034 - val_mae: 1.5996\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0704 - mae: 1.5556 - val_loss: 1.0589 - val_mae: 1.5539\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0539 - mae: 1.5387 - val_loss: 1.0756 - val_mae: 1.5675\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0670 - mae: 1.5565 - val_loss: 1.0674 - val_mae: 1.5537\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0747 - mae: 1.5636 - val_loss: 1.0658 - val_mae: 1.5450\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0505 - mae: 1.5400 - val_loss: 1.1107 - val_mae: 1.6168\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0363 - mae: 1.5215 - val_loss: 1.0610 - val_mae: 1.5575\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0345 - mae: 1.5214 - val_loss: 1.0833 - val_mae: 1.5814\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0388 - mae: 1.5252 - val_loss: 1.0824 - val_mae: 1.5768\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0395 - mae: 1.5272 - val_loss: 1.0600 - val_mae: 1.5528\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0354 - mae: 1.5198 - val_loss: 1.1038 - val_mae: 1.6079\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0261 - mae: 1.5091 - val_loss: 1.0736 - val_mae: 1.5711\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0271 - mae: 1.5129 - val_loss: 1.0836 - val_mae: 1.5855\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0239 - mae: 1.5071 - val_loss: 1.0415 - val_mae: 1.5374\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0288 - mae: 1.5175 - val_loss: 1.0776 - val_mae: 1.5754\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0217 - mae: 1.5052 - val_loss: 1.0652 - val_mae: 1.5645\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0298 - mae: 1.5141 - val_loss: 1.0694 - val_mae: 1.5606\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0244 - mae: 1.5114 - val_loss: 1.0236 - val_mae: 1.5092\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0264 - mae: 1.5128 - val_loss: 1.0350 - val_mae: 1.5205\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0098 - mae: 1.4936 - val_loss: 1.0510 - val_mae: 1.5510\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0026 - mae: 1.4826 - val_loss: 1.0591 - val_mae: 1.5500\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0144 - mae: 1.4975 - val_loss: 1.0597 - val_mae: 1.5432\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0245 - mae: 1.5104 - val_loss: 1.1004 - val_mae: 1.5928\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9994 - mae: 1.4831 - val_loss: 1.0642 - val_mae: 1.5500\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0126 - mae: 1.4958 - val_loss: 1.0620 - val_mae: 1.5550\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9944 - mae: 1.4783 - val_loss: 1.0335 - val_mae: 1.5331\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0132 - mae: 1.4961 - val_loss: 1.0674 - val_mae: 1.5630\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0138 - mae: 1.4982 - val_loss: 1.0576 - val_mae: 1.5502\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0152 - mae: 1.5015 - val_loss: 1.0368 - val_mae: 1.5276\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9943 - mae: 1.4742 - val_loss: 1.0639 - val_mae: 1.5518\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0077 - mae: 1.4878 - val_loss: 1.0538 - val_mae: 1.5397\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0064 - mae: 1.4881 - val_loss: 1.0564 - val_mae: 1.5459\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9997 - mae: 1.4794 - val_loss: 1.0318 - val_mae: 1.5132\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9944 - mae: 1.4712 - val_loss: 1.0612 - val_mae: 1.5555\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0007 - mae: 1.4821 - val_loss: 1.0608 - val_mae: 1.5432\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0097 - mae: 1.4951 - val_loss: 1.0336 - val_mae: 1.5115\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9914 - mae: 1.4741 - val_loss: 1.0098 - val_mae: 1.5007\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9920 - mae: 1.4744 - val_loss: 1.0425 - val_mae: 1.5362\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9929 - mae: 1.4773 - val_loss: 1.0371 - val_mae: 1.5296\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9870 - mae: 1.4643 - val_loss: 1.0126 - val_mae: 1.5008\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9942 - mae: 1.4766 - val_loss: 1.0144 - val_mae: 1.4926\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9832 - mae: 1.4638 - val_loss: 1.0036 - val_mae: 1.4901\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9812 - mae: 1.4658 - val_loss: 0.9900 - val_mae: 1.4693\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9967 - mae: 1.4788 - val_loss: 1.0002 - val_mae: 1.4833\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9900 - mae: 1.4750 - val_loss: 1.0269 - val_mae: 1.5141\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9787 - mae: 1.4579 - val_loss: 1.0037 - val_mae: 1.4866\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9806 - mae: 1.4618 - val_loss: 1.0338 - val_mae: 1.5234\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9816 - mae: 1.4638 - val_loss: 1.0441 - val_mae: 1.5184\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0001 - mae: 1.4850 - val_loss: 1.0374 - val_mae: 1.5119\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9821 - mae: 1.4640 - val_loss: 1.0272 - val_mae: 1.5171\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9787 - mae: 1.4622 - val_loss: 1.0045 - val_mae: 1.4806\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9787 - mae: 1.4587 - val_loss: 1.0078 - val_mae: 1.4842\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9933 - mae: 1.4790 - val_loss: 1.0280 - val_mae: 1.5158\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9811 - mae: 1.4603 - val_loss: 0.9897 - val_mae: 1.4771\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9763 - mae: 1.4557 - val_loss: 1.0037 - val_mae: 1.4891\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9834 - mae: 1.4654 - val_loss: 1.0523 - val_mae: 1.5405\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9792 - mae: 1.4564 - val_loss: 1.0168 - val_mae: 1.4935\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9767 - mae: 1.4559 - val_loss: 1.0447 - val_mae: 1.5294\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9748 - mae: 1.4523 - val_loss: 0.9855 - val_mae: 1.4689\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9693 - mae: 1.4512 - val_loss: 0.9954 - val_mae: 1.4762\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9604 - mae: 1.4408 - val_loss: 0.9868 - val_mae: 1.4615\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9767 - mae: 1.4596 - val_loss: 0.9797 - val_mae: 1.4518\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9692 - mae: 1.4493 - val_loss: 1.0064 - val_mae: 1.4881\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9757 - mae: 1.4533 - val_loss: 0.9989 - val_mae: 1.4823\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9854 - mae: 1.4645 - val_loss: 1.0169 - val_mae: 1.4918\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9914 - mae: 1.4691 - val_loss: 0.9788 - val_mae: 1.4543\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9791 - mae: 1.4585 - val_loss: 0.9829 - val_mae: 1.4541\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9664 - mae: 1.4425 - val_loss: 1.0269 - val_mae: 1.5094\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9748 - mae: 1.4527 - val_loss: 0.9777 - val_mae: 1.4559\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9673 - mae: 1.4450 - val_loss: 0.9650 - val_mae: 1.4390\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9507 - mae: 1.4277 - val_loss: 0.9858 - val_mae: 1.4629\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9369 - mae: 1.4114 - val_loss: 0.9683 - val_mae: 1.4496\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9728 - mae: 1.4532 - val_loss: 1.0023 - val_mae: 1.4885\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9634 - mae: 1.4446 - val_loss: 1.0133 - val_mae: 1.4949\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9656 - mae: 1.4485 - val_loss: 1.0279 - val_mae: 1.5183\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9646 - mae: 1.4453 - val_loss: 1.0009 - val_mae: 1.4709\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9766 - mae: 1.4580 - val_loss: 0.9770 - val_mae: 1.4522\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9599 - mae: 1.4398 - val_loss: 0.9943 - val_mae: 1.4804\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9739 - mae: 1.4532 - val_loss: 1.0100 - val_mae: 1.4764\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9597 - mae: 1.4426 - val_loss: 0.9928 - val_mae: 1.4715\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9614 - mae: 1.4409 - val_loss: 0.9940 - val_mae: 1.4690\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9519 - mae: 1.4295 - val_loss: 1.0294 - val_mae: 1.5126\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9682 - mae: 1.4472 - val_loss: 0.9937 - val_mae: 1.4691\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9590 - mae: 1.4351 - val_loss: 0.9698 - val_mae: 1.4366\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9483 - mae: 1.4249 - val_loss: 0.9754 - val_mae: 1.4505\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9540 - mae: 1.4333 - val_loss: 0.9591 - val_mae: 1.4344\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9502 - mae: 1.4284 - val_loss: 1.0110 - val_mae: 1.4865\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9548 - mae: 1.4321 - val_loss: 1.0168 - val_mae: 1.4979\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9601 - mae: 1.4370 - val_loss: 1.0081 - val_mae: 1.4822\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9651 - mae: 1.4455 - val_loss: 0.9799 - val_mae: 1.4538\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9497 - mae: 1.4243 - val_loss: 0.9712 - val_mae: 1.4429\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9591 - mae: 1.4341 - val_loss: 0.9687 - val_mae: 1.4427\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9597 - mae: 1.4309 - val_loss: 0.9463 - val_mae: 1.4128\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9497 - mae: 1.4235 - val_loss: 0.9784 - val_mae: 1.4552\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9590 - mae: 1.4376 - val_loss: 0.9826 - val_mae: 1.4601\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9544 - mae: 1.4333 - val_loss: 0.9773 - val_mae: 1.4535\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9555 - mae: 1.4299 - val_loss: 0.9617 - val_mae: 1.4303\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9463 - mae: 1.4216 - val_loss: 0.9824 - val_mae: 1.4528\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9301 - mae: 1.4020 - val_loss: 0.9944 - val_mae: 1.4788\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9423 - mae: 1.4168 - val_loss: 0.9442 - val_mae: 1.4163\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9480 - mae: 1.4269 - val_loss: 0.9601 - val_mae: 1.4345\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9565 - mae: 1.4342 - val_loss: 0.9459 - val_mae: 1.4116\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9330 - mae: 1.4064 - val_loss: 0.9476 - val_mae: 1.4180\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9306 - mae: 1.4056 - val_loss: 0.9765 - val_mae: 1.4512\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9327 - mae: 1.4086 - val_loss: 0.9967 - val_mae: 1.4719\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9508 - mae: 1.4292 - val_loss: 0.9595 - val_mae: 1.4226\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9534 - mae: 1.4288 - val_loss: 0.9383 - val_mae: 1.4031\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9655 - mae: 1.4448 - val_loss: 0.9513 - val_mae: 1.4137\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9579 - mae: 1.4329 - val_loss: 0.9486 - val_mae: 1.4217\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9551 - mae: 1.4311 - val_loss: 0.9526 - val_mae: 1.4203\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9553 - mae: 1.4361 - val_loss: 0.9853 - val_mae: 1.4672\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9183 - mae: 1.3921 - val_loss: 0.9783 - val_mae: 1.4450\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9363 - mae: 1.4106 - val_loss: 0.9704 - val_mae: 1.4521\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9529 - mae: 1.4275 - val_loss: 0.9775 - val_mae: 1.4446\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9453 - mae: 1.4195 - val_loss: 1.0069 - val_mae: 1.4863\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9441 - mae: 1.4201 - val_loss: 1.0047 - val_mae: 1.4805\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9286 - mae: 1.3989 - val_loss: 0.9990 - val_mae: 1.4727\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9408 - mae: 1.4143 - val_loss: 0.9526 - val_mae: 1.4184\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9395 - mae: 1.4113 - val_loss: 0.9772 - val_mae: 1.4529\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9417 - mae: 1.4177 - val_loss: 0.9541 - val_mae: 1.4186\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9433 - mae: 1.4157 - val_loss: 0.9569 - val_mae: 1.4209\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9364 - mae: 1.4107 - val_loss: 0.9730 - val_mae: 1.4419\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9250 - mae: 1.3973 - val_loss: 0.9762 - val_mae: 1.4576\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9410 - mae: 1.4109 - val_loss: 0.9620 - val_mae: 1.4305\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9459 - mae: 1.4162 - val_loss: 0.9615 - val_mae: 1.4339\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9268 - mae: 1.3973 - val_loss: 0.9512 - val_mae: 1.4265\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9458 - mae: 1.4172 - val_loss: 0.9810 - val_mae: 1.4506\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9363 - mae: 1.4100 - val_loss: 0.9989 - val_mae: 1.4781\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9491 - mae: 1.4253 - val_loss: 0.9787 - val_mae: 1.4568\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9277 - mae: 1.4021 - val_loss: 0.9963 - val_mae: 1.4773\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9242 - mae: 1.3922 - val_loss: 0.9852 - val_mae: 1.4550\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9413 - mae: 1.4175 - val_loss: 0.9895 - val_mae: 1.4762\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9399 - mae: 1.4147 - val_loss: 0.9641 - val_mae: 1.4311\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9239 - mae: 1.3947 - val_loss: 0.9946 - val_mae: 1.4703\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9313 - mae: 1.4014 - val_loss: 0.9721 - val_mae: 1.4418\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9428 - mae: 1.4142 - val_loss: 0.9493 - val_mae: 1.4179\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9430 - mae: 1.4200 - val_loss: 0.9544 - val_mae: 1.4199\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9363 - mae: 1.4098 - val_loss: 0.9472 - val_mae: 1.4094\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9268 - mae: 1.3991 - val_loss: 0.9692 - val_mae: 1.4470\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9210 - mae: 1.3933 - val_loss: 0.9774 - val_mae: 1.4521\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9071 - mae: 1.3749 - val_loss: 0.9699 - val_mae: 1.4328\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9283 - mae: 1.3987 - val_loss: 0.9991 - val_mae: 1.4732\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9396 - mae: 1.4134 - val_loss: 0.9712 - val_mae: 1.4450\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9350 - mae: 1.4071 - val_loss: 1.0113 - val_mae: 1.4896\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9315 - mae: 1.4035 - val_loss: 0.9930 - val_mae: 1.4576\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9249 - mae: 1.3954 - val_loss: 0.9639 - val_mae: 1.4184\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9276 - mae: 1.3973 - val_loss: 0.9616 - val_mae: 1.4219\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9242 - mae: 1.3918 - val_loss: 0.9878 - val_mae: 1.4555\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9248 - mae: 1.3950 - val_loss: 0.9756 - val_mae: 1.4313\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9308 - mae: 1.4027 - val_loss: 0.9461 - val_mae: 1.4015\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9216 - mae: 1.3927 - val_loss: 0.9448 - val_mae: 1.4090\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9287 - mae: 1.3946 - val_loss: 0.9393 - val_mae: 1.3980\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9329 - mae: 1.4049 - val_loss: 0.9477 - val_mae: 1.4172\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9285 - mae: 1.3999 - val_loss: 0.9255 - val_mae: 1.3735\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9275 - mae: 1.3925 - val_loss: 0.9678 - val_mae: 1.4403\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9280 - mae: 1.3946 - val_loss: 0.9752 - val_mae: 1.4331\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9370 - mae: 1.4060 - val_loss: 0.9674 - val_mae: 1.4237\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9124 - mae: 1.3799 - val_loss: 0.9770 - val_mae: 1.4331\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9165 - mae: 1.3890 - val_loss: 0.9593 - val_mae: 1.4190\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9482 - mae: 1.4220 - val_loss: 1.0015 - val_mae: 1.4701\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9115 - mae: 1.3807 - val_loss: 0.9410 - val_mae: 1.4040\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9408 - mae: 1.4072 - val_loss: 0.9176 - val_mae: 1.3811\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9260 - mae: 1.3947 - val_loss: 0.9402 - val_mae: 1.4104\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9184 - mae: 1.3922 - val_loss: 0.9325 - val_mae: 1.3963\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9163 - mae: 1.3882 - val_loss: 0.9445 - val_mae: 1.4142\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9183 - mae: 1.3902 - val_loss: 0.9554 - val_mae: 1.4208\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9294 - mae: 1.3954 - val_loss: 0.9620 - val_mae: 1.4237\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9334 - mae: 1.4054 - val_loss: 0.9373 - val_mae: 1.3879\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9165 - mae: 1.3822 - val_loss: 0.9583 - val_mae: 1.4207\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9285 - mae: 1.3992 - val_loss: 0.9315 - val_mae: 1.3883\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9275 - mae: 1.3960 - val_loss: 0.9084 - val_mae: 1.3588\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9215 - mae: 1.3875 - val_loss: 0.9366 - val_mae: 1.4083\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9250 - mae: 1.3955 - val_loss: 0.9449 - val_mae: 1.4069\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9194 - mae: 1.3864 - val_loss: 0.9281 - val_mae: 1.3899\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9168 - mae: 1.3830 - val_loss: 0.9247 - val_mae: 1.3839\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9116 - mae: 1.3801 - val_loss: 0.9244 - val_mae: 1.3730\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9170 - mae: 1.3833 - val_loss: 0.9351 - val_mae: 1.3897\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9199 - mae: 1.3884 - val_loss: 0.9348 - val_mae: 1.3866\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9026 - mae: 1.3654 - val_loss: 0.9195 - val_mae: 1.3747\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9037 - mae: 1.3712 - val_loss: 0.8991 - val_mae: 1.3519\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8982 - mae: 1.3631 - val_loss: 0.9318 - val_mae: 1.3847\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9185 - mae: 1.3877 - val_loss: 0.9470 - val_mae: 1.4072\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9197 - mae: 1.3887 - val_loss: 0.9419 - val_mae: 1.4015\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9095 - mae: 1.3762 - val_loss: 0.9184 - val_mae: 1.3805\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9240 - mae: 1.3909 - val_loss: 0.9641 - val_mae: 1.4239\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9150 - mae: 1.3801 - val_loss: 0.9180 - val_mae: 1.3675\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9023 - mae: 1.3700 - val_loss: 0.9398 - val_mae: 1.4028\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9131 - mae: 1.3819 - val_loss: 0.9207 - val_mae: 1.3768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:41:26,411] Trial 24 finished with value: 0.9206854104995728 and parameters: {'dropout_2': 0.2687344645398298, 'dropout_3': 0.38387040982896936, 'dropout_4': 0.5148907706612649, 'dropout_5': 0.5340868576430888, 'learning_rate': 0.0018832952225169325, 'epochs': 200, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 3s 7ms/step - loss: 1.4934 - mae: 2.0471 - val_loss: 1.2495 - val_mae: 1.7236\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2147 - mae: 1.7291 - val_loss: 1.2332 - val_mae: 1.7237\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1359 - mae: 1.6313 - val_loss: 1.1439 - val_mae: 1.6200\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1039 - mae: 1.5927 - val_loss: 1.1073 - val_mae: 1.5902\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0969 - mae: 1.5821 - val_loss: 1.0847 - val_mae: 1.5535\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0862 - mae: 1.5767 - val_loss: 1.1485 - val_mae: 1.6340\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0853 - mae: 1.5757 - val_loss: 1.0889 - val_mae: 1.5777\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0682 - mae: 1.5527 - val_loss: 1.1154 - val_mae: 1.6098\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0749 - mae: 1.5631 - val_loss: 1.0784 - val_mae: 1.5487\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0524 - mae: 1.5385 - val_loss: 1.0855 - val_mae: 1.5656\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0573 - mae: 1.5455 - val_loss: 1.0724 - val_mae: 1.5631\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 1s 11ms/step - loss: 1.0641 - mae: 1.5540 - val_loss: 1.0905 - val_mae: 1.5841\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0680 - mae: 1.5583 - val_loss: 1.0672 - val_mae: 1.5461\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0467 - mae: 1.5374 - val_loss: 1.0595 - val_mae: 1.5532\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0422 - mae: 1.5327 - val_loss: 1.0912 - val_mae: 1.5865\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0531 - mae: 1.5364 - val_loss: 1.0624 - val_mae: 1.5631\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0524 - mae: 1.5352 - val_loss: 1.0649 - val_mae: 1.5569\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0394 - mae: 1.5286 - val_loss: 1.0695 - val_mae: 1.5614\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0362 - mae: 1.5245 - val_loss: 1.0612 - val_mae: 1.5585\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0215 - mae: 1.5067 - val_loss: 1.0872 - val_mae: 1.5881\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0316 - mae: 1.5193 - val_loss: 1.0521 - val_mae: 1.5521\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0363 - mae: 1.5239 - val_loss: 1.0437 - val_mae: 1.5334\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0181 - mae: 1.5041 - val_loss: 1.0694 - val_mae: 1.5729\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0356 - mae: 1.5261 - val_loss: 1.0352 - val_mae: 1.5378\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0268 - mae: 1.5128 - val_loss: 1.0627 - val_mae: 1.5536\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0088 - mae: 1.4950 - val_loss: 1.0487 - val_mae: 1.5282\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0250 - mae: 1.5108 - val_loss: 1.0692 - val_mae: 1.5721\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0289 - mae: 1.5189 - val_loss: 1.0612 - val_mae: 1.5424\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0196 - mae: 1.5067 - val_loss: 1.0456 - val_mae: 1.5217\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0099 - mae: 1.4900 - val_loss: 1.0590 - val_mae: 1.5509\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0151 - mae: 1.4978 - val_loss: 1.0342 - val_mae: 1.5286\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0061 - mae: 1.4907 - val_loss: 1.0504 - val_mae: 1.5338\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0177 - mae: 1.5024 - val_loss: 1.0614 - val_mae: 1.5487\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0081 - mae: 1.4871 - val_loss: 1.0695 - val_mae: 1.5694\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9995 - mae: 1.4810 - val_loss: 1.0654 - val_mae: 1.5623\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9973 - mae: 1.4756 - val_loss: 1.0947 - val_mae: 1.5860\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0117 - mae: 1.4954 - val_loss: 1.0092 - val_mae: 1.4887\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0068 - mae: 1.4920 - val_loss: 1.0752 - val_mae: 1.5674\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9849 - mae: 1.4692 - val_loss: 1.0571 - val_mae: 1.5561\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0001 - mae: 1.4822 - val_loss: 1.0283 - val_mae: 1.5063\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9975 - mae: 1.4782 - val_loss: 1.0390 - val_mae: 1.5107\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9901 - mae: 1.4694 - val_loss: 1.0776 - val_mae: 1.5652\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9881 - mae: 1.4700 - val_loss: 1.0365 - val_mae: 1.5106\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9759 - mae: 1.4524 - val_loss: 1.1205 - val_mae: 1.6011\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9948 - mae: 1.4755 - val_loss: 1.0605 - val_mae: 1.5462\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9802 - mae: 1.4602 - val_loss: 1.0623 - val_mae: 1.5602\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9907 - mae: 1.4723 - val_loss: 1.0510 - val_mae: 1.5332\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9829 - mae: 1.4594 - val_loss: 1.0390 - val_mae: 1.5242\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9834 - mae: 1.4635 - val_loss: 1.0660 - val_mae: 1.5392\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9893 - mae: 1.4715 - val_loss: 1.0539 - val_mae: 1.5479\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9808 - mae: 1.4588 - val_loss: 1.0700 - val_mae: 1.5734\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9908 - mae: 1.4712 - val_loss: 1.0507 - val_mae: 1.5450\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9695 - mae: 1.4471 - val_loss: 1.0754 - val_mae: 1.5700\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9829 - mae: 1.4665 - val_loss: 1.0834 - val_mae: 1.5713\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9878 - mae: 1.4678 - val_loss: 1.0573 - val_mae: 1.5479\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9773 - mae: 1.4549 - val_loss: 1.0419 - val_mae: 1.5323\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9675 - mae: 1.4457 - val_loss: 1.0227 - val_mae: 1.5040\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9706 - mae: 1.4472 - val_loss: 1.0586 - val_mae: 1.5460\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9657 - mae: 1.4441 - val_loss: 1.0441 - val_mae: 1.5403\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9750 - mae: 1.4546 - val_loss: 1.0420 - val_mae: 1.5360\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9755 - mae: 1.4524 - val_loss: 1.0569 - val_mae: 1.5594\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9702 - mae: 1.4454 - val_loss: 1.0482 - val_mae: 1.5475\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9769 - mae: 1.4564 - val_loss: 1.0510 - val_mae: 1.5591\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9547 - mae: 1.4339 - val_loss: 0.9942 - val_mae: 1.4837\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9645 - mae: 1.4425 - val_loss: 1.0514 - val_mae: 1.5446\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9648 - mae: 1.4401 - val_loss: 1.0226 - val_mae: 1.5178\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9571 - mae: 1.4380 - val_loss: 1.0367 - val_mae: 1.5308\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9713 - mae: 1.4484 - val_loss: 1.0580 - val_mae: 1.5561\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9784 - mae: 1.4568 - val_loss: 1.0193 - val_mae: 1.5185\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9761 - mae: 1.4509 - val_loss: 1.0281 - val_mae: 1.5216\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9543 - mae: 1.4319 - val_loss: 1.0391 - val_mae: 1.5331\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9691 - mae: 1.4516 - val_loss: 1.0626 - val_mae: 1.5561\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9679 - mae: 1.4482 - val_loss: 1.0313 - val_mae: 1.5272\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9692 - mae: 1.4503 - val_loss: 1.0577 - val_mae: 1.5450\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9853 - mae: 1.4680 - val_loss: 0.9961 - val_mae: 1.4840\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9644 - mae: 1.4428 - val_loss: 1.0306 - val_mae: 1.5163\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9755 - mae: 1.4555 - val_loss: 1.0423 - val_mae: 1.5204\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9826 - mae: 1.4625 - val_loss: 1.0097 - val_mae: 1.4936\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9577 - mae: 1.4301 - val_loss: 0.9962 - val_mae: 1.4781\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9587 - mae: 1.4351 - val_loss: 1.0429 - val_mae: 1.5339\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9541 - mae: 1.4320 - val_loss: 1.0002 - val_mae: 1.4729\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9488 - mae: 1.4212 - val_loss: 0.9616 - val_mae: 1.4348\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9652 - mae: 1.4436 - val_loss: 0.9809 - val_mae: 1.4663\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9615 - mae: 1.4418 - val_loss: 1.0106 - val_mae: 1.5014\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9716 - mae: 1.4495 - val_loss: 1.0323 - val_mae: 1.5218\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9565 - mae: 1.4308 - val_loss: 1.0290 - val_mae: 1.5114\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9596 - mae: 1.4346 - val_loss: 1.0561 - val_mae: 1.5473\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9636 - mae: 1.4440 - val_loss: 1.0157 - val_mae: 1.5022\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9726 - mae: 1.4482 - val_loss: 0.9844 - val_mae: 1.4683\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9638 - mae: 1.4413 - val_loss: 0.9675 - val_mae: 1.4496\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9696 - mae: 1.4482 - val_loss: 1.0101 - val_mae: 1.4881\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9645 - mae: 1.4399 - val_loss: 0.9814 - val_mae: 1.4637\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9804 - mae: 1.4598 - val_loss: 1.0130 - val_mae: 1.4919\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9733 - mae: 1.4524 - val_loss: 1.0166 - val_mae: 1.5052\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9511 - mae: 1.4310 - val_loss: 1.0032 - val_mae: 1.4862\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9585 - mae: 1.4349 - val_loss: 1.0037 - val_mae: 1.4880\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9663 - mae: 1.4433 - val_loss: 1.0122 - val_mae: 1.4977\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9617 - mae: 1.4360 - val_loss: 0.9635 - val_mae: 1.4405\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9595 - mae: 1.4393 - val_loss: 0.9763 - val_mae: 1.4434\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9578 - mae: 1.4326 - val_loss: 1.0213 - val_mae: 1.5104\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9534 - mae: 1.4306 - val_loss: 0.9999 - val_mae: 1.4834\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9621 - mae: 1.4401 - val_loss: 0.9812 - val_mae: 1.4625\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9522 - mae: 1.4257 - val_loss: 0.9876 - val_mae: 1.4767\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9551 - mae: 1.4337 - val_loss: 0.9875 - val_mae: 1.4831\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9591 - mae: 1.4379 - val_loss: 0.9983 - val_mae: 1.4850\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9509 - mae: 1.4275 - val_loss: 0.9938 - val_mae: 1.4804\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9623 - mae: 1.4387 - val_loss: 0.9714 - val_mae: 1.4408\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9656 - mae: 1.4436 - val_loss: 0.9701 - val_mae: 1.4461\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9702 - mae: 1.4445 - val_loss: 0.9653 - val_mae: 1.4399\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9319 - mae: 1.4075 - val_loss: 0.9825 - val_mae: 1.4663\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9435 - mae: 1.4158 - val_loss: 0.9749 - val_mae: 1.4536\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9574 - mae: 1.4377 - val_loss: 0.9994 - val_mae: 1.4743\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9330 - mae: 1.4072 - val_loss: 0.9478 - val_mae: 1.4134\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9480 - mae: 1.4262 - val_loss: 0.9658 - val_mae: 1.4428\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9594 - mae: 1.4379 - val_loss: 0.9430 - val_mae: 1.4187\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9594 - mae: 1.4372 - val_loss: 0.9780 - val_mae: 1.4542\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9459 - mae: 1.4197 - val_loss: 0.9954 - val_mae: 1.4649\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9564 - mae: 1.4306 - val_loss: 1.0103 - val_mae: 1.4919\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9536 - mae: 1.4257 - val_loss: 0.9962 - val_mae: 1.4753\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9488 - mae: 1.4205 - val_loss: 0.9880 - val_mae: 1.4789\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9447 - mae: 1.4195 - val_loss: 0.9503 - val_mae: 1.4367\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9731 - mae: 1.4520 - val_loss: 0.9642 - val_mae: 1.4503\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9533 - mae: 1.4301 - val_loss: 1.0001 - val_mae: 1.4791\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9436 - mae: 1.4167 - val_loss: 1.0056 - val_mae: 1.4983\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9482 - mae: 1.4219 - val_loss: 0.9591 - val_mae: 1.4400\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9505 - mae: 1.4293 - val_loss: 0.9377 - val_mae: 1.4072\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9368 - mae: 1.4095 - val_loss: 0.9329 - val_mae: 1.4115\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9592 - mae: 1.4344 - val_loss: 0.9625 - val_mae: 1.4529\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9408 - mae: 1.4171 - val_loss: 0.9544 - val_mae: 1.4384\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9440 - mae: 1.4186 - val_loss: 0.9506 - val_mae: 1.4354\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9560 - mae: 1.4341 - val_loss: 1.0406 - val_mae: 1.5436\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9453 - mae: 1.4194 - val_loss: 0.9736 - val_mae: 1.4504\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9496 - mae: 1.4283 - val_loss: 0.9692 - val_mae: 1.4517\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9284 - mae: 1.4010 - val_loss: 0.9309 - val_mae: 1.4150\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9542 - mae: 1.4301 - val_loss: 0.9564 - val_mae: 1.4313\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9321 - mae: 1.4059 - val_loss: 0.9319 - val_mae: 1.4113\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9532 - mae: 1.4278 - val_loss: 0.9314 - val_mae: 1.4083\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9504 - mae: 1.4305 - val_loss: 0.9281 - val_mae: 1.4068\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9371 - mae: 1.4106 - val_loss: 0.9529 - val_mae: 1.4339\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9340 - mae: 1.4112 - val_loss: 0.9339 - val_mae: 1.4144\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9141 - mae: 1.3850 - val_loss: 0.9232 - val_mae: 1.3958\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9389 - mae: 1.4138 - val_loss: 0.9233 - val_mae: 1.3936\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9389 - mae: 1.4125 - val_loss: 0.9524 - val_mae: 1.4204\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9400 - mae: 1.4118 - val_loss: 0.9052 - val_mae: 1.3736\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9499 - mae: 1.4256 - val_loss: 0.9128 - val_mae: 1.3812\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9364 - mae: 1.4099 - val_loss: 0.9471 - val_mae: 1.4165\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9374 - mae: 1.4121 - val_loss: 0.9295 - val_mae: 1.4061\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9326 - mae: 1.4008 - val_loss: 0.9467 - val_mae: 1.4173\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9324 - mae: 1.4049 - val_loss: 0.9688 - val_mae: 1.4429\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9393 - mae: 1.4175 - val_loss: 0.9728 - val_mae: 1.4363\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9391 - mae: 1.4146 - val_loss: 0.9300 - val_mae: 1.3964\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9527 - mae: 1.4308 - val_loss: 0.9570 - val_mae: 1.4274\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9256 - mae: 1.3979 - val_loss: 0.9307 - val_mae: 1.3961\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9481 - mae: 1.4185 - val_loss: 0.9418 - val_mae: 1.4024\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9418 - mae: 1.4133 - val_loss: 0.9119 - val_mae: 1.3844\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9490 - mae: 1.4229 - val_loss: 0.9386 - val_mae: 1.4202\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9348 - mae: 1.4070 - val_loss: 1.0010 - val_mae: 1.4831\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9367 - mae: 1.4089 - val_loss: 0.9979 - val_mae: 1.4710\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9320 - mae: 1.4054 - val_loss: 0.9786 - val_mae: 1.4532\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9403 - mae: 1.4128 - val_loss: 0.9282 - val_mae: 1.3915\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9487 - mae: 1.4237 - val_loss: 0.9549 - val_mae: 1.4170\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9208 - mae: 1.3906 - val_loss: 0.9716 - val_mae: 1.4412\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9243 - mae: 1.3960 - val_loss: 0.9784 - val_mae: 1.4556\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9420 - mae: 1.4139 - val_loss: 1.0212 - val_mae: 1.4994\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9373 - mae: 1.4132 - val_loss: 0.9617 - val_mae: 1.4412\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9362 - mae: 1.4132 - val_loss: 0.9672 - val_mae: 1.4408\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9262 - mae: 1.3977 - val_loss: 0.9417 - val_mae: 1.4199\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9322 - mae: 1.4087 - val_loss: 0.9481 - val_mae: 1.4254\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9331 - mae: 1.4084 - val_loss: 0.9585 - val_mae: 1.4152\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9252 - mae: 1.4019 - val_loss: 0.9476 - val_mae: 1.4126\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9317 - mae: 1.4016 - val_loss: 0.9923 - val_mae: 1.4630\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9184 - mae: 1.3847 - val_loss: 0.9756 - val_mae: 1.4439\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9254 - mae: 1.3994 - val_loss: 0.9694 - val_mae: 1.4398\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9282 - mae: 1.4022 - val_loss: 0.9584 - val_mae: 1.4326\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9339 - mae: 1.4098 - val_loss: 0.9364 - val_mae: 1.4097\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9224 - mae: 1.3944 - val_loss: 0.9328 - val_mae: 1.3953\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9163 - mae: 1.3882 - val_loss: 1.0377 - val_mae: 1.5193\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9453 - mae: 1.4182 - val_loss: 0.9402 - val_mae: 1.4191\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9268 - mae: 1.4046 - val_loss: 0.9556 - val_mae: 1.4250\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9276 - mae: 1.4007 - val_loss: 0.9503 - val_mae: 1.4166\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9138 - mae: 1.3852 - val_loss: 0.9388 - val_mae: 1.4077\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9117 - mae: 1.3815 - val_loss: 0.9270 - val_mae: 1.3918\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9256 - mae: 1.3940 - val_loss: 0.9105 - val_mae: 1.3662\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9243 - mae: 1.3978 - val_loss: 0.9699 - val_mae: 1.4401\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9227 - mae: 1.3954 - val_loss: 0.9342 - val_mae: 1.3966\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9280 - mae: 1.4023 - val_loss: 0.9540 - val_mae: 1.4237\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9207 - mae: 1.3925 - val_loss: 0.9301 - val_mae: 1.3939\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9363 - mae: 1.4076 - val_loss: 0.9524 - val_mae: 1.4192\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9125 - mae: 1.3808 - val_loss: 0.9227 - val_mae: 1.3824\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9150 - mae: 1.3861 - val_loss: 0.9865 - val_mae: 1.4554\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9221 - mae: 1.3957 - val_loss: 0.9864 - val_mae: 1.4574\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9206 - mae: 1.3877 - val_loss: 0.9941 - val_mae: 1.4610\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9213 - mae: 1.3899 - val_loss: 0.9463 - val_mae: 1.4125\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9078 - mae: 1.3814 - val_loss: 0.9783 - val_mae: 1.4536\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9080 - mae: 1.3796 - val_loss: 0.9504 - val_mae: 1.4072\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9321 - mae: 1.4032 - val_loss: 0.9441 - val_mae: 1.3984\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9254 - mae: 1.3942 - val_loss: 0.9355 - val_mae: 1.3947\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9280 - mae: 1.3992 - val_loss: 0.9970 - val_mae: 1.4543\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9005 - mae: 1.3655 - val_loss: 0.9205 - val_mae: 1.3799\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9119 - mae: 1.3816 - val_loss: 0.9294 - val_mae: 1.3879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:43:29,593] Trial 25 finished with value: 0.9294252395629883 and parameters: {'dropout_2': 0.2866729749749726, 'dropout_3': 0.3912132876191002, 'dropout_4': 0.28257920867563147, 'dropout_5': 0.5107834915239602, 'learning_rate': 0.0027375587397614576, 'epochs': 200, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 3s 7ms/step - loss: 1.5609 - mae: 2.1186 - val_loss: 1.3100 - val_mae: 1.7871\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3555 - mae: 1.8947 - val_loss: 1.2859 - val_mae: 1.7674\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2956 - mae: 1.8206 - val_loss: 1.2376 - val_mae: 1.7165\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2364 - mae: 1.7540 - val_loss: 1.1677 - val_mae: 1.6435\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2039 - mae: 1.7128 - val_loss: 1.1520 - val_mae: 1.6273\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1683 - mae: 1.6696 - val_loss: 1.1400 - val_mae: 1.6107\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1482 - mae: 1.6493 - val_loss: 1.1499 - val_mae: 1.6216\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1222 - mae: 1.6161 - val_loss: 1.1280 - val_mae: 1.5927\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1217 - mae: 1.6154 - val_loss: 1.1106 - val_mae: 1.5825\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1163 - mae: 1.6088 - val_loss: 1.1037 - val_mae: 1.5757\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1025 - mae: 1.5910 - val_loss: 1.1133 - val_mae: 1.5870\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0764 - mae: 1.5637 - val_loss: 1.1290 - val_mae: 1.6112\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0924 - mae: 1.5839 - val_loss: 1.1132 - val_mae: 1.5942\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0606 - mae: 1.5487 - val_loss: 1.0853 - val_mae: 1.5663\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0720 - mae: 1.5614 - val_loss: 1.0935 - val_mae: 1.5739\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0489 - mae: 1.5350 - val_loss: 1.0758 - val_mae: 1.5508\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0589 - mae: 1.5462 - val_loss: 1.0642 - val_mae: 1.5312\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0475 - mae: 1.5368 - val_loss: 1.0689 - val_mae: 1.5392\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0351 - mae: 1.5182 - val_loss: 1.0637 - val_mae: 1.5452\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0656 - mae: 1.5563 - val_loss: 1.0687 - val_mae: 1.5483\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0401 - mae: 1.5277 - val_loss: 1.0756 - val_mae: 1.5509\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0425 - mae: 1.5302 - val_loss: 1.0497 - val_mae: 1.5244\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0251 - mae: 1.5121 - val_loss: 1.0446 - val_mae: 1.5218\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0231 - mae: 1.5124 - val_loss: 1.0792 - val_mae: 1.5649\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0189 - mae: 1.5076 - val_loss: 1.0279 - val_mae: 1.5040\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0181 - mae: 1.5051 - val_loss: 1.0389 - val_mae: 1.5130\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0248 - mae: 1.5112 - val_loss: 1.0416 - val_mae: 1.5201\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0051 - mae: 1.4902 - val_loss: 1.0716 - val_mae: 1.5535\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0172 - mae: 1.5054 - val_loss: 1.0434 - val_mae: 1.5300\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0205 - mae: 1.5082 - val_loss: 1.0347 - val_mae: 1.5131\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9920 - mae: 1.4750 - val_loss: 1.0324 - val_mae: 1.5093\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0006 - mae: 1.4851 - val_loss: 1.0291 - val_mae: 1.5107\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0093 - mae: 1.4952 - val_loss: 1.0291 - val_mae: 1.5118\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0014 - mae: 1.4835 - val_loss: 1.0298 - val_mae: 1.5073\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9896 - mae: 1.4735 - val_loss: 1.0257 - val_mae: 1.5047\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0073 - mae: 1.4942 - val_loss: 1.0275 - val_mae: 1.5034\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9906 - mae: 1.4750 - val_loss: 1.0073 - val_mae: 1.4817\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0084 - mae: 1.4925 - val_loss: 1.0123 - val_mae: 1.4838\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9918 - mae: 1.4742 - val_loss: 1.0302 - val_mae: 1.4983\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9935 - mae: 1.4784 - val_loss: 1.0160 - val_mae: 1.4889\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9822 - mae: 1.4663 - val_loss: 1.0322 - val_mae: 1.5060\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9790 - mae: 1.4624 - val_loss: 1.0228 - val_mae: 1.5001\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9801 - mae: 1.4620 - val_loss: 1.0099 - val_mae: 1.4851\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9839 - mae: 1.4727 - val_loss: 0.9972 - val_mae: 1.4702\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9821 - mae: 1.4678 - val_loss: 1.0154 - val_mae: 1.4924\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9672 - mae: 1.4527 - val_loss: 0.9878 - val_mae: 1.4571\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9822 - mae: 1.4670 - val_loss: 1.0054 - val_mae: 1.4759\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9796 - mae: 1.4621 - val_loss: 1.0045 - val_mae: 1.4714\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9824 - mae: 1.4622 - val_loss: 1.0093 - val_mae: 1.4826\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9648 - mae: 1.4463 - val_loss: 1.0076 - val_mae: 1.4787\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9716 - mae: 1.4514 - val_loss: 0.9932 - val_mae: 1.4563\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9568 - mae: 1.4329 - val_loss: 0.9954 - val_mae: 1.4643\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9685 - mae: 1.4483 - val_loss: 1.0114 - val_mae: 1.4746\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9619 - mae: 1.4413 - val_loss: 0.9970 - val_mae: 1.4589\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9543 - mae: 1.4376 - val_loss: 0.9935 - val_mae: 1.4593\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9643 - mae: 1.4448 - val_loss: 0.9715 - val_mae: 1.4359\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9505 - mae: 1.4330 - val_loss: 1.0165 - val_mae: 1.4847\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9481 - mae: 1.4265 - val_loss: 0.9640 - val_mae: 1.4322\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9573 - mae: 1.4358 - val_loss: 0.9893 - val_mae: 1.4521\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9434 - mae: 1.4199 - val_loss: 1.0179 - val_mae: 1.4900\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9690 - mae: 1.4496 - val_loss: 0.9947 - val_mae: 1.4660\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9556 - mae: 1.4359 - val_loss: 0.9830 - val_mae: 1.4454\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9481 - mae: 1.4289 - val_loss: 0.9821 - val_mae: 1.4435\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9553 - mae: 1.4381 - val_loss: 0.9834 - val_mae: 1.4484\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9497 - mae: 1.4286 - val_loss: 0.9740 - val_mae: 1.4382\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9304 - mae: 1.4082 - val_loss: 0.9857 - val_mae: 1.4535\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9543 - mae: 1.4362 - val_loss: 0.9797 - val_mae: 1.4411\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9334 - mae: 1.4122 - val_loss: 1.0052 - val_mae: 1.4657\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9243 - mae: 1.4010 - val_loss: 0.9632 - val_mae: 1.4272\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9296 - mae: 1.4091 - val_loss: 0.9643 - val_mae: 1.4262\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9497 - mae: 1.4293 - val_loss: 0.9747 - val_mae: 1.4424\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9461 - mae: 1.4251 - val_loss: 0.9519 - val_mae: 1.4185\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9385 - mae: 1.4189 - val_loss: 0.9625 - val_mae: 1.4266\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9514 - mae: 1.4292 - val_loss: 0.9820 - val_mae: 1.4551\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9382 - mae: 1.4158 - val_loss: 0.9566 - val_mae: 1.4264\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9385 - mae: 1.4155 - val_loss: 0.9526 - val_mae: 1.4094\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9487 - mae: 1.4271 - val_loss: 0.9855 - val_mae: 1.4602\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9311 - mae: 1.4094 - val_loss: 0.9980 - val_mae: 1.4635\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9374 - mae: 1.4176 - val_loss: 0.9667 - val_mae: 1.4311\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9178 - mae: 1.3912 - val_loss: 0.9670 - val_mae: 1.4339\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9343 - mae: 1.4133 - val_loss: 0.9748 - val_mae: 1.4336\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9360 - mae: 1.4130 - val_loss: 0.9884 - val_mae: 1.4477\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9359 - mae: 1.4107 - val_loss: 0.9785 - val_mae: 1.4354\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9373 - mae: 1.4170 - val_loss: 0.9703 - val_mae: 1.4306\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9516 - mae: 1.4313 - val_loss: 0.9491 - val_mae: 1.4146\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9238 - mae: 1.3975 - val_loss: 0.9598 - val_mae: 1.4186\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9264 - mae: 1.4053 - val_loss: 0.9975 - val_mae: 1.4604\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9115 - mae: 1.3850 - val_loss: 0.9596 - val_mae: 1.4243\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9231 - mae: 1.3960 - val_loss: 0.9534 - val_mae: 1.4114\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9148 - mae: 1.3907 - val_loss: 0.9492 - val_mae: 1.4049\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9182 - mae: 1.3910 - val_loss: 0.9640 - val_mae: 1.4311\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9199 - mae: 1.3961 - val_loss: 0.9649 - val_mae: 1.4303\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9086 - mae: 1.3792 - val_loss: 0.9633 - val_mae: 1.4252\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9198 - mae: 1.3962 - val_loss: 0.9285 - val_mae: 1.3893\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9329 - mae: 1.4131 - val_loss: 0.9674 - val_mae: 1.4254\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9047 - mae: 1.3830 - val_loss: 0.9566 - val_mae: 1.4208\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9225 - mae: 1.3976 - val_loss: 0.9716 - val_mae: 1.4312\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9224 - mae: 1.3944 - val_loss: 0.9823 - val_mae: 1.4335\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9078 - mae: 1.3827 - val_loss: 0.9701 - val_mae: 1.4293\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9029 - mae: 1.3759 - val_loss: 0.9630 - val_mae: 1.4217\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9262 - mae: 1.4037 - val_loss: 0.9528 - val_mae: 1.4164\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9070 - mae: 1.3816 - val_loss: 0.9416 - val_mae: 1.4126\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9124 - mae: 1.3904 - val_loss: 0.9895 - val_mae: 1.4438\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9153 - mae: 1.3925 - val_loss: 1.0174 - val_mae: 1.4760\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9214 - mae: 1.4002 - val_loss: 0.9707 - val_mae: 1.4367\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8993 - mae: 1.3739 - val_loss: 0.9472 - val_mae: 1.4102\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9124 - mae: 1.3869 - val_loss: 0.9602 - val_mae: 1.4182\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8909 - mae: 1.3609 - val_loss: 1.0696 - val_mae: 1.5511\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9080 - mae: 1.3819 - val_loss: 0.9781 - val_mae: 1.4352\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9229 - mae: 1.3997 - val_loss: 0.9623 - val_mae: 1.4243\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9126 - mae: 1.3860 - val_loss: 0.9812 - val_mae: 1.4455\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8959 - mae: 1.3675 - val_loss: 0.9267 - val_mae: 1.3892\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8903 - mae: 1.3631 - val_loss: 0.9466 - val_mae: 1.4152\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9088 - mae: 1.3844 - val_loss: 0.9608 - val_mae: 1.4285\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9083 - mae: 1.3826 - val_loss: 0.9450 - val_mae: 1.4115\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8866 - mae: 1.3565 - val_loss: 0.9450 - val_mae: 1.4089\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9014 - mae: 1.3726 - val_loss: 0.9792 - val_mae: 1.4425\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9132 - mae: 1.3879 - val_loss: 0.9532 - val_mae: 1.4170\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8975 - mae: 1.3684 - val_loss: 0.9506 - val_mae: 1.4083\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9071 - mae: 1.3806 - val_loss: 0.9501 - val_mae: 1.4091\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8905 - mae: 1.3630 - val_loss: 0.9257 - val_mae: 1.3842\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8874 - mae: 1.3590 - val_loss: 0.9396 - val_mae: 1.3974\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9007 - mae: 1.3751 - val_loss: 0.9291 - val_mae: 1.3819\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9065 - mae: 1.3763 - val_loss: 0.9439 - val_mae: 1.3995\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9010 - mae: 1.3716 - val_loss: 0.9398 - val_mae: 1.4003\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9182 - mae: 1.3966 - val_loss: 0.9552 - val_mae: 1.4107\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9149 - mae: 1.3863 - val_loss: 0.9396 - val_mae: 1.4047\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9079 - mae: 1.3786 - val_loss: 0.9562 - val_mae: 1.4056\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8937 - mae: 1.3657 - val_loss: 0.9059 - val_mae: 1.3596\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8905 - mae: 1.3589 - val_loss: 0.9087 - val_mae: 1.3660\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8794 - mae: 1.3481 - val_loss: 0.9372 - val_mae: 1.3990\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9096 - mae: 1.3858 - val_loss: 0.9465 - val_mae: 1.4044\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8992 - mae: 1.3697 - val_loss: 0.9292 - val_mae: 1.3837\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8985 - mae: 1.3729 - val_loss: 0.9632 - val_mae: 1.4309\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9066 - mae: 1.3800 - val_loss: 0.9532 - val_mae: 1.4155\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8988 - mae: 1.3731 - val_loss: 0.9373 - val_mae: 1.3962\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8930 - mae: 1.3672 - val_loss: 0.9177 - val_mae: 1.3794\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8929 - mae: 1.3656 - val_loss: 0.9379 - val_mae: 1.3941\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8857 - mae: 1.3573 - val_loss: 0.9974 - val_mae: 1.4611\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8766 - mae: 1.3422 - val_loss: 0.9558 - val_mae: 1.4114\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8838 - mae: 1.3526 - val_loss: 0.9415 - val_mae: 1.3949\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8879 - mae: 1.3595 - val_loss: 0.9343 - val_mae: 1.3938\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8891 - mae: 1.3606 - val_loss: 0.9291 - val_mae: 1.3896\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8880 - mae: 1.3590 - val_loss: 0.9635 - val_mae: 1.4234\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8757 - mae: 1.3461 - val_loss: 0.9544 - val_mae: 1.4116\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8797 - mae: 1.3507 - val_loss: 0.9537 - val_mae: 1.4069\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8848 - mae: 1.3545 - val_loss: 0.9350 - val_mae: 1.3941\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9029 - mae: 1.3748 - val_loss: 0.9283 - val_mae: 1.3845\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8864 - mae: 1.3583 - val_loss: 0.9305 - val_mae: 1.3923\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8797 - mae: 1.3491 - val_loss: 0.9253 - val_mae: 1.3822\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8713 - mae: 1.3399 - val_loss: 0.9269 - val_mae: 1.3836\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8776 - mae: 1.3473 - val_loss: 0.9071 - val_mae: 1.3623\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8662 - mae: 1.3346 - val_loss: 0.9046 - val_mae: 1.3570\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8965 - mae: 1.3656 - val_loss: 0.9159 - val_mae: 1.3703\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8949 - mae: 1.3632 - val_loss: 0.9270 - val_mae: 1.3804\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8547 - mae: 1.3231 - val_loss: 0.9178 - val_mae: 1.3722\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8835 - mae: 1.3517 - val_loss: 0.8880 - val_mae: 1.3386\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.8904 - mae: 1.3620 - val_loss: 0.9428 - val_mae: 1.3956\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8893 - mae: 1.3634 - val_loss: 0.9228 - val_mae: 1.3847\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8702 - mae: 1.3397 - val_loss: 0.9498 - val_mae: 1.4059\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8831 - mae: 1.3526 - val_loss: 0.9208 - val_mae: 1.3712\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8934 - mae: 1.3632 - val_loss: 0.9228 - val_mae: 1.3750\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8764 - mae: 1.3481 - val_loss: 0.9233 - val_mae: 1.3820\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8727 - mae: 1.3397 - val_loss: 0.9180 - val_mae: 1.3755\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8580 - mae: 1.3292 - val_loss: 0.9461 - val_mae: 1.4025\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8603 - mae: 1.3280 - val_loss: 0.9481 - val_mae: 1.4086\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8940 - mae: 1.3666 - val_loss: 0.9254 - val_mae: 1.3796\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8867 - mae: 1.3537 - val_loss: 0.9380 - val_mae: 1.3932\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8763 - mae: 1.3469 - val_loss: 0.9139 - val_mae: 1.3736\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8615 - mae: 1.3264 - val_loss: 0.8993 - val_mae: 1.3562\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8638 - mae: 1.3346 - val_loss: 0.9037 - val_mae: 1.3598\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8841 - mae: 1.3545 - val_loss: 0.9266 - val_mae: 1.3802\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8666 - mae: 1.3327 - val_loss: 0.9474 - val_mae: 1.4054\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8739 - mae: 1.3435 - val_loss: 0.9125 - val_mae: 1.3677\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8793 - mae: 1.3483 - val_loss: 0.9311 - val_mae: 1.3819\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8865 - mae: 1.3588 - val_loss: 0.9219 - val_mae: 1.3791\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8789 - mae: 1.3480 - val_loss: 0.9474 - val_mae: 1.3993\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8740 - mae: 1.3393 - val_loss: 0.9165 - val_mae: 1.3715\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8752 - mae: 1.3444 - val_loss: 0.9251 - val_mae: 1.3761\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8689 - mae: 1.3389 - val_loss: 0.9297 - val_mae: 1.3835\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8732 - mae: 1.3409 - val_loss: 0.9546 - val_mae: 1.4108\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8692 - mae: 1.3357 - val_loss: 0.9300 - val_mae: 1.3816\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8647 - mae: 1.3335 - val_loss: 0.9503 - val_mae: 1.4133\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8665 - mae: 1.3323 - val_loss: 0.9281 - val_mae: 1.3840\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8635 - mae: 1.3293 - val_loss: 0.9579 - val_mae: 1.4169\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8538 - mae: 1.3183 - val_loss: 0.9496 - val_mae: 1.4017\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8694 - mae: 1.3354 - val_loss: 0.9121 - val_mae: 1.3671\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.8550 - mae: 1.3238 - val_loss: 0.9235 - val_mae: 1.3818\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8661 - mae: 1.3332 - val_loss: 0.9296 - val_mae: 1.3851\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8604 - mae: 1.3290 - val_loss: 0.9245 - val_mae: 1.3836\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8581 - mae: 1.3242 - val_loss: 0.9278 - val_mae: 1.3788\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8806 - mae: 1.3485 - val_loss: 0.9209 - val_mae: 1.3726\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8855 - mae: 1.3503 - val_loss: 0.9207 - val_mae: 1.3754\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8638 - mae: 1.3303 - val_loss: 0.9052 - val_mae: 1.3559\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8716 - mae: 1.3391 - val_loss: 0.9069 - val_mae: 1.3601\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8733 - mae: 1.3392 - val_loss: 0.9073 - val_mae: 1.3656\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8498 - mae: 1.3163 - val_loss: 0.9067 - val_mae: 1.3581\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8576 - mae: 1.3204 - val_loss: 0.9210 - val_mae: 1.3724\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8516 - mae: 1.3157 - val_loss: 0.9220 - val_mae: 1.3685\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8639 - mae: 1.3293 - val_loss: 0.9219 - val_mae: 1.3677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:45:27,180] Trial 26 finished with value: 0.921909511089325 and parameters: {'dropout_2': 0.27103161977792256, 'dropout_3': 0.5054327135575557, 'dropout_4': 0.4287917940087913, 'dropout_5': 0.39756104237867806, 'learning_rate': 0.0007826495273393185, 'epochs': 200, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "108/108 [==============================] - 3s 7ms/step - loss: 1.2390 - mae: 1.7395 - val_loss: 1.2444 - val_mae: 1.7682\n",
            "Epoch 2/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1562 - mae: 1.6423 - val_loss: 1.1799 - val_mae: 1.6312\n",
            "Epoch 3/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1491 - mae: 1.6324 - val_loss: 1.1455 - val_mae: 1.6401\n",
            "Epoch 4/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1498 - mae: 1.6376 - val_loss: 1.1803 - val_mae: 1.6728\n",
            "Epoch 5/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1295 - mae: 1.6139 - val_loss: 1.1714 - val_mae: 1.6308\n",
            "Epoch 6/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1464 - mae: 1.6271 - val_loss: 1.1957 - val_mae: 1.6760\n",
            "Epoch 7/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1371 - mae: 1.6185 - val_loss: 1.1943 - val_mae: 1.6764\n",
            "Epoch 8/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1202 - mae: 1.6040 - val_loss: 1.1289 - val_mae: 1.6065\n",
            "Epoch 9/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1285 - mae: 1.6167 - val_loss: 1.1585 - val_mae: 1.6532\n",
            "Epoch 10/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1177 - mae: 1.6056 - val_loss: 1.1762 - val_mae: 1.6574\n",
            "Epoch 11/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1146 - mae: 1.5995 - val_loss: 1.1248 - val_mae: 1.5953\n",
            "Epoch 12/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1245 - mae: 1.6087 - val_loss: 1.1209 - val_mae: 1.5893\n",
            "Epoch 13/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1257 - mae: 1.6118 - val_loss: 1.1623 - val_mae: 1.6833\n",
            "Epoch 14/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1097 - mae: 1.5961 - val_loss: 1.1567 - val_mae: 1.6320\n",
            "Epoch 15/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1138 - mae: 1.6004 - val_loss: 1.1493 - val_mae: 1.6673\n",
            "Epoch 16/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1082 - mae: 1.5951 - val_loss: 1.1882 - val_mae: 1.6411\n",
            "Epoch 17/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1089 - mae: 1.5943 - val_loss: 1.1299 - val_mae: 1.6042\n",
            "Epoch 18/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0991 - mae: 1.5839 - val_loss: 1.0927 - val_mae: 1.5676\n",
            "Epoch 19/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1046 - mae: 1.5880 - val_loss: 1.1457 - val_mae: 1.6380\n",
            "Epoch 20/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1049 - mae: 1.5940 - val_loss: 1.1602 - val_mae: 1.6791\n",
            "Epoch 21/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1010 - mae: 1.5892 - val_loss: 1.1980 - val_mae: 1.6919\n",
            "Epoch 22/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0896 - mae: 1.5739 - val_loss: 1.2292 - val_mae: 1.7294\n",
            "Epoch 23/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1028 - mae: 1.5872 - val_loss: 1.1345 - val_mae: 1.6570\n",
            "Epoch 24/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1045 - mae: 1.5883 - val_loss: 1.1646 - val_mae: 1.6440\n",
            "Epoch 25/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1018 - mae: 1.5866 - val_loss: 1.1623 - val_mae: 1.6616\n",
            "Epoch 26/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0848 - mae: 1.5662 - val_loss: 1.1528 - val_mae: 1.6457\n",
            "Epoch 27/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0978 - mae: 1.5843 - val_loss: 1.1967 - val_mae: 1.7114\n",
            "Epoch 28/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1050 - mae: 1.5908 - val_loss: 1.0838 - val_mae: 1.5737\n",
            "Epoch 29/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0822 - mae: 1.5680 - val_loss: 1.1519 - val_mae: 1.6376\n",
            "Epoch 30/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1116 - mae: 1.6017 - val_loss: 1.2097 - val_mae: 1.6641\n",
            "Epoch 31/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0807 - mae: 1.5665 - val_loss: 1.1154 - val_mae: 1.5875\n",
            "Epoch 32/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1002 - mae: 1.5845 - val_loss: 1.2355 - val_mae: 1.7560\n",
            "Epoch 33/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0963 - mae: 1.5825 - val_loss: 1.1249 - val_mae: 1.6124\n",
            "Epoch 34/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0920 - mae: 1.5794 - val_loss: 1.1446 - val_mae: 1.6628\n",
            "Epoch 35/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0813 - mae: 1.5692 - val_loss: 1.1825 - val_mae: 1.6735\n",
            "Epoch 36/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0852 - mae: 1.5719 - val_loss: 1.1805 - val_mae: 1.6913\n",
            "Epoch 37/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0784 - mae: 1.5620 - val_loss: 1.1336 - val_mae: 1.5922\n",
            "Epoch 38/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0669 - mae: 1.5528 - val_loss: 1.1034 - val_mae: 1.5865\n",
            "Epoch 39/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0770 - mae: 1.5675 - val_loss: 1.1025 - val_mae: 1.5853\n",
            "Epoch 40/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0854 - mae: 1.5758 - val_loss: 1.1733 - val_mae: 1.6828\n",
            "Epoch 41/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0890 - mae: 1.5758 - val_loss: 1.1574 - val_mae: 1.6803\n",
            "Epoch 42/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0778 - mae: 1.5615 - val_loss: 1.1112 - val_mae: 1.5796\n",
            "Epoch 43/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0881 - mae: 1.5744 - val_loss: 1.1766 - val_mae: 1.6596\n",
            "Epoch 44/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0804 - mae: 1.5655 - val_loss: 1.1082 - val_mae: 1.6078\n",
            "Epoch 45/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0911 - mae: 1.5793 - val_loss: 1.1444 - val_mae: 1.5967\n",
            "Epoch 46/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0890 - mae: 1.5733 - val_loss: 1.1136 - val_mae: 1.6140\n",
            "Epoch 47/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0761 - mae: 1.5590 - val_loss: 1.1160 - val_mae: 1.5860\n",
            "Epoch 48/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0858 - mae: 1.5710 - val_loss: 1.1075 - val_mae: 1.5809\n",
            "Epoch 49/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0657 - mae: 1.5501 - val_loss: 1.1341 - val_mae: 1.6001\n",
            "Epoch 50/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0750 - mae: 1.5593 - val_loss: 1.1592 - val_mae: 1.6437\n",
            "Epoch 51/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0768 - mae: 1.5635 - val_loss: 1.1409 - val_mae: 1.6548\n",
            "Epoch 52/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0846 - mae: 1.5699 - val_loss: 1.1786 - val_mae: 1.6913\n",
            "Epoch 53/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0867 - mae: 1.5686 - val_loss: 1.1174 - val_mae: 1.6225\n",
            "Epoch 54/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0902 - mae: 1.5747 - val_loss: 1.1561 - val_mae: 1.6761\n",
            "Epoch 55/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0846 - mae: 1.5714 - val_loss: 1.0938 - val_mae: 1.5684\n",
            "Epoch 56/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0747 - mae: 1.5532 - val_loss: 1.1226 - val_mae: 1.5887\n",
            "Epoch 57/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0750 - mae: 1.5561 - val_loss: 1.1210 - val_mae: 1.6099\n",
            "Epoch 58/100\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0816 - mae: 1.5668 - val_loss: 1.0801 - val_mae: 1.6011\n",
            "Epoch 59/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0791 - mae: 1.5621 - val_loss: 1.1235 - val_mae: 1.6093\n",
            "Epoch 60/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0772 - mae: 1.5635 - val_loss: 1.1031 - val_mae: 1.5681\n",
            "Epoch 61/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0615 - mae: 1.5441 - val_loss: 1.0711 - val_mae: 1.5286\n",
            "Epoch 62/100\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0751 - mae: 1.5585 - val_loss: 1.1395 - val_mae: 1.6230\n",
            "Epoch 63/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0716 - mae: 1.5539 - val_loss: 1.0819 - val_mae: 1.5770\n",
            "Epoch 64/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0769 - mae: 1.5603 - val_loss: 1.0884 - val_mae: 1.5813\n",
            "Epoch 65/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0659 - mae: 1.5544 - val_loss: 1.1097 - val_mae: 1.5765\n",
            "Epoch 66/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0656 - mae: 1.5484 - val_loss: 1.1308 - val_mae: 1.6282\n",
            "Epoch 67/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0766 - mae: 1.5614 - val_loss: 1.1349 - val_mae: 1.6116\n",
            "Epoch 68/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0598 - mae: 1.5418 - val_loss: 1.0847 - val_mae: 1.5365\n",
            "Epoch 69/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0711 - mae: 1.5604 - val_loss: 1.1389 - val_mae: 1.6316\n",
            "Epoch 70/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0685 - mae: 1.5480 - val_loss: 1.0797 - val_mae: 1.5614\n",
            "Epoch 71/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0586 - mae: 1.5409 - val_loss: 1.0913 - val_mae: 1.5526\n",
            "Epoch 72/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0815 - mae: 1.5627 - val_loss: 1.1864 - val_mae: 1.6439\n",
            "Epoch 73/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0753 - mae: 1.5580 - val_loss: 1.0922 - val_mae: 1.5375\n",
            "Epoch 74/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0770 - mae: 1.5604 - val_loss: 1.1291 - val_mae: 1.5918\n",
            "Epoch 75/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0739 - mae: 1.5572 - val_loss: 1.0681 - val_mae: 1.5603\n",
            "Epoch 76/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0698 - mae: 1.5545 - val_loss: 1.0796 - val_mae: 1.5567\n",
            "Epoch 77/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0944 - mae: 1.5822 - val_loss: 1.1763 - val_mae: 1.6342\n",
            "Epoch 78/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0708 - mae: 1.5526 - val_loss: 1.1211 - val_mae: 1.5744\n",
            "Epoch 79/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0720 - mae: 1.5538 - val_loss: 1.0953 - val_mae: 1.5696\n",
            "Epoch 80/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0842 - mae: 1.5719 - val_loss: 1.1187 - val_mae: 1.6010\n",
            "Epoch 81/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0820 - mae: 1.5712 - val_loss: 1.1418 - val_mae: 1.5874\n",
            "Epoch 82/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0787 - mae: 1.5631 - val_loss: 1.1671 - val_mae: 1.6935\n",
            "Epoch 83/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0773 - mae: 1.5630 - val_loss: 1.1252 - val_mae: 1.6256\n",
            "Epoch 84/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0698 - mae: 1.5564 - val_loss: 1.1816 - val_mae: 1.6896\n",
            "Epoch 85/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0528 - mae: 1.5350 - val_loss: 1.0699 - val_mae: 1.5505\n",
            "Epoch 86/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0688 - mae: 1.5530 - val_loss: 1.1386 - val_mae: 1.6293\n",
            "Epoch 87/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0566 - mae: 1.5360 - val_loss: 1.1571 - val_mae: 1.6138\n",
            "Epoch 88/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0631 - mae: 1.5457 - val_loss: 1.1500 - val_mae: 1.6670\n",
            "Epoch 89/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0607 - mae: 1.5463 - val_loss: 1.1711 - val_mae: 1.6365\n",
            "Epoch 90/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0637 - mae: 1.5508 - val_loss: 1.0584 - val_mae: 1.5357\n",
            "Epoch 91/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0530 - mae: 1.5402 - val_loss: 1.1292 - val_mae: 1.6241\n",
            "Epoch 92/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0680 - mae: 1.5473 - val_loss: 1.1389 - val_mae: 1.6608\n",
            "Epoch 93/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0670 - mae: 1.5546 - val_loss: 1.1447 - val_mae: 1.6604\n",
            "Epoch 94/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0655 - mae: 1.5565 - val_loss: 1.0612 - val_mae: 1.5523\n",
            "Epoch 95/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0602 - mae: 1.5487 - val_loss: 1.1690 - val_mae: 1.6374\n",
            "Epoch 96/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0634 - mae: 1.5476 - val_loss: 1.0969 - val_mae: 1.5660\n",
            "Epoch 97/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0523 - mae: 1.5321 - val_loss: 1.0845 - val_mae: 1.5746\n",
            "Epoch 98/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0584 - mae: 1.5472 - val_loss: 1.1457 - val_mae: 1.6382\n",
            "Epoch 99/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0568 - mae: 1.5434 - val_loss: 1.1419 - val_mae: 1.6370\n",
            "Epoch 100/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0415 - mae: 1.5281 - val_loss: 1.0975 - val_mae: 1.5645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:46:25,912] Trial 27 finished with value: 1.0975167751312256 and parameters: {'dropout_2': 0.3976265083972391, 'dropout_3': 0.3304148638825642, 'dropout_4': 0.6027993881916452, 'dropout_5': 0.5359673700832308, 'learning_rate': 0.023617149502866093, 'epochs': 100, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "108/108 [==============================] - 2s 6ms/step - loss: 1.6847 - mae: 2.2521 - val_loss: 1.3271 - val_mae: 1.8247\n",
            "Epoch 2/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4637 - mae: 2.0046 - val_loss: 1.2878 - val_mae: 1.7465\n",
            "Epoch 3/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2827 - mae: 1.8010 - val_loss: 1.2298 - val_mae: 1.6984\n",
            "Epoch 4/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2152 - mae: 1.7172 - val_loss: 1.2052 - val_mae: 1.6897\n",
            "Epoch 5/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1924 - mae: 1.6840 - val_loss: 1.1619 - val_mae: 1.6424\n",
            "Epoch 6/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1594 - mae: 1.6500 - val_loss: 1.1495 - val_mae: 1.6283\n",
            "Epoch 7/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1503 - mae: 1.6367 - val_loss: 1.1329 - val_mae: 1.6110\n",
            "Epoch 8/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1578 - mae: 1.6464 - val_loss: 1.1413 - val_mae: 1.6277\n",
            "Epoch 9/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1235 - mae: 1.6117 - val_loss: 1.1385 - val_mae: 1.6386\n",
            "Epoch 10/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1193 - mae: 1.6082 - val_loss: 1.1480 - val_mae: 1.6370\n",
            "Epoch 11/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1196 - mae: 1.6061 - val_loss: 1.1407 - val_mae: 1.6358\n",
            "Epoch 12/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1209 - mae: 1.6110 - val_loss: 1.1115 - val_mae: 1.5934\n",
            "Epoch 13/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1231 - mae: 1.6124 - val_loss: 1.1259 - val_mae: 1.6116\n",
            "Epoch 14/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0988 - mae: 1.5888 - val_loss: 1.1018 - val_mae: 1.5879\n",
            "Epoch 15/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0962 - mae: 1.5874 - val_loss: 1.1384 - val_mae: 1.6418\n",
            "Epoch 16/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0899 - mae: 1.5818 - val_loss: 1.1315 - val_mae: 1.6332\n",
            "Epoch 17/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1002 - mae: 1.5874 - val_loss: 1.1236 - val_mae: 1.6236\n",
            "Epoch 18/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0952 - mae: 1.5881 - val_loss: 1.0747 - val_mae: 1.5611\n",
            "Epoch 19/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0851 - mae: 1.5730 - val_loss: 1.0865 - val_mae: 1.5714\n",
            "Epoch 20/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1001 - mae: 1.5945 - val_loss: 1.0893 - val_mae: 1.5742\n",
            "Epoch 21/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0712 - mae: 1.5585 - val_loss: 1.0835 - val_mae: 1.5692\n",
            "Epoch 22/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0843 - mae: 1.5745 - val_loss: 1.0757 - val_mae: 1.5623\n",
            "Epoch 23/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0782 - mae: 1.5668 - val_loss: 1.1164 - val_mae: 1.6145\n",
            "Epoch 24/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0871 - mae: 1.5822 - val_loss: 1.0845 - val_mae: 1.5820\n",
            "Epoch 25/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0718 - mae: 1.5624 - val_loss: 1.0575 - val_mae: 1.5489\n",
            "Epoch 26/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0602 - mae: 1.5550 - val_loss: 1.1162 - val_mae: 1.6187\n",
            "Epoch 27/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0580 - mae: 1.5532 - val_loss: 1.1068 - val_mae: 1.6043\n",
            "Epoch 28/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0709 - mae: 1.5610 - val_loss: 1.1069 - val_mae: 1.6095\n",
            "Epoch 29/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0482 - mae: 1.5404 - val_loss: 1.0953 - val_mae: 1.6004\n",
            "Epoch 30/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0723 - mae: 1.5663 - val_loss: 1.0788 - val_mae: 1.5801\n",
            "Epoch 31/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0533 - mae: 1.5418 - val_loss: 1.0380 - val_mae: 1.5287\n",
            "Epoch 32/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0511 - mae: 1.5398 - val_loss: 1.0649 - val_mae: 1.5549\n",
            "Epoch 33/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0491 - mae: 1.5404 - val_loss: 1.0600 - val_mae: 1.5452\n",
            "Epoch 34/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0501 - mae: 1.5421 - val_loss: 1.0329 - val_mae: 1.5181\n",
            "Epoch 35/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0477 - mae: 1.5363 - val_loss: 1.0556 - val_mae: 1.5468\n",
            "Epoch 36/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0455 - mae: 1.5369 - val_loss: 1.0332 - val_mae: 1.5198\n",
            "Epoch 37/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0372 - mae: 1.5230 - val_loss: 1.0865 - val_mae: 1.5836\n",
            "Epoch 38/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0408 - mae: 1.5291 - val_loss: 1.0251 - val_mae: 1.5100\n",
            "Epoch 39/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0442 - mae: 1.5319 - val_loss: 1.0443 - val_mae: 1.5388\n",
            "Epoch 40/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0238 - mae: 1.5081 - val_loss: 1.0605 - val_mae: 1.5605\n",
            "Epoch 41/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0471 - mae: 1.5389 - val_loss: 1.0486 - val_mae: 1.5410\n",
            "Epoch 42/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0411 - mae: 1.5312 - val_loss: 1.0728 - val_mae: 1.5686\n",
            "Epoch 43/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0438 - mae: 1.5308 - val_loss: 1.0657 - val_mae: 1.5574\n",
            "Epoch 44/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0218 - mae: 1.5047 - val_loss: 1.0265 - val_mae: 1.5166\n",
            "Epoch 45/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0350 - mae: 1.5234 - val_loss: 1.0485 - val_mae: 1.5324\n",
            "Epoch 46/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0283 - mae: 1.5108 - val_loss: 1.0452 - val_mae: 1.5307\n",
            "Epoch 47/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0292 - mae: 1.5158 - val_loss: 1.0828 - val_mae: 1.5821\n",
            "Epoch 48/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0185 - mae: 1.5071 - val_loss: 1.0561 - val_mae: 1.5392\n",
            "Epoch 49/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0199 - mae: 1.5039 - val_loss: 1.0548 - val_mae: 1.5476\n",
            "Epoch 50/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0111 - mae: 1.4931 - val_loss: 1.0317 - val_mae: 1.5231\n",
            "Epoch 51/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0069 - mae: 1.4905 - val_loss: 1.0350 - val_mae: 1.5238\n",
            "Epoch 52/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0178 - mae: 1.5015 - val_loss: 1.0534 - val_mae: 1.5429\n",
            "Epoch 53/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0342 - mae: 1.5194 - val_loss: 1.0543 - val_mae: 1.5466\n",
            "Epoch 54/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0188 - mae: 1.5020 - val_loss: 1.0660 - val_mae: 1.5506\n",
            "Epoch 55/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0213 - mae: 1.5072 - val_loss: 1.0703 - val_mae: 1.5676\n",
            "Epoch 56/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0106 - mae: 1.4929 - val_loss: 1.0396 - val_mae: 1.5255\n",
            "Epoch 57/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0031 - mae: 1.4857 - val_loss: 1.0659 - val_mae: 1.5580\n",
            "Epoch 58/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0067 - mae: 1.4920 - val_loss: 1.1093 - val_mae: 1.6030\n",
            "Epoch 59/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0165 - mae: 1.5013 - val_loss: 1.0530 - val_mae: 1.5353\n",
            "Epoch 60/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0032 - mae: 1.4880 - val_loss: 1.0335 - val_mae: 1.5086\n",
            "Epoch 61/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0036 - mae: 1.4854 - val_loss: 1.0479 - val_mae: 1.5277\n",
            "Epoch 62/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0050 - mae: 1.4897 - val_loss: 1.0238 - val_mae: 1.5043\n",
            "Epoch 63/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0159 - mae: 1.4987 - val_loss: 1.0177 - val_mae: 1.5038\n",
            "Epoch 64/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9999 - mae: 1.4819 - val_loss: 1.0313 - val_mae: 1.5095\n",
            "Epoch 65/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0021 - mae: 1.4818 - val_loss: 1.0375 - val_mae: 1.5217\n",
            "Epoch 66/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0046 - mae: 1.4877 - val_loss: 1.0299 - val_mae: 1.5098\n",
            "Epoch 67/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9872 - mae: 1.4671 - val_loss: 1.0449 - val_mae: 1.5369\n",
            "Epoch 68/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0033 - mae: 1.4855 - val_loss: 1.0435 - val_mae: 1.5271\n",
            "Epoch 69/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0143 - mae: 1.4955 - val_loss: 1.0254 - val_mae: 1.5091\n",
            "Epoch 70/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0040 - mae: 1.4829 - val_loss: 1.0360 - val_mae: 1.5277\n",
            "Epoch 71/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0053 - mae: 1.4892 - val_loss: 1.0402 - val_mae: 1.5271\n",
            "Epoch 72/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9826 - mae: 1.4600 - val_loss: 1.0436 - val_mae: 1.5354\n",
            "Epoch 73/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9761 - mae: 1.4561 - val_loss: 1.0492 - val_mae: 1.5489\n",
            "Epoch 74/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0158 - mae: 1.5038 - val_loss: 1.0563 - val_mae: 1.5555\n",
            "Epoch 75/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0041 - mae: 1.4877 - val_loss: 1.0226 - val_mae: 1.5143\n",
            "Epoch 76/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9916 - mae: 1.4722 - val_loss: 1.0462 - val_mae: 1.5328\n",
            "Epoch 77/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9977 - mae: 1.4785 - val_loss: 1.0486 - val_mae: 1.5389\n",
            "Epoch 78/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9944 - mae: 1.4746 - val_loss: 1.0301 - val_mae: 1.5060\n",
            "Epoch 79/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9811 - mae: 1.4578 - val_loss: 1.0245 - val_mae: 1.5071\n",
            "Epoch 80/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9845 - mae: 1.4631 - val_loss: 1.0550 - val_mae: 1.5471\n",
            "Epoch 81/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0019 - mae: 1.4828 - val_loss: 1.0587 - val_mae: 1.5568\n",
            "Epoch 82/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9994 - mae: 1.4781 - val_loss: 1.0455 - val_mae: 1.5262\n",
            "Epoch 83/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9992 - mae: 1.4771 - val_loss: 1.0233 - val_mae: 1.5010\n",
            "Epoch 84/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0046 - mae: 1.4866 - val_loss: 1.0410 - val_mae: 1.5288\n",
            "Epoch 85/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9950 - mae: 1.4746 - val_loss: 1.0332 - val_mae: 1.5185\n",
            "Epoch 86/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9900 - mae: 1.4710 - val_loss: 1.0308 - val_mae: 1.5136\n",
            "Epoch 87/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9570 - mae: 1.4328 - val_loss: 1.0157 - val_mae: 1.5020\n",
            "Epoch 88/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9804 - mae: 1.4609 - val_loss: 1.0172 - val_mae: 1.5019\n",
            "Epoch 89/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9896 - mae: 1.4735 - val_loss: 1.0485 - val_mae: 1.5427\n",
            "Epoch 90/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9863 - mae: 1.4661 - val_loss: 1.0543 - val_mae: 1.5352\n",
            "Epoch 91/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9697 - mae: 1.4491 - val_loss: 1.0146 - val_mae: 1.4971\n",
            "Epoch 92/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9855 - mae: 1.4667 - val_loss: 1.0335 - val_mae: 1.5187\n",
            "Epoch 93/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9866 - mae: 1.4668 - val_loss: 1.0593 - val_mae: 1.5501\n",
            "Epoch 94/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9717 - mae: 1.4510 - val_loss: 1.0400 - val_mae: 1.5183\n",
            "Epoch 95/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9855 - mae: 1.4636 - val_loss: 1.0268 - val_mae: 1.5016\n",
            "Epoch 96/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9681 - mae: 1.4458 - val_loss: 1.0339 - val_mae: 1.5143\n",
            "Epoch 97/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9806 - mae: 1.4619 - val_loss: 1.0171 - val_mae: 1.4877\n",
            "Epoch 98/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9792 - mae: 1.4570 - val_loss: 1.0155 - val_mae: 1.4989\n",
            "Epoch 99/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9674 - mae: 1.4453 - val_loss: 1.0346 - val_mae: 1.5137\n",
            "Epoch 100/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9716 - mae: 1.4493 - val_loss: 1.0067 - val_mae: 1.4753\n",
            "Epoch 101/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9753 - mae: 1.4553 - val_loss: 1.0257 - val_mae: 1.4984\n",
            "Epoch 102/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9758 - mae: 1.4540 - val_loss: 1.0038 - val_mae: 1.4768\n",
            "Epoch 103/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9727 - mae: 1.4510 - val_loss: 1.0149 - val_mae: 1.4941\n",
            "Epoch 104/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9694 - mae: 1.4457 - val_loss: 1.0434 - val_mae: 1.5214\n",
            "Epoch 105/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9715 - mae: 1.4484 - val_loss: 1.0377 - val_mae: 1.5142\n",
            "Epoch 106/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9687 - mae: 1.4457 - val_loss: 0.9997 - val_mae: 1.4638\n",
            "Epoch 107/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9663 - mae: 1.4414 - val_loss: 1.0272 - val_mae: 1.4995\n",
            "Epoch 108/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9706 - mae: 1.4461 - val_loss: 1.0176 - val_mae: 1.4922\n",
            "Epoch 109/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9739 - mae: 1.4494 - val_loss: 1.0136 - val_mae: 1.4916\n",
            "Epoch 110/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9803 - mae: 1.4620 - val_loss: 1.0093 - val_mae: 1.4760\n",
            "Epoch 111/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9739 - mae: 1.4516 - val_loss: 0.9925 - val_mae: 1.4701\n",
            "Epoch 112/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9578 - mae: 1.4377 - val_loss: 1.0109 - val_mae: 1.4856\n",
            "Epoch 113/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9589 - mae: 1.4379 - val_loss: 1.0288 - val_mae: 1.5091\n",
            "Epoch 114/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9674 - mae: 1.4414 - val_loss: 1.0191 - val_mae: 1.4947\n",
            "Epoch 115/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9810 - mae: 1.4622 - val_loss: 1.0245 - val_mae: 1.5020\n",
            "Epoch 116/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9739 - mae: 1.4495 - val_loss: 1.0162 - val_mae: 1.4908\n",
            "Epoch 117/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9618 - mae: 1.4416 - val_loss: 1.0001 - val_mae: 1.4663\n",
            "Epoch 118/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9559 - mae: 1.4322 - val_loss: 1.0040 - val_mae: 1.4738\n",
            "Epoch 119/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9691 - mae: 1.4450 - val_loss: 1.0189 - val_mae: 1.4854\n",
            "Epoch 120/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9547 - mae: 1.4313 - val_loss: 1.0089 - val_mae: 1.4791\n",
            "Epoch 121/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9619 - mae: 1.4379 - val_loss: 1.0190 - val_mae: 1.4954\n",
            "Epoch 122/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9776 - mae: 1.4574 - val_loss: 1.0481 - val_mae: 1.5275\n",
            "Epoch 123/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9450 - mae: 1.4208 - val_loss: 1.0137 - val_mae: 1.4911\n",
            "Epoch 124/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9698 - mae: 1.4486 - val_loss: 1.0232 - val_mae: 1.4969\n",
            "Epoch 125/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9632 - mae: 1.4401 - val_loss: 1.0070 - val_mae: 1.4740\n",
            "Epoch 126/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9673 - mae: 1.4439 - val_loss: 1.0301 - val_mae: 1.4990\n",
            "Epoch 127/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9475 - mae: 1.4213 - val_loss: 1.0203 - val_mae: 1.4946\n",
            "Epoch 128/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9598 - mae: 1.4357 - val_loss: 0.9965 - val_mae: 1.4652\n",
            "Epoch 129/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9456 - mae: 1.4235 - val_loss: 1.0064 - val_mae: 1.4748\n",
            "Epoch 130/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9641 - mae: 1.4361 - val_loss: 1.0423 - val_mae: 1.5080\n",
            "Epoch 131/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9466 - mae: 1.4182 - val_loss: 0.9867 - val_mae: 1.4587\n",
            "Epoch 132/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9562 - mae: 1.4308 - val_loss: 1.0094 - val_mae: 1.4861\n",
            "Epoch 133/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9507 - mae: 1.4203 - val_loss: 0.9986 - val_mae: 1.4685\n",
            "Epoch 134/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9510 - mae: 1.4253 - val_loss: 1.0151 - val_mae: 1.4847\n",
            "Epoch 135/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9527 - mae: 1.4277 - val_loss: 1.0222 - val_mae: 1.4976\n",
            "Epoch 136/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9580 - mae: 1.4339 - val_loss: 1.0173 - val_mae: 1.4934\n",
            "Epoch 137/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9470 - mae: 1.4207 - val_loss: 1.0016 - val_mae: 1.4694\n",
            "Epoch 138/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9355 - mae: 1.4068 - val_loss: 1.0590 - val_mae: 1.5446\n",
            "Epoch 139/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9583 - mae: 1.4349 - val_loss: 0.9914 - val_mae: 1.4626\n",
            "Epoch 140/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9442 - mae: 1.4156 - val_loss: 0.9853 - val_mae: 1.4562\n",
            "Epoch 141/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9422 - mae: 1.4180 - val_loss: 1.0102 - val_mae: 1.4874\n",
            "Epoch 142/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9692 - mae: 1.4459 - val_loss: 0.9881 - val_mae: 1.4595\n",
            "Epoch 143/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9564 - mae: 1.4307 - val_loss: 1.0105 - val_mae: 1.4897\n",
            "Epoch 144/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9551 - mae: 1.4272 - val_loss: 0.9910 - val_mae: 1.4575\n",
            "Epoch 145/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9469 - mae: 1.4212 - val_loss: 1.0159 - val_mae: 1.4943\n",
            "Epoch 146/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9445 - mae: 1.4200 - val_loss: 1.0094 - val_mae: 1.4818\n",
            "Epoch 147/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9542 - mae: 1.4280 - val_loss: 0.9926 - val_mae: 1.4629\n",
            "Epoch 148/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9506 - mae: 1.4249 - val_loss: 1.0066 - val_mae: 1.4777\n",
            "Epoch 149/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9439 - mae: 1.4181 - val_loss: 1.0146 - val_mae: 1.4820\n",
            "Epoch 150/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9221 - mae: 1.3944 - val_loss: 0.9722 - val_mae: 1.4405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:47:49,983] Trial 28 finished with value: 0.9722071886062622 and parameters: {'dropout_2': 0.15647183444150736, 'dropout_3': 0.7238249124847883, 'dropout_4': 0.5069571495021171, 'dropout_5': 0.6329142468139375, 'learning_rate': 0.0015150615167323403, 'epochs': 150, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "27/27 [==============================] - 2s 17ms/step - loss: 1.6397 - mae: 2.1871 - val_loss: 1.3336 - val_mae: 1.8170\n",
            "Epoch 2/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.4219 - mae: 1.9370 - val_loss: 1.3022 - val_mae: 1.7843\n",
            "Epoch 3/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.3121 - mae: 1.8110 - val_loss: 1.2737 - val_mae: 1.7594\n",
            "Epoch 4/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.2475 - mae: 1.7357 - val_loss: 1.2424 - val_mae: 1.7321\n",
            "Epoch 5/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.2050 - mae: 1.6898 - val_loss: 1.2294 - val_mae: 1.7211\n",
            "Epoch 6/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1816 - mae: 1.6627 - val_loss: 1.1979 - val_mae: 1.6832\n",
            "Epoch 7/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1621 - mae: 1.6447 - val_loss: 1.1739 - val_mae: 1.6602\n",
            "Epoch 8/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1583 - mae: 1.6441 - val_loss: 1.1683 - val_mae: 1.6685\n",
            "Epoch 9/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1428 - mae: 1.6284 - val_loss: 1.1730 - val_mae: 1.6845\n",
            "Epoch 10/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1473 - mae: 1.6368 - val_loss: 1.1473 - val_mae: 1.6559\n",
            "Epoch 11/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.1286 - mae: 1.6154 - val_loss: 1.1450 - val_mae: 1.6381\n",
            "Epoch 12/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.1072 - mae: 1.5907 - val_loss: 1.1363 - val_mae: 1.6455\n",
            "Epoch 13/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1200 - mae: 1.6054 - val_loss: 1.1361 - val_mae: 1.6423\n",
            "Epoch 14/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0913 - mae: 1.5758 - val_loss: 1.1322 - val_mae: 1.6388\n",
            "Epoch 15/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1192 - mae: 1.6065 - val_loss: 1.1455 - val_mae: 1.6483\n",
            "Epoch 16/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.1108 - mae: 1.5972 - val_loss: 1.1521 - val_mae: 1.6652\n",
            "Epoch 17/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1110 - mae: 1.5971 - val_loss: 1.1621 - val_mae: 1.6689\n",
            "Epoch 18/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0984 - mae: 1.5775 - val_loss: 1.1304 - val_mae: 1.6406\n",
            "Epoch 19/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1003 - mae: 1.5864 - val_loss: 1.1248 - val_mae: 1.6302\n",
            "Epoch 20/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1075 - mae: 1.5947 - val_loss: 1.1194 - val_mae: 1.6204\n",
            "Epoch 21/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0978 - mae: 1.5851 - val_loss: 1.1161 - val_mae: 1.6118\n",
            "Epoch 22/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0907 - mae: 1.5736 - val_loss: 1.1317 - val_mae: 1.6327\n",
            "Epoch 23/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0887 - mae: 1.5743 - val_loss: 1.1254 - val_mae: 1.6245\n",
            "Epoch 24/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0973 - mae: 1.5829 - val_loss: 1.1469 - val_mae: 1.6390\n",
            "Epoch 25/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.1014 - mae: 1.5846 - val_loss: 1.1146 - val_mae: 1.6049\n",
            "Epoch 26/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.1083 - mae: 1.5951 - val_loss: 1.1173 - val_mae: 1.6197\n",
            "Epoch 27/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0898 - mae: 1.5737 - val_loss: 1.1118 - val_mae: 1.6077\n",
            "Epoch 28/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0874 - mae: 1.5669 - val_loss: 1.1028 - val_mae: 1.5960\n",
            "Epoch 29/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0987 - mae: 1.5794 - val_loss: 1.1096 - val_mae: 1.6095\n",
            "Epoch 30/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0811 - mae: 1.5675 - val_loss: 1.1230 - val_mae: 1.6100\n",
            "Epoch 31/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0855 - mae: 1.5705 - val_loss: 1.1134 - val_mae: 1.5963\n",
            "Epoch 32/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0842 - mae: 1.5696 - val_loss: 1.1317 - val_mae: 1.6245\n",
            "Epoch 33/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0701 - mae: 1.5518 - val_loss: 1.1311 - val_mae: 1.6232\n",
            "Epoch 34/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0923 - mae: 1.5736 - val_loss: 1.1325 - val_mae: 1.6319\n",
            "Epoch 35/150\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.0818 - mae: 1.5605 - val_loss: 1.1547 - val_mae: 1.6509\n",
            "Epoch 36/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0682 - mae: 1.5483 - val_loss: 1.1131 - val_mae: 1.6117\n",
            "Epoch 37/150\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.0944 - mae: 1.5811 - val_loss: 1.1341 - val_mae: 1.6248\n",
            "Epoch 38/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0743 - mae: 1.5599 - val_loss: 1.1251 - val_mae: 1.6169\n",
            "Epoch 39/150\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.0824 - mae: 1.5635 - val_loss: 1.1013 - val_mae: 1.5853\n",
            "Epoch 40/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0749 - mae: 1.5542 - val_loss: 1.0974 - val_mae: 1.5839\n",
            "Epoch 41/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0947 - mae: 1.5778 - val_loss: 1.1069 - val_mae: 1.5934\n",
            "Epoch 42/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0811 - mae: 1.5632 - val_loss: 1.1155 - val_mae: 1.5991\n",
            "Epoch 43/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0609 - mae: 1.5436 - val_loss: 1.0993 - val_mae: 1.5896\n",
            "Epoch 44/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0720 - mae: 1.5561 - val_loss: 1.1012 - val_mae: 1.5784\n",
            "Epoch 45/150\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.0630 - mae: 1.5435 - val_loss: 1.0895 - val_mae: 1.5769\n",
            "Epoch 46/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0705 - mae: 1.5535 - val_loss: 1.0838 - val_mae: 1.5735\n",
            "Epoch 47/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0658 - mae: 1.5493 - val_loss: 1.1012 - val_mae: 1.5896\n",
            "Epoch 48/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0870 - mae: 1.5693 - val_loss: 1.1133 - val_mae: 1.5950\n",
            "Epoch 49/150\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 1.0748 - mae: 1.5576 - val_loss: 1.1009 - val_mae: 1.5827\n",
            "Epoch 50/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0668 - mae: 1.5503 - val_loss: 1.0920 - val_mae: 1.5722\n",
            "Epoch 51/150\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.0774 - mae: 1.5546 - val_loss: 1.1045 - val_mae: 1.5773\n",
            "Epoch 52/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0939 - mae: 1.5754 - val_loss: 1.1085 - val_mae: 1.5807\n",
            "Epoch 53/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0759 - mae: 1.5545 - val_loss: 1.1266 - val_mae: 1.6091\n",
            "Epoch 54/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0709 - mae: 1.5490 - val_loss: 1.1054 - val_mae: 1.5892\n",
            "Epoch 55/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0766 - mae: 1.5578 - val_loss: 1.1047 - val_mae: 1.5791\n",
            "Epoch 56/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0875 - mae: 1.5717 - val_loss: 1.1256 - val_mae: 1.6006\n",
            "Epoch 57/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0719 - mae: 1.5497 - val_loss: 1.1392 - val_mae: 1.6241\n",
            "Epoch 58/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0773 - mae: 1.5566 - val_loss: 1.1343 - val_mae: 1.6246\n",
            "Epoch 59/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0793 - mae: 1.5612 - val_loss: 1.0915 - val_mae: 1.5874\n",
            "Epoch 60/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0619 - mae: 1.5400 - val_loss: 1.1331 - val_mae: 1.6234\n",
            "Epoch 61/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0791 - mae: 1.5577 - val_loss: 1.1226 - val_mae: 1.6138\n",
            "Epoch 62/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0744 - mae: 1.5565 - val_loss: 1.1291 - val_mae: 1.6216\n",
            "Epoch 63/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0832 - mae: 1.5620 - val_loss: 1.1280 - val_mae: 1.6187\n",
            "Epoch 64/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0734 - mae: 1.5502 - val_loss: 1.1168 - val_mae: 1.6119\n",
            "Epoch 65/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0639 - mae: 1.5426 - val_loss: 1.1103 - val_mae: 1.5948\n",
            "Epoch 66/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0563 - mae: 1.5335 - val_loss: 1.1072 - val_mae: 1.5909\n",
            "Epoch 67/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0472 - mae: 1.5273 - val_loss: 1.1198 - val_mae: 1.5969\n",
            "Epoch 68/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0648 - mae: 1.5463 - val_loss: 1.1267 - val_mae: 1.6117\n",
            "Epoch 69/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0542 - mae: 1.5366 - val_loss: 1.0855 - val_mae: 1.5719\n",
            "Epoch 70/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0724 - mae: 1.5516 - val_loss: 1.1328 - val_mae: 1.6241\n",
            "Epoch 71/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0485 - mae: 1.5294 - val_loss: 1.1252 - val_mae: 1.6106\n",
            "Epoch 72/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0722 - mae: 1.5526 - val_loss: 1.1261 - val_mae: 1.6044\n",
            "Epoch 73/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0570 - mae: 1.5370 - val_loss: 1.0987 - val_mae: 1.5849\n",
            "Epoch 74/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0677 - mae: 1.5492 - val_loss: 1.0775 - val_mae: 1.5706\n",
            "Epoch 75/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0563 - mae: 1.5384 - val_loss: 1.1067 - val_mae: 1.5931\n",
            "Epoch 76/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0692 - mae: 1.5495 - val_loss: 1.1102 - val_mae: 1.5955\n",
            "Epoch 77/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0507 - mae: 1.5276 - val_loss: 1.1092 - val_mae: 1.5975\n",
            "Epoch 78/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0624 - mae: 1.5462 - val_loss: 1.1050 - val_mae: 1.5847\n",
            "Epoch 79/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0490 - mae: 1.5285 - val_loss: 1.0991 - val_mae: 1.5816\n",
            "Epoch 80/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0611 - mae: 1.5444 - val_loss: 1.1056 - val_mae: 1.5889\n",
            "Epoch 81/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0656 - mae: 1.5450 - val_loss: 1.0946 - val_mae: 1.5814\n",
            "Epoch 82/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0534 - mae: 1.5333 - val_loss: 1.1320 - val_mae: 1.6136\n",
            "Epoch 83/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0610 - mae: 1.5433 - val_loss: 1.0887 - val_mae: 1.5756\n",
            "Epoch 84/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0576 - mae: 1.5360 - val_loss: 1.1065 - val_mae: 1.5968\n",
            "Epoch 85/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0751 - mae: 1.5559 - val_loss: 1.0738 - val_mae: 1.5655\n",
            "Epoch 86/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0541 - mae: 1.5341 - val_loss: 1.0871 - val_mae: 1.5792\n",
            "Epoch 87/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0474 - mae: 1.5290 - val_loss: 1.1140 - val_mae: 1.6072\n",
            "Epoch 88/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0481 - mae: 1.5334 - val_loss: 1.1085 - val_mae: 1.5877\n",
            "Epoch 89/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0390 - mae: 1.5192 - val_loss: 1.0835 - val_mae: 1.5727\n",
            "Epoch 90/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0478 - mae: 1.5333 - val_loss: 1.1072 - val_mae: 1.5963\n",
            "Epoch 91/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0559 - mae: 1.5384 - val_loss: 1.0928 - val_mae: 1.5830\n",
            "Epoch 92/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0665 - mae: 1.5511 - val_loss: 1.0765 - val_mae: 1.5686\n",
            "Epoch 93/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0521 - mae: 1.5338 - val_loss: 1.0679 - val_mae: 1.5578\n",
            "Epoch 94/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0473 - mae: 1.5279 - val_loss: 1.0788 - val_mae: 1.5728\n",
            "Epoch 95/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0648 - mae: 1.5450 - val_loss: 1.0657 - val_mae: 1.5526\n",
            "Epoch 96/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0613 - mae: 1.5425 - val_loss: 1.0945 - val_mae: 1.5849\n",
            "Epoch 97/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0507 - mae: 1.5327 - val_loss: 1.1007 - val_mae: 1.5902\n",
            "Epoch 98/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0503 - mae: 1.5279 - val_loss: 1.0933 - val_mae: 1.5852\n",
            "Epoch 99/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0651 - mae: 1.5443 - val_loss: 1.0985 - val_mae: 1.5903\n",
            "Epoch 100/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0558 - mae: 1.5342 - val_loss: 1.0685 - val_mae: 1.5555\n",
            "Epoch 101/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0492 - mae: 1.5306 - val_loss: 1.0634 - val_mae: 1.5518\n",
            "Epoch 102/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0560 - mae: 1.5338 - val_loss: 1.0828 - val_mae: 1.5735\n",
            "Epoch 103/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0560 - mae: 1.5356 - val_loss: 1.0730 - val_mae: 1.5585\n",
            "Epoch 104/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0486 - mae: 1.5288 - val_loss: 1.0717 - val_mae: 1.5510\n",
            "Epoch 105/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0542 - mae: 1.5344 - val_loss: 1.0659 - val_mae: 1.5420\n",
            "Epoch 106/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0573 - mae: 1.5352 - val_loss: 1.0655 - val_mae: 1.5507\n",
            "Epoch 107/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0563 - mae: 1.5391 - val_loss: 1.0740 - val_mae: 1.5551\n",
            "Epoch 108/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0592 - mae: 1.5368 - val_loss: 1.0592 - val_mae: 1.5462\n",
            "Epoch 109/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0499 - mae: 1.5285 - val_loss: 1.0512 - val_mae: 1.5417\n",
            "Epoch 110/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0461 - mae: 1.5270 - val_loss: 1.0719 - val_mae: 1.5545\n",
            "Epoch 111/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0653 - mae: 1.5488 - val_loss: 1.0660 - val_mae: 1.5439\n",
            "Epoch 112/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0410 - mae: 1.5265 - val_loss: 1.0694 - val_mae: 1.5469\n",
            "Epoch 113/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0600 - mae: 1.5381 - val_loss: 1.0830 - val_mae: 1.5664\n",
            "Epoch 114/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0395 - mae: 1.5217 - val_loss: 1.0452 - val_mae: 1.5278\n",
            "Epoch 115/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0561 - mae: 1.5399 - val_loss: 1.0757 - val_mae: 1.5639\n",
            "Epoch 116/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0475 - mae: 1.5291 - val_loss: 1.0499 - val_mae: 1.5304\n",
            "Epoch 117/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0489 - mae: 1.5274 - val_loss: 1.0539 - val_mae: 1.5387\n",
            "Epoch 118/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0517 - mae: 1.5316 - val_loss: 1.0483 - val_mae: 1.5394\n",
            "Epoch 119/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0206 - mae: 1.5031 - val_loss: 1.0584 - val_mae: 1.5441\n",
            "Epoch 120/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0481 - mae: 1.5303 - val_loss: 1.0645 - val_mae: 1.5461\n",
            "Epoch 121/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0560 - mae: 1.5406 - val_loss: 1.0601 - val_mae: 1.5378\n",
            "Epoch 122/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0527 - mae: 1.5383 - val_loss: 1.0637 - val_mae: 1.5546\n",
            "Epoch 123/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0538 - mae: 1.5361 - val_loss: 1.0641 - val_mae: 1.5435\n",
            "Epoch 124/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0339 - mae: 1.5118 - val_loss: 1.0441 - val_mae: 1.5285\n",
            "Epoch 125/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0481 - mae: 1.5335 - val_loss: 1.0731 - val_mae: 1.5673\n",
            "Epoch 126/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0439 - mae: 1.5269 - val_loss: 1.0815 - val_mae: 1.5719\n",
            "Epoch 127/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0477 - mae: 1.5298 - val_loss: 1.0664 - val_mae: 1.5611\n",
            "Epoch 128/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0532 - mae: 1.5399 - val_loss: 1.0867 - val_mae: 1.5798\n",
            "Epoch 129/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0355 - mae: 1.5163 - val_loss: 1.1007 - val_mae: 1.5976\n",
            "Epoch 130/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0361 - mae: 1.5142 - val_loss: 1.0675 - val_mae: 1.5524\n",
            "Epoch 131/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0315 - mae: 1.5121 - val_loss: 1.0748 - val_mae: 1.5677\n",
            "Epoch 132/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0353 - mae: 1.5192 - val_loss: 1.0439 - val_mae: 1.5307\n",
            "Epoch 133/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0391 - mae: 1.5223 - val_loss: 1.0578 - val_mae: 1.5511\n",
            "Epoch 134/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0492 - mae: 1.5316 - val_loss: 1.0746 - val_mae: 1.5712\n",
            "Epoch 135/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0426 - mae: 1.5244 - val_loss: 1.0707 - val_mae: 1.5568\n",
            "Epoch 136/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0396 - mae: 1.5248 - val_loss: 1.0897 - val_mae: 1.5868\n",
            "Epoch 137/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0461 - mae: 1.5264 - val_loss: 1.0734 - val_mae: 1.5667\n",
            "Epoch 138/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0589 - mae: 1.5446 - val_loss: 1.0782 - val_mae: 1.5732\n",
            "Epoch 139/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0344 - mae: 1.5193 - val_loss: 1.0742 - val_mae: 1.5612\n",
            "Epoch 140/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0410 - mae: 1.5247 - val_loss: 1.0677 - val_mae: 1.5539\n",
            "Epoch 141/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0467 - mae: 1.5256 - val_loss: 1.0932 - val_mae: 1.5796\n",
            "Epoch 142/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0405 - mae: 1.5208 - val_loss: 1.0689 - val_mae: 1.5596\n",
            "Epoch 143/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0373 - mae: 1.5233 - val_loss: 1.0753 - val_mae: 1.5609\n",
            "Epoch 144/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0576 - mae: 1.5383 - val_loss: 1.0617 - val_mae: 1.5506\n",
            "Epoch 145/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0470 - mae: 1.5290 - val_loss: 1.0663 - val_mae: 1.5478\n",
            "Epoch 146/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0414 - mae: 1.5261 - val_loss: 1.0445 - val_mae: 1.5280\n",
            "Epoch 147/150\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0352 - mae: 1.5180 - val_loss: 1.0668 - val_mae: 1.5557\n",
            "Epoch 148/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0427 - mae: 1.5264 - val_loss: 1.0633 - val_mae: 1.5602\n",
            "Epoch 149/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0404 - mae: 1.5217 - val_loss: 1.0474 - val_mae: 1.5325\n",
            "Epoch 150/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0447 - mae: 1.5285 - val_loss: 1.0373 - val_mae: 1.5244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:48:26,645] Trial 29 finished with value: 1.0372792482376099 and parameters: {'dropout_2': 0.6673988570622252, 'dropout_3': 0.6342278792011897, 'dropout_4': 0.19660301302047256, 'dropout_5': 0.7354754204623262, 'learning_rate': 0.004166293940037002, 'epochs': 150, 'batch_size': 128}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 2s 7ms/step - loss: 1.5547 - mae: 2.1118 - val_loss: 1.3201 - val_mae: 1.7933\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4816 - mae: 2.0272 - val_loss: 1.3040 - val_mae: 1.7689\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3856 - mae: 1.9227 - val_loss: 1.2663 - val_mae: 1.7368\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3357 - mae: 1.8651 - val_loss: 1.2193 - val_mae: 1.6947\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2787 - mae: 1.7990 - val_loss: 1.1908 - val_mae: 1.6737\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2544 - mae: 1.7701 - val_loss: 1.1617 - val_mae: 1.6458\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2440 - mae: 1.7588 - val_loss: 1.1619 - val_mae: 1.6438\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2096 - mae: 1.7167 - val_loss: 1.1527 - val_mae: 1.6311\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1915 - mae: 1.6966 - val_loss: 1.1454 - val_mae: 1.6220\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1755 - mae: 1.6790 - val_loss: 1.1425 - val_mae: 1.6293\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1621 - mae: 1.6642 - val_loss: 1.1378 - val_mae: 1.6164\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1596 - mae: 1.6608 - val_loss: 1.1348 - val_mae: 1.6111\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1704 - mae: 1.6687 - val_loss: 1.1414 - val_mae: 1.6129\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1469 - mae: 1.6458 - val_loss: 1.1367 - val_mae: 1.6138\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1468 - mae: 1.6429 - val_loss: 1.1306 - val_mae: 1.6101\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1414 - mae: 1.6329 - val_loss: 1.1348 - val_mae: 1.6167\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1364 - mae: 1.6308 - val_loss: 1.1162 - val_mae: 1.5981\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1337 - mae: 1.6284 - val_loss: 1.1176 - val_mae: 1.5970\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1277 - mae: 1.6204 - val_loss: 1.1083 - val_mae: 1.5844\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1184 - mae: 1.6090 - val_loss: 1.1165 - val_mae: 1.5985\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1115 - mae: 1.5998 - val_loss: 1.1233 - val_mae: 1.6069\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1183 - mae: 1.6108 - val_loss: 1.1105 - val_mae: 1.5920\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1168 - mae: 1.6100 - val_loss: 1.1117 - val_mae: 1.5922\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0879 - mae: 1.5805 - val_loss: 1.0995 - val_mae: 1.5795\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0958 - mae: 1.5892 - val_loss: 1.0993 - val_mae: 1.5858\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0895 - mae: 1.5816 - val_loss: 1.1025 - val_mae: 1.5899\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0872 - mae: 1.5752 - val_loss: 1.0885 - val_mae: 1.5690\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0723 - mae: 1.5622 - val_loss: 1.0821 - val_mae: 1.5657\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0724 - mae: 1.5620 - val_loss: 1.0767 - val_mae: 1.5589\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0806 - mae: 1.5705 - val_loss: 1.0825 - val_mae: 1.5699\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0933 - mae: 1.5831 - val_loss: 1.0967 - val_mae: 1.5834\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0765 - mae: 1.5673 - val_loss: 1.0810 - val_mae: 1.5694\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0613 - mae: 1.5495 - val_loss: 1.0772 - val_mae: 1.5685\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0576 - mae: 1.5460 - val_loss: 1.0801 - val_mae: 1.5708\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0560 - mae: 1.5426 - val_loss: 1.0604 - val_mae: 1.5481\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0557 - mae: 1.5446 - val_loss: 1.0571 - val_mae: 1.5417\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0486 - mae: 1.5378 - val_loss: 1.0610 - val_mae: 1.5523\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0493 - mae: 1.5417 - val_loss: 1.0545 - val_mae: 1.5422\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0545 - mae: 1.5476 - val_loss: 1.0595 - val_mae: 1.5485\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0466 - mae: 1.5342 - val_loss: 1.0385 - val_mae: 1.5199\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0616 - mae: 1.5539 - val_loss: 1.0495 - val_mae: 1.5268\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0300 - mae: 1.5172 - val_loss: 1.0512 - val_mae: 1.5396\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0404 - mae: 1.5307 - val_loss: 1.0391 - val_mae: 1.5228\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0311 - mae: 1.5171 - val_loss: 1.0397 - val_mae: 1.5212\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0455 - mae: 1.5350 - val_loss: 1.0469 - val_mae: 1.5308\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0382 - mae: 1.5297 - val_loss: 1.0416 - val_mae: 1.5233\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0341 - mae: 1.5205 - val_loss: 1.0361 - val_mae: 1.5130\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0261 - mae: 1.5187 - val_loss: 1.0256 - val_mae: 1.5042\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0357 - mae: 1.5284 - val_loss: 1.0308 - val_mae: 1.5092\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0233 - mae: 1.5145 - val_loss: 1.0303 - val_mae: 1.5142\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0259 - mae: 1.5133 - val_loss: 1.0280 - val_mae: 1.5111\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0277 - mae: 1.5188 - val_loss: 1.0278 - val_mae: 1.5152\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0168 - mae: 1.5040 - val_loss: 1.0216 - val_mae: 1.5085\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0135 - mae: 1.5033 - val_loss: 1.0300 - val_mae: 1.5157\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0032 - mae: 1.4888 - val_loss: 1.0113 - val_mae: 1.4940\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9999 - mae: 1.4848 - val_loss: 1.0101 - val_mae: 1.4874\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0022 - mae: 1.4959 - val_loss: 1.0062 - val_mae: 1.4839\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9989 - mae: 1.4867 - val_loss: 0.9982 - val_mae: 1.4794\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0153 - mae: 1.5045 - val_loss: 1.0146 - val_mae: 1.4978\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0198 - mae: 1.5108 - val_loss: 1.0011 - val_mae: 1.4797\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0033 - mae: 1.4934 - val_loss: 1.0179 - val_mae: 1.5046\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0082 - mae: 1.4930 - val_loss: 1.0201 - val_mae: 1.5045\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9997 - mae: 1.4848 - val_loss: 1.0120 - val_mae: 1.4969\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0033 - mae: 1.4887 - val_loss: 0.9929 - val_mae: 1.4672\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9993 - mae: 1.4850 - val_loss: 1.0001 - val_mae: 1.4813\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9813 - mae: 1.4647 - val_loss: 1.0080 - val_mae: 1.4836\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9898 - mae: 1.4773 - val_loss: 1.0038 - val_mae: 1.4830\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9982 - mae: 1.4866 - val_loss: 1.0031 - val_mae: 1.4826\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9760 - mae: 1.4627 - val_loss: 0.9900 - val_mae: 1.4691\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9871 - mae: 1.4735 - val_loss: 1.0006 - val_mae: 1.4793\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9803 - mae: 1.4659 - val_loss: 0.9931 - val_mae: 1.4741\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9946 - mae: 1.4838 - val_loss: 0.9876 - val_mae: 1.4690\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9909 - mae: 1.4761 - val_loss: 1.0072 - val_mae: 1.4855\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9818 - mae: 1.4658 - val_loss: 0.9885 - val_mae: 1.4668\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9830 - mae: 1.4671 - val_loss: 0.9850 - val_mae: 1.4577\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9741 - mae: 1.4588 - val_loss: 0.9836 - val_mae: 1.4528\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9694 - mae: 1.4518 - val_loss: 0.9824 - val_mae: 1.4530\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9809 - mae: 1.4665 - val_loss: 0.9924 - val_mae: 1.4671\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9768 - mae: 1.4634 - val_loss: 0.9777 - val_mae: 1.4441\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9832 - mae: 1.4648 - val_loss: 0.9851 - val_mae: 1.4510\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9796 - mae: 1.4649 - val_loss: 0.9702 - val_mae: 1.4450\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9717 - mae: 1.4566 - val_loss: 0.9789 - val_mae: 1.4492\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9642 - mae: 1.4496 - val_loss: 0.9960 - val_mae: 1.4696\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9647 - mae: 1.4489 - val_loss: 0.9803 - val_mae: 1.4467\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9425 - mae: 1.4191 - val_loss: 0.9800 - val_mae: 1.4575\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9549 - mae: 1.4395 - val_loss: 0.9760 - val_mae: 1.4452\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9660 - mae: 1.4479 - val_loss: 0.9743 - val_mae: 1.4414\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9650 - mae: 1.4465 - val_loss: 0.9741 - val_mae: 1.4447\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9535 - mae: 1.4346 - val_loss: 0.9548 - val_mae: 1.4331\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9443 - mae: 1.4213 - val_loss: 0.9578 - val_mae: 1.4263\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9460 - mae: 1.4248 - val_loss: 0.9612 - val_mae: 1.4263\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9689 - mae: 1.4519 - val_loss: 0.9657 - val_mae: 1.4350\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9571 - mae: 1.4363 - val_loss: 0.9606 - val_mae: 1.4261\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9470 - mae: 1.4266 - val_loss: 0.9632 - val_mae: 1.4272\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9564 - mae: 1.4313 - val_loss: 0.9718 - val_mae: 1.4429\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9411 - mae: 1.4201 - val_loss: 0.9676 - val_mae: 1.4322\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9430 - mae: 1.4211 - val_loss: 0.9611 - val_mae: 1.4301\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9308 - mae: 1.4104 - val_loss: 0.9631 - val_mae: 1.4296\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9332 - mae: 1.4128 - val_loss: 0.9611 - val_mae: 1.4265\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9262 - mae: 1.4024 - val_loss: 0.9555 - val_mae: 1.4235\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9605 - mae: 1.4394 - val_loss: 0.9608 - val_mae: 1.4252\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9437 - mae: 1.4240 - val_loss: 0.9587 - val_mae: 1.4286\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9354 - mae: 1.4116 - val_loss: 0.9597 - val_mae: 1.4247\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9355 - mae: 1.4137 - val_loss: 0.9599 - val_mae: 1.4212\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9344 - mae: 1.4102 - val_loss: 0.9475 - val_mae: 1.4088\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9321 - mae: 1.4109 - val_loss: 0.9584 - val_mae: 1.4158\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9485 - mae: 1.4269 - val_loss: 0.9604 - val_mae: 1.4208\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9201 - mae: 1.3956 - val_loss: 0.9595 - val_mae: 1.4213\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9417 - mae: 1.4215 - val_loss: 0.9566 - val_mae: 1.4195\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9319 - mae: 1.4124 - val_loss: 0.9442 - val_mae: 1.4057\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9318 - mae: 1.4090 - val_loss: 0.9579 - val_mae: 1.4129\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9109 - mae: 1.3839 - val_loss: 0.9537 - val_mae: 1.4122\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9322 - mae: 1.4090 - val_loss: 0.9563 - val_mae: 1.4146\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9252 - mae: 1.4026 - val_loss: 0.9534 - val_mae: 1.4070\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9429 - mae: 1.4197 - val_loss: 0.9554 - val_mae: 1.4151\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9313 - mae: 1.4103 - val_loss: 0.9556 - val_mae: 1.4120\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9284 - mae: 1.4047 - val_loss: 0.9647 - val_mae: 1.4201\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9264 - mae: 1.4039 - val_loss: 0.9535 - val_mae: 1.4147\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9325 - mae: 1.4076 - val_loss: 0.9495 - val_mae: 1.4062\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9162 - mae: 1.3903 - val_loss: 0.9687 - val_mae: 1.4193\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9190 - mae: 1.3951 - val_loss: 0.9500 - val_mae: 1.4026\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9055 - mae: 1.3787 - val_loss: 0.9432 - val_mae: 1.4019\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9152 - mae: 1.3869 - val_loss: 0.9446 - val_mae: 1.4004\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9211 - mae: 1.3959 - val_loss: 0.9535 - val_mae: 1.4068\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9150 - mae: 1.3909 - val_loss: 0.9486 - val_mae: 1.4054\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9174 - mae: 1.3933 - val_loss: 0.9473 - val_mae: 1.4066\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9153 - mae: 1.3898 - val_loss: 0.9555 - val_mae: 1.4069\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9219 - mae: 1.3988 - val_loss: 0.9554 - val_mae: 1.4142\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9028 - mae: 1.3742 - val_loss: 0.9543 - val_mae: 1.4139\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8958 - mae: 1.3682 - val_loss: 0.9326 - val_mae: 1.3862\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9165 - mae: 1.3925 - val_loss: 0.9456 - val_mae: 1.3948\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 1s 11ms/step - loss: 0.9338 - mae: 1.4099 - val_loss: 0.9529 - val_mae: 1.4025\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9149 - mae: 1.3948 - val_loss: 0.9361 - val_mae: 1.3948\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9228 - mae: 1.3981 - val_loss: 0.9501 - val_mae: 1.4023\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9220 - mae: 1.3985 - val_loss: 0.9488 - val_mae: 1.4079\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8933 - mae: 1.3621 - val_loss: 0.9389 - val_mae: 1.4001\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9210 - mae: 1.3986 - val_loss: 0.9464 - val_mae: 1.4046\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9139 - mae: 1.3864 - val_loss: 0.9534 - val_mae: 1.4120\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9007 - mae: 1.3743 - val_loss: 0.9322 - val_mae: 1.3975\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8978 - mae: 1.3707 - val_loss: 0.9502 - val_mae: 1.4021\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9021 - mae: 1.3744 - val_loss: 0.9272 - val_mae: 1.3783\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9045 - mae: 1.3767 - val_loss: 0.9366 - val_mae: 1.3869\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8953 - mae: 1.3686 - val_loss: 0.9357 - val_mae: 1.3921\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8879 - mae: 1.3605 - val_loss: 0.9396 - val_mae: 1.3931\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9029 - mae: 1.3803 - val_loss: 0.9599 - val_mae: 1.4133\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9051 - mae: 1.3796 - val_loss: 0.9405 - val_mae: 1.3928\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9070 - mae: 1.3802 - val_loss: 0.9407 - val_mae: 1.3991\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8922 - mae: 1.3645 - val_loss: 0.9491 - val_mae: 1.4036\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9054 - mae: 1.3807 - val_loss: 0.9384 - val_mae: 1.3964\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8964 - mae: 1.3692 - val_loss: 0.9328 - val_mae: 1.3916\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9007 - mae: 1.3699 - val_loss: 0.9501 - val_mae: 1.4135\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9106 - mae: 1.3838 - val_loss: 0.9425 - val_mae: 1.3985\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8994 - mae: 1.3697 - val_loss: 0.9390 - val_mae: 1.3944\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8796 - mae: 1.3498 - val_loss: 0.9323 - val_mae: 1.3859\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9075 - mae: 1.3813 - val_loss: 0.9555 - val_mae: 1.4121\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8797 - mae: 1.3481 - val_loss: 0.9372 - val_mae: 1.3920\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8909 - mae: 1.3604 - val_loss: 0.9415 - val_mae: 1.3978\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8880 - mae: 1.3584 - val_loss: 0.9357 - val_mae: 1.3915\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9056 - mae: 1.3808 - val_loss: 0.9582 - val_mae: 1.4108\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8799 - mae: 1.3483 - val_loss: 0.9242 - val_mae: 1.3780\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8948 - mae: 1.3688 - val_loss: 0.9334 - val_mae: 1.3895\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8861 - mae: 1.3587 - val_loss: 0.9257 - val_mae: 1.3722\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8725 - mae: 1.3374 - val_loss: 0.9321 - val_mae: 1.3893\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8734 - mae: 1.3412 - val_loss: 0.9296 - val_mae: 1.3849\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8975 - mae: 1.3716 - val_loss: 0.9448 - val_mae: 1.3922\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8884 - mae: 1.3562 - val_loss: 0.9222 - val_mae: 1.3691\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8970 - mae: 1.3657 - val_loss: 0.9290 - val_mae: 1.3723\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8912 - mae: 1.3610 - val_loss: 0.9329 - val_mae: 1.3764\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8877 - mae: 1.3560 - val_loss: 0.9366 - val_mae: 1.3820\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8824 - mae: 1.3530 - val_loss: 0.9397 - val_mae: 1.3914\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8919 - mae: 1.3603 - val_loss: 0.9326 - val_mae: 1.3849\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8810 - mae: 1.3531 - val_loss: 0.9149 - val_mae: 1.3612\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8700 - mae: 1.3350 - val_loss: 0.9285 - val_mae: 1.3787\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8939 - mae: 1.3665 - val_loss: 0.9295 - val_mae: 1.3866\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8853 - mae: 1.3541 - val_loss: 0.9317 - val_mae: 1.3810\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8774 - mae: 1.3461 - val_loss: 0.9219 - val_mae: 1.3756\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8746 - mae: 1.3421 - val_loss: 0.9429 - val_mae: 1.3921\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8844 - mae: 1.3512 - val_loss: 0.9245 - val_mae: 1.3741\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8622 - mae: 1.3271 - val_loss: 0.9306 - val_mae: 1.3835\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8818 - mae: 1.3515 - val_loss: 0.9242 - val_mae: 1.3809\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8709 - mae: 1.3402 - val_loss: 0.9210 - val_mae: 1.3764\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8818 - mae: 1.3498 - val_loss: 0.9279 - val_mae: 1.3792\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8729 - mae: 1.3435 - val_loss: 0.9366 - val_mae: 1.3840\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8865 - mae: 1.3558 - val_loss: 0.9321 - val_mae: 1.3825\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8793 - mae: 1.3501 - val_loss: 0.9351 - val_mae: 1.3879\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8745 - mae: 1.3367 - val_loss: 0.9263 - val_mae: 1.3763\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8708 - mae: 1.3354 - val_loss: 0.9304 - val_mae: 1.3837\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8716 - mae: 1.3423 - val_loss: 0.9281 - val_mae: 1.3775\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8872 - mae: 1.3584 - val_loss: 0.9312 - val_mae: 1.3803\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8743 - mae: 1.3427 - val_loss: 0.9159 - val_mae: 1.3586\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8719 - mae: 1.3398 - val_loss: 0.9375 - val_mae: 1.3866\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8737 - mae: 1.3399 - val_loss: 0.9112 - val_mae: 1.3526\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8742 - mae: 1.3401 - val_loss: 0.9142 - val_mae: 1.3588\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8707 - mae: 1.3385 - val_loss: 0.9194 - val_mae: 1.3652\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8652 - mae: 1.3350 - val_loss: 0.9042 - val_mae: 1.3512\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8542 - mae: 1.3172 - val_loss: 0.9215 - val_mae: 1.3694\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8684 - mae: 1.3382 - val_loss: 0.9170 - val_mae: 1.3672\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8617 - mae: 1.3282 - val_loss: 0.9104 - val_mae: 1.3616\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8784 - mae: 1.3452 - val_loss: 0.9142 - val_mae: 1.3588\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8648 - mae: 1.3310 - val_loss: 0.9124 - val_mae: 1.3608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:50:21,262] Trial 30 finished with value: 0.9124104976654053 and parameters: {'dropout_2': 0.10003438417175137, 'dropout_3': 0.8170488065950537, 'dropout_4': 0.38470245055073593, 'dropout_5': 0.2228502844489258, 'learning_rate': 0.00041042108578602857, 'epochs': 200, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 2s 6ms/step - loss: 1.6321 - mae: 2.1985 - val_loss: 1.3649 - val_mae: 1.8916\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5237 - mae: 2.0812 - val_loss: 1.3302 - val_mae: 1.8350\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4497 - mae: 1.9977 - val_loss: 1.3022 - val_mae: 1.7947\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3961 - mae: 1.9360 - val_loss: 1.2693 - val_mae: 1.7579\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3466 - mae: 1.8831 - val_loss: 1.2350 - val_mae: 1.7273\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3169 - mae: 1.8442 - val_loss: 1.2075 - val_mae: 1.6996\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2733 - mae: 1.7990 - val_loss: 1.1922 - val_mae: 1.6811\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2754 - mae: 1.7954 - val_loss: 1.1859 - val_mae: 1.6727\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2407 - mae: 1.7560 - val_loss: 1.1677 - val_mae: 1.6465\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2072 - mae: 1.7176 - val_loss: 1.1615 - val_mae: 1.6468\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1862 - mae: 1.6925 - val_loss: 1.1540 - val_mae: 1.6394\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1869 - mae: 1.6948 - val_loss: 1.1466 - val_mae: 1.6297\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1803 - mae: 1.6851 - val_loss: 1.1356 - val_mae: 1.6195\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1564 - mae: 1.6583 - val_loss: 1.1296 - val_mae: 1.6080\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1544 - mae: 1.6569 - val_loss: 1.1338 - val_mae: 1.6134\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1440 - mae: 1.6448 - val_loss: 1.1289 - val_mae: 1.6055\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1331 - mae: 1.6303 - val_loss: 1.1357 - val_mae: 1.6145\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1395 - mae: 1.6387 - val_loss: 1.1252 - val_mae: 1.6062\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1251 - mae: 1.6222 - val_loss: 1.1286 - val_mae: 1.6077\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1114 - mae: 1.6062 - val_loss: 1.1156 - val_mae: 1.5978\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1102 - mae: 1.6071 - val_loss: 1.1180 - val_mae: 1.6015\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0959 - mae: 1.5879 - val_loss: 1.1076 - val_mae: 1.5959\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1090 - mae: 1.6024 - val_loss: 1.1083 - val_mae: 1.5962\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0919 - mae: 1.5861 - val_loss: 1.1029 - val_mae: 1.5908\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0950 - mae: 1.5866 - val_loss: 1.0870 - val_mae: 1.5719\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0837 - mae: 1.5772 - val_loss: 1.0953 - val_mae: 1.5870\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0978 - mae: 1.5970 - val_loss: 1.0882 - val_mae: 1.5735\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0867 - mae: 1.5798 - val_loss: 1.0985 - val_mae: 1.5856\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0948 - mae: 1.5902 - val_loss: 1.0946 - val_mae: 1.5805\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0940 - mae: 1.5859 - val_loss: 1.0950 - val_mae: 1.5802\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0789 - mae: 1.5688 - val_loss: 1.0935 - val_mae: 1.5795\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0646 - mae: 1.5518 - val_loss: 1.0868 - val_mae: 1.5764\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0863 - mae: 1.5783 - val_loss: 1.0849 - val_mae: 1.5758\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0663 - mae: 1.5600 - val_loss: 1.0802 - val_mae: 1.5715\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0567 - mae: 1.5472 - val_loss: 1.0786 - val_mae: 1.5718\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0593 - mae: 1.5497 - val_loss: 1.0738 - val_mae: 1.5649\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0611 - mae: 1.5490 - val_loss: 1.0818 - val_mae: 1.5785\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0643 - mae: 1.5591 - val_loss: 1.0774 - val_mae: 1.5705\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0483 - mae: 1.5384 - val_loss: 1.0659 - val_mae: 1.5604\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0585 - mae: 1.5511 - val_loss: 1.0656 - val_mae: 1.5554\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0552 - mae: 1.5439 - val_loss: 1.0581 - val_mae: 1.5467\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0477 - mae: 1.5403 - val_loss: 1.0695 - val_mae: 1.5656\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0451 - mae: 1.5355 - val_loss: 1.0553 - val_mae: 1.5457\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0472 - mae: 1.5380 - val_loss: 1.0443 - val_mae: 1.5294\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0291 - mae: 1.5183 - val_loss: 1.0494 - val_mae: 1.5345\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0426 - mae: 1.5306 - val_loss: 1.0503 - val_mae: 1.5357\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0449 - mae: 1.5381 - val_loss: 1.0450 - val_mae: 1.5217\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0352 - mae: 1.5278 - val_loss: 1.0472 - val_mae: 1.5272\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0310 - mae: 1.5187 - val_loss: 1.0335 - val_mae: 1.5151\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0482 - mae: 1.5412 - val_loss: 1.0469 - val_mae: 1.5308\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0247 - mae: 1.5169 - val_loss: 1.0470 - val_mae: 1.5346\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0234 - mae: 1.5104 - val_loss: 1.0402 - val_mae: 1.5241\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0287 - mae: 1.5183 - val_loss: 1.0462 - val_mae: 1.5337\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0283 - mae: 1.5193 - val_loss: 1.0421 - val_mae: 1.5306\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0314 - mae: 1.5250 - val_loss: 1.0445 - val_mae: 1.5336\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0045 - mae: 1.4914 - val_loss: 1.0317 - val_mae: 1.5168\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0059 - mae: 1.4951 - val_loss: 1.0422 - val_mae: 1.5294\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0299 - mae: 1.5197 - val_loss: 1.0292 - val_mae: 1.5084\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0120 - mae: 1.5030 - val_loss: 1.0409 - val_mae: 1.5280\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0170 - mae: 1.5062 - val_loss: 1.0213 - val_mae: 1.5021\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0082 - mae: 1.4976 - val_loss: 1.0270 - val_mae: 1.5089\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0133 - mae: 1.5041 - val_loss: 1.0241 - val_mae: 1.5090\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0174 - mae: 1.5082 - val_loss: 1.0197 - val_mae: 1.5027\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0045 - mae: 1.4970 - val_loss: 1.0102 - val_mae: 1.4925\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9939 - mae: 1.4884 - val_loss: 1.0269 - val_mae: 1.5140\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0108 - mae: 1.5007 - val_loss: 1.0258 - val_mae: 1.5051\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0059 - mae: 1.4934 - val_loss: 1.0242 - val_mae: 1.5055\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0102 - mae: 1.5037 - val_loss: 1.0229 - val_mae: 1.4994\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0015 - mae: 1.4865 - val_loss: 1.0234 - val_mae: 1.5060\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9982 - mae: 1.4845 - val_loss: 1.0278 - val_mae: 1.5060\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9861 - mae: 1.4732 - val_loss: 1.0261 - val_mae: 1.5097\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9949 - mae: 1.4835 - val_loss: 1.0203 - val_mae: 1.5033\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9835 - mae: 1.4687 - val_loss: 1.0189 - val_mae: 1.5043\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9837 - mae: 1.4725 - val_loss: 1.0176 - val_mae: 1.5046\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0038 - mae: 1.4932 - val_loss: 1.0110 - val_mae: 1.4967\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9866 - mae: 1.4759 - val_loss: 1.0138 - val_mae: 1.4932\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9841 - mae: 1.4673 - val_loss: 1.0175 - val_mae: 1.5004\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0027 - mae: 1.4927 - val_loss: 1.0247 - val_mae: 1.5072\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0035 - mae: 1.4887 - val_loss: 1.0201 - val_mae: 1.5035\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0017 - mae: 1.4901 - val_loss: 1.0169 - val_mae: 1.4934\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9822 - mae: 1.4685 - val_loss: 1.0212 - val_mae: 1.5016\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9818 - mae: 1.4687 - val_loss: 1.0123 - val_mae: 1.4874\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9678 - mae: 1.4511 - val_loss: 0.9965 - val_mae: 1.4759\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9944 - mae: 1.4817 - val_loss: 0.9863 - val_mae: 1.4622\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9806 - mae: 1.4703 - val_loss: 0.9854 - val_mae: 1.4623\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9688 - mae: 1.4502 - val_loss: 0.9932 - val_mae: 1.4702\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9867 - mae: 1.4717 - val_loss: 0.9856 - val_mae: 1.4624\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9873 - mae: 1.4732 - val_loss: 0.9854 - val_mae: 1.4657\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9856 - mae: 1.4716 - val_loss: 0.9884 - val_mae: 1.4661\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9802 - mae: 1.4634 - val_loss: 0.9904 - val_mae: 1.4686\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9748 - mae: 1.4613 - val_loss: 0.9943 - val_mae: 1.4697\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9839 - mae: 1.4706 - val_loss: 0.9952 - val_mae: 1.4713\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9834 - mae: 1.4717 - val_loss: 1.0068 - val_mae: 1.4794\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9730 - mae: 1.4569 - val_loss: 0.9872 - val_mae: 1.4589\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9756 - mae: 1.4582 - val_loss: 0.9801 - val_mae: 1.4476\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9722 - mae: 1.4542 - val_loss: 0.9858 - val_mae: 1.4578\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9672 - mae: 1.4480 - val_loss: 1.0035 - val_mae: 1.4748\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9560 - mae: 1.4368 - val_loss: 1.0039 - val_mae: 1.4795\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9562 - mae: 1.4361 - val_loss: 0.9834 - val_mae: 1.4532\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9626 - mae: 1.4433 - val_loss: 0.9971 - val_mae: 1.4715\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9808 - mae: 1.4660 - val_loss: 0.9961 - val_mae: 1.4672\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9579 - mae: 1.4381 - val_loss: 0.9827 - val_mae: 1.4546\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9598 - mae: 1.4413 - val_loss: 0.9911 - val_mae: 1.4579\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9636 - mae: 1.4485 - val_loss: 0.9892 - val_mae: 1.4547\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9604 - mae: 1.4392 - val_loss: 0.9968 - val_mae: 1.4610\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9589 - mae: 1.4422 - val_loss: 1.0111 - val_mae: 1.4787\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9691 - mae: 1.4528 - val_loss: 0.9998 - val_mae: 1.4705\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9683 - mae: 1.4509 - val_loss: 1.0011 - val_mae: 1.4677\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9499 - mae: 1.4322 - val_loss: 0.9900 - val_mae: 1.4572\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9665 - mae: 1.4492 - val_loss: 1.0049 - val_mae: 1.4749\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9462 - mae: 1.4267 - val_loss: 1.0224 - val_mae: 1.4870\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9358 - mae: 1.4140 - val_loss: 1.0050 - val_mae: 1.4748\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9324 - mae: 1.4146 - val_loss: 1.0046 - val_mae: 1.4742\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9538 - mae: 1.4364 - val_loss: 1.0329 - val_mae: 1.4994\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9481 - mae: 1.4261 - val_loss: 0.9984 - val_mae: 1.4620\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9447 - mae: 1.4229 - val_loss: 1.0183 - val_mae: 1.4803\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9519 - mae: 1.4331 - val_loss: 1.0188 - val_mae: 1.4795\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9569 - mae: 1.4363 - val_loss: 0.9965 - val_mae: 1.4559\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9504 - mae: 1.4288 - val_loss: 1.0087 - val_mae: 1.4705\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9299 - mae: 1.4083 - val_loss: 1.0175 - val_mae: 1.4765\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9415 - mae: 1.4184 - val_loss: 1.0111 - val_mae: 1.4727\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9424 - mae: 1.4215 - val_loss: 1.0027 - val_mae: 1.4620\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9351 - mae: 1.4110 - val_loss: 1.0093 - val_mae: 1.4700\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9313 - mae: 1.4098 - val_loss: 1.0194 - val_mae: 1.4837\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9348 - mae: 1.4108 - val_loss: 1.0319 - val_mae: 1.4922\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9295 - mae: 1.4065 - val_loss: 1.0309 - val_mae: 1.4917\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9450 - mae: 1.4235 - val_loss: 0.9923 - val_mae: 1.4552\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9322 - mae: 1.4062 - val_loss: 0.9895 - val_mae: 1.4492\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9324 - mae: 1.4105 - val_loss: 0.9818 - val_mae: 1.4454\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9205 - mae: 1.3960 - val_loss: 1.0093 - val_mae: 1.4638\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9293 - mae: 1.4052 - val_loss: 1.0171 - val_mae: 1.4731\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9430 - mae: 1.4193 - val_loss: 0.9874 - val_mae: 1.4467\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9141 - mae: 1.3858 - val_loss: 1.0000 - val_mae: 1.4593\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9190 - mae: 1.3956 - val_loss: 1.0085 - val_mae: 1.4678\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9308 - mae: 1.4074 - val_loss: 1.0097 - val_mae: 1.4686\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9331 - mae: 1.4111 - val_loss: 1.0284 - val_mae: 1.4901\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9456 - mae: 1.4240 - val_loss: 0.9939 - val_mae: 1.4513\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9121 - mae: 1.3866 - val_loss: 1.0069 - val_mae: 1.4650\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9366 - mae: 1.4161 - val_loss: 1.0034 - val_mae: 1.4655\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9319 - mae: 1.4088 - val_loss: 1.0396 - val_mae: 1.4995\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9189 - mae: 1.3941 - val_loss: 1.0395 - val_mae: 1.4975\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9283 - mae: 1.4032 - val_loss: 1.0204 - val_mae: 1.4773\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9379 - mae: 1.4155 - val_loss: 0.9987 - val_mae: 1.4612\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9211 - mae: 1.3959 - val_loss: 1.0107 - val_mae: 1.4676\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9294 - mae: 1.4028 - val_loss: 1.0156 - val_mae: 1.4710\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9392 - mae: 1.4142 - val_loss: 1.0346 - val_mae: 1.4873\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9310 - mae: 1.4062 - val_loss: 1.0161 - val_mae: 1.4754\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9183 - mae: 1.3903 - val_loss: 1.0069 - val_mae: 1.4579\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9218 - mae: 1.3938 - val_loss: 1.0102 - val_mae: 1.4630\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9138 - mae: 1.3872 - val_loss: 1.0172 - val_mae: 1.4651\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9042 - mae: 1.3769 - val_loss: 1.0247 - val_mae: 1.4825\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9247 - mae: 1.3985 - val_loss: 1.0171 - val_mae: 1.4754\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9136 - mae: 1.3886 - val_loss: 1.0125 - val_mae: 1.4688\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9077 - mae: 1.3804 - val_loss: 1.0162 - val_mae: 1.4715\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9161 - mae: 1.3923 - val_loss: 1.0292 - val_mae: 1.4836\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9203 - mae: 1.3903 - val_loss: 0.9757 - val_mae: 1.4290\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9137 - mae: 1.3839 - val_loss: 1.0156 - val_mae: 1.4654\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9121 - mae: 1.3842 - val_loss: 0.9981 - val_mae: 1.4524\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9155 - mae: 1.3888 - val_loss: 1.0261 - val_mae: 1.4794\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9002 - mae: 1.3714 - val_loss: 0.9915 - val_mae: 1.4432\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9080 - mae: 1.3768 - val_loss: 1.0128 - val_mae: 1.4657\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9178 - mae: 1.3881 - val_loss: 1.0138 - val_mae: 1.4666\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8966 - mae: 1.3674 - val_loss: 1.0066 - val_mae: 1.4546\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9031 - mae: 1.3707 - val_loss: 1.0080 - val_mae: 1.4601\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8999 - mae: 1.3686 - val_loss: 0.9873 - val_mae: 1.4383\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9122 - mae: 1.3904 - val_loss: 1.0228 - val_mae: 1.4727\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8957 - mae: 1.3601 - val_loss: 1.0246 - val_mae: 1.4709\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9052 - mae: 1.3731 - val_loss: 1.0054 - val_mae: 1.4579\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8998 - mae: 1.3719 - val_loss: 0.9982 - val_mae: 1.4499\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9002 - mae: 1.3687 - val_loss: 1.0093 - val_mae: 1.4583\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9091 - mae: 1.3808 - val_loss: 1.0253 - val_mae: 1.4787\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9043 - mae: 1.3775 - val_loss: 0.9995 - val_mae: 1.4520\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8887 - mae: 1.3589 - val_loss: 1.0117 - val_mae: 1.4630\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8931 - mae: 1.3593 - val_loss: 1.0330 - val_mae: 1.4812\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9303 - mae: 1.4036 - val_loss: 0.9996 - val_mae: 1.4487\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8958 - mae: 1.3655 - val_loss: 0.9935 - val_mae: 1.4418\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8949 - mae: 1.3651 - val_loss: 1.0174 - val_mae: 1.4630\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8982 - mae: 1.3685 - val_loss: 1.0235 - val_mae: 1.4796\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9033 - mae: 1.3695 - val_loss: 1.0159 - val_mae: 1.4622\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8877 - mae: 1.3564 - val_loss: 1.0206 - val_mae: 1.4638\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9025 - mae: 1.3672 - val_loss: 1.0011 - val_mae: 1.4445\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8967 - mae: 1.3631 - val_loss: 1.0051 - val_mae: 1.4516\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9035 - mae: 1.3702 - val_loss: 1.0041 - val_mae: 1.4541\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9086 - mae: 1.3783 - val_loss: 1.0160 - val_mae: 1.4704\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8898 - mae: 1.3584 - val_loss: 1.0090 - val_mae: 1.4641\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8825 - mae: 1.3486 - val_loss: 1.0003 - val_mae: 1.4498\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9039 - mae: 1.3744 - val_loss: 1.0176 - val_mae: 1.4674\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9040 - mae: 1.3744 - val_loss: 1.0044 - val_mae: 1.4525\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8777 - mae: 1.3409 - val_loss: 1.0092 - val_mae: 1.4576\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9017 - mae: 1.3699 - val_loss: 1.0037 - val_mae: 1.4580\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8892 - mae: 1.3554 - val_loss: 0.9939 - val_mae: 1.4396\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9010 - mae: 1.3681 - val_loss: 1.0285 - val_mae: 1.4752\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8875 - mae: 1.3567 - val_loss: 0.9983 - val_mae: 1.4426\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8948 - mae: 1.3644 - val_loss: 0.9700 - val_mae: 1.4142\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8789 - mae: 1.3439 - val_loss: 0.9849 - val_mae: 1.4283\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8846 - mae: 1.3521 - val_loss: 0.9819 - val_mae: 1.4241\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8826 - mae: 1.3474 - val_loss: 0.9979 - val_mae: 1.4383\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8880 - mae: 1.3525 - val_loss: 1.0109 - val_mae: 1.4516\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8800 - mae: 1.3499 - val_loss: 0.9994 - val_mae: 1.4447\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8949 - mae: 1.3643 - val_loss: 0.9984 - val_mae: 1.4398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:52:45,296] Trial 31 finished with value: 0.9983814358711243 and parameters: {'dropout_2': 0.16415596597466323, 'dropout_3': 0.8027420471497464, 'dropout_4': 0.37488010929361143, 'dropout_5': 0.19873869637597405, 'learning_rate': 0.00043222136846796084, 'epochs': 200, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 2s 6ms/step - loss: 1.7019 - mae: 2.2696 - val_loss: 1.3747 - val_mae: 1.9140\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5818 - mae: 2.1400 - val_loss: 1.3686 - val_mae: 1.9008\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5047 - mae: 2.0566 - val_loss: 1.3435 - val_mae: 1.8609\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4617 - mae: 2.0074 - val_loss: 1.3103 - val_mae: 1.8070\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3800 - mae: 1.9099 - val_loss: 1.2774 - val_mae: 1.7712\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3795 - mae: 1.9089 - val_loss: 1.2552 - val_mae: 1.7367\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3470 - mae: 1.8694 - val_loss: 1.2446 - val_mae: 1.7224\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3156 - mae: 1.8337 - val_loss: 1.2237 - val_mae: 1.6966\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3027 - mae: 1.8203 - val_loss: 1.2190 - val_mae: 1.6884\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2755 - mae: 1.7857 - val_loss: 1.2082 - val_mae: 1.6724\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2669 - mae: 1.7746 - val_loss: 1.1950 - val_mae: 1.6581\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2650 - mae: 1.7720 - val_loss: 1.1941 - val_mae: 1.6562\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2318 - mae: 1.7339 - val_loss: 1.2042 - val_mae: 1.6642\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2197 - mae: 1.7201 - val_loss: 1.1892 - val_mae: 1.6513\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2275 - mae: 1.7265 - val_loss: 1.1842 - val_mae: 1.6433\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2224 - mae: 1.7183 - val_loss: 1.1765 - val_mae: 1.6344\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1933 - mae: 1.6897 - val_loss: 1.1723 - val_mae: 1.6346\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1960 - mae: 1.6909 - val_loss: 1.1620 - val_mae: 1.6285\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1740 - mae: 1.6650 - val_loss: 1.1662 - val_mae: 1.6335\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1657 - mae: 1.6552 - val_loss: 1.1549 - val_mae: 1.6206\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1640 - mae: 1.6514 - val_loss: 1.1532 - val_mae: 1.6213\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1785 - mae: 1.6697 - val_loss: 1.1566 - val_mae: 1.6247\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1542 - mae: 1.6413 - val_loss: 1.1563 - val_mae: 1.6258\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1590 - mae: 1.6505 - val_loss: 1.1405 - val_mae: 1.6115\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1577 - mae: 1.6492 - val_loss: 1.1400 - val_mae: 1.6135\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1500 - mae: 1.6383 - val_loss: 1.1390 - val_mae: 1.6116\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1497 - mae: 1.6401 - val_loss: 1.1324 - val_mae: 1.6045\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1375 - mae: 1.6300 - val_loss: 1.1287 - val_mae: 1.6051\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1490 - mae: 1.6409 - val_loss: 1.1396 - val_mae: 1.6139\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1338 - mae: 1.6243 - val_loss: 1.1321 - val_mae: 1.6075\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1302 - mae: 1.6205 - val_loss: 1.1258 - val_mae: 1.6014\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1101 - mae: 1.5997 - val_loss: 1.1253 - val_mae: 1.6048\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1326 - mae: 1.6217 - val_loss: 1.1263 - val_mae: 1.6059\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1108 - mae: 1.6011 - val_loss: 1.1255 - val_mae: 1.6108\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1143 - mae: 1.6047 - val_loss: 1.1095 - val_mae: 1.5910\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1062 - mae: 1.5926 - val_loss: 1.1079 - val_mae: 1.5899\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1155 - mae: 1.6058 - val_loss: 1.1050 - val_mae: 1.5892\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1021 - mae: 1.5931 - val_loss: 1.1013 - val_mae: 1.5826\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0835 - mae: 1.5724 - val_loss: 1.1048 - val_mae: 1.5919\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1012 - mae: 1.5937 - val_loss: 1.1045 - val_mae: 1.5934\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1015 - mae: 1.5937 - val_loss: 1.0997 - val_mae: 1.5900\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0924 - mae: 1.5865 - val_loss: 1.1002 - val_mae: 1.5863\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1097 - mae: 1.6034 - val_loss: 1.0903 - val_mae: 1.5713\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0970 - mae: 1.5869 - val_loss: 1.0957 - val_mae: 1.5746\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0702 - mae: 1.5567 - val_loss: 1.1018 - val_mae: 1.5853\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0965 - mae: 1.5908 - val_loss: 1.0970 - val_mae: 1.5794\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0948 - mae: 1.5889 - val_loss: 1.0996 - val_mae: 1.5880\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0669 - mae: 1.5586 - val_loss: 1.0839 - val_mae: 1.5686\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0757 - mae: 1.5683 - val_loss: 1.0802 - val_mae: 1.5638\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0684 - mae: 1.5549 - val_loss: 1.0868 - val_mae: 1.5729\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0729 - mae: 1.5646 - val_loss: 1.0734 - val_mae: 1.5567\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0705 - mae: 1.5624 - val_loss: 1.0928 - val_mae: 1.5825\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0804 - mae: 1.5775 - val_loss: 1.0832 - val_mae: 1.5699\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0818 - mae: 1.5781 - val_loss: 1.0727 - val_mae: 1.5589\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0800 - mae: 1.5684 - val_loss: 1.0821 - val_mae: 1.5692\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0733 - mae: 1.5677 - val_loss: 1.0685 - val_mae: 1.5536\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0745 - mae: 1.5691 - val_loss: 1.0725 - val_mae: 1.5553\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0606 - mae: 1.5558 - val_loss: 1.0775 - val_mae: 1.5633\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0663 - mae: 1.5590 - val_loss: 1.0909 - val_mae: 1.5783\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0543 - mae: 1.5475 - val_loss: 1.0762 - val_mae: 1.5627\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0715 - mae: 1.5652 - val_loss: 1.0798 - val_mae: 1.5653\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0542 - mae: 1.5474 - val_loss: 1.0656 - val_mae: 1.5529\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0580 - mae: 1.5536 - val_loss: 1.0638 - val_mae: 1.5502\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0489 - mae: 1.5429 - val_loss: 1.0646 - val_mae: 1.5541\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0483 - mae: 1.5440 - val_loss: 1.0737 - val_mae: 1.5661\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0534 - mae: 1.5467 - val_loss: 1.0554 - val_mae: 1.5414\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0414 - mae: 1.5310 - val_loss: 1.0514 - val_mae: 1.5381\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0457 - mae: 1.5356 - val_loss: 1.0475 - val_mae: 1.5348\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0433 - mae: 1.5359 - val_loss: 1.0592 - val_mae: 1.5504\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0463 - mae: 1.5383 - val_loss: 1.0564 - val_mae: 1.5467\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0434 - mae: 1.5395 - val_loss: 1.0502 - val_mae: 1.5344\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0500 - mae: 1.5411 - val_loss: 1.0550 - val_mae: 1.5395\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0394 - mae: 1.5326 - val_loss: 1.0517 - val_mae: 1.5393\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0516 - mae: 1.5485 - val_loss: 1.0504 - val_mae: 1.5429\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0352 - mae: 1.5268 - val_loss: 1.0616 - val_mae: 1.5539\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0339 - mae: 1.5300 - val_loss: 1.0430 - val_mae: 1.5341\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0409 - mae: 1.5346 - val_loss: 1.0434 - val_mae: 1.5342\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0257 - mae: 1.5161 - val_loss: 1.0384 - val_mae: 1.5282\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0412 - mae: 1.5372 - val_loss: 1.0491 - val_mae: 1.5399\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0309 - mae: 1.5256 - val_loss: 1.0432 - val_mae: 1.5305\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0339 - mae: 1.5285 - val_loss: 1.0488 - val_mae: 1.5382\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0260 - mae: 1.5193 - val_loss: 1.0395 - val_mae: 1.5244\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0307 - mae: 1.5233 - val_loss: 1.0457 - val_mae: 1.5312\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0188 - mae: 1.5099 - val_loss: 1.0425 - val_mae: 1.5258\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0151 - mae: 1.5041 - val_loss: 1.0332 - val_mae: 1.5199\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0284 - mae: 1.5194 - val_loss: 1.0386 - val_mae: 1.5273\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0170 - mae: 1.5099 - val_loss: 1.0301 - val_mae: 1.5156\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0241 - mae: 1.5125 - val_loss: 1.0421 - val_mae: 1.5345\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0276 - mae: 1.5157 - val_loss: 1.0302 - val_mae: 1.5146\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0378 - mae: 1.5348 - val_loss: 1.0175 - val_mae: 1.5007\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0023 - mae: 1.4926 - val_loss: 1.0255 - val_mae: 1.5118\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0095 - mae: 1.4986 - val_loss: 1.0347 - val_mae: 1.5214\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0034 - mae: 1.4945 - val_loss: 1.0195 - val_mae: 1.5048\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0083 - mae: 1.4997 - val_loss: 1.0273 - val_mae: 1.5163\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0173 - mae: 1.5071 - val_loss: 1.0245 - val_mae: 1.5109\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0091 - mae: 1.5021 - val_loss: 1.0257 - val_mae: 1.5111\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9987 - mae: 1.4910 - val_loss: 1.0221 - val_mae: 1.5107\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0072 - mae: 1.4984 - val_loss: 1.0291 - val_mae: 1.5177\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0064 - mae: 1.4988 - val_loss: 1.0285 - val_mae: 1.5163\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0130 - mae: 1.5051 - val_loss: 1.0390 - val_mae: 1.5301\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0083 - mae: 1.5012 - val_loss: 1.0231 - val_mae: 1.5092\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0005 - mae: 1.4870 - val_loss: 1.0209 - val_mae: 1.5086\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0015 - mae: 1.4897 - val_loss: 1.0211 - val_mae: 1.5118\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9965 - mae: 1.4868 - val_loss: 1.0146 - val_mae: 1.4969\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9993 - mae: 1.4905 - val_loss: 1.0130 - val_mae: 1.4937\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9942 - mae: 1.4849 - val_loss: 1.0128 - val_mae: 1.4956\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0031 - mae: 1.4930 - val_loss: 1.0205 - val_mae: 1.5064\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9976 - mae: 1.4932 - val_loss: 1.0098 - val_mae: 1.4942\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9957 - mae: 1.4868 - val_loss: 1.0086 - val_mae: 1.4905\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9986 - mae: 1.4869 - val_loss: 1.0194 - val_mae: 1.5018\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9906 - mae: 1.4814 - val_loss: 1.0184 - val_mae: 1.5087\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0007 - mae: 1.4932 - val_loss: 1.0094 - val_mae: 1.4933\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9840 - mae: 1.4724 - val_loss: 1.0155 - val_mae: 1.4992\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9875 - mae: 1.4772 - val_loss: 1.0202 - val_mae: 1.5084\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0103 - mae: 1.4986 - val_loss: 1.0198 - val_mae: 1.5069\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0072 - mae: 1.4957 - val_loss: 1.0228 - val_mae: 1.5083\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9959 - mae: 1.4857 - val_loss: 1.0180 - val_mae: 1.4961\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9760 - mae: 1.4639 - val_loss: 1.0067 - val_mae: 1.4868\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9966 - mae: 1.4890 - val_loss: 1.0214 - val_mae: 1.5074\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9793 - mae: 1.4669 - val_loss: 1.0094 - val_mae: 1.4940\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9941 - mae: 1.4856 - val_loss: 1.0226 - val_mae: 1.5048\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9892 - mae: 1.4793 - val_loss: 1.0143 - val_mae: 1.4987\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0132 - mae: 1.5055 - val_loss: 1.0040 - val_mae: 1.4808\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9899 - mae: 1.4743 - val_loss: 0.9966 - val_mae: 1.4789\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9858 - mae: 1.4728 - val_loss: 0.9921 - val_mae: 1.4704\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9802 - mae: 1.4668 - val_loss: 1.0062 - val_mae: 1.4883\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9743 - mae: 1.4593 - val_loss: 0.9978 - val_mae: 1.4738\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9943 - mae: 1.4823 - val_loss: 1.0124 - val_mae: 1.4983\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9745 - mae: 1.4628 - val_loss: 0.9978 - val_mae: 1.4788\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9762 - mae: 1.4663 - val_loss: 0.9985 - val_mae: 1.4759\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9878 - mae: 1.4782 - val_loss: 1.0039 - val_mae: 1.4860\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9750 - mae: 1.4622 - val_loss: 0.9965 - val_mae: 1.4730\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9791 - mae: 1.4702 - val_loss: 0.9860 - val_mae: 1.4592\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9921 - mae: 1.4820 - val_loss: 0.9970 - val_mae: 1.4739\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9839 - mae: 1.4698 - val_loss: 1.0006 - val_mae: 1.4765\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9712 - mae: 1.4574 - val_loss: 0.9906 - val_mae: 1.4658\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9793 - mae: 1.4629 - val_loss: 0.9988 - val_mae: 1.4747\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9755 - mae: 1.4645 - val_loss: 1.0049 - val_mae: 1.4772\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9887 - mae: 1.4738 - val_loss: 0.9994 - val_mae: 1.4718\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9803 - mae: 1.4717 - val_loss: 1.0141 - val_mae: 1.4958\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9790 - mae: 1.4647 - val_loss: 1.0049 - val_mae: 1.4827\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9672 - mae: 1.4505 - val_loss: 0.9973 - val_mae: 1.4744\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9795 - mae: 1.4640 - val_loss: 0.9968 - val_mae: 1.4722\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9792 - mae: 1.4610 - val_loss: 0.9924 - val_mae: 1.4686\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9713 - mae: 1.4611 - val_loss: 0.9960 - val_mae: 1.4681\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9553 - mae: 1.4410 - val_loss: 0.9946 - val_mae: 1.4744\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9813 - mae: 1.4676 - val_loss: 0.9964 - val_mae: 1.4772\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9600 - mae: 1.4456 - val_loss: 1.0020 - val_mae: 1.4792\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9567 - mae: 1.4408 - val_loss: 1.0018 - val_mae: 1.4835\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9672 - mae: 1.4522 - val_loss: 0.9826 - val_mae: 1.4536\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9809 - mae: 1.4652 - val_loss: 0.9997 - val_mae: 1.4741\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9706 - mae: 1.4560 - val_loss: 0.9943 - val_mae: 1.4722\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9656 - mae: 1.4503 - val_loss: 0.9822 - val_mae: 1.4543\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9632 - mae: 1.4537 - val_loss: 0.9879 - val_mae: 1.4597\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9732 - mae: 1.4603 - val_loss: 0.9938 - val_mae: 1.4737\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9598 - mae: 1.4437 - val_loss: 0.9923 - val_mae: 1.4661\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9599 - mae: 1.4458 - val_loss: 0.9966 - val_mae: 1.4689\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9633 - mae: 1.4467 - val_loss: 0.9958 - val_mae: 1.4672\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9524 - mae: 1.4338 - val_loss: 1.0048 - val_mae: 1.4826\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9644 - mae: 1.4487 - val_loss: 1.0061 - val_mae: 1.4793\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9745 - mae: 1.4615 - val_loss: 0.9967 - val_mae: 1.4673\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9780 - mae: 1.4619 - val_loss: 0.9991 - val_mae: 1.4720\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9647 - mae: 1.4496 - val_loss: 0.9991 - val_mae: 1.4771\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9704 - mae: 1.4527 - val_loss: 0.9981 - val_mae: 1.4658\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9477 - mae: 1.4324 - val_loss: 1.0038 - val_mae: 1.4801\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9702 - mae: 1.4553 - val_loss: 0.9973 - val_mae: 1.4716\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9562 - mae: 1.4442 - val_loss: 0.9831 - val_mae: 1.4495\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9508 - mae: 1.4344 - val_loss: 1.0031 - val_mae: 1.4731\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9668 - mae: 1.4513 - val_loss: 0.9819 - val_mae: 1.4515\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9529 - mae: 1.4402 - val_loss: 0.9944 - val_mae: 1.4643\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9529 - mae: 1.4360 - val_loss: 0.9952 - val_mae: 1.4619\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9578 - mae: 1.4418 - val_loss: 0.9898 - val_mae: 1.4600\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9522 - mae: 1.4352 - val_loss: 0.9949 - val_mae: 1.4642\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9398 - mae: 1.4215 - val_loss: 1.0011 - val_mae: 1.4799\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9552 - mae: 1.4417 - val_loss: 0.9899 - val_mae: 1.4644\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9477 - mae: 1.4269 - val_loss: 0.9854 - val_mae: 1.4553\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9465 - mae: 1.4261 - val_loss: 0.9950 - val_mae: 1.4692\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9588 - mae: 1.4426 - val_loss: 0.9863 - val_mae: 1.4578\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9452 - mae: 1.4263 - val_loss: 0.9844 - val_mae: 1.4554\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9338 - mae: 1.4145 - val_loss: 0.9880 - val_mae: 1.4586\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9483 - mae: 1.4344 - val_loss: 0.9897 - val_mae: 1.4587\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9407 - mae: 1.4218 - val_loss: 0.9826 - val_mae: 1.4503\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9435 - mae: 1.4251 - val_loss: 0.9862 - val_mae: 1.4585\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9496 - mae: 1.4308 - val_loss: 1.0061 - val_mae: 1.4788\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9374 - mae: 1.4179 - val_loss: 0.9827 - val_mae: 1.4473\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9527 - mae: 1.4374 - val_loss: 0.9821 - val_mae: 1.4540\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9503 - mae: 1.4308 - val_loss: 0.9745 - val_mae: 1.4392\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9321 - mae: 1.4102 - val_loss: 0.9863 - val_mae: 1.4539\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9463 - mae: 1.4303 - val_loss: 0.9779 - val_mae: 1.4436\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9442 - mae: 1.4249 - val_loss: 1.0036 - val_mae: 1.4719\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9464 - mae: 1.4271 - val_loss: 0.9898 - val_mae: 1.4567\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9306 - mae: 1.4082 - val_loss: 0.9881 - val_mae: 1.4541\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9369 - mae: 1.4183 - val_loss: 0.9910 - val_mae: 1.4582\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9291 - mae: 1.4083 - val_loss: 0.9900 - val_mae: 1.4553\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9493 - mae: 1.4305 - val_loss: 0.9799 - val_mae: 1.4426\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9511 - mae: 1.4300 - val_loss: 0.9849 - val_mae: 1.4470\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9571 - mae: 1.4416 - val_loss: 1.0007 - val_mae: 1.4725\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9383 - mae: 1.4189 - val_loss: 0.9949 - val_mae: 1.4616\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9307 - mae: 1.4059 - val_loss: 0.9923 - val_mae: 1.4585\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9456 - mae: 1.4274 - val_loss: 0.9935 - val_mae: 1.4641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:55:09,307] Trial 32 finished with value: 0.9934736490249634 and parameters: {'dropout_2': 0.1132668297999947, 'dropout_3': 0.8839145477145939, 'dropout_4': 0.497684247674, 'dropout_5': 0.28408498096395635, 'learning_rate': 0.00040974315080858536, 'epochs': 200, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 3s 7ms/step - loss: 1.6048 - mae: 2.1655 - val_loss: 1.3183 - val_mae: 1.7777\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4019 - mae: 1.9422 - val_loss: 1.2744 - val_mae: 1.7264\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3104 - mae: 1.8418 - val_loss: 1.2428 - val_mae: 1.7159\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2669 - mae: 1.7841 - val_loss: 1.2160 - val_mae: 1.7087\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.2111 - mae: 1.7218 - val_loss: 1.1899 - val_mae: 1.6812\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1798 - mae: 1.6837 - val_loss: 1.1885 - val_mae: 1.6864\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1689 - mae: 1.6694 - val_loss: 1.1495 - val_mae: 1.6308\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1453 - mae: 1.6435 - val_loss: 1.1431 - val_mae: 1.6297\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1262 - mae: 1.6213 - val_loss: 1.1394 - val_mae: 1.6310\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1215 - mae: 1.6160 - val_loss: 1.1294 - val_mae: 1.6219\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1055 - mae: 1.5965 - val_loss: 1.1467 - val_mae: 1.6431\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1150 - mae: 1.6095 - val_loss: 1.1084 - val_mae: 1.5951\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1029 - mae: 1.5923 - val_loss: 1.1245 - val_mae: 1.6177\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0882 - mae: 1.5743 - val_loss: 1.1276 - val_mae: 1.6186\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0914 - mae: 1.5876 - val_loss: 1.0987 - val_mae: 1.5881\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0717 - mae: 1.5652 - val_loss: 1.1013 - val_mae: 1.5841\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0786 - mae: 1.5730 - val_loss: 1.0993 - val_mae: 1.5884\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0771 - mae: 1.5723 - val_loss: 1.1023 - val_mae: 1.6015\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0668 - mae: 1.5604 - val_loss: 1.1402 - val_mae: 1.6425\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0615 - mae: 1.5523 - val_loss: 1.0959 - val_mae: 1.5942\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0604 - mae: 1.5505 - val_loss: 1.1089 - val_mae: 1.6065\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0674 - mae: 1.5597 - val_loss: 1.1228 - val_mae: 1.6267\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0560 - mae: 1.5484 - val_loss: 1.1096 - val_mae: 1.6132\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0662 - mae: 1.5610 - val_loss: 1.0701 - val_mae: 1.5605\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0574 - mae: 1.5479 - val_loss: 1.0729 - val_mae: 1.5652\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0793 - mae: 1.5771 - val_loss: 1.0658 - val_mae: 1.5489\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0487 - mae: 1.5422 - val_loss: 1.0744 - val_mae: 1.5662\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0354 - mae: 1.5292 - val_loss: 1.0863 - val_mae: 1.5844\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0493 - mae: 1.5422 - val_loss: 1.0558 - val_mae: 1.5380\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0437 - mae: 1.5357 - val_loss: 1.0536 - val_mae: 1.5330\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0413 - mae: 1.5309 - val_loss: 1.0602 - val_mae: 1.5522\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0233 - mae: 1.5123 - val_loss: 1.0610 - val_mae: 1.5542\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0404 - mae: 1.5295 - val_loss: 1.0725 - val_mae: 1.5642\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0436 - mae: 1.5340 - val_loss: 1.0678 - val_mae: 1.5513\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0346 - mae: 1.5247 - val_loss: 1.0561 - val_mae: 1.5403\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0281 - mae: 1.5155 - val_loss: 1.0590 - val_mae: 1.5490\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0344 - mae: 1.5234 - val_loss: 1.0448 - val_mae: 1.5331\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0257 - mae: 1.5101 - val_loss: 1.0514 - val_mae: 1.5370\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0338 - mae: 1.5226 - val_loss: 1.0822 - val_mae: 1.5693\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0361 - mae: 1.5219 - val_loss: 1.0564 - val_mae: 1.5435\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0140 - mae: 1.4975 - val_loss: 1.0341 - val_mae: 1.5161\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0200 - mae: 1.5098 - val_loss: 1.0539 - val_mae: 1.5420\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0255 - mae: 1.5139 - val_loss: 1.0616 - val_mae: 1.5435\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0174 - mae: 1.5008 - val_loss: 1.0456 - val_mae: 1.5299\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0160 - mae: 1.5015 - val_loss: 1.0439 - val_mae: 1.5291\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0218 - mae: 1.5093 - val_loss: 1.0471 - val_mae: 1.5285\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0240 - mae: 1.5111 - val_loss: 1.0473 - val_mae: 1.5270\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0166 - mae: 1.5037 - val_loss: 1.0375 - val_mae: 1.5254\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0169 - mae: 1.5050 - val_loss: 1.0557 - val_mae: 1.5416\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0109 - mae: 1.4954 - val_loss: 1.0545 - val_mae: 1.5308\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0073 - mae: 1.4912 - val_loss: 1.0358 - val_mae: 1.5052\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9942 - mae: 1.4833 - val_loss: 1.0375 - val_mae: 1.5176\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9861 - mae: 1.4697 - val_loss: 1.0340 - val_mae: 1.5143\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0071 - mae: 1.4922 - val_loss: 1.0703 - val_mae: 1.5434\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0065 - mae: 1.4912 - val_loss: 1.0378 - val_mae: 1.5190\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0084 - mae: 1.4942 - val_loss: 1.0365 - val_mae: 1.5112\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0126 - mae: 1.4985 - val_loss: 1.0109 - val_mae: 1.4897\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9971 - mae: 1.4786 - val_loss: 1.0012 - val_mae: 1.4778\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9940 - mae: 1.4774 - val_loss: 1.0197 - val_mae: 1.4981\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9932 - mae: 1.4759 - val_loss: 1.0188 - val_mae: 1.5078\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9868 - mae: 1.4703 - val_loss: 1.0131 - val_mae: 1.4954\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0070 - mae: 1.4907 - val_loss: 1.0352 - val_mae: 1.5068\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9974 - mae: 1.4808 - val_loss: 1.0341 - val_mae: 1.5086\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0085 - mae: 1.4948 - val_loss: 1.0324 - val_mae: 1.5177\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9956 - mae: 1.4789 - val_loss: 1.0170 - val_mae: 1.4896\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0152 - mae: 1.5005 - val_loss: 1.0437 - val_mae: 1.5200\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0007 - mae: 1.4863 - val_loss: 1.0221 - val_mae: 1.5005\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9755 - mae: 1.4544 - val_loss: 1.0270 - val_mae: 1.4930\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9823 - mae: 1.4659 - val_loss: 1.0389 - val_mae: 1.5070\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9964 - mae: 1.4843 - val_loss: 1.0232 - val_mae: 1.4853\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0080 - mae: 1.4913 - val_loss: 1.0171 - val_mae: 1.4933\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9896 - mae: 1.4744 - val_loss: 1.0296 - val_mae: 1.5055\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9903 - mae: 1.4723 - val_loss: 1.0217 - val_mae: 1.4984\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9867 - mae: 1.4699 - val_loss: 1.0265 - val_mae: 1.5050\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9790 - mae: 1.4633 - val_loss: 1.0202 - val_mae: 1.4919\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9963 - mae: 1.4845 - val_loss: 1.0366 - val_mae: 1.5123\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9663 - mae: 1.4467 - val_loss: 0.9941 - val_mae: 1.4591\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9730 - mae: 1.4516 - val_loss: 1.0019 - val_mae: 1.4723\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9885 - mae: 1.4716 - val_loss: 1.0046 - val_mae: 1.4893\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9782 - mae: 1.4607 - val_loss: 1.0227 - val_mae: 1.5031\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9880 - mae: 1.4725 - val_loss: 1.0131 - val_mae: 1.4960\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9724 - mae: 1.4530 - val_loss: 0.9952 - val_mae: 1.4650\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9744 - mae: 1.4537 - val_loss: 0.9932 - val_mae: 1.4627\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9841 - mae: 1.4649 - val_loss: 1.0216 - val_mae: 1.5021\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9855 - mae: 1.4680 - val_loss: 0.9888 - val_mae: 1.4640\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9818 - mae: 1.4652 - val_loss: 1.0039 - val_mae: 1.4801\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9710 - mae: 1.4499 - val_loss: 0.9842 - val_mae: 1.4503\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9721 - mae: 1.4508 - val_loss: 0.9940 - val_mae: 1.4557\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9623 - mae: 1.4400 - val_loss: 0.9837 - val_mae: 1.4524\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9835 - mae: 1.4615 - val_loss: 0.9841 - val_mae: 1.4499\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9752 - mae: 1.4558 - val_loss: 1.0032 - val_mae: 1.4810\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9595 - mae: 1.4408 - val_loss: 1.0077 - val_mae: 1.4807\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9572 - mae: 1.4341 - val_loss: 1.0361 - val_mae: 1.5092\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9664 - mae: 1.4467 - val_loss: 0.9895 - val_mae: 1.4558\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9715 - mae: 1.4520 - val_loss: 0.9880 - val_mae: 1.4587\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9740 - mae: 1.4539 - val_loss: 0.9879 - val_mae: 1.4565\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9691 - mae: 1.4474 - val_loss: 0.9935 - val_mae: 1.4667\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9486 - mae: 1.4293 - val_loss: 0.9739 - val_mae: 1.4402\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9449 - mae: 1.4242 - val_loss: 0.9930 - val_mae: 1.4625\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9633 - mae: 1.4396 - val_loss: 1.0105 - val_mae: 1.4805\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9551 - mae: 1.4348 - val_loss: 0.9592 - val_mae: 1.4249\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9784 - mae: 1.4616 - val_loss: 0.9969 - val_mae: 1.4640\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9643 - mae: 1.4432 - val_loss: 1.0448 - val_mae: 1.5302\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9556 - mae: 1.4375 - val_loss: 1.0064 - val_mae: 1.4843\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9644 - mae: 1.4471 - val_loss: 1.0247 - val_mae: 1.5011\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9680 - mae: 1.4419 - val_loss: 0.9856 - val_mae: 1.4555\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9490 - mae: 1.4259 - val_loss: 1.0011 - val_mae: 1.4622\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9567 - mae: 1.4380 - val_loss: 0.9767 - val_mae: 1.4493\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9667 - mae: 1.4456 - val_loss: 0.9874 - val_mae: 1.4618\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9547 - mae: 1.4343 - val_loss: 1.0071 - val_mae: 1.4792\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9347 - mae: 1.4092 - val_loss: 0.9789 - val_mae: 1.4492\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9459 - mae: 1.4189 - val_loss: 0.9922 - val_mae: 1.4699\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9413 - mae: 1.4152 - val_loss: 0.9847 - val_mae: 1.4485\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9586 - mae: 1.4357 - val_loss: 0.9953 - val_mae: 1.4669\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9473 - mae: 1.4221 - val_loss: 0.9742 - val_mae: 1.4445\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9536 - mae: 1.4292 - val_loss: 0.9699 - val_mae: 1.4350\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9319 - mae: 1.4083 - val_loss: 0.9814 - val_mae: 1.4507\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9666 - mae: 1.4500 - val_loss: 0.9553 - val_mae: 1.4259\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9472 - mae: 1.4223 - val_loss: 1.0113 - val_mae: 1.4789\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9629 - mae: 1.4428 - val_loss: 0.9787 - val_mae: 1.4547\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9278 - mae: 1.3986 - val_loss: 0.9712 - val_mae: 1.4317\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9356 - mae: 1.4093 - val_loss: 0.9544 - val_mae: 1.4193\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9425 - mae: 1.4199 - val_loss: 0.9744 - val_mae: 1.4439\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9512 - mae: 1.4239 - val_loss: 0.9579 - val_mae: 1.4220\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9352 - mae: 1.4096 - val_loss: 0.9816 - val_mae: 1.4460\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9391 - mae: 1.4142 - val_loss: 0.9759 - val_mae: 1.4474\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9473 - mae: 1.4222 - val_loss: 0.9714 - val_mae: 1.4381\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9466 - mae: 1.4234 - val_loss: 0.9727 - val_mae: 1.4384\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9448 - mae: 1.4206 - val_loss: 0.9866 - val_mae: 1.4544\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9430 - mae: 1.4181 - val_loss: 0.9665 - val_mae: 1.4319\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9357 - mae: 1.4117 - val_loss: 0.9935 - val_mae: 1.4581\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9228 - mae: 1.3958 - val_loss: 0.9581 - val_mae: 1.4247\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9378 - mae: 1.4138 - val_loss: 0.9767 - val_mae: 1.4390\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9359 - mae: 1.4093 - val_loss: 0.9757 - val_mae: 1.4363\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9312 - mae: 1.4043 - val_loss: 0.9539 - val_mae: 1.4196\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9478 - mae: 1.4256 - val_loss: 0.9814 - val_mae: 1.4612\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9353 - mae: 1.4118 - val_loss: 0.9681 - val_mae: 1.4326\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9201 - mae: 1.3923 - val_loss: 0.9666 - val_mae: 1.4301\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9225 - mae: 1.3937 - val_loss: 0.9787 - val_mae: 1.4452\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9250 - mae: 1.3988 - val_loss: 0.9693 - val_mae: 1.4371\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9492 - mae: 1.4255 - val_loss: 0.9738 - val_mae: 1.4449\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9388 - mae: 1.4144 - val_loss: 0.9773 - val_mae: 1.4371\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9374 - mae: 1.4151 - val_loss: 0.9770 - val_mae: 1.4420\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9301 - mae: 1.3998 - val_loss: 0.9550 - val_mae: 1.4145\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9391 - mae: 1.4097 - val_loss: 0.9810 - val_mae: 1.4371\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9318 - mae: 1.4060 - val_loss: 1.0039 - val_mae: 1.4700\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9320 - mae: 1.4042 - val_loss: 0.9639 - val_mae: 1.4194\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9269 - mae: 1.4009 - val_loss: 0.9918 - val_mae: 1.4449\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9167 - mae: 1.3893 - val_loss: 0.9979 - val_mae: 1.4602\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9247 - mae: 1.3942 - val_loss: 0.9435 - val_mae: 1.4074\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9285 - mae: 1.3998 - val_loss: 0.9742 - val_mae: 1.4431\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9351 - mae: 1.4096 - val_loss: 0.9936 - val_mae: 1.4614\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9460 - mae: 1.4204 - val_loss: 0.9858 - val_mae: 1.4499\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9376 - mae: 1.4091 - val_loss: 0.9858 - val_mae: 1.4483\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9262 - mae: 1.3978 - val_loss: 0.9567 - val_mae: 1.4201\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9216 - mae: 1.3985 - val_loss: 0.9673 - val_mae: 1.4358\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9318 - mae: 1.4074 - val_loss: 0.9509 - val_mae: 1.4069\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9323 - mae: 1.4067 - val_loss: 0.9638 - val_mae: 1.4284\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9142 - mae: 1.3876 - val_loss: 0.9698 - val_mae: 1.4283\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9414 - mae: 1.4138 - val_loss: 0.9850 - val_mae: 1.4468\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9164 - mae: 1.3867 - val_loss: 0.9855 - val_mae: 1.4508\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9276 - mae: 1.3990 - val_loss: 0.9846 - val_mae: 1.4507\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9170 - mae: 1.3877 - val_loss: 0.9780 - val_mae: 1.4449\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9232 - mae: 1.3902 - val_loss: 0.9991 - val_mae: 1.4605\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9163 - mae: 1.3856 - val_loss: 0.9917 - val_mae: 1.4584\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9270 - mae: 1.4014 - val_loss: 0.9994 - val_mae: 1.4730\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9064 - mae: 1.3781 - val_loss: 1.0069 - val_mae: 1.4753\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9026 - mae: 1.3751 - val_loss: 0.9775 - val_mae: 1.4429\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9292 - mae: 1.4011 - val_loss: 1.0070 - val_mae: 1.4845\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9306 - mae: 1.4070 - val_loss: 1.0003 - val_mae: 1.4582\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9292 - mae: 1.4031 - val_loss: 0.9856 - val_mae: 1.4528\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9162 - mae: 1.3866 - val_loss: 0.9752 - val_mae: 1.4335\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9264 - mae: 1.3979 - val_loss: 0.9869 - val_mae: 1.4405\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9157 - mae: 1.3855 - val_loss: 1.0020 - val_mae: 1.4637\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9273 - mae: 1.3997 - val_loss: 0.9880 - val_mae: 1.4559\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9185 - mae: 1.3895 - val_loss: 0.9435 - val_mae: 1.4028\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9166 - mae: 1.3823 - val_loss: 0.9667 - val_mae: 1.4296\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9066 - mae: 1.3740 - val_loss: 0.9730 - val_mae: 1.4349\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9127 - mae: 1.3855 - val_loss: 0.9601 - val_mae: 1.4170\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9288 - mae: 1.3991 - val_loss: 0.9986 - val_mae: 1.4538\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9254 - mae: 1.3967 - val_loss: 0.9729 - val_mae: 1.4353\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9377 - mae: 1.4120 - val_loss: 0.9775 - val_mae: 1.4397\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9270 - mae: 1.3981 - val_loss: 0.9823 - val_mae: 1.4414\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9122 - mae: 1.3841 - val_loss: 0.9744 - val_mae: 1.4331\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9184 - mae: 1.3916 - val_loss: 0.9831 - val_mae: 1.4312\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9309 - mae: 1.3999 - val_loss: 0.9843 - val_mae: 1.4434\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9116 - mae: 1.3841 - val_loss: 0.9776 - val_mae: 1.4339\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9039 - mae: 1.3726 - val_loss: 0.9783 - val_mae: 1.4329\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9256 - mae: 1.3947 - val_loss: 0.9924 - val_mae: 1.4493\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9217 - mae: 1.3912 - val_loss: 0.9718 - val_mae: 1.4307\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9118 - mae: 1.3813 - val_loss: 0.9786 - val_mae: 1.4431\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9054 - mae: 1.3770 - val_loss: 0.9845 - val_mae: 1.4408\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9276 - mae: 1.3990 - val_loss: 0.9918 - val_mae: 1.4621\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9335 - mae: 1.4087 - val_loss: 0.9992 - val_mae: 1.4770\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9183 - mae: 1.3877 - val_loss: 0.9685 - val_mae: 1.4304\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9262 - mae: 1.3958 - val_loss: 0.9593 - val_mae: 1.4264\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9128 - mae: 1.3801 - val_loss: 0.9934 - val_mae: 1.4703\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9301 - mae: 1.4025 - val_loss: 0.9764 - val_mae: 1.4425\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9180 - mae: 1.3933 - val_loss: 0.9629 - val_mae: 1.4237\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9081 - mae: 1.3826 - val_loss: 0.9748 - val_mae: 1.4264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:57:33,323] Trial 33 finished with value: 0.9747785329818726 and parameters: {'dropout_2': 0.2602961366718497, 'dropout_3': 0.8078229711636163, 'dropout_4': 0.3113812678257235, 'dropout_5': 0.42920084490551835, 'learning_rate': 0.0011188715287461714, 'epochs': 200, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "27/27 [==============================] - 2s 16ms/step - loss: 1.3472 - mae: 1.8660 - val_loss: 1.2830 - val_mae: 1.7679\n",
            "Epoch 2/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.1933 - mae: 1.6877 - val_loss: 1.2480 - val_mae: 1.7044\n",
            "Epoch 3/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1662 - mae: 1.6557 - val_loss: 1.2035 - val_mae: 1.6563\n",
            "Epoch 4/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1216 - mae: 1.6089 - val_loss: 1.1677 - val_mae: 1.6297\n",
            "Epoch 5/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1226 - mae: 1.6123 - val_loss: 1.1580 - val_mae: 1.6200\n",
            "Epoch 6/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1015 - mae: 1.5883 - val_loss: 1.1414 - val_mae: 1.6193\n",
            "Epoch 7/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0830 - mae: 1.5651 - val_loss: 1.1584 - val_mae: 1.6513\n",
            "Epoch 8/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0776 - mae: 1.5627 - val_loss: 1.1598 - val_mae: 1.6402\n",
            "Epoch 9/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0696 - mae: 1.5555 - val_loss: 1.1478 - val_mae: 1.6399\n",
            "Epoch 10/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0586 - mae: 1.5476 - val_loss: 1.1007 - val_mae: 1.5909\n",
            "Epoch 11/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0515 - mae: 1.5374 - val_loss: 1.1188 - val_mae: 1.6145\n",
            "Epoch 12/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0591 - mae: 1.5485 - val_loss: 1.0943 - val_mae: 1.5762\n",
            "Epoch 13/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0361 - mae: 1.5210 - val_loss: 1.0799 - val_mae: 1.5704\n",
            "Epoch 14/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0424 - mae: 1.5299 - val_loss: 1.0958 - val_mae: 1.5945\n",
            "Epoch 15/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0397 - mae: 1.5240 - val_loss: 1.0923 - val_mae: 1.5837\n",
            "Epoch 16/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0390 - mae: 1.5259 - val_loss: 1.0625 - val_mae: 1.5562\n",
            "Epoch 17/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0333 - mae: 1.5206 - val_loss: 1.0477 - val_mae: 1.5483\n",
            "Epoch 18/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0205 - mae: 1.5072 - val_loss: 1.0426 - val_mae: 1.5398\n",
            "Epoch 19/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0160 - mae: 1.4982 - val_loss: 1.0480 - val_mae: 1.5476\n",
            "Epoch 20/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0239 - mae: 1.5109 - val_loss: 1.0655 - val_mae: 1.5590\n",
            "Epoch 21/200\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0217 - mae: 1.5046 - val_loss: 1.0395 - val_mae: 1.5368\n",
            "Epoch 22/200\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0049 - mae: 1.4875 - val_loss: 1.0637 - val_mae: 1.5682\n",
            "Epoch 23/200\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0226 - mae: 1.5054 - val_loss: 1.0768 - val_mae: 1.5710\n",
            "Epoch 24/200\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0069 - mae: 1.4933 - val_loss: 1.0699 - val_mae: 1.5605\n",
            "Epoch 25/200\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0254 - mae: 1.5121 - val_loss: 1.0583 - val_mae: 1.5534\n",
            "Epoch 26/200\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0077 - mae: 1.4888 - val_loss: 1.0344 - val_mae: 1.5069\n",
            "Epoch 27/200\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0100 - mae: 1.4951 - val_loss: 1.0895 - val_mae: 1.5841\n",
            "Epoch 28/200\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0101 - mae: 1.4925 - val_loss: 1.0366 - val_mae: 1.5078\n",
            "Epoch 29/200\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0106 - mae: 1.4921 - val_loss: 1.0572 - val_mae: 1.5218\n",
            "Epoch 30/200\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0076 - mae: 1.4878 - val_loss: 1.0405 - val_mae: 1.5213\n",
            "Epoch 31/200\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.9993 - mae: 1.4792 - val_loss: 1.0398 - val_mae: 1.5194\n",
            "Epoch 32/200\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9962 - mae: 1.4750 - val_loss: 1.0253 - val_mae: 1.4975\n",
            "Epoch 33/200\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0111 - mae: 1.4918 - val_loss: 1.0262 - val_mae: 1.4933\n",
            "Epoch 34/200\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0070 - mae: 1.4876 - val_loss: 1.0265 - val_mae: 1.5006\n",
            "Epoch 35/200\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9820 - mae: 1.4603 - val_loss: 1.0151 - val_mae: 1.4855\n",
            "Epoch 36/200\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9934 - mae: 1.4748 - val_loss: 1.0168 - val_mae: 1.4992\n",
            "Epoch 37/200\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0073 - mae: 1.4900 - val_loss: 1.0163 - val_mae: 1.4954\n",
            "Epoch 38/200\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9803 - mae: 1.4616 - val_loss: 1.0206 - val_mae: 1.4909\n",
            "Epoch 39/200\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9974 - mae: 1.4781 - val_loss: 1.0565 - val_mae: 1.5406\n",
            "Epoch 40/200\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9752 - mae: 1.4526 - val_loss: 1.0101 - val_mae: 1.4928\n",
            "Epoch 41/200\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9803 - mae: 1.4611 - val_loss: 1.0476 - val_mae: 1.5114\n",
            "Epoch 42/200\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9730 - mae: 1.4548 - val_loss: 1.0570 - val_mae: 1.5401\n",
            "Epoch 43/200\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9890 - mae: 1.4677 - val_loss: 1.0663 - val_mae: 1.5529\n",
            "Epoch 44/200\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9950 - mae: 1.4774 - val_loss: 1.0710 - val_mae: 1.5625\n",
            "Epoch 45/200\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9823 - mae: 1.4636 - val_loss: 1.0267 - val_mae: 1.5067\n",
            "Epoch 46/200\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9834 - mae: 1.4650 - val_loss: 1.0242 - val_mae: 1.5031\n",
            "Epoch 47/200\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9754 - mae: 1.4534 - val_loss: 1.0353 - val_mae: 1.5210\n",
            "Epoch 48/200\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9761 - mae: 1.4555 - val_loss: 1.0307 - val_mae: 1.5180\n",
            "Epoch 49/200\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9751 - mae: 1.4575 - val_loss: 1.0881 - val_mae: 1.5793\n",
            "Epoch 50/200\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9794 - mae: 1.4622 - val_loss: 1.0102 - val_mae: 1.4914\n",
            "Epoch 51/200\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9820 - mae: 1.4626 - val_loss: 0.9976 - val_mae: 1.4804\n",
            "Epoch 52/200\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9684 - mae: 1.4437 - val_loss: 1.0004 - val_mae: 1.4906\n",
            "Epoch 53/200\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9837 - mae: 1.4654 - val_loss: 1.0210 - val_mae: 1.5125\n",
            "Epoch 54/200\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 0.9767 - mae: 1.4561 - val_loss: 1.0249 - val_mae: 1.5142\n",
            "Epoch 55/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9717 - mae: 1.4488 - val_loss: 1.0483 - val_mae: 1.5376\n",
            "Epoch 56/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9606 - mae: 1.4389 - val_loss: 1.0075 - val_mae: 1.4885\n",
            "Epoch 57/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9719 - mae: 1.4527 - val_loss: 1.0419 - val_mae: 1.5283\n",
            "Epoch 58/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9736 - mae: 1.4550 - val_loss: 1.0544 - val_mae: 1.5481\n",
            "Epoch 59/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9824 - mae: 1.4652 - val_loss: 1.0284 - val_mae: 1.5155\n",
            "Epoch 60/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9730 - mae: 1.4520 - val_loss: 1.0223 - val_mae: 1.4971\n",
            "Epoch 61/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9738 - mae: 1.4524 - val_loss: 1.0071 - val_mae: 1.4931\n",
            "Epoch 62/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9655 - mae: 1.4455 - val_loss: 0.9934 - val_mae: 1.4775\n",
            "Epoch 63/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9643 - mae: 1.4434 - val_loss: 1.0016 - val_mae: 1.4838\n",
            "Epoch 64/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9554 - mae: 1.4346 - val_loss: 1.0186 - val_mae: 1.4946\n",
            "Epoch 65/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9624 - mae: 1.4402 - val_loss: 1.0092 - val_mae: 1.4866\n",
            "Epoch 66/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9665 - mae: 1.4450 - val_loss: 1.0257 - val_mae: 1.5071\n",
            "Epoch 67/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9696 - mae: 1.4492 - val_loss: 1.0327 - val_mae: 1.5092\n",
            "Epoch 68/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9653 - mae: 1.4434 - val_loss: 0.9885 - val_mae: 1.4661\n",
            "Epoch 69/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9496 - mae: 1.4264 - val_loss: 1.0237 - val_mae: 1.5082\n",
            "Epoch 70/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9644 - mae: 1.4406 - val_loss: 1.0159 - val_mae: 1.4861\n",
            "Epoch 71/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9615 - mae: 1.4384 - val_loss: 1.0429 - val_mae: 1.5186\n",
            "Epoch 72/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9726 - mae: 1.4498 - val_loss: 1.0431 - val_mae: 1.5199\n",
            "Epoch 73/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9602 - mae: 1.4351 - val_loss: 1.0268 - val_mae: 1.5098\n",
            "Epoch 74/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9794 - mae: 1.4597 - val_loss: 1.0298 - val_mae: 1.4962\n",
            "Epoch 75/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9556 - mae: 1.4330 - val_loss: 1.0385 - val_mae: 1.5185\n",
            "Epoch 76/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9498 - mae: 1.4247 - val_loss: 1.0394 - val_mae: 1.5136\n",
            "Epoch 77/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9451 - mae: 1.4238 - val_loss: 1.0325 - val_mae: 1.5092\n",
            "Epoch 78/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9573 - mae: 1.4370 - val_loss: 1.0527 - val_mae: 1.5246\n",
            "Epoch 79/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9669 - mae: 1.4424 - val_loss: 1.0542 - val_mae: 1.5166\n",
            "Epoch 80/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9554 - mae: 1.4301 - val_loss: 1.0886 - val_mae: 1.5611\n",
            "Epoch 81/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9647 - mae: 1.4400 - val_loss: 1.0440 - val_mae: 1.5250\n",
            "Epoch 82/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9599 - mae: 1.4371 - val_loss: 1.0231 - val_mae: 1.5014\n",
            "Epoch 83/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9589 - mae: 1.4366 - val_loss: 1.0162 - val_mae: 1.4848\n",
            "Epoch 84/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9467 - mae: 1.4254 - val_loss: 1.0323 - val_mae: 1.5014\n",
            "Epoch 85/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9666 - mae: 1.4452 - val_loss: 1.0545 - val_mae: 1.5394\n",
            "Epoch 86/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9666 - mae: 1.4434 - val_loss: 1.0260 - val_mae: 1.4931\n",
            "Epoch 87/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9588 - mae: 1.4353 - val_loss: 1.0131 - val_mae: 1.4793\n",
            "Epoch 88/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9459 - mae: 1.4210 - val_loss: 1.0148 - val_mae: 1.4782\n",
            "Epoch 89/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9589 - mae: 1.4332 - val_loss: 1.0313 - val_mae: 1.5089\n",
            "Epoch 90/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9414 - mae: 1.4156 - val_loss: 1.0434 - val_mae: 1.5247\n",
            "Epoch 91/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9419 - mae: 1.4181 - val_loss: 0.9943 - val_mae: 1.4703\n",
            "Epoch 92/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9195 - mae: 1.3945 - val_loss: 1.0097 - val_mae: 1.4789\n",
            "Epoch 93/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9571 - mae: 1.4366 - val_loss: 1.0162 - val_mae: 1.4975\n",
            "Epoch 94/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9528 - mae: 1.4316 - val_loss: 1.0188 - val_mae: 1.4972\n",
            "Epoch 95/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9350 - mae: 1.4071 - val_loss: 1.0022 - val_mae: 1.4695\n",
            "Epoch 96/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9654 - mae: 1.4440 - val_loss: 1.0608 - val_mae: 1.5347\n",
            "Epoch 97/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9590 - mae: 1.4368 - val_loss: 1.0173 - val_mae: 1.4882\n",
            "Epoch 98/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9452 - mae: 1.4188 - val_loss: 1.0353 - val_mae: 1.5130\n",
            "Epoch 99/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9583 - mae: 1.4381 - val_loss: 1.0094 - val_mae: 1.4797\n",
            "Epoch 100/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9461 - mae: 1.4224 - val_loss: 1.0278 - val_mae: 1.4893\n",
            "Epoch 101/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9532 - mae: 1.4269 - val_loss: 1.0063 - val_mae: 1.4718\n",
            "Epoch 102/200\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9376 - mae: 1.4141 - val_loss: 1.0075 - val_mae: 1.4733\n",
            "Epoch 103/200\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9523 - mae: 1.4289 - val_loss: 1.0327 - val_mae: 1.5148\n",
            "Epoch 104/200\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9581 - mae: 1.4360 - val_loss: 1.0203 - val_mae: 1.4858\n",
            "Epoch 105/200\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9507 - mae: 1.4280 - val_loss: 1.0368 - val_mae: 1.5057\n",
            "Epoch 106/200\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9465 - mae: 1.4201 - val_loss: 1.0185 - val_mae: 1.4884\n",
            "Epoch 107/200\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9362 - mae: 1.4124 - val_loss: 1.0384 - val_mae: 1.5133\n",
            "Epoch 108/200\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9418 - mae: 1.4142 - val_loss: 1.0317 - val_mae: 1.4885\n",
            "Epoch 109/200\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9266 - mae: 1.3986 - val_loss: 1.0220 - val_mae: 1.4947\n",
            "Epoch 110/200\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9400 - mae: 1.4129 - val_loss: 1.0255 - val_mae: 1.5003\n",
            "Epoch 111/200\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 0.9378 - mae: 1.4108 - val_loss: 0.9885 - val_mae: 1.4628\n",
            "Epoch 112/200\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 0.9443 - mae: 1.4174 - val_loss: 0.9924 - val_mae: 1.4684\n",
            "Epoch 113/200\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9275 - mae: 1.4022 - val_loss: 1.0157 - val_mae: 1.4865\n",
            "Epoch 114/200\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9348 - mae: 1.4093 - val_loss: 0.9935 - val_mae: 1.4642\n",
            "Epoch 115/200\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9422 - mae: 1.4187 - val_loss: 1.0106 - val_mae: 1.4816\n",
            "Epoch 116/200\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 0.9405 - mae: 1.4165 - val_loss: 0.9936 - val_mae: 1.4585\n",
            "Epoch 117/200\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9457 - mae: 1.4195 - val_loss: 1.0379 - val_mae: 1.5060\n",
            "Epoch 118/200\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9544 - mae: 1.4309 - val_loss: 1.0460 - val_mae: 1.5135\n",
            "Epoch 119/200\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 0.9364 - mae: 1.4076 - val_loss: 1.0108 - val_mae: 1.4795\n",
            "Epoch 120/200\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 0.9533 - mae: 1.4293 - val_loss: 1.1027 - val_mae: 1.5888\n",
            "Epoch 121/200\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9278 - mae: 1.3984 - val_loss: 0.9723 - val_mae: 1.4341\n",
            "Epoch 122/200\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9358 - mae: 1.4054 - val_loss: 0.9821 - val_mae: 1.4530\n",
            "Epoch 123/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9452 - mae: 1.4170 - val_loss: 1.0014 - val_mae: 1.4691\n",
            "Epoch 124/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9514 - mae: 1.4277 - val_loss: 0.9887 - val_mae: 1.4620\n",
            "Epoch 125/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9409 - mae: 1.4114 - val_loss: 0.9589 - val_mae: 1.4353\n",
            "Epoch 126/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9242 - mae: 1.3988 - val_loss: 0.9943 - val_mae: 1.4814\n",
            "Epoch 127/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9503 - mae: 1.4256 - val_loss: 0.9712 - val_mae: 1.4462\n",
            "Epoch 128/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9421 - mae: 1.4183 - val_loss: 1.0056 - val_mae: 1.4878\n",
            "Epoch 129/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9305 - mae: 1.4000 - val_loss: 1.0155 - val_mae: 1.4813\n",
            "Epoch 130/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9234 - mae: 1.3968 - val_loss: 1.0032 - val_mae: 1.4808\n",
            "Epoch 131/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9475 - mae: 1.4205 - val_loss: 0.9780 - val_mae: 1.4467\n",
            "Epoch 132/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9136 - mae: 1.3896 - val_loss: 0.9835 - val_mae: 1.4466\n",
            "Epoch 133/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9474 - mae: 1.4241 - val_loss: 1.0090 - val_mae: 1.4746\n",
            "Epoch 134/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9306 - mae: 1.3994 - val_loss: 0.9918 - val_mae: 1.4658\n",
            "Epoch 135/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9227 - mae: 1.3917 - val_loss: 1.0064 - val_mae: 1.4714\n",
            "Epoch 136/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9147 - mae: 1.3851 - val_loss: 1.0297 - val_mae: 1.4965\n",
            "Epoch 137/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9234 - mae: 1.3906 - val_loss: 1.0229 - val_mae: 1.4872\n",
            "Epoch 138/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9304 - mae: 1.4013 - val_loss: 1.0074 - val_mae: 1.4751\n",
            "Epoch 139/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9291 - mae: 1.4001 - val_loss: 1.0468 - val_mae: 1.4907\n",
            "Epoch 140/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9442 - mae: 1.4159 - val_loss: 1.0214 - val_mae: 1.4926\n",
            "Epoch 141/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9310 - mae: 1.3994 - val_loss: 0.9945 - val_mae: 1.4627\n",
            "Epoch 142/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9295 - mae: 1.3982 - val_loss: 0.9891 - val_mae: 1.4506\n",
            "Epoch 143/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9330 - mae: 1.4058 - val_loss: 0.9984 - val_mae: 1.4664\n",
            "Epoch 144/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9386 - mae: 1.4099 - val_loss: 1.0251 - val_mae: 1.4868\n",
            "Epoch 145/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9297 - mae: 1.4016 - val_loss: 0.9941 - val_mae: 1.4598\n",
            "Epoch 146/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9391 - mae: 1.4106 - val_loss: 0.9839 - val_mae: 1.4509\n",
            "Epoch 147/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9364 - mae: 1.4082 - val_loss: 0.9697 - val_mae: 1.4341\n",
            "Epoch 148/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9327 - mae: 1.4021 - val_loss: 1.0047 - val_mae: 1.4739\n",
            "Epoch 149/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9284 - mae: 1.3986 - val_loss: 0.9891 - val_mae: 1.4548\n",
            "Epoch 150/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9189 - mae: 1.3889 - val_loss: 0.9704 - val_mae: 1.4390\n",
            "Epoch 151/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9164 - mae: 1.3871 - val_loss: 1.0239 - val_mae: 1.4955\n",
            "Epoch 152/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9282 - mae: 1.3992 - val_loss: 0.9898 - val_mae: 1.4529\n",
            "Epoch 153/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9140 - mae: 1.3808 - val_loss: 0.9805 - val_mae: 1.4447\n",
            "Epoch 154/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9243 - mae: 1.3945 - val_loss: 0.9996 - val_mae: 1.4729\n",
            "Epoch 155/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9200 - mae: 1.3889 - val_loss: 0.9978 - val_mae: 1.4560\n",
            "Epoch 156/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9257 - mae: 1.3955 - val_loss: 0.9613 - val_mae: 1.4281\n",
            "Epoch 157/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9339 - mae: 1.4062 - val_loss: 0.9862 - val_mae: 1.4573\n",
            "Epoch 158/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9068 - mae: 1.3768 - val_loss: 1.0153 - val_mae: 1.4965\n",
            "Epoch 159/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9191 - mae: 1.3899 - val_loss: 1.0039 - val_mae: 1.4778\n",
            "Epoch 160/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9283 - mae: 1.4029 - val_loss: 0.9797 - val_mae: 1.4414\n",
            "Epoch 161/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9268 - mae: 1.4022 - val_loss: 0.9867 - val_mae: 1.4399\n",
            "Epoch 162/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9280 - mae: 1.3989 - val_loss: 1.0536 - val_mae: 1.5193\n",
            "Epoch 163/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9187 - mae: 1.3899 - val_loss: 0.9653 - val_mae: 1.4299\n",
            "Epoch 164/200\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 0.9166 - mae: 1.3867 - val_loss: 0.9673 - val_mae: 1.4330\n",
            "Epoch 165/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9242 - mae: 1.3964 - val_loss: 0.9735 - val_mae: 1.4467\n",
            "Epoch 166/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9023 - mae: 1.3746 - val_loss: 0.9519 - val_mae: 1.4159\n",
            "Epoch 167/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9266 - mae: 1.3999 - val_loss: 0.9630 - val_mae: 1.4279\n",
            "Epoch 168/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9098 - mae: 1.3782 - val_loss: 0.9698 - val_mae: 1.4346\n",
            "Epoch 169/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9268 - mae: 1.3964 - val_loss: 0.9831 - val_mae: 1.4488\n",
            "Epoch 170/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9373 - mae: 1.4095 - val_loss: 0.9883 - val_mae: 1.4570\n",
            "Epoch 171/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9270 - mae: 1.3980 - val_loss: 0.9554 - val_mae: 1.4237\n",
            "Epoch 172/200\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9170 - mae: 1.3903 - val_loss: 1.0059 - val_mae: 1.4708\n",
            "Epoch 173/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9310 - mae: 1.4031 - val_loss: 0.9891 - val_mae: 1.4640\n",
            "Epoch 174/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9209 - mae: 1.3965 - val_loss: 0.9831 - val_mae: 1.4573\n",
            "Epoch 175/200\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9115 - mae: 1.3828 - val_loss: 0.9709 - val_mae: 1.4395\n",
            "Epoch 176/200\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9192 - mae: 1.3889 - val_loss: 0.9903 - val_mae: 1.4544\n",
            "Epoch 177/200\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 0.9122 - mae: 1.3826 - val_loss: 0.9851 - val_mae: 1.4544\n",
            "Epoch 178/200\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 0.9286 - mae: 1.4013 - val_loss: 0.9723 - val_mae: 1.4312\n",
            "Epoch 179/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9213 - mae: 1.3896 - val_loss: 0.9726 - val_mae: 1.4344\n",
            "Epoch 180/200\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9338 - mae: 1.4065 - val_loss: 0.9978 - val_mae: 1.4611\n",
            "Epoch 181/200\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9315 - mae: 1.4012 - val_loss: 0.9963 - val_mae: 1.4563\n",
            "Epoch 182/200\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 0.9109 - mae: 1.3816 - val_loss: 1.0015 - val_mae: 1.4576\n",
            "Epoch 183/200\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9308 - mae: 1.4023 - val_loss: 0.9757 - val_mae: 1.4311\n",
            "Epoch 184/200\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9089 - mae: 1.3764 - val_loss: 1.0012 - val_mae: 1.4564\n",
            "Epoch 185/200\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9252 - mae: 1.3966 - val_loss: 1.0068 - val_mae: 1.4732\n",
            "Epoch 186/200\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9104 - mae: 1.3795 - val_loss: 0.9971 - val_mae: 1.4518\n",
            "Epoch 187/200\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9244 - mae: 1.3954 - val_loss: 0.9550 - val_mae: 1.4170\n",
            "Epoch 188/200\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9133 - mae: 1.3818 - val_loss: 0.9661 - val_mae: 1.4212\n",
            "Epoch 189/200\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.9150 - mae: 1.3833 - val_loss: 0.9618 - val_mae: 1.4308\n",
            "Epoch 190/200\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 0.9145 - mae: 1.3821 - val_loss: 0.9571 - val_mae: 1.4216\n",
            "Epoch 191/200\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9113 - mae: 1.3816 - val_loss: 0.9587 - val_mae: 1.4244\n",
            "Epoch 192/200\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9082 - mae: 1.3777 - val_loss: 0.9846 - val_mae: 1.4575\n",
            "Epoch 193/200\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9182 - mae: 1.3925 - val_loss: 0.9519 - val_mae: 1.4115\n",
            "Epoch 194/200\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9291 - mae: 1.4052 - val_loss: 0.9868 - val_mae: 1.4464\n",
            "Epoch 195/200\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9060 - mae: 1.3775 - val_loss: 0.9740 - val_mae: 1.4478\n",
            "Epoch 196/200\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 0.9192 - mae: 1.3871 - val_loss: 0.9504 - val_mae: 1.4113\n",
            "Epoch 197/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9047 - mae: 1.3705 - val_loss: 0.9596 - val_mae: 1.4230\n",
            "Epoch 198/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9209 - mae: 1.3924 - val_loss: 0.9429 - val_mae: 1.4081\n",
            "Epoch 199/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9133 - mae: 1.3830 - val_loss: 0.9578 - val_mae: 1.4231\n",
            "Epoch 200/200\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9149 - mae: 1.3875 - val_loss: 0.9543 - val_mae: 1.4018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:58:57,329] Trial 34 finished with value: 0.9543360471725464 and parameters: {'dropout_2': 0.3526240082979934, 'dropout_3': 0.6777238897284674, 'dropout_4': 0.41284697732031994, 'dropout_5': 0.344527165493702, 'learning_rate': 0.0050615757629477376, 'epochs': 200, 'batch_size': 128}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "108/108 [==============================] - 3s 7ms/step - loss: 1.3611 - mae: 1.8955 - val_loss: 1.3052 - val_mae: 1.8055\n",
            "Epoch 2/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2296 - mae: 1.7401 - val_loss: 1.2681 - val_mae: 1.7664\n",
            "Epoch 3/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1866 - mae: 1.6871 - val_loss: 1.2208 - val_mae: 1.7192\n",
            "Epoch 4/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1432 - mae: 1.6360 - val_loss: 1.1772 - val_mae: 1.6831\n",
            "Epoch 5/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1274 - mae: 1.6162 - val_loss: 1.1305 - val_mae: 1.6223\n",
            "Epoch 6/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1100 - mae: 1.6022 - val_loss: 1.1551 - val_mae: 1.6485\n",
            "Epoch 7/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1176 - mae: 1.6041 - val_loss: 1.1998 - val_mae: 1.7014\n",
            "Epoch 8/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1060 - mae: 1.5950 - val_loss: 1.1717 - val_mae: 1.6757\n",
            "Epoch 9/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0937 - mae: 1.5841 - val_loss: 1.1503 - val_mae: 1.6500\n",
            "Epoch 10/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0874 - mae: 1.5787 - val_loss: 1.1214 - val_mae: 1.6198\n",
            "Epoch 11/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0801 - mae: 1.5681 - val_loss: 1.0910 - val_mae: 1.5919\n",
            "Epoch 12/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0813 - mae: 1.5699 - val_loss: 1.1006 - val_mae: 1.6012\n",
            "Epoch 13/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0516 - mae: 1.5391 - val_loss: 1.0606 - val_mae: 1.5570\n",
            "Epoch 14/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0535 - mae: 1.5454 - val_loss: 1.0898 - val_mae: 1.5804\n",
            "Epoch 15/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0448 - mae: 1.5286 - val_loss: 1.1158 - val_mae: 1.6202\n",
            "Epoch 16/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0508 - mae: 1.5396 - val_loss: 1.0837 - val_mae: 1.5640\n",
            "Epoch 17/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0531 - mae: 1.5438 - val_loss: 1.0692 - val_mae: 1.5732\n",
            "Epoch 18/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0302 - mae: 1.5169 - val_loss: 1.0677 - val_mae: 1.5634\n",
            "Epoch 19/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0345 - mae: 1.5187 - val_loss: 1.0872 - val_mae: 1.5833\n",
            "Epoch 20/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0346 - mae: 1.5207 - val_loss: 1.1124 - val_mae: 1.5995\n",
            "Epoch 21/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0268 - mae: 1.5143 - val_loss: 1.0823 - val_mae: 1.5678\n",
            "Epoch 22/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0164 - mae: 1.5008 - val_loss: 1.0500 - val_mae: 1.5286\n",
            "Epoch 23/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0209 - mae: 1.5037 - val_loss: 1.0814 - val_mae: 1.5750\n",
            "Epoch 24/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0229 - mae: 1.5122 - val_loss: 1.1677 - val_mae: 1.6631\n",
            "Epoch 25/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0264 - mae: 1.5132 - val_loss: 1.0528 - val_mae: 1.5397\n",
            "Epoch 26/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0322 - mae: 1.5208 - val_loss: 1.0927 - val_mae: 1.5842\n",
            "Epoch 27/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0200 - mae: 1.5083 - val_loss: 1.0786 - val_mae: 1.5659\n",
            "Epoch 28/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0236 - mae: 1.5072 - val_loss: 1.0542 - val_mae: 1.5246\n",
            "Epoch 29/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0120 - mae: 1.4964 - val_loss: 1.0400 - val_mae: 1.5222\n",
            "Epoch 30/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0210 - mae: 1.5074 - val_loss: 1.0353 - val_mae: 1.5125\n",
            "Epoch 31/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0133 - mae: 1.5000 - val_loss: 1.0607 - val_mae: 1.5532\n",
            "Epoch 32/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0144 - mae: 1.5027 - val_loss: 1.0272 - val_mae: 1.5132\n",
            "Epoch 33/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0063 - mae: 1.4869 - val_loss: 1.0649 - val_mae: 1.5523\n",
            "Epoch 34/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0196 - mae: 1.5012 - val_loss: 1.0489 - val_mae: 1.5306\n",
            "Epoch 35/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0229 - mae: 1.5101 - val_loss: 1.0757 - val_mae: 1.5524\n",
            "Epoch 36/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0201 - mae: 1.5027 - val_loss: 1.0593 - val_mae: 1.5422\n",
            "Epoch 37/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0021 - mae: 1.4870 - val_loss: 1.0687 - val_mae: 1.5615\n",
            "Epoch 38/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9978 - mae: 1.4777 - val_loss: 1.0555 - val_mae: 1.5356\n",
            "Epoch 39/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9876 - mae: 1.4723 - val_loss: 1.0076 - val_mae: 1.4907\n",
            "Epoch 40/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0095 - mae: 1.4922 - val_loss: 1.0624 - val_mae: 1.5566\n",
            "Epoch 41/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9931 - mae: 1.4782 - val_loss: 1.0805 - val_mae: 1.5706\n",
            "Epoch 42/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9965 - mae: 1.4773 - val_loss: 1.0412 - val_mae: 1.5272\n",
            "Epoch 43/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9914 - mae: 1.4719 - val_loss: 1.0525 - val_mae: 1.5304\n",
            "Epoch 44/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9907 - mae: 1.4702 - val_loss: 1.0475 - val_mae: 1.5303\n",
            "Epoch 45/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0051 - mae: 1.4875 - val_loss: 1.0795 - val_mae: 1.5566\n",
            "Epoch 46/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9953 - mae: 1.4730 - val_loss: 1.0707 - val_mae: 1.5417\n",
            "Epoch 47/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9769 - mae: 1.4543 - val_loss: 1.0145 - val_mae: 1.4916\n",
            "Epoch 48/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9675 - mae: 1.4469 - val_loss: 1.0431 - val_mae: 1.5075\n",
            "Epoch 49/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9826 - mae: 1.4643 - val_loss: 1.0491 - val_mae: 1.5273\n",
            "Epoch 50/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9905 - mae: 1.4686 - val_loss: 1.0512 - val_mae: 1.5344\n",
            "Epoch 51/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9664 - mae: 1.4449 - val_loss: 1.0582 - val_mae: 1.5328\n",
            "Epoch 52/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9940 - mae: 1.4726 - val_loss: 1.0576 - val_mae: 1.5340\n",
            "Epoch 53/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9829 - mae: 1.4641 - val_loss: 1.0972 - val_mae: 1.5634\n",
            "Epoch 54/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9796 - mae: 1.4583 - val_loss: 1.0398 - val_mae: 1.5160\n",
            "Epoch 55/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9603 - mae: 1.4342 - val_loss: 1.0477 - val_mae: 1.5355\n",
            "Epoch 56/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9701 - mae: 1.4507 - val_loss: 1.0739 - val_mae: 1.5612\n",
            "Epoch 57/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9845 - mae: 1.4643 - val_loss: 1.0447 - val_mae: 1.5103\n",
            "Epoch 58/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9877 - mae: 1.4684 - val_loss: 1.0295 - val_mae: 1.5132\n",
            "Epoch 59/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9578 - mae: 1.4347 - val_loss: 1.0190 - val_mae: 1.4882\n",
            "Epoch 60/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9635 - mae: 1.4386 - val_loss: 1.0667 - val_mae: 1.5453\n",
            "Epoch 61/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9755 - mae: 1.4539 - val_loss: 1.0312 - val_mae: 1.4970\n",
            "Epoch 62/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9675 - mae: 1.4436 - val_loss: 1.0395 - val_mae: 1.5243\n",
            "Epoch 63/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9706 - mae: 1.4483 - val_loss: 1.0410 - val_mae: 1.5073\n",
            "Epoch 64/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9707 - mae: 1.4428 - val_loss: 1.0649 - val_mae: 1.5445\n",
            "Epoch 65/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9608 - mae: 1.4333 - val_loss: 1.0397 - val_mae: 1.5140\n",
            "Epoch 66/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9617 - mae: 1.4405 - val_loss: 1.0441 - val_mae: 1.5113\n",
            "Epoch 67/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9609 - mae: 1.4358 - val_loss: 1.0365 - val_mae: 1.4954\n",
            "Epoch 68/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9593 - mae: 1.4314 - val_loss: 1.0450 - val_mae: 1.5113\n",
            "Epoch 69/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9656 - mae: 1.4418 - val_loss: 1.0963 - val_mae: 1.5699\n",
            "Epoch 70/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9637 - mae: 1.4389 - val_loss: 1.1097 - val_mae: 1.5973\n",
            "Epoch 71/100\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9879 - mae: 1.4661 - val_loss: 1.0913 - val_mae: 1.5635\n",
            "Epoch 72/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9637 - mae: 1.4349 - val_loss: 1.0526 - val_mae: 1.5194\n",
            "Epoch 73/100\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9596 - mae: 1.4319 - val_loss: 1.0970 - val_mae: 1.5730\n",
            "Epoch 74/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9656 - mae: 1.4430 - val_loss: 1.0659 - val_mae: 1.5438\n",
            "Epoch 75/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9567 - mae: 1.4285 - val_loss: 1.0154 - val_mae: 1.4871\n",
            "Epoch 76/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9627 - mae: 1.4368 - val_loss: 1.0638 - val_mae: 1.5264\n",
            "Epoch 77/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9533 - mae: 1.4288 - val_loss: 1.0509 - val_mae: 1.5180\n",
            "Epoch 78/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9532 - mae: 1.4268 - val_loss: 1.0204 - val_mae: 1.4804\n",
            "Epoch 79/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9510 - mae: 1.4216 - val_loss: 1.0200 - val_mae: 1.4861\n",
            "Epoch 80/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9522 - mae: 1.4268 - val_loss: 1.0540 - val_mae: 1.5194\n",
            "Epoch 81/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9474 - mae: 1.4167 - val_loss: 1.0322 - val_mae: 1.5100\n",
            "Epoch 82/100\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9500 - mae: 1.4281 - val_loss: 1.0565 - val_mae: 1.5378\n",
            "Epoch 83/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9483 - mae: 1.4199 - val_loss: 1.0872 - val_mae: 1.5685\n",
            "Epoch 84/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9617 - mae: 1.4372 - val_loss: 1.0607 - val_mae: 1.5378\n",
            "Epoch 85/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9598 - mae: 1.4408 - val_loss: 1.0481 - val_mae: 1.5137\n",
            "Epoch 86/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9580 - mae: 1.4317 - val_loss: 1.0174 - val_mae: 1.4809\n",
            "Epoch 87/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9480 - mae: 1.4226 - val_loss: 1.0138 - val_mae: 1.4901\n",
            "Epoch 88/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9609 - mae: 1.4359 - val_loss: 1.0200 - val_mae: 1.4975\n",
            "Epoch 89/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9700 - mae: 1.4490 - val_loss: 1.0534 - val_mae: 1.5333\n",
            "Epoch 90/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9547 - mae: 1.4291 - val_loss: 1.0106 - val_mae: 1.4824\n",
            "Epoch 91/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9568 - mae: 1.4329 - val_loss: 1.0177 - val_mae: 1.4937\n",
            "Epoch 92/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9368 - mae: 1.4121 - val_loss: 1.0287 - val_mae: 1.5063\n",
            "Epoch 93/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9410 - mae: 1.4162 - val_loss: 1.0005 - val_mae: 1.4620\n",
            "Epoch 94/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9546 - mae: 1.4258 - val_loss: 1.0006 - val_mae: 1.4749\n",
            "Epoch 95/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9323 - mae: 1.4073 - val_loss: 1.0132 - val_mae: 1.4895\n",
            "Epoch 96/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9329 - mae: 1.4035 - val_loss: 0.9855 - val_mae: 1.4529\n",
            "Epoch 97/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9236 - mae: 1.3918 - val_loss: 0.9853 - val_mae: 1.4677\n",
            "Epoch 98/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9317 - mae: 1.4050 - val_loss: 1.0325 - val_mae: 1.4982\n",
            "Epoch 99/100\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9279 - mae: 1.3981 - val_loss: 0.9779 - val_mae: 1.4526\n",
            "Epoch 100/100\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9416 - mae: 1.4118 - val_loss: 0.9886 - val_mae: 1.4549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 11:59:53,048] Trial 35 finished with value: 0.9886423349380493 and parameters: {'dropout_2': 0.16657058147964598, 'dropout_3': 0.7821541949123529, 'dropout_4': 0.47345277145789383, 'dropout_5': 0.16160572314805385, 'learning_rate': 0.0022888462194131597, 'epochs': 100, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "108/108 [==============================] - 4s 9ms/step - loss: 1.2182 - mae: 1.7167 - val_loss: 1.2085 - val_mae: 1.6897\n",
            "Epoch 2/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1181 - mae: 1.6018 - val_loss: 1.1472 - val_mae: 1.6441\n",
            "Epoch 3/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1202 - mae: 1.6083 - val_loss: 1.1254 - val_mae: 1.6200\n",
            "Epoch 4/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0832 - mae: 1.5675 - val_loss: 1.1450 - val_mae: 1.6254\n",
            "Epoch 5/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0797 - mae: 1.5633 - val_loss: 1.2006 - val_mae: 1.6737\n",
            "Epoch 6/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1012 - mae: 1.5817 - val_loss: 1.1532 - val_mae: 1.6484\n",
            "Epoch 7/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0741 - mae: 1.5609 - val_loss: 1.1226 - val_mae: 1.6126\n",
            "Epoch 8/50\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0794 - mae: 1.5645 - val_loss: 1.1663 - val_mae: 1.6773\n",
            "Epoch 9/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0734 - mae: 1.5602 - val_loss: 1.1497 - val_mae: 1.6224\n",
            "Epoch 10/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0733 - mae: 1.5568 - val_loss: 1.1558 - val_mae: 1.6633\n",
            "Epoch 11/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0752 - mae: 1.5597 - val_loss: 1.1646 - val_mae: 1.6603\n",
            "Epoch 12/50\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0767 - mae: 1.5634 - val_loss: 1.1651 - val_mae: 1.6886\n",
            "Epoch 13/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0783 - mae: 1.5629 - val_loss: 1.1005 - val_mae: 1.5781\n",
            "Epoch 14/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0679 - mae: 1.5544 - val_loss: 1.1656 - val_mae: 1.6675\n",
            "Epoch 15/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0683 - mae: 1.5531 - val_loss: 1.1492 - val_mae: 1.6612\n",
            "Epoch 16/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0563 - mae: 1.5389 - val_loss: 1.1449 - val_mae: 1.6357\n",
            "Epoch 17/50\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0436 - mae: 1.5280 - val_loss: 1.1763 - val_mae: 1.6956\n",
            "Epoch 18/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0336 - mae: 1.5171 - val_loss: 1.0972 - val_mae: 1.6025\n",
            "Epoch 19/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0373 - mae: 1.5230 - val_loss: 1.1431 - val_mae: 1.6344\n",
            "Epoch 20/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0545 - mae: 1.5392 - val_loss: 1.1348 - val_mae: 1.6373\n",
            "Epoch 21/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0427 - mae: 1.5257 - val_loss: 1.1464 - val_mae: 1.6586\n",
            "Epoch 22/50\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0338 - mae: 1.5192 - val_loss: 1.1094 - val_mae: 1.6149\n",
            "Epoch 23/50\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0444 - mae: 1.5287 - val_loss: 1.1352 - val_mae: 1.6353\n",
            "Epoch 24/50\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0391 - mae: 1.5232 - val_loss: 1.1380 - val_mae: 1.6435\n",
            "Epoch 25/50\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0430 - mae: 1.5291 - val_loss: 1.1423 - val_mae: 1.6343\n",
            "Epoch 26/50\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0427 - mae: 1.5305 - val_loss: 1.2010 - val_mae: 1.6878\n",
            "Epoch 27/50\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0437 - mae: 1.5304 - val_loss: 1.1154 - val_mae: 1.6237\n",
            "Epoch 28/50\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0345 - mae: 1.5194 - val_loss: 1.1184 - val_mae: 1.6149\n",
            "Epoch 29/50\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0317 - mae: 1.5189 - val_loss: 1.1232 - val_mae: 1.6289\n",
            "Epoch 30/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0235 - mae: 1.5086 - val_loss: 1.1540 - val_mae: 1.6620\n",
            "Epoch 31/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0413 - mae: 1.5259 - val_loss: 1.0754 - val_mae: 1.5705\n",
            "Epoch 32/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0231 - mae: 1.5102 - val_loss: 1.1020 - val_mae: 1.5984\n",
            "Epoch 33/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0090 - mae: 1.4926 - val_loss: 1.1307 - val_mae: 1.6532\n",
            "Epoch 34/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0430 - mae: 1.5282 - val_loss: 1.1435 - val_mae: 1.6358\n",
            "Epoch 35/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0252 - mae: 1.5087 - val_loss: 1.0876 - val_mae: 1.6014\n",
            "Epoch 36/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0231 - mae: 1.5101 - val_loss: 1.1583 - val_mae: 1.6665\n",
            "Epoch 37/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0189 - mae: 1.5019 - val_loss: 1.1446 - val_mae: 1.6337\n",
            "Epoch 38/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0453 - mae: 1.5262 - val_loss: 1.1153 - val_mae: 1.6247\n",
            "Epoch 39/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0244 - mae: 1.5038 - val_loss: 1.1219 - val_mae: 1.6228\n",
            "Epoch 40/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0399 - mae: 1.5243 - val_loss: 1.0963 - val_mae: 1.5828\n",
            "Epoch 41/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0199 - mae: 1.5077 - val_loss: 1.1144 - val_mae: 1.5981\n",
            "Epoch 42/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0199 - mae: 1.4992 - val_loss: 1.0774 - val_mae: 1.5592\n",
            "Epoch 43/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0162 - mae: 1.4983 - val_loss: 1.0659 - val_mae: 1.5608\n",
            "Epoch 44/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0219 - mae: 1.5062 - val_loss: 1.0668 - val_mae: 1.5240\n",
            "Epoch 45/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0218 - mae: 1.5059 - val_loss: 1.1013 - val_mae: 1.5725\n",
            "Epoch 46/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0259 - mae: 1.5075 - val_loss: 1.0563 - val_mae: 1.5327\n",
            "Epoch 47/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0262 - mae: 1.5153 - val_loss: 1.0886 - val_mae: 1.5929\n",
            "Epoch 48/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0251 - mae: 1.5094 - val_loss: 1.1484 - val_mae: 1.6676\n",
            "Epoch 49/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0067 - mae: 1.4923 - val_loss: 1.0996 - val_mae: 1.5675\n",
            "Epoch 50/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0204 - mae: 1.5036 - val_loss: 1.1160 - val_mae: 1.6346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 12:00:21,706] Trial 36 finished with value: 1.116047739982605 and parameters: {'dropout_2': 0.32882487340990235, 'dropout_3': 0.5752606332526408, 'dropout_4': 0.22230456514993724, 'dropout_5': 0.23501979578697912, 'learning_rate': 0.010988429762939582, 'epochs': 50, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "14/14 [==============================] - 3s 30ms/step - loss: 1.7777 - mae: 2.3406 - val_loss: 1.3433 - val_mae: 1.8059\n",
            "Epoch 2/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.7613 - mae: 2.3259 - val_loss: 1.3463 - val_mae: 1.8130\n",
            "Epoch 3/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.7664 - mae: 2.3355 - val_loss: 1.3507 - val_mae: 1.8223\n",
            "Epoch 4/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.7291 - mae: 2.2947 - val_loss: 1.3519 - val_mae: 1.8260\n",
            "Epoch 5/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.7416 - mae: 2.3092 - val_loss: 1.3538 - val_mae: 1.8301\n",
            "Epoch 6/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.7481 - mae: 2.3147 - val_loss: 1.3557 - val_mae: 1.8336\n",
            "Epoch 7/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.7509 - mae: 2.3157 - val_loss: 1.3575 - val_mae: 1.8371\n",
            "Epoch 8/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.7224 - mae: 2.2889 - val_loss: 1.3607 - val_mae: 1.8424\n",
            "Epoch 9/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.7127 - mae: 2.2765 - val_loss: 1.3627 - val_mae: 1.8451\n",
            "Epoch 10/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.7094 - mae: 2.2744 - val_loss: 1.3647 - val_mae: 1.8481\n",
            "Epoch 11/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.7254 - mae: 2.2894 - val_loss: 1.3666 - val_mae: 1.8505\n",
            "Epoch 12/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.6870 - mae: 2.2503 - val_loss: 1.3677 - val_mae: 1.8521\n",
            "Epoch 13/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.7021 - mae: 2.2645 - val_loss: 1.3699 - val_mae: 1.8548\n",
            "Epoch 14/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.6987 - mae: 2.2607 - val_loss: 1.3708 - val_mae: 1.8559\n",
            "Epoch 15/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.6860 - mae: 2.2501 - val_loss: 1.3712 - val_mae: 1.8567\n",
            "Epoch 16/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.6803 - mae: 2.2433 - val_loss: 1.3714 - val_mae: 1.8567\n",
            "Epoch 17/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.6893 - mae: 2.2496 - val_loss: 1.3720 - val_mae: 1.8576\n",
            "Epoch 18/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.6683 - mae: 2.2265 - val_loss: 1.3729 - val_mae: 1.8592\n",
            "Epoch 19/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.6959 - mae: 2.2548 - val_loss: 1.3725 - val_mae: 1.8583\n",
            "Epoch 20/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.6946 - mae: 2.2531 - val_loss: 1.3716 - val_mae: 1.8572\n",
            "Epoch 21/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.6587 - mae: 2.2185 - val_loss: 1.3700 - val_mae: 1.8554\n",
            "Epoch 22/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.6478 - mae: 2.2094 - val_loss: 1.3689 - val_mae: 1.8546\n",
            "Epoch 23/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.6602 - mae: 2.2173 - val_loss: 1.3673 - val_mae: 1.8522\n",
            "Epoch 24/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.6486 - mae: 2.2073 - val_loss: 1.3660 - val_mae: 1.8519\n",
            "Epoch 25/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.6593 - mae: 2.2177 - val_loss: 1.3651 - val_mae: 1.8515\n",
            "Epoch 26/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.6966 - mae: 2.2584 - val_loss: 1.3642 - val_mae: 1.8515\n",
            "Epoch 27/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.6185 - mae: 2.1759 - val_loss: 1.3632 - val_mae: 1.8503\n",
            "Epoch 28/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.6326 - mae: 2.1900 - val_loss: 1.3614 - val_mae: 1.8487\n",
            "Epoch 29/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.6346 - mae: 2.1951 - val_loss: 1.3604 - val_mae: 1.8475\n",
            "Epoch 30/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.6219 - mae: 2.1778 - val_loss: 1.3580 - val_mae: 1.8448\n",
            "Epoch 31/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.6259 - mae: 2.1829 - val_loss: 1.3575 - val_mae: 1.8448\n",
            "Epoch 32/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.6194 - mae: 2.1770 - val_loss: 1.3558 - val_mae: 1.8427\n",
            "Epoch 33/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.6105 - mae: 2.1640 - val_loss: 1.3545 - val_mae: 1.8412\n",
            "Epoch 34/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.6056 - mae: 2.1600 - val_loss: 1.3527 - val_mae: 1.8389\n",
            "Epoch 35/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.5901 - mae: 2.1432 - val_loss: 1.3492 - val_mae: 1.8343\n",
            "Epoch 36/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.6081 - mae: 2.1631 - val_loss: 1.3471 - val_mae: 1.8323\n",
            "Epoch 37/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.5887 - mae: 2.1398 - val_loss: 1.3444 - val_mae: 1.8296\n",
            "Epoch 38/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.5909 - mae: 2.1428 - val_loss: 1.3401 - val_mae: 1.8255\n",
            "Epoch 39/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.5721 - mae: 2.1226 - val_loss: 1.3370 - val_mae: 1.8231\n",
            "Epoch 40/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.5981 - mae: 2.1508 - val_loss: 1.3333 - val_mae: 1.8195\n",
            "Epoch 41/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.5951 - mae: 2.1474 - val_loss: 1.3307 - val_mae: 1.8179\n",
            "Epoch 42/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.5698 - mae: 2.1229 - val_loss: 1.3273 - val_mae: 1.8144\n",
            "Epoch 43/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.5701 - mae: 2.1222 - val_loss: 1.3232 - val_mae: 1.8102\n",
            "Epoch 44/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.5650 - mae: 2.1152 - val_loss: 1.3214 - val_mae: 1.8080\n",
            "Epoch 45/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.5935 - mae: 2.1453 - val_loss: 1.3186 - val_mae: 1.8053\n",
            "Epoch 46/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.6127 - mae: 2.1703 - val_loss: 1.3150 - val_mae: 1.8012\n",
            "Epoch 47/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.5860 - mae: 2.1398 - val_loss: 1.3104 - val_mae: 1.7966\n",
            "Epoch 48/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.5861 - mae: 2.1388 - val_loss: 1.3076 - val_mae: 1.7939\n",
            "Epoch 49/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.5734 - mae: 2.1236 - val_loss: 1.3036 - val_mae: 1.7893\n",
            "Epoch 50/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.5460 - mae: 2.0946 - val_loss: 1.3022 - val_mae: 1.7878\n",
            "Epoch 51/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.5522 - mae: 2.1003 - val_loss: 1.3004 - val_mae: 1.7859\n",
            "Epoch 52/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.5233 - mae: 2.0713 - val_loss: 1.2981 - val_mae: 1.7835\n",
            "Epoch 53/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.5272 - mae: 2.0749 - val_loss: 1.2971 - val_mae: 1.7827\n",
            "Epoch 54/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.5488 - mae: 2.0990 - val_loss: 1.2948 - val_mae: 1.7805\n",
            "Epoch 55/200\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.5538 - mae: 2.1031 - val_loss: 1.2923 - val_mae: 1.7783\n",
            "Epoch 56/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.5604 - mae: 2.1117 - val_loss: 1.2907 - val_mae: 1.7769\n",
            "Epoch 57/200\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.5467 - mae: 2.0980 - val_loss: 1.2886 - val_mae: 1.7743\n",
            "Epoch 58/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.5609 - mae: 2.1136 - val_loss: 1.2870 - val_mae: 1.7723\n",
            "Epoch 59/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.5446 - mae: 2.0957 - val_loss: 1.2839 - val_mae: 1.7688\n",
            "Epoch 60/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.5305 - mae: 2.0807 - val_loss: 1.2816 - val_mae: 1.7664\n",
            "Epoch 61/200\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.4994 - mae: 2.0455 - val_loss: 1.2777 - val_mae: 1.7619\n",
            "Epoch 62/200\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.5284 - mae: 2.0759 - val_loss: 1.2745 - val_mae: 1.7583\n",
            "Epoch 63/200\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.5203 - mae: 2.0673 - val_loss: 1.2705 - val_mae: 1.7541\n",
            "Epoch 64/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.5305 - mae: 2.0813 - val_loss: 1.2667 - val_mae: 1.7503\n",
            "Epoch 65/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.5197 - mae: 2.0656 - val_loss: 1.2655 - val_mae: 1.7492\n",
            "Epoch 66/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.4956 - mae: 2.0408 - val_loss: 1.2631 - val_mae: 1.7468\n",
            "Epoch 67/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.5043 - mae: 2.0468 - val_loss: 1.2611 - val_mae: 1.7448\n",
            "Epoch 68/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.4972 - mae: 2.0428 - val_loss: 1.2609 - val_mae: 1.7448\n",
            "Epoch 69/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.5219 - mae: 2.0707 - val_loss: 1.2593 - val_mae: 1.7443\n",
            "Epoch 70/200\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.5494 - mae: 2.0978 - val_loss: 1.2578 - val_mae: 1.7424\n",
            "Epoch 71/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.5086 - mae: 2.0512 - val_loss: 1.2564 - val_mae: 1.7407\n",
            "Epoch 72/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.5022 - mae: 2.0512 - val_loss: 1.2552 - val_mae: 1.7391\n",
            "Epoch 73/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.5065 - mae: 2.0523 - val_loss: 1.2530 - val_mae: 1.7374\n",
            "Epoch 74/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.5082 - mae: 2.0552 - val_loss: 1.2508 - val_mae: 1.7349\n",
            "Epoch 75/200\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.5002 - mae: 2.0461 - val_loss: 1.2491 - val_mae: 1.7324\n",
            "Epoch 76/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.4928 - mae: 2.0376 - val_loss: 1.2475 - val_mae: 1.7307\n",
            "Epoch 77/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.4925 - mae: 2.0366 - val_loss: 1.2458 - val_mae: 1.7291\n",
            "Epoch 78/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.4927 - mae: 2.0351 - val_loss: 1.2427 - val_mae: 1.7258\n",
            "Epoch 79/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.4873 - mae: 2.0311 - val_loss: 1.2407 - val_mae: 1.7234\n",
            "Epoch 80/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.5019 - mae: 2.0464 - val_loss: 1.2382 - val_mae: 1.7209\n",
            "Epoch 81/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.4941 - mae: 2.0391 - val_loss: 1.2372 - val_mae: 1.7196\n",
            "Epoch 82/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.4803 - mae: 2.0246 - val_loss: 1.2343 - val_mae: 1.7167\n",
            "Epoch 83/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.4610 - mae: 2.0023 - val_loss: 1.2318 - val_mae: 1.7141\n",
            "Epoch 84/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4857 - mae: 2.0290 - val_loss: 1.2302 - val_mae: 1.7120\n",
            "Epoch 85/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4851 - mae: 2.0307 - val_loss: 1.2285 - val_mae: 1.7098\n",
            "Epoch 86/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4663 - mae: 2.0087 - val_loss: 1.2261 - val_mae: 1.7074\n",
            "Epoch 87/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.4826 - mae: 2.0240 - val_loss: 1.2248 - val_mae: 1.7065\n",
            "Epoch 88/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4531 - mae: 1.9927 - val_loss: 1.2230 - val_mae: 1.7048\n",
            "Epoch 89/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.4532 - mae: 1.9936 - val_loss: 1.2216 - val_mae: 1.7032\n",
            "Epoch 90/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.4279 - mae: 1.9633 - val_loss: 1.2180 - val_mae: 1.6995\n",
            "Epoch 91/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4496 - mae: 1.9884 - val_loss: 1.2161 - val_mae: 1.6972\n",
            "Epoch 92/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4598 - mae: 1.9982 - val_loss: 1.2144 - val_mae: 1.6950\n",
            "Epoch 93/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.4613 - mae: 2.0026 - val_loss: 1.2129 - val_mae: 1.6938\n",
            "Epoch 94/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4700 - mae: 2.0132 - val_loss: 1.2120 - val_mae: 1.6929\n",
            "Epoch 95/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.4566 - mae: 1.9954 - val_loss: 1.2107 - val_mae: 1.6915\n",
            "Epoch 96/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.4477 - mae: 1.9870 - val_loss: 1.2092 - val_mae: 1.6899\n",
            "Epoch 97/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4647 - mae: 2.0039 - val_loss: 1.2079 - val_mae: 1.6886\n",
            "Epoch 98/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4510 - mae: 1.9909 - val_loss: 1.2056 - val_mae: 1.6855\n",
            "Epoch 99/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.4398 - mae: 1.9781 - val_loss: 1.2032 - val_mae: 1.6831\n",
            "Epoch 100/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.4640 - mae: 2.0042 - val_loss: 1.2011 - val_mae: 1.6805\n",
            "Epoch 101/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4516 - mae: 1.9911 - val_loss: 1.2008 - val_mae: 1.6800\n",
            "Epoch 102/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4525 - mae: 1.9906 - val_loss: 1.1997 - val_mae: 1.6789\n",
            "Epoch 103/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.4339 - mae: 1.9719 - val_loss: 1.1985 - val_mae: 1.6774\n",
            "Epoch 104/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.4449 - mae: 1.9842 - val_loss: 1.1973 - val_mae: 1.6761\n",
            "Epoch 105/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.4490 - mae: 1.9874 - val_loss: 1.1948 - val_mae: 1.6739\n",
            "Epoch 106/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.4288 - mae: 1.9668 - val_loss: 1.1944 - val_mae: 1.6730\n",
            "Epoch 107/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.4524 - mae: 1.9893 - val_loss: 1.1933 - val_mae: 1.6716\n",
            "Epoch 108/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.4433 - mae: 1.9802 - val_loss: 1.1916 - val_mae: 1.6692\n",
            "Epoch 109/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.4346 - mae: 1.9702 - val_loss: 1.1907 - val_mae: 1.6676\n",
            "Epoch 110/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3935 - mae: 1.9276 - val_loss: 1.1894 - val_mae: 1.6658\n",
            "Epoch 111/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.4159 - mae: 1.9505 - val_loss: 1.1877 - val_mae: 1.6635\n",
            "Epoch 112/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.4048 - mae: 1.9418 - val_loss: 1.1852 - val_mae: 1.6608\n",
            "Epoch 113/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.3998 - mae: 1.9381 - val_loss: 1.1831 - val_mae: 1.6588\n",
            "Epoch 114/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4220 - mae: 1.9578 - val_loss: 1.1825 - val_mae: 1.6585\n",
            "Epoch 115/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.4082 - mae: 1.9427 - val_loss: 1.1806 - val_mae: 1.6564\n",
            "Epoch 116/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3888 - mae: 1.9243 - val_loss: 1.1797 - val_mae: 1.6560\n",
            "Epoch 117/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.4293 - mae: 1.9686 - val_loss: 1.1798 - val_mae: 1.6563\n",
            "Epoch 118/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4161 - mae: 1.9521 - val_loss: 1.1804 - val_mae: 1.6567\n",
            "Epoch 119/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4013 - mae: 1.9348 - val_loss: 1.1791 - val_mae: 1.6555\n",
            "Epoch 120/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4269 - mae: 1.9605 - val_loss: 1.1784 - val_mae: 1.6550\n",
            "Epoch 121/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3772 - mae: 1.9078 - val_loss: 1.1766 - val_mae: 1.6530\n",
            "Epoch 122/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4141 - mae: 1.9486 - val_loss: 1.1767 - val_mae: 1.6528\n",
            "Epoch 123/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3993 - mae: 1.9320 - val_loss: 1.1756 - val_mae: 1.6517\n",
            "Epoch 124/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3969 - mae: 1.9313 - val_loss: 1.1731 - val_mae: 1.6488\n",
            "Epoch 125/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3890 - mae: 1.9213 - val_loss: 1.1723 - val_mae: 1.6481\n",
            "Epoch 126/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.4048 - mae: 1.9386 - val_loss: 1.1700 - val_mae: 1.6452\n",
            "Epoch 127/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3723 - mae: 1.9046 - val_loss: 1.1694 - val_mae: 1.6445\n",
            "Epoch 128/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3811 - mae: 1.9122 - val_loss: 1.1682 - val_mae: 1.6433\n",
            "Epoch 129/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3609 - mae: 1.8902 - val_loss: 1.1673 - val_mae: 1.6419\n",
            "Epoch 130/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3839 - mae: 1.9153 - val_loss: 1.1663 - val_mae: 1.6404\n",
            "Epoch 131/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3681 - mae: 1.8989 - val_loss: 1.1666 - val_mae: 1.6406\n",
            "Epoch 132/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.4195 - mae: 1.9536 - val_loss: 1.1648 - val_mae: 1.6384\n",
            "Epoch 133/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3819 - mae: 1.9131 - val_loss: 1.1641 - val_mae: 1.6377\n",
            "Epoch 134/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3956 - mae: 1.9318 - val_loss: 1.1634 - val_mae: 1.6366\n",
            "Epoch 135/200\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.3925 - mae: 1.9267 - val_loss: 1.1626 - val_mae: 1.6360\n",
            "Epoch 136/200\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.3680 - mae: 1.8987 - val_loss: 1.1630 - val_mae: 1.6365\n",
            "Epoch 137/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3744 - mae: 1.9043 - val_loss: 1.1624 - val_mae: 1.6357\n",
            "Epoch 138/200\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.3668 - mae: 1.8981 - val_loss: 1.1620 - val_mae: 1.6353\n",
            "Epoch 139/200\n",
            "14/14 [==============================] - 0s 20ms/step - loss: 1.3905 - mae: 1.9234 - val_loss: 1.1610 - val_mae: 1.6345\n",
            "Epoch 140/200\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.3777 - mae: 1.9088 - val_loss: 1.1599 - val_mae: 1.6330\n",
            "Epoch 141/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3836 - mae: 1.9131 - val_loss: 1.1586 - val_mae: 1.6313\n",
            "Epoch 142/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3875 - mae: 1.9183 - val_loss: 1.1574 - val_mae: 1.6297\n",
            "Epoch 143/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3455 - mae: 1.8740 - val_loss: 1.1579 - val_mae: 1.6306\n",
            "Epoch 144/200\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.3656 - mae: 1.8938 - val_loss: 1.1570 - val_mae: 1.6298\n",
            "Epoch 145/200\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.3816 - mae: 1.9109 - val_loss: 1.1562 - val_mae: 1.6287\n",
            "Epoch 146/200\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.3743 - mae: 1.9065 - val_loss: 1.1564 - val_mae: 1.6288\n",
            "Epoch 147/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3366 - mae: 1.8645 - val_loss: 1.1561 - val_mae: 1.6286\n",
            "Epoch 148/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3761 - mae: 1.9066 - val_loss: 1.1546 - val_mae: 1.6271\n",
            "Epoch 149/200\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.3529 - mae: 1.8812 - val_loss: 1.1530 - val_mae: 1.6256\n",
            "Epoch 150/200\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.3723 - mae: 1.8992 - val_loss: 1.1521 - val_mae: 1.6245\n",
            "Epoch 151/200\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 1.3620 - mae: 1.8908 - val_loss: 1.1510 - val_mae: 1.6235\n",
            "Epoch 152/200\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3549 - mae: 1.8835 - val_loss: 1.1492 - val_mae: 1.6214\n",
            "Epoch 153/200\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.3226 - mae: 1.8488 - val_loss: 1.1496 - val_mae: 1.6220\n",
            "Epoch 154/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3475 - mae: 1.8756 - val_loss: 1.1490 - val_mae: 1.6217\n",
            "Epoch 155/200\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.3471 - mae: 1.8706 - val_loss: 1.1487 - val_mae: 1.6207\n",
            "Epoch 156/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3384 - mae: 1.8659 - val_loss: 1.1473 - val_mae: 1.6194\n",
            "Epoch 157/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3327 - mae: 1.8604 - val_loss: 1.1477 - val_mae: 1.6196\n",
            "Epoch 158/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3380 - mae: 1.8634 - val_loss: 1.1468 - val_mae: 1.6184\n",
            "Epoch 159/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3552 - mae: 1.8827 - val_loss: 1.1464 - val_mae: 1.6179\n",
            "Epoch 160/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3498 - mae: 1.8776 - val_loss: 1.1456 - val_mae: 1.6171\n",
            "Epoch 161/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3530 - mae: 1.8845 - val_loss: 1.1435 - val_mae: 1.6147\n",
            "Epoch 162/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3453 - mae: 1.8714 - val_loss: 1.1425 - val_mae: 1.6137\n",
            "Epoch 163/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3418 - mae: 1.8657 - val_loss: 1.1427 - val_mae: 1.6138\n",
            "Epoch 164/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3562 - mae: 1.8817 - val_loss: 1.1427 - val_mae: 1.6138\n",
            "Epoch 165/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.3318 - mae: 1.8581 - val_loss: 1.1410 - val_mae: 1.6122\n",
            "Epoch 166/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3330 - mae: 1.8594 - val_loss: 1.1394 - val_mae: 1.6106\n",
            "Epoch 167/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3568 - mae: 1.8843 - val_loss: 1.1385 - val_mae: 1.6097\n",
            "Epoch 168/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.3592 - mae: 1.8857 - val_loss: 1.1374 - val_mae: 1.6085\n",
            "Epoch 169/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3281 - mae: 1.8564 - val_loss: 1.1369 - val_mae: 1.6081\n",
            "Epoch 170/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3443 - mae: 1.8679 - val_loss: 1.1367 - val_mae: 1.6077\n",
            "Epoch 171/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3164 - mae: 1.8376 - val_loss: 1.1371 - val_mae: 1.6083\n",
            "Epoch 172/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.3121 - mae: 1.8342 - val_loss: 1.1363 - val_mae: 1.6068\n",
            "Epoch 173/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3521 - mae: 1.8790 - val_loss: 1.1362 - val_mae: 1.6063\n",
            "Epoch 174/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3314 - mae: 1.8567 - val_loss: 1.1368 - val_mae: 1.6074\n",
            "Epoch 175/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3466 - mae: 1.8745 - val_loss: 1.1361 - val_mae: 1.6066\n",
            "Epoch 176/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3388 - mae: 1.8630 - val_loss: 1.1365 - val_mae: 1.6070\n",
            "Epoch 177/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.3193 - mae: 1.8419 - val_loss: 1.1371 - val_mae: 1.6081\n",
            "Epoch 178/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3411 - mae: 1.8671 - val_loss: 1.1363 - val_mae: 1.6069\n",
            "Epoch 179/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3160 - mae: 1.8380 - val_loss: 1.1373 - val_mae: 1.6078\n",
            "Epoch 180/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3348 - mae: 1.8611 - val_loss: 1.1368 - val_mae: 1.6072\n",
            "Epoch 181/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.3152 - mae: 1.8384 - val_loss: 1.1358 - val_mae: 1.6060\n",
            "Epoch 182/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3255 - mae: 1.8463 - val_loss: 1.1369 - val_mae: 1.6078\n",
            "Epoch 183/200\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.3007 - mae: 1.8186 - val_loss: 1.1371 - val_mae: 1.6079\n",
            "Epoch 184/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3142 - mae: 1.8337 - val_loss: 1.1367 - val_mae: 1.6072\n",
            "Epoch 185/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.3272 - mae: 1.8490 - val_loss: 1.1354 - val_mae: 1.6054\n",
            "Epoch 186/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3099 - mae: 1.8342 - val_loss: 1.1355 - val_mae: 1.6056\n",
            "Epoch 187/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3173 - mae: 1.8410 - val_loss: 1.1359 - val_mae: 1.6059\n",
            "Epoch 188/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3191 - mae: 1.8423 - val_loss: 1.1353 - val_mae: 1.6050\n",
            "Epoch 189/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3341 - mae: 1.8591 - val_loss: 1.1344 - val_mae: 1.6037\n",
            "Epoch 190/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3445 - mae: 1.8694 - val_loss: 1.1344 - val_mae: 1.6037\n",
            "Epoch 191/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3139 - mae: 1.8375 - val_loss: 1.1334 - val_mae: 1.6029\n",
            "Epoch 192/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3114 - mae: 1.8346 - val_loss: 1.1323 - val_mae: 1.6015\n",
            "Epoch 193/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3295 - mae: 1.8497 - val_loss: 1.1320 - val_mae: 1.6009\n",
            "Epoch 194/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3076 - mae: 1.8271 - val_loss: 1.1318 - val_mae: 1.6003\n",
            "Epoch 195/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3044 - mae: 1.8247 - val_loss: 1.1316 - val_mae: 1.6001\n",
            "Epoch 196/200\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3149 - mae: 1.8387 - val_loss: 1.1310 - val_mae: 1.5994\n",
            "Epoch 197/200\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3075 - mae: 1.8266 - val_loss: 1.1300 - val_mae: 1.5986\n",
            "Epoch 198/200\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3347 - mae: 1.8586 - val_loss: 1.1287 - val_mae: 1.5970\n",
            "Epoch 199/200\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3128 - mae: 1.8327 - val_loss: 1.1286 - val_mae: 1.5964\n",
            "Epoch 200/200\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.3076 - mae: 1.8308 - val_loss: 1.1287 - val_mae: 1.5967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 12:01:06,669] Trial 37 finished with value: 1.1286945343017578 and parameters: {'dropout_2': 0.1831008823395608, 'dropout_3': 0.49911717303916603, 'dropout_4': 0.5272798716135507, 'dropout_5': 0.5035110952703578, 'learning_rate': 3.747797638220542e-05, 'epochs': 200, 'batch_size': 256}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "27/27 [==============================] - 3s 17ms/step - loss: 1.7128 - mae: 2.2746 - val_loss: 1.3813 - val_mae: 1.8958\n",
            "Epoch 2/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.7094 - mae: 2.2735 - val_loss: 1.4111 - val_mae: 1.9533\n",
            "Epoch 3/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.7135 - mae: 2.2746 - val_loss: 1.4519 - val_mae: 2.0075\n",
            "Epoch 4/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.7220 - mae: 2.2838 - val_loss: 1.4837 - val_mae: 2.0421\n",
            "Epoch 5/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.7188 - mae: 2.2821 - val_loss: 1.5052 - val_mae: 2.0654\n",
            "Epoch 6/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.7089 - mae: 2.2716 - val_loss: 1.5221 - val_mae: 2.0869\n",
            "Epoch 7/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6922 - mae: 2.2543 - val_loss: 1.5340 - val_mae: 2.1019\n",
            "Epoch 8/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.7059 - mae: 2.2702 - val_loss: 1.5448 - val_mae: 2.1154\n",
            "Epoch 9/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.7066 - mae: 2.2682 - val_loss: 1.5507 - val_mae: 2.1228\n",
            "Epoch 10/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6944 - mae: 2.2568 - val_loss: 1.5516 - val_mae: 2.1243\n",
            "Epoch 11/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6845 - mae: 2.2447 - val_loss: 1.5547 - val_mae: 2.1283\n",
            "Epoch 12/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.7114 - mae: 2.2727 - val_loss: 1.5546 - val_mae: 2.1283\n",
            "Epoch 13/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.7055 - mae: 2.2655 - val_loss: 1.5577 - val_mae: 2.1322\n",
            "Epoch 14/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.7076 - mae: 2.2671 - val_loss: 1.5582 - val_mae: 2.1330\n",
            "Epoch 15/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6905 - mae: 2.2499 - val_loss: 1.5597 - val_mae: 2.1349\n",
            "Epoch 16/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6753 - mae: 2.2321 - val_loss: 1.5588 - val_mae: 2.1341\n",
            "Epoch 17/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.7011 - mae: 2.2635 - val_loss: 1.5570 - val_mae: 2.1322\n",
            "Epoch 18/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6953 - mae: 2.2543 - val_loss: 1.5515 - val_mae: 2.1258\n",
            "Epoch 19/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6677 - mae: 2.2254 - val_loss: 1.5516 - val_mae: 2.1262\n",
            "Epoch 20/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6933 - mae: 2.2498 - val_loss: 1.5509 - val_mae: 2.1255\n",
            "Epoch 21/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6893 - mae: 2.2497 - val_loss: 1.5496 - val_mae: 2.1243\n",
            "Epoch 22/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6790 - mae: 2.2403 - val_loss: 1.5497 - val_mae: 2.1246\n",
            "Epoch 23/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6773 - mae: 2.2363 - val_loss: 1.5457 - val_mae: 2.1200\n",
            "Epoch 24/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6831 - mae: 2.2406 - val_loss: 1.5481 - val_mae: 2.1232\n",
            "Epoch 25/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.6706 - mae: 2.2288 - val_loss: 1.5469 - val_mae: 2.1220\n",
            "Epoch 26/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6552 - mae: 2.2137 - val_loss: 1.5437 - val_mae: 2.1184\n",
            "Epoch 27/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6627 - mae: 2.2219 - val_loss: 1.5417 - val_mae: 2.1163\n",
            "Epoch 28/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6483 - mae: 2.2091 - val_loss: 1.5351 - val_mae: 2.1084\n",
            "Epoch 29/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6467 - mae: 2.2033 - val_loss: 1.5306 - val_mae: 2.1032\n",
            "Epoch 30/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6599 - mae: 2.2183 - val_loss: 1.5297 - val_mae: 2.1024\n",
            "Epoch 31/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6521 - mae: 2.2082 - val_loss: 1.5262 - val_mae: 2.0983\n",
            "Epoch 32/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6503 - mae: 2.2057 - val_loss: 1.5258 - val_mae: 2.0981\n",
            "Epoch 33/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6530 - mae: 2.2101 - val_loss: 1.5192 - val_mae: 2.0906\n",
            "Epoch 34/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6486 - mae: 2.2053 - val_loss: 1.5159 - val_mae: 2.0867\n",
            "Epoch 35/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6376 - mae: 2.1963 - val_loss: 1.5154 - val_mae: 2.0863\n",
            "Epoch 36/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6762 - mae: 2.2352 - val_loss: 1.5143 - val_mae: 2.0852\n",
            "Epoch 37/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6440 - mae: 2.1987 - val_loss: 1.5111 - val_mae: 2.0817\n",
            "Epoch 38/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6666 - mae: 2.2268 - val_loss: 1.5117 - val_mae: 2.0825\n",
            "Epoch 39/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6132 - mae: 2.1675 - val_loss: 1.5069 - val_mae: 2.0768\n",
            "Epoch 40/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.6437 - mae: 2.2024 - val_loss: 1.5058 - val_mae: 2.0754\n",
            "Epoch 41/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6389 - mae: 2.1913 - val_loss: 1.5014 - val_mae: 2.0702\n",
            "Epoch 42/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6272 - mae: 2.1817 - val_loss: 1.4998 - val_mae: 2.0684\n",
            "Epoch 43/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6349 - mae: 2.1905 - val_loss: 1.4987 - val_mae: 2.0671\n",
            "Epoch 44/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.6318 - mae: 2.1875 - val_loss: 1.4953 - val_mae: 2.0631\n",
            "Epoch 45/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.6252 - mae: 2.1786 - val_loss: 1.4917 - val_mae: 2.0590\n",
            "Epoch 46/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.6039 - mae: 2.1566 - val_loss: 1.4873 - val_mae: 2.0536\n",
            "Epoch 47/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.6003 - mae: 2.1541 - val_loss: 1.4858 - val_mae: 2.0522\n",
            "Epoch 48/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.6340 - mae: 2.1884 - val_loss: 1.4854 - val_mae: 2.0522\n",
            "Epoch 49/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.6062 - mae: 2.1586 - val_loss: 1.4842 - val_mae: 2.0507\n",
            "Epoch 50/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.6256 - mae: 2.1800 - val_loss: 1.4798 - val_mae: 2.0457\n",
            "Epoch 51/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.6143 - mae: 2.1689 - val_loss: 1.4769 - val_mae: 2.0420\n",
            "Epoch 52/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.6208 - mae: 2.1776 - val_loss: 1.4770 - val_mae: 2.0425\n",
            "Epoch 53/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.6152 - mae: 2.1721 - val_loss: 1.4757 - val_mae: 2.0410\n",
            "Epoch 54/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.6127 - mae: 2.1703 - val_loss: 1.4736 - val_mae: 2.0384\n",
            "Epoch 55/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.6003 - mae: 2.1506 - val_loss: 1.4715 - val_mae: 2.0360\n",
            "Epoch 56/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5917 - mae: 2.1412 - val_loss: 1.4673 - val_mae: 2.0310\n",
            "Epoch 57/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5925 - mae: 2.1480 - val_loss: 1.4658 - val_mae: 2.0293\n",
            "Epoch 58/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5918 - mae: 2.1443 - val_loss: 1.4624 - val_mae: 2.0251\n",
            "Epoch 59/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.6006 - mae: 2.1514 - val_loss: 1.4580 - val_mae: 2.0196\n",
            "Epoch 60/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5889 - mae: 2.1430 - val_loss: 1.4574 - val_mae: 2.0191\n",
            "Epoch 61/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5967 - mae: 2.1502 - val_loss: 1.4531 - val_mae: 2.0140\n",
            "Epoch 62/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.6088 - mae: 2.1626 - val_loss: 1.4548 - val_mae: 2.0165\n",
            "Epoch 63/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5956 - mae: 2.1498 - val_loss: 1.4525 - val_mae: 2.0135\n",
            "Epoch 64/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.5972 - mae: 2.1493 - val_loss: 1.4487 - val_mae: 2.0088\n",
            "Epoch 65/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5989 - mae: 2.1517 - val_loss: 1.4476 - val_mae: 2.0075\n",
            "Epoch 66/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5921 - mae: 2.1443 - val_loss: 1.4449 - val_mae: 2.0044\n",
            "Epoch 67/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.6076 - mae: 2.1619 - val_loss: 1.4421 - val_mae: 2.0006\n",
            "Epoch 68/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5873 - mae: 2.1353 - val_loss: 1.4407 - val_mae: 1.9992\n",
            "Epoch 69/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.5886 - mae: 2.1432 - val_loss: 1.4399 - val_mae: 1.9988\n",
            "Epoch 70/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.5801 - mae: 2.1299 - val_loss: 1.4375 - val_mae: 1.9955\n",
            "Epoch 71/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5797 - mae: 2.1328 - val_loss: 1.4381 - val_mae: 1.9964\n",
            "Epoch 72/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5746 - mae: 2.1243 - val_loss: 1.4340 - val_mae: 1.9912\n",
            "Epoch 73/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5818 - mae: 2.1340 - val_loss: 1.4310 - val_mae: 1.9870\n",
            "Epoch 74/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.5594 - mae: 2.1080 - val_loss: 1.4289 - val_mae: 1.9846\n",
            "Epoch 75/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5563 - mae: 2.1067 - val_loss: 1.4263 - val_mae: 1.9814\n",
            "Epoch 76/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5753 - mae: 2.1263 - val_loss: 1.4264 - val_mae: 1.9815\n",
            "Epoch 77/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5807 - mae: 2.1288 - val_loss: 1.4238 - val_mae: 1.9783\n",
            "Epoch 78/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.5616 - mae: 2.1100 - val_loss: 1.4241 - val_mae: 1.9784\n",
            "Epoch 79/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.5829 - mae: 2.1315 - val_loss: 1.4209 - val_mae: 1.9748\n",
            "Epoch 80/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5524 - mae: 2.1007 - val_loss: 1.4204 - val_mae: 1.9742\n",
            "Epoch 81/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5558 - mae: 2.1043 - val_loss: 1.4164 - val_mae: 1.9691\n",
            "Epoch 82/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.5704 - mae: 2.1191 - val_loss: 1.4138 - val_mae: 1.9658\n",
            "Epoch 83/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5613 - mae: 2.1098 - val_loss: 1.4127 - val_mae: 1.9644\n",
            "Epoch 84/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.5740 - mae: 2.1236 - val_loss: 1.4102 - val_mae: 1.9615\n",
            "Epoch 85/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.5497 - mae: 2.0991 - val_loss: 1.4088 - val_mae: 1.9599\n",
            "Epoch 86/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.5718 - mae: 2.1263 - val_loss: 1.4067 - val_mae: 1.9572\n",
            "Epoch 87/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.5559 - mae: 2.1079 - val_loss: 1.4048 - val_mae: 1.9547\n",
            "Epoch 88/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.5650 - mae: 2.1130 - val_loss: 1.4038 - val_mae: 1.9537\n",
            "Epoch 89/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5606 - mae: 2.1097 - val_loss: 1.3999 - val_mae: 1.9491\n",
            "Epoch 90/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.5400 - mae: 2.0876 - val_loss: 1.3974 - val_mae: 1.9460\n",
            "Epoch 91/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5491 - mae: 2.0949 - val_loss: 1.3964 - val_mae: 1.9447\n",
            "Epoch 92/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.5532 - mae: 2.1005 - val_loss: 1.3964 - val_mae: 1.9448\n",
            "Epoch 93/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.5384 - mae: 2.0831 - val_loss: 1.3939 - val_mae: 1.9416\n",
            "Epoch 94/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5428 - mae: 2.0902 - val_loss: 1.3904 - val_mae: 1.9365\n",
            "Epoch 95/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5516 - mae: 2.0992 - val_loss: 1.3898 - val_mae: 1.9351\n",
            "Epoch 96/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5256 - mae: 2.0722 - val_loss: 1.3865 - val_mae: 1.9310\n",
            "Epoch 97/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.5397 - mae: 2.0875 - val_loss: 1.3843 - val_mae: 1.9281\n",
            "Epoch 98/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.5481 - mae: 2.0935 - val_loss: 1.3820 - val_mae: 1.9253\n",
            "Epoch 99/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5281 - mae: 2.0762 - val_loss: 1.3811 - val_mae: 1.9241\n",
            "Epoch 100/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.5321 - mae: 2.0767 - val_loss: 1.3791 - val_mae: 1.9215\n",
            "Epoch 101/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5131 - mae: 2.0608 - val_loss: 1.3782 - val_mae: 1.9201\n",
            "Epoch 102/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5415 - mae: 2.0891 - val_loss: 1.3771 - val_mae: 1.9185\n",
            "Epoch 103/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.5478 - mae: 2.0962 - val_loss: 1.3755 - val_mae: 1.9168\n",
            "Epoch 104/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5147 - mae: 2.0600 - val_loss: 1.3747 - val_mae: 1.9157\n",
            "Epoch 105/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5288 - mae: 2.0768 - val_loss: 1.3727 - val_mae: 1.9128\n",
            "Epoch 106/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5318 - mae: 2.0800 - val_loss: 1.3716 - val_mae: 1.9115\n",
            "Epoch 107/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5371 - mae: 2.0858 - val_loss: 1.3701 - val_mae: 1.9099\n",
            "Epoch 108/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.5078 - mae: 2.0521 - val_loss: 1.3693 - val_mae: 1.9091\n",
            "Epoch 109/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.5115 - mae: 2.0565 - val_loss: 1.3669 - val_mae: 1.9058\n",
            "Epoch 110/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.5196 - mae: 2.0648 - val_loss: 1.3643 - val_mae: 1.9023\n",
            "Epoch 111/150\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.5002 - mae: 2.0437 - val_loss: 1.3617 - val_mae: 1.8990\n",
            "Epoch 112/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.5232 - mae: 2.0686 - val_loss: 1.3608 - val_mae: 1.8980\n",
            "Epoch 113/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5340 - mae: 2.0797 - val_loss: 1.3590 - val_mae: 1.8956\n",
            "Epoch 114/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5204 - mae: 2.0614 - val_loss: 1.3574 - val_mae: 1.8937\n",
            "Epoch 115/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.4980 - mae: 2.0400 - val_loss: 1.3563 - val_mae: 1.8925\n",
            "Epoch 116/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5050 - mae: 2.0459 - val_loss: 1.3543 - val_mae: 1.8900\n",
            "Epoch 117/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.5039 - mae: 2.0482 - val_loss: 1.3541 - val_mae: 1.8894\n",
            "Epoch 118/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5087 - mae: 2.0510 - val_loss: 1.3523 - val_mae: 1.8877\n",
            "Epoch 119/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5193 - mae: 2.0626 - val_loss: 1.3504 - val_mae: 1.8855\n",
            "Epoch 120/150\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.5106 - mae: 2.0537 - val_loss: 1.3496 - val_mae: 1.8844\n",
            "Epoch 121/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.4951 - mae: 2.0387 - val_loss: 1.3476 - val_mae: 1.8823\n",
            "Epoch 122/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.4942 - mae: 2.0359 - val_loss: 1.3460 - val_mae: 1.8804\n",
            "Epoch 123/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5089 - mae: 2.0533 - val_loss: 1.3443 - val_mae: 1.8776\n",
            "Epoch 124/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.5083 - mae: 2.0507 - val_loss: 1.3438 - val_mae: 1.8772\n",
            "Epoch 125/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.4792 - mae: 2.0220 - val_loss: 1.3423 - val_mae: 1.8756\n",
            "Epoch 126/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.4998 - mae: 2.0431 - val_loss: 1.3400 - val_mae: 1.8729\n",
            "Epoch 127/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5043 - mae: 2.0468 - val_loss: 1.3385 - val_mae: 1.8708\n",
            "Epoch 128/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5002 - mae: 2.0459 - val_loss: 1.3380 - val_mae: 1.8700\n",
            "Epoch 129/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.4957 - mae: 2.0380 - val_loss: 1.3385 - val_mae: 1.8704\n",
            "Epoch 130/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.4925 - mae: 2.0375 - val_loss: 1.3379 - val_mae: 1.8698\n",
            "Epoch 131/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.4940 - mae: 2.0359 - val_loss: 1.3359 - val_mae: 1.8677\n",
            "Epoch 132/150\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.4866 - mae: 2.0306 - val_loss: 1.3356 - val_mae: 1.8672\n",
            "Epoch 133/150\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.4982 - mae: 2.0438 - val_loss: 1.3349 - val_mae: 1.8666\n",
            "Epoch 134/150\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.4784 - mae: 2.0219 - val_loss: 1.3327 - val_mae: 1.8634\n",
            "Epoch 135/150\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.4854 - mae: 2.0266 - val_loss: 1.3306 - val_mae: 1.8610\n",
            "Epoch 136/150\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.4794 - mae: 2.0177 - val_loss: 1.3294 - val_mae: 1.8590\n",
            "Epoch 137/150\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.4782 - mae: 2.0192 - val_loss: 1.3270 - val_mae: 1.8553\n",
            "Epoch 138/150\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.4834 - mae: 2.0302 - val_loss: 1.3252 - val_mae: 1.8532\n",
            "Epoch 139/150\n",
            "27/27 [==============================] - 1s 20ms/step - loss: 1.4981 - mae: 2.0399 - val_loss: 1.3249 - val_mae: 1.8529\n",
            "Epoch 140/150\n",
            "27/27 [==============================] - 0s 18ms/step - loss: 1.4857 - mae: 2.0263 - val_loss: 1.3231 - val_mae: 1.8503\n",
            "Epoch 141/150\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 1.4716 - mae: 2.0095 - val_loss: 1.3210 - val_mae: 1.8475\n",
            "Epoch 142/150\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 1.4813 - mae: 2.0243 - val_loss: 1.3189 - val_mae: 1.8448\n",
            "Epoch 143/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.4768 - mae: 2.0202 - val_loss: 1.3171 - val_mae: 1.8421\n",
            "Epoch 144/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.4927 - mae: 2.0320 - val_loss: 1.3167 - val_mae: 1.8421\n",
            "Epoch 145/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.4810 - mae: 2.0219 - val_loss: 1.3164 - val_mae: 1.8417\n",
            "Epoch 146/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.4615 - mae: 2.0016 - val_loss: 1.3149 - val_mae: 1.8396\n",
            "Epoch 147/150\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.4703 - mae: 2.0086 - val_loss: 1.3154 - val_mae: 1.8404\n",
            "Epoch 148/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.4566 - mae: 1.9971 - val_loss: 1.3138 - val_mae: 1.8383\n",
            "Epoch 149/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.4760 - mae: 2.0160 - val_loss: 1.3134 - val_mae: 1.8378\n",
            "Epoch 150/150\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.4731 - mae: 2.0134 - val_loss: 1.3114 - val_mae: 1.8349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 12:01:50,261] Trial 38 finished with value: 1.3113930225372314 and parameters: {'dropout_2': 0.2628511268804141, 'dropout_3': 0.38971160493926893, 'dropout_4': 0.6000696497336975, 'dropout_5': 0.4472339754809921, 'learning_rate': 1.2553779150663017e-05, 'epochs': 150, 'batch_size': 128}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "108/108 [==============================] - 3s 10ms/step - loss: 1.7834 - mae: 2.3570 - val_loss: 1.3845 - val_mae: 1.8898\n",
            "Epoch 2/50\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.7529 - mae: 2.3250 - val_loss: 1.4108 - val_mae: 1.9330\n",
            "Epoch 3/50\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.7425 - mae: 2.3121 - val_loss: 1.4232 - val_mae: 1.9515\n",
            "Epoch 4/50\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.7473 - mae: 2.3187 - val_loss: 1.4267 - val_mae: 1.9586\n",
            "Epoch 5/50\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.7654 - mae: 2.3352 - val_loss: 1.4339 - val_mae: 1.9673\n",
            "Epoch 6/50\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.7358 - mae: 2.3049 - val_loss: 1.4363 - val_mae: 1.9711\n",
            "Epoch 7/50\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.7278 - mae: 2.2966 - val_loss: 1.4370 - val_mae: 1.9718\n",
            "Epoch 8/50\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.7385 - mae: 2.3082 - val_loss: 1.4358 - val_mae: 1.9694\n",
            "Epoch 9/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7605 - mae: 2.3287 - val_loss: 1.4362 - val_mae: 1.9696\n",
            "Epoch 10/50\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.7342 - mae: 2.3031 - val_loss: 1.4371 - val_mae: 1.9703\n",
            "Epoch 11/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7469 - mae: 2.3181 - val_loss: 1.4328 - val_mae: 1.9665\n",
            "Epoch 12/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7371 - mae: 2.3023 - val_loss: 1.4400 - val_mae: 1.9759\n",
            "Epoch 13/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7525 - mae: 2.3225 - val_loss: 1.4338 - val_mae: 1.9677\n",
            "Epoch 14/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7239 - mae: 2.2918 - val_loss: 1.4290 - val_mae: 1.9607\n",
            "Epoch 15/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7501 - mae: 2.3198 - val_loss: 1.4305 - val_mae: 1.9635\n",
            "Epoch 16/50\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.7148 - mae: 2.2823 - val_loss: 1.4267 - val_mae: 1.9601\n",
            "Epoch 17/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7564 - mae: 2.3287 - val_loss: 1.4323 - val_mae: 1.9659\n",
            "Epoch 18/50\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.7379 - mae: 2.3095 - val_loss: 1.4240 - val_mae: 1.9557\n",
            "Epoch 19/50\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.7041 - mae: 2.2704 - val_loss: 1.4278 - val_mae: 1.9612\n",
            "Epoch 20/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7387 - mae: 2.3061 - val_loss: 1.4285 - val_mae: 1.9604\n",
            "Epoch 21/50\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.7316 - mae: 2.2982 - val_loss: 1.4341 - val_mae: 1.9675\n",
            "Epoch 22/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7249 - mae: 2.2962 - val_loss: 1.4325 - val_mae: 1.9670\n",
            "Epoch 23/50\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.7182 - mae: 2.2879 - val_loss: 1.4350 - val_mae: 1.9683\n",
            "Epoch 24/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7346 - mae: 2.3047 - val_loss: 1.4304 - val_mae: 1.9625\n",
            "Epoch 25/50\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.7154 - mae: 2.2833 - val_loss: 1.4330 - val_mae: 1.9669\n",
            "Epoch 26/50\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.7355 - mae: 2.3050 - val_loss: 1.4270 - val_mae: 1.9601\n",
            "Epoch 27/50\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.7367 - mae: 2.3041 - val_loss: 1.4299 - val_mae: 1.9622\n",
            "Epoch 28/50\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.7044 - mae: 2.2715 - val_loss: 1.4296 - val_mae: 1.9635\n",
            "Epoch 29/50\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.7008 - mae: 2.2672 - val_loss: 1.4289 - val_mae: 1.9632\n",
            "Epoch 30/50\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.7420 - mae: 2.3127 - val_loss: 1.4288 - val_mae: 1.9625\n",
            "Epoch 31/50\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.7197 - mae: 2.2900 - val_loss: 1.4253 - val_mae: 1.9578\n",
            "Epoch 32/50\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.7206 - mae: 2.2865 - val_loss: 1.4292 - val_mae: 1.9632\n",
            "Epoch 33/50\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.7117 - mae: 2.2801 - val_loss: 1.4285 - val_mae: 1.9619\n",
            "Epoch 34/50\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.7199 - mae: 2.2857 - val_loss: 1.4315 - val_mae: 1.9681\n",
            "Epoch 35/50\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.7228 - mae: 2.2904 - val_loss: 1.4313 - val_mae: 1.9655\n",
            "Epoch 36/50\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.7391 - mae: 2.3092 - val_loss: 1.4303 - val_mae: 1.9637\n",
            "Epoch 37/50\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.7247 - mae: 2.2942 - val_loss: 1.4291 - val_mae: 1.9621\n",
            "Epoch 38/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7265 - mae: 2.2931 - val_loss: 1.4272 - val_mae: 1.9613\n",
            "Epoch 39/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7305 - mae: 2.3000 - val_loss: 1.4277 - val_mae: 1.9607\n",
            "Epoch 40/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7318 - mae: 2.3007 - val_loss: 1.4273 - val_mae: 1.9617\n",
            "Epoch 41/50\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.7285 - mae: 2.2987 - val_loss: 1.4256 - val_mae: 1.9604\n",
            "Epoch 42/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7098 - mae: 2.2784 - val_loss: 1.4282 - val_mae: 1.9627\n",
            "Epoch 43/50\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.7089 - mae: 2.2761 - val_loss: 1.4253 - val_mae: 1.9591\n",
            "Epoch 44/50\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.7207 - mae: 2.2890 - val_loss: 1.4259 - val_mae: 1.9602\n",
            "Epoch 45/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7058 - mae: 2.2707 - val_loss: 1.4242 - val_mae: 1.9580\n",
            "Epoch 46/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7237 - mae: 2.2925 - val_loss: 1.4258 - val_mae: 1.9601\n",
            "Epoch 47/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7135 - mae: 2.2804 - val_loss: 1.4221 - val_mae: 1.9566\n",
            "Epoch 48/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7169 - mae: 2.2828 - val_loss: 1.4283 - val_mae: 1.9628\n",
            "Epoch 49/50\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7116 - mae: 2.2787 - val_loss: 1.4251 - val_mae: 1.9574\n",
            "Epoch 50/50\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.7185 - mae: 2.2861 - val_loss: 1.4200 - val_mae: 1.9527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 12:02:20,545] Trial 39 finished with value: 1.4200003147125244 and parameters: {'dropout_2': 0.10029906364158343, 'dropout_3': 0.427930196406866, 'dropout_4': 0.7228838009538843, 'dropout_5': 0.32021634050996495, 'learning_rate': 1.5291516598807379e-06, 'epochs': 50, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 3s 10ms/step - loss: 1.2797 - mae: 1.7747 - val_loss: 1.2104 - val_mae: 1.6760\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1966 - mae: 1.6856 - val_loss: 1.1403 - val_mae: 1.6231\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1630 - mae: 1.6560 - val_loss: 1.1402 - val_mae: 1.6071\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1429 - mae: 1.6251 - val_loss: 1.1936 - val_mae: 1.6870\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1432 - mae: 1.6334 - val_loss: 1.1923 - val_mae: 1.6824\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1106 - mae: 1.5995 - val_loss: 1.2153 - val_mae: 1.7508\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1422 - mae: 1.6334 - val_loss: 1.1794 - val_mae: 1.6643\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1223 - mae: 1.6101 - val_loss: 1.2220 - val_mae: 1.7557\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1228 - mae: 1.6094 - val_loss: 1.2622 - val_mae: 1.7854\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1088 - mae: 1.5943 - val_loss: 1.2168 - val_mae: 1.7227\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1085 - mae: 1.5977 - val_loss: 1.1701 - val_mae: 1.6859\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1126 - mae: 1.6006 - val_loss: 1.2453 - val_mae: 1.7157\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1087 - mae: 1.5977 - val_loss: 1.1907 - val_mae: 1.6642\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1036 - mae: 1.5906 - val_loss: 1.1575 - val_mae: 1.6272\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1190 - mae: 1.6116 - val_loss: 1.1756 - val_mae: 1.6551\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1141 - mae: 1.5984 - val_loss: 1.1645 - val_mae: 1.6543\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1018 - mae: 1.5850 - val_loss: 1.1695 - val_mae: 1.6559\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0872 - mae: 1.5710 - val_loss: 1.1589 - val_mae: 1.6831\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1032 - mae: 1.5908 - val_loss: 1.2028 - val_mae: 1.7014\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0900 - mae: 1.5798 - val_loss: 1.1462 - val_mae: 1.6336\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0932 - mae: 1.5827 - val_loss: 1.1669 - val_mae: 1.6359\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1052 - mae: 1.5926 - val_loss: 1.1630 - val_mae: 1.6567\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0882 - mae: 1.5742 - val_loss: 1.1988 - val_mae: 1.7119\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0846 - mae: 1.5739 - val_loss: 1.1696 - val_mae: 1.6350\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0890 - mae: 1.5708 - val_loss: 1.1935 - val_mae: 1.7028\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0901 - mae: 1.5779 - val_loss: 1.1803 - val_mae: 1.6624\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0888 - mae: 1.5799 - val_loss: 1.2160 - val_mae: 1.7097\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0745 - mae: 1.5575 - val_loss: 1.1433 - val_mae: 1.6634\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0874 - mae: 1.5748 - val_loss: 1.1783 - val_mae: 1.6944\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0820 - mae: 1.5662 - val_loss: 1.1450 - val_mae: 1.6236\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0953 - mae: 1.5840 - val_loss: 1.1299 - val_mae: 1.6189\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0976 - mae: 1.5861 - val_loss: 1.1995 - val_mae: 1.7206\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0778 - mae: 1.5675 - val_loss: 1.1221 - val_mae: 1.6232\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0690 - mae: 1.5560 - val_loss: 1.0975 - val_mae: 1.5822\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0700 - mae: 1.5569 - val_loss: 1.2058 - val_mae: 1.6579\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0806 - mae: 1.5681 - val_loss: 1.1375 - val_mae: 1.6172\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0796 - mae: 1.5649 - val_loss: 1.1536 - val_mae: 1.6226\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0956 - mae: 1.5834 - val_loss: 1.1244 - val_mae: 1.5804\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0927 - mae: 1.5794 - val_loss: 1.1739 - val_mae: 1.6243\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0764 - mae: 1.5633 - val_loss: 1.1305 - val_mae: 1.6455\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0795 - mae: 1.5711 - val_loss: 1.1500 - val_mae: 1.6337\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0737 - mae: 1.5604 - val_loss: 1.1336 - val_mae: 1.6229\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0787 - mae: 1.5626 - val_loss: 1.1668 - val_mae: 1.7013\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0701 - mae: 1.5540 - val_loss: 1.1123 - val_mae: 1.5804\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0805 - mae: 1.5698 - val_loss: 1.1342 - val_mae: 1.6217\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0654 - mae: 1.5495 - val_loss: 1.1373 - val_mae: 1.6589\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0896 - mae: 1.5787 - val_loss: 1.1126 - val_mae: 1.5621\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0699 - mae: 1.5564 - val_loss: 1.1491 - val_mae: 1.6565\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0767 - mae: 1.5630 - val_loss: 1.1079 - val_mae: 1.5872\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0780 - mae: 1.5660 - val_loss: 1.1408 - val_mae: 1.5982\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0597 - mae: 1.5455 - val_loss: 1.1709 - val_mae: 1.6619\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0781 - mae: 1.5661 - val_loss: 1.1330 - val_mae: 1.5818\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0693 - mae: 1.5584 - val_loss: 1.1261 - val_mae: 1.6151\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0650 - mae: 1.5497 - val_loss: 1.1072 - val_mae: 1.5769\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0716 - mae: 1.5580 - val_loss: 1.1053 - val_mae: 1.5720\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0672 - mae: 1.5508 - val_loss: 1.0854 - val_mae: 1.5699\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0801 - mae: 1.5658 - val_loss: 1.1648 - val_mae: 1.6287\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0700 - mae: 1.5563 - val_loss: 1.1239 - val_mae: 1.6106\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0888 - mae: 1.5752 - val_loss: 1.0784 - val_mae: 1.5797\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0643 - mae: 1.5492 - val_loss: 1.1796 - val_mae: 1.6739\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0816 - mae: 1.5703 - val_loss: 1.2052 - val_mae: 1.6951\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0863 - mae: 1.5759 - val_loss: 1.0996 - val_mae: 1.5818\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0633 - mae: 1.5498 - val_loss: 1.1765 - val_mae: 1.6513\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0706 - mae: 1.5562 - val_loss: 1.1796 - val_mae: 1.6308\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0698 - mae: 1.5531 - val_loss: 1.1336 - val_mae: 1.6469\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0594 - mae: 1.5434 - val_loss: 1.1434 - val_mae: 1.5917\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0742 - mae: 1.5568 - val_loss: 1.1863 - val_mae: 1.6460\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0447 - mae: 1.5321 - val_loss: 1.1164 - val_mae: 1.6117\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0756 - mae: 1.5582 - val_loss: 1.1157 - val_mae: 1.5639\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0507 - mae: 1.5370 - val_loss: 1.1237 - val_mae: 1.5934\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0762 - mae: 1.5630 - val_loss: 1.0981 - val_mae: 1.5518\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0695 - mae: 1.5542 - val_loss: 1.1296 - val_mae: 1.5955\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0699 - mae: 1.5525 - val_loss: 1.1006 - val_mae: 1.5696\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0548 - mae: 1.5369 - val_loss: 1.1270 - val_mae: 1.6536\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0500 - mae: 1.5397 - val_loss: 1.1290 - val_mae: 1.6065\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0640 - mae: 1.5563 - val_loss: 1.0912 - val_mae: 1.5866\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0778 - mae: 1.5649 - val_loss: 1.1084 - val_mae: 1.5993\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0669 - mae: 1.5512 - val_loss: 1.0957 - val_mae: 1.6176\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0647 - mae: 1.5476 - val_loss: 1.1138 - val_mae: 1.6221\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0867 - mae: 1.5746 - val_loss: 1.1766 - val_mae: 1.6473\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0688 - mae: 1.5562 - val_loss: 1.1671 - val_mae: 1.6299\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0599 - mae: 1.5431 - val_loss: 1.1008 - val_mae: 1.5866\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0659 - mae: 1.5576 - val_loss: 1.1569 - val_mae: 1.6466\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0504 - mae: 1.5408 - val_loss: 1.1905 - val_mae: 1.6727\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0525 - mae: 1.5322 - val_loss: 1.1457 - val_mae: 1.6624\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0722 - mae: 1.5579 - val_loss: 1.1069 - val_mae: 1.5638\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0514 - mae: 1.5383 - val_loss: 1.0819 - val_mae: 1.5971\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0657 - mae: 1.5557 - val_loss: 1.1450 - val_mae: 1.6041\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0595 - mae: 1.5481 - val_loss: 1.1062 - val_mae: 1.5668\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0630 - mae: 1.5513 - val_loss: 1.1234 - val_mae: 1.5919\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0614 - mae: 1.5510 - val_loss: 1.0930 - val_mae: 1.5843\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0656 - mae: 1.5552 - val_loss: 1.0715 - val_mae: 1.5335\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0585 - mae: 1.5422 - val_loss: 1.0779 - val_mae: 1.5458\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0593 - mae: 1.5455 - val_loss: 1.0957 - val_mae: 1.5631\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0599 - mae: 1.5469 - val_loss: 1.0968 - val_mae: 1.5930\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0443 - mae: 1.5331 - val_loss: 1.1424 - val_mae: 1.6316\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0532 - mae: 1.5387 - val_loss: 1.0824 - val_mae: 1.5468\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0531 - mae: 1.5409 - val_loss: 1.1046 - val_mae: 1.5749\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0628 - mae: 1.5494 - val_loss: 1.1437 - val_mae: 1.5977\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0410 - mae: 1.5257 - val_loss: 1.0815 - val_mae: 1.5912\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0388 - mae: 1.5304 - val_loss: 1.0844 - val_mae: 1.5677\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0549 - mae: 1.5390 - val_loss: 1.0821 - val_mae: 1.5796\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0528 - mae: 1.5393 - val_loss: 1.0856 - val_mae: 1.5560\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0569 - mae: 1.5428 - val_loss: 1.1019 - val_mae: 1.5588\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0569 - mae: 1.5404 - val_loss: 1.1327 - val_mae: 1.5780\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0468 - mae: 1.5312 - val_loss: 1.1485 - val_mae: 1.6497\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0451 - mae: 1.5312 - val_loss: 1.0663 - val_mae: 1.5490\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0503 - mae: 1.5385 - val_loss: 1.1489 - val_mae: 1.6309\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0549 - mae: 1.5431 - val_loss: 1.0894 - val_mae: 1.5825\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0608 - mae: 1.5478 - val_loss: 1.1064 - val_mae: 1.5767\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0272 - mae: 1.5086 - val_loss: 1.0894 - val_mae: 1.5757\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0444 - mae: 1.5281 - val_loss: 1.0967 - val_mae: 1.5732\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0577 - mae: 1.5456 - val_loss: 1.0798 - val_mae: 1.5534\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0347 - mae: 1.5214 - val_loss: 1.1009 - val_mae: 1.5536\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0567 - mae: 1.5440 - val_loss: 1.0885 - val_mae: 1.5901\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0368 - mae: 1.5199 - val_loss: 1.1436 - val_mae: 1.6362\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0530 - mae: 1.5339 - val_loss: 1.1181 - val_mae: 1.6166\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0322 - mae: 1.5187 - val_loss: 1.1587 - val_mae: 1.6564\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0252 - mae: 1.5091 - val_loss: 1.1138 - val_mae: 1.5856\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0365 - mae: 1.5241 - val_loss: 1.1350 - val_mae: 1.5978\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0415 - mae: 1.5302 - val_loss: 1.1449 - val_mae: 1.6041\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0430 - mae: 1.5283 - val_loss: 1.1553 - val_mae: 1.6503\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0442 - mae: 1.5295 - val_loss: 1.0841 - val_mae: 1.5623\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0351 - mae: 1.5215 - val_loss: 1.1083 - val_mae: 1.6039\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0399 - mae: 1.5213 - val_loss: 1.0520 - val_mae: 1.5238\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0364 - mae: 1.5245 - val_loss: 1.1012 - val_mae: 1.5501\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0391 - mae: 1.5224 - val_loss: 1.0608 - val_mae: 1.5202\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0379 - mae: 1.5210 - val_loss: 1.0728 - val_mae: 1.5536\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0523 - mae: 1.5378 - val_loss: 1.1484 - val_mae: 1.6325\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0333 - mae: 1.5212 - val_loss: 1.1330 - val_mae: 1.6054\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0438 - mae: 1.5285 - val_loss: 1.0778 - val_mae: 1.5349\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0354 - mae: 1.5185 - val_loss: 1.1082 - val_mae: 1.5937\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0386 - mae: 1.5233 - val_loss: 1.1110 - val_mae: 1.5721\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0363 - mae: 1.5189 - val_loss: 1.0760 - val_mae: 1.5374\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0441 - mae: 1.5263 - val_loss: 1.0667 - val_mae: 1.5709\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0246 - mae: 1.5070 - val_loss: 1.0861 - val_mae: 1.5773\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0309 - mae: 1.5139 - val_loss: 1.1303 - val_mae: 1.6158\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0219 - mae: 1.5004 - val_loss: 1.0754 - val_mae: 1.5553\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0224 - mae: 1.5030 - val_loss: 1.1307 - val_mae: 1.6265\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0494 - mae: 1.5347 - val_loss: 1.0537 - val_mae: 1.5244\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0457 - mae: 1.5328 - val_loss: 1.0876 - val_mae: 1.5498\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0482 - mae: 1.5304 - val_loss: 1.1678 - val_mae: 1.6320\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0352 - mae: 1.5164 - val_loss: 1.1186 - val_mae: 1.5837\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0246 - mae: 1.5091 - val_loss: 1.0882 - val_mae: 1.5719\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0360 - mae: 1.5257 - val_loss: 1.0301 - val_mae: 1.5203\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0382 - mae: 1.5263 - val_loss: 1.0938 - val_mae: 1.5595\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0333 - mae: 1.5171 - val_loss: 1.0852 - val_mae: 1.5661\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0185 - mae: 1.5005 - val_loss: 1.1096 - val_mae: 1.5798\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0211 - mae: 1.5051 - val_loss: 1.0971 - val_mae: 1.5695\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0275 - mae: 1.5136 - val_loss: 1.1244 - val_mae: 1.5840\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0330 - mae: 1.5164 - val_loss: 1.0939 - val_mae: 1.5943\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0441 - mae: 1.5284 - val_loss: 1.1143 - val_mae: 1.5888\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0502 - mae: 1.5353 - val_loss: 1.0574 - val_mae: 1.5245\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0403 - mae: 1.5265 - val_loss: 1.0683 - val_mae: 1.5540\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0161 - mae: 1.5037 - val_loss: 1.1009 - val_mae: 1.5673\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0240 - mae: 1.5084 - val_loss: 1.0566 - val_mae: 1.5474\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0412 - mae: 1.5261 - val_loss: 1.0778 - val_mae: 1.5609\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0269 - mae: 1.5093 - val_loss: 1.0974 - val_mae: 1.5619\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0439 - mae: 1.5295 - val_loss: 1.0818 - val_mae: 1.5609\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0358 - mae: 1.5219 - val_loss: 1.1351 - val_mae: 1.5889\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0320 - mae: 1.5126 - val_loss: 1.0782 - val_mae: 1.5445\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0333 - mae: 1.5168 - val_loss: 1.0769 - val_mae: 1.5689\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0484 - mae: 1.5358 - val_loss: 1.1646 - val_mae: 1.6276\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0439 - mae: 1.5262 - val_loss: 1.1117 - val_mae: 1.5642\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0392 - mae: 1.5248 - val_loss: 1.0544 - val_mae: 1.5119\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0441 - mae: 1.5276 - val_loss: 1.1452 - val_mae: 1.6043\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0463 - mae: 1.5305 - val_loss: 1.0739 - val_mae: 1.5524\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0534 - mae: 1.5376 - val_loss: 1.0847 - val_mae: 1.5400\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0391 - mae: 1.5215 - val_loss: 1.0968 - val_mae: 1.5918\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0185 - mae: 1.5040 - val_loss: 1.1355 - val_mae: 1.6181\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0570 - mae: 1.5447 - val_loss: 1.1267 - val_mae: 1.6079\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0597 - mae: 1.5482 - val_loss: 1.1225 - val_mae: 1.5776\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0436 - mae: 1.5240 - val_loss: 1.1211 - val_mae: 1.5683\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0325 - mae: 1.5148 - val_loss: 1.1071 - val_mae: 1.5925\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0519 - mae: 1.5407 - val_loss: 1.1126 - val_mae: 1.5717\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0428 - mae: 1.5249 - val_loss: 1.0884 - val_mae: 1.5572\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0531 - mae: 1.5385 - val_loss: 1.0759 - val_mae: 1.5321\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0352 - mae: 1.5192 - val_loss: 1.0757 - val_mae: 1.5301\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0668 - mae: 1.5491 - val_loss: 1.1085 - val_mae: 1.5754\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0371 - mae: 1.5219 - val_loss: 1.0881 - val_mae: 1.5630\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0481 - mae: 1.5314 - val_loss: 1.0669 - val_mae: 1.5302\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0417 - mae: 1.5228 - val_loss: 1.0913 - val_mae: 1.5687\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0340 - mae: 1.5139 - val_loss: 1.0469 - val_mae: 1.5353\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0314 - mae: 1.5121 - val_loss: 1.1037 - val_mae: 1.5647\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0300 - mae: 1.5147 - val_loss: 1.1098 - val_mae: 1.5697\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0268 - mae: 1.5092 - val_loss: 1.0797 - val_mae: 1.5378\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0333 - mae: 1.5155 - val_loss: 1.0825 - val_mae: 1.5586\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0366 - mae: 1.5210 - val_loss: 1.0763 - val_mae: 1.5513\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0574 - mae: 1.5431 - val_loss: 1.1211 - val_mae: 1.5795\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0476 - mae: 1.5280 - val_loss: 1.0522 - val_mae: 1.5640\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0380 - mae: 1.5233 - val_loss: 1.0402 - val_mae: 1.5134\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0364 - mae: 1.5200 - val_loss: 1.0782 - val_mae: 1.5266\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0311 - mae: 1.5125 - val_loss: 1.1659 - val_mae: 1.6217\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0421 - mae: 1.5278 - val_loss: 1.0841 - val_mae: 1.5705\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0372 - mae: 1.5222 - val_loss: 1.0634 - val_mae: 1.5557\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0351 - mae: 1.5236 - val_loss: 1.1386 - val_mae: 1.5908\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0431 - mae: 1.5207 - val_loss: 1.0316 - val_mae: 1.5020\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0503 - mae: 1.5291 - val_loss: 1.0365 - val_mae: 1.4875\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0410 - mae: 1.5233 - val_loss: 1.1016 - val_mae: 1.5488\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0346 - mae: 1.5157 - val_loss: 1.1334 - val_mae: 1.5877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 12:04:45,045] Trial 40 finished with value: 1.1333931684494019 and parameters: {'dropout_2': 0.3826875062619946, 'dropout_3': 0.8476202200589535, 'dropout_4': 0.4029109388024551, 'dropout_5': 0.25656198580958206, 'learning_rate': 0.02108648081364339, 'epochs': 200, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 3s 9ms/step - loss: 1.6106 - mae: 2.1713 - val_loss: 1.3147 - val_mae: 1.7961\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.4255 - mae: 1.9695 - val_loss: 1.2795 - val_mae: 1.7633\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.3034 - mae: 1.8329 - val_loss: 1.2353 - val_mae: 1.7089\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2909 - mae: 1.8147 - val_loss: 1.1964 - val_mae: 1.6769\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2269 - mae: 1.7416 - val_loss: 1.1571 - val_mae: 1.6335\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1864 - mae: 1.6955 - val_loss: 1.1340 - val_mae: 1.6157\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1770 - mae: 1.6890 - val_loss: 1.1237 - val_mae: 1.6046\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1598 - mae: 1.6676 - val_loss: 1.1392 - val_mae: 1.6235\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1526 - mae: 1.6541 - val_loss: 1.1250 - val_mae: 1.6118\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1150 - mae: 1.6136 - val_loss: 1.1010 - val_mae: 1.5819\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1018 - mae: 1.5957 - val_loss: 1.1052 - val_mae: 1.5901\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1133 - mae: 1.6120 - val_loss: 1.0973 - val_mae: 1.5766\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0781 - mae: 1.5729 - val_loss: 1.0950 - val_mae: 1.5801\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0760 - mae: 1.5710 - val_loss: 1.0837 - val_mae: 1.5655\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0744 - mae: 1.5673 - val_loss: 1.0726 - val_mae: 1.5545\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0783 - mae: 1.5732 - val_loss: 1.0785 - val_mae: 1.5523\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0625 - mae: 1.5562 - val_loss: 1.0735 - val_mae: 1.5522\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0701 - mae: 1.5631 - val_loss: 1.0847 - val_mae: 1.5707\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0535 - mae: 1.5459 - val_loss: 1.0613 - val_mae: 1.5410\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0477 - mae: 1.5372 - val_loss: 1.0642 - val_mae: 1.5406\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0561 - mae: 1.5521 - val_loss: 1.0519 - val_mae: 1.5296\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0401 - mae: 1.5321 - val_loss: 1.0586 - val_mae: 1.5347\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0418 - mae: 1.5307 - val_loss: 1.0390 - val_mae: 1.5235\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0385 - mae: 1.5325 - val_loss: 1.0390 - val_mae: 1.5183\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0127 - mae: 1.5044 - val_loss: 1.0500 - val_mae: 1.5335\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0228 - mae: 1.5112 - val_loss: 1.0686 - val_mae: 1.5485\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0053 - mae: 1.4944 - val_loss: 1.0530 - val_mae: 1.5236\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0254 - mae: 1.5175 - val_loss: 1.0448 - val_mae: 1.5206\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0172 - mae: 1.5055 - val_loss: 1.0444 - val_mae: 1.5173\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0229 - mae: 1.5127 - val_loss: 1.0333 - val_mae: 1.5118\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0167 - mae: 1.5081 - val_loss: 1.0343 - val_mae: 1.5149\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0034 - mae: 1.4902 - val_loss: 1.0226 - val_mae: 1.5006\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0201 - mae: 1.5102 - val_loss: 1.0321 - val_mae: 1.5050\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0002 - mae: 1.4884 - val_loss: 1.0333 - val_mae: 1.5036\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0066 - mae: 1.4947 - val_loss: 1.0261 - val_mae: 1.4975\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9957 - mae: 1.4804 - val_loss: 1.0176 - val_mae: 1.4883\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0042 - mae: 1.4945 - val_loss: 1.0340 - val_mae: 1.5122\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9990 - mae: 1.4863 - val_loss: 1.0522 - val_mae: 1.5314\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9796 - mae: 1.4621 - val_loss: 1.0239 - val_mae: 1.4979\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9970 - mae: 1.4848 - val_loss: 1.0142 - val_mae: 1.4890\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9851 - mae: 1.4705 - val_loss: 1.0850 - val_mae: 1.5729\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9930 - mae: 1.4762 - val_loss: 1.0060 - val_mae: 1.4817\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9814 - mae: 1.4619 - val_loss: 1.0337 - val_mae: 1.5182\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9797 - mae: 1.4619 - val_loss: 0.9975 - val_mae: 1.4748\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9974 - mae: 1.4853 - val_loss: 0.9921 - val_mae: 1.4632\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9933 - mae: 1.4759 - val_loss: 0.9832 - val_mae: 1.4537\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9805 - mae: 1.4639 - val_loss: 0.9944 - val_mae: 1.4663\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9917 - mae: 1.4774 - val_loss: 1.0049 - val_mae: 1.4801\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9753 - mae: 1.4564 - val_loss: 0.9794 - val_mae: 1.4542\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9741 - mae: 1.4558 - val_loss: 0.9816 - val_mae: 1.4520\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9671 - mae: 1.4494 - val_loss: 0.9835 - val_mae: 1.4588\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9704 - mae: 1.4548 - val_loss: 0.9978 - val_mae: 1.4709\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9705 - mae: 1.4542 - val_loss: 0.9986 - val_mae: 1.4694\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9818 - mae: 1.4706 - val_loss: 0.9837 - val_mae: 1.4569\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9810 - mae: 1.4653 - val_loss: 0.9892 - val_mae: 1.4651\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9748 - mae: 1.4556 - val_loss: 0.9909 - val_mae: 1.4572\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9625 - mae: 1.4425 - val_loss: 0.9928 - val_mae: 1.4662\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9561 - mae: 1.4320 - val_loss: 0.9970 - val_mae: 1.4673\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9613 - mae: 1.4409 - val_loss: 0.9874 - val_mae: 1.4634\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9603 - mae: 1.4409 - val_loss: 0.9735 - val_mae: 1.4465\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9664 - mae: 1.4454 - val_loss: 0.9880 - val_mae: 1.4607\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9487 - mae: 1.4292 - val_loss: 0.9911 - val_mae: 1.4600\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9582 - mae: 1.4409 - val_loss: 0.9750 - val_mae: 1.4467\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9451 - mae: 1.4244 - val_loss: 0.9785 - val_mae: 1.4470\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9722 - mae: 1.4560 - val_loss: 0.9709 - val_mae: 1.4476\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9641 - mae: 1.4473 - val_loss: 0.9813 - val_mae: 1.4495\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9414 - mae: 1.4160 - val_loss: 0.9708 - val_mae: 1.4427\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9442 - mae: 1.4217 - val_loss: 0.9782 - val_mae: 1.4527\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9625 - mae: 1.4411 - val_loss: 0.9994 - val_mae: 1.4742\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9556 - mae: 1.4335 - val_loss: 0.9844 - val_mae: 1.4542\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9530 - mae: 1.4313 - val_loss: 0.9888 - val_mae: 1.4677\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9454 - mae: 1.4190 - val_loss: 0.9812 - val_mae: 1.4503\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9412 - mae: 1.4200 - val_loss: 0.9674 - val_mae: 1.4293\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9558 - mae: 1.4363 - val_loss: 0.9633 - val_mae: 1.4359\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9485 - mae: 1.4215 - val_loss: 0.9761 - val_mae: 1.4453\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9628 - mae: 1.4429 - val_loss: 0.9926 - val_mae: 1.4585\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9501 - mae: 1.4277 - val_loss: 0.9578 - val_mae: 1.4279\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9484 - mae: 1.4255 - val_loss: 0.9710 - val_mae: 1.4393\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9581 - mae: 1.4396 - val_loss: 0.9618 - val_mae: 1.4305\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9424 - mae: 1.4180 - val_loss: 0.9613 - val_mae: 1.4319\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9408 - mae: 1.4171 - val_loss: 0.9541 - val_mae: 1.4253\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9519 - mae: 1.4259 - val_loss: 0.9494 - val_mae: 1.4170\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9295 - mae: 1.4004 - val_loss: 0.9888 - val_mae: 1.4520\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9371 - mae: 1.4126 - val_loss: 0.9621 - val_mae: 1.4295\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9318 - mae: 1.4050 - val_loss: 0.9611 - val_mae: 1.4314\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9364 - mae: 1.4139 - val_loss: 0.9575 - val_mae: 1.4223\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9358 - mae: 1.4135 - val_loss: 0.9693 - val_mae: 1.4263\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9267 - mae: 1.4022 - val_loss: 0.9423 - val_mae: 1.4047\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9253 - mae: 1.4012 - val_loss: 0.9556 - val_mae: 1.4211\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9337 - mae: 1.4097 - val_loss: 0.9456 - val_mae: 1.4160\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9319 - mae: 1.4039 - val_loss: 0.9525 - val_mae: 1.4186\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9397 - mae: 1.4140 - val_loss: 0.9372 - val_mae: 1.4009\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9415 - mae: 1.4175 - val_loss: 0.9782 - val_mae: 1.4399\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9207 - mae: 1.3931 - val_loss: 0.9485 - val_mae: 1.4160\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9339 - mae: 1.4107 - val_loss: 0.9657 - val_mae: 1.4331\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9284 - mae: 1.4055 - val_loss: 0.9408 - val_mae: 1.4074\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9284 - mae: 1.4034 - val_loss: 0.9654 - val_mae: 1.4288\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9270 - mae: 1.4015 - val_loss: 0.9629 - val_mae: 1.4223\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9247 - mae: 1.3917 - val_loss: 0.9371 - val_mae: 1.4000\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9229 - mae: 1.3940 - val_loss: 0.9428 - val_mae: 1.4030\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9098 - mae: 1.3839 - val_loss: 0.9359 - val_mae: 1.3977\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9265 - mae: 1.4005 - val_loss: 0.9489 - val_mae: 1.4039\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9158 - mae: 1.3873 - val_loss: 0.9485 - val_mae: 1.4088\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9278 - mae: 1.4012 - val_loss: 0.9341 - val_mae: 1.4046\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9112 - mae: 1.3807 - val_loss: 0.9444 - val_mae: 1.4045\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8978 - mae: 1.3700 - val_loss: 0.9258 - val_mae: 1.3897\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8966 - mae: 1.3681 - val_loss: 0.9281 - val_mae: 1.3928\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9111 - mae: 1.3858 - val_loss: 0.9349 - val_mae: 1.3937\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9073 - mae: 1.3788 - val_loss: 0.9494 - val_mae: 1.4074\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9181 - mae: 1.3928 - val_loss: 0.9355 - val_mae: 1.3886\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9124 - mae: 1.3866 - val_loss: 0.9230 - val_mae: 1.3800\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9095 - mae: 1.3813 - val_loss: 0.9151 - val_mae: 1.3786\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9240 - mae: 1.3962 - val_loss: 0.9349 - val_mae: 1.3994\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9226 - mae: 1.3957 - val_loss: 0.9560 - val_mae: 1.4200\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9286 - mae: 1.3995 - val_loss: 0.9353 - val_mae: 1.4032\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9158 - mae: 1.3885 - val_loss: 0.9429 - val_mae: 1.4089\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9061 - mae: 1.3739 - val_loss: 0.9294 - val_mae: 1.3922\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8890 - mae: 1.3568 - val_loss: 0.9480 - val_mae: 1.4063\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9147 - mae: 1.3855 - val_loss: 0.9462 - val_mae: 1.4033\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9082 - mae: 1.3754 - val_loss: 0.9164 - val_mae: 1.3743\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9114 - mae: 1.3822 - val_loss: 0.9252 - val_mae: 1.3864\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9065 - mae: 1.3769 - val_loss: 0.9527 - val_mae: 1.4110\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9169 - mae: 1.3883 - val_loss: 0.9435 - val_mae: 1.4026\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9209 - mae: 1.3952 - val_loss: 0.9328 - val_mae: 1.3913\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9063 - mae: 1.3769 - val_loss: 0.9338 - val_mae: 1.3907\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9044 - mae: 1.3753 - val_loss: 0.9354 - val_mae: 1.3972\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8904 - mae: 1.3621 - val_loss: 0.9033 - val_mae: 1.3571\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9100 - mae: 1.3802 - val_loss: 0.9225 - val_mae: 1.3781\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9198 - mae: 1.3909 - val_loss: 0.9173 - val_mae: 1.3781\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8858 - mae: 1.3515 - val_loss: 0.9225 - val_mae: 1.3867\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8965 - mae: 1.3654 - val_loss: 0.9342 - val_mae: 1.3964\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9049 - mae: 1.3734 - val_loss: 0.9559 - val_mae: 1.4093\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9111 - mae: 1.3811 - val_loss: 0.9291 - val_mae: 1.3906\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8955 - mae: 1.3621 - val_loss: 0.9483 - val_mae: 1.4105\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8934 - mae: 1.3634 - val_loss: 0.9321 - val_mae: 1.3969\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8925 - mae: 1.3602 - val_loss: 0.9322 - val_mae: 1.3952\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9095 - mae: 1.3755 - val_loss: 0.9410 - val_mae: 1.4090\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9002 - mae: 1.3717 - val_loss: 0.9287 - val_mae: 1.3884\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9049 - mae: 1.3768 - val_loss: 0.9463 - val_mae: 1.4140\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9052 - mae: 1.3722 - val_loss: 0.9384 - val_mae: 1.4042\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8896 - mae: 1.3588 - val_loss: 0.9359 - val_mae: 1.3975\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8919 - mae: 1.3583 - val_loss: 0.9289 - val_mae: 1.3920\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8879 - mae: 1.3559 - val_loss: 0.9118 - val_mae: 1.3827\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8880 - mae: 1.3543 - val_loss: 0.9436 - val_mae: 1.3943\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8897 - mae: 1.3572 - val_loss: 0.9302 - val_mae: 1.3920\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8901 - mae: 1.3576 - val_loss: 0.9493 - val_mae: 1.4064\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8997 - mae: 1.3685 - val_loss: 0.9402 - val_mae: 1.3979\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8891 - mae: 1.3602 - val_loss: 0.9394 - val_mae: 1.3974\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8910 - mae: 1.3588 - val_loss: 0.9352 - val_mae: 1.3964\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8926 - mae: 1.3598 - val_loss: 0.9429 - val_mae: 1.4017\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8894 - mae: 1.3528 - val_loss: 0.9538 - val_mae: 1.4201\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8886 - mae: 1.3559 - val_loss: 0.9553 - val_mae: 1.4172\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8863 - mae: 1.3530 - val_loss: 0.9294 - val_mae: 1.3870\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9007 - mae: 1.3719 - val_loss: 0.9288 - val_mae: 1.3853\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8893 - mae: 1.3540 - val_loss: 0.9546 - val_mae: 1.4128\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8877 - mae: 1.3585 - val_loss: 0.9175 - val_mae: 1.3806\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8810 - mae: 1.3421 - val_loss: 0.9285 - val_mae: 1.3995\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8884 - mae: 1.3541 - val_loss: 0.9024 - val_mae: 1.3635\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8934 - mae: 1.3622 - val_loss: 0.9187 - val_mae: 1.3782\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8782 - mae: 1.3426 - val_loss: 0.9097 - val_mae: 1.3734\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9001 - mae: 1.3670 - val_loss: 0.9059 - val_mae: 1.3592\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8954 - mae: 1.3618 - val_loss: 0.9223 - val_mae: 1.3817\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8858 - mae: 1.3555 - val_loss: 0.9230 - val_mae: 1.3811\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8720 - mae: 1.3354 - val_loss: 0.9112 - val_mae: 1.3629\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8852 - mae: 1.3512 - val_loss: 0.9008 - val_mae: 1.3514\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8699 - mae: 1.3332 - val_loss: 0.9051 - val_mae: 1.3611\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8950 - mae: 1.3652 - val_loss: 0.9024 - val_mae: 1.3639\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8713 - mae: 1.3380 - val_loss: 0.9101 - val_mae: 1.3601\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8832 - mae: 1.3451 - val_loss: 0.8828 - val_mae: 1.3461\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8836 - mae: 1.3507 - val_loss: 0.8952 - val_mae: 1.3590\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8720 - mae: 1.3396 - val_loss: 0.8938 - val_mae: 1.3503\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8629 - mae: 1.3298 - val_loss: 0.9149 - val_mae: 1.3745\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8814 - mae: 1.3497 - val_loss: 0.9032 - val_mae: 1.3634\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8705 - mae: 1.3356 - val_loss: 0.9033 - val_mae: 1.3590\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8819 - mae: 1.3473 - val_loss: 0.9086 - val_mae: 1.3633\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8921 - mae: 1.3566 - val_loss: 0.9319 - val_mae: 1.3838\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8714 - mae: 1.3343 - val_loss: 0.8854 - val_mae: 1.3421\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8694 - mae: 1.3318 - val_loss: 0.9014 - val_mae: 1.3541\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8788 - mae: 1.3457 - val_loss: 0.9128 - val_mae: 1.3714\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8750 - mae: 1.3408 - val_loss: 0.8845 - val_mae: 1.3445\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8783 - mae: 1.3424 - val_loss: 0.9099 - val_mae: 1.3674\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8702 - mae: 1.3334 - val_loss: 0.9152 - val_mae: 1.3756\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8826 - mae: 1.3473 - val_loss: 0.8918 - val_mae: 1.3436\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8750 - mae: 1.3340 - val_loss: 0.9025 - val_mae: 1.3637\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8770 - mae: 1.3425 - val_loss: 0.9183 - val_mae: 1.3792\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8688 - mae: 1.3325 - val_loss: 0.8956 - val_mae: 1.3512\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8495 - mae: 1.3104 - val_loss: 0.8855 - val_mae: 1.3389\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8742 - mae: 1.3361 - val_loss: 0.9038 - val_mae: 1.3635\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8800 - mae: 1.3488 - val_loss: 0.9123 - val_mae: 1.3677\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8711 - mae: 1.3355 - val_loss: 0.9036 - val_mae: 1.3572\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8589 - mae: 1.3247 - val_loss: 0.8866 - val_mae: 1.3352\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8726 - mae: 1.3409 - val_loss: 0.8851 - val_mae: 1.3405\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8721 - mae: 1.3358 - val_loss: 0.8773 - val_mae: 1.3351\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8782 - mae: 1.3414 - val_loss: 0.8857 - val_mae: 1.3426\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8900 - mae: 1.3570 - val_loss: 0.8995 - val_mae: 1.3518\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8662 - mae: 1.3258 - val_loss: 0.8803 - val_mae: 1.3308\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8678 - mae: 1.3302 - val_loss: 0.8627 - val_mae: 1.3150\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8470 - mae: 1.3049 - val_loss: 0.8915 - val_mae: 1.3309\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8772 - mae: 1.3349 - val_loss: 0.8872 - val_mae: 1.3313\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8388 - mae: 1.2941 - val_loss: 0.8845 - val_mae: 1.3334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 12:07:09,098] Trial 41 finished with value: 0.884486973285675 and parameters: {'dropout_2': 0.2779534973785829, 'dropout_3': 0.5197843457565545, 'dropout_4': 0.4334020639055236, 'dropout_5': 0.39440137123543983, 'learning_rate': 0.0007379615867712541, 'epochs': 200, 'batch_size': 32}. Best is trial 2 with value: 0.8499419093132019.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 3s 7ms/step - loss: 1.6657 - mae: 2.2274 - val_loss: 1.2959 - val_mae: 1.7826\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.4543 - mae: 1.9994 - val_loss: 1.2548 - val_mae: 1.7402\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3785 - mae: 1.9118 - val_loss: 1.2122 - val_mae: 1.6935\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2997 - mae: 1.8258 - val_loss: 1.1897 - val_mae: 1.6623\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2661 - mae: 1.7870 - val_loss: 1.1781 - val_mae: 1.6489\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2402 - mae: 1.7576 - val_loss: 1.1497 - val_mae: 1.6144\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2145 - mae: 1.7243 - val_loss: 1.1583 - val_mae: 1.6238\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1938 - mae: 1.7005 - val_loss: 1.1303 - val_mae: 1.5992\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1566 - mae: 1.6575 - val_loss: 1.1465 - val_mae: 1.6215\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1465 - mae: 1.6468 - val_loss: 1.1427 - val_mae: 1.6152\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1363 - mae: 1.6307 - val_loss: 1.1365 - val_mae: 1.6043\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1064 - mae: 1.5994 - val_loss: 1.1411 - val_mae: 1.6141\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1164 - mae: 1.6087 - val_loss: 1.0928 - val_mae: 1.5573\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0920 - mae: 1.5819 - val_loss: 1.1083 - val_mae: 1.5784\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0964 - mae: 1.5874 - val_loss: 1.1210 - val_mae: 1.5918\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0797 - mae: 1.5677 - val_loss: 1.0943 - val_mae: 1.5622\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0718 - mae: 1.5615 - val_loss: 1.0824 - val_mae: 1.5512\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0669 - mae: 1.5524 - val_loss: 1.0829 - val_mae: 1.5461\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0576 - mae: 1.5455 - val_loss: 1.0779 - val_mae: 1.5454\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0479 - mae: 1.5322 - val_loss: 1.0718 - val_mae: 1.5450\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0483 - mae: 1.5384 - val_loss: 1.0756 - val_mae: 1.5474\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0348 - mae: 1.5219 - val_loss: 1.0718 - val_mae: 1.5403\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0501 - mae: 1.5375 - val_loss: 1.0802 - val_mae: 1.5548\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0277 - mae: 1.5148 - val_loss: 1.0661 - val_mae: 1.5354\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0235 - mae: 1.5117 - val_loss: 1.0653 - val_mae: 1.5325\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0307 - mae: 1.5193 - val_loss: 1.0554 - val_mae: 1.5266\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0298 - mae: 1.5174 - val_loss: 1.0743 - val_mae: 1.5555\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0237 - mae: 1.5070 - val_loss: 1.0589 - val_mae: 1.5334\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0147 - mae: 1.4990 - val_loss: 1.0480 - val_mae: 1.5140\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0044 - mae: 1.4903 - val_loss: 1.0512 - val_mae: 1.5188\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0135 - mae: 1.4986 - val_loss: 1.0610 - val_mae: 1.5337\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9989 - mae: 1.4804 - val_loss: 1.0806 - val_mae: 1.5528\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0116 - mae: 1.4968 - val_loss: 1.0600 - val_mae: 1.5305\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0060 - mae: 1.4914 - val_loss: 1.0656 - val_mae: 1.5380\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9891 - mae: 1.4716 - val_loss: 1.0682 - val_mae: 1.5406\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9943 - mae: 1.4779 - val_loss: 1.0729 - val_mae: 1.5470\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9799 - mae: 1.4594 - val_loss: 1.0548 - val_mae: 1.5307\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0030 - mae: 1.4900 - val_loss: 1.0955 - val_mae: 1.5758\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9906 - mae: 1.4740 - val_loss: 1.0676 - val_mae: 1.5431\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9699 - mae: 1.4497 - val_loss: 1.0741 - val_mae: 1.5523\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9772 - mae: 1.4587 - val_loss: 1.0582 - val_mae: 1.5354\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9706 - mae: 1.4528 - val_loss: 1.0462 - val_mae: 1.5212\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9698 - mae: 1.4510 - val_loss: 1.0262 - val_mae: 1.4965\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9788 - mae: 1.4598 - val_loss: 1.0782 - val_mae: 1.5538\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9755 - mae: 1.4574 - val_loss: 1.0614 - val_mae: 1.5336\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9641 - mae: 1.4463 - val_loss: 1.0514 - val_mae: 1.5201\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9789 - mae: 1.4672 - val_loss: 1.0402 - val_mae: 1.5101\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9639 - mae: 1.4434 - val_loss: 1.0245 - val_mae: 1.4962\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9652 - mae: 1.4461 - val_loss: 1.0168 - val_mae: 1.4891\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9557 - mae: 1.4357 - val_loss: 1.0233 - val_mae: 1.4988\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9555 - mae: 1.4378 - val_loss: 1.0232 - val_mae: 1.5003\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9420 - mae: 1.4238 - val_loss: 1.0392 - val_mae: 1.5117\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9623 - mae: 1.4453 - val_loss: 1.0840 - val_mae: 1.5591\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9527 - mae: 1.4350 - val_loss: 1.0615 - val_mae: 1.5306\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9463 - mae: 1.4261 - val_loss: 0.9950 - val_mae: 1.4691\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9550 - mae: 1.4348 - val_loss: 1.0835 - val_mae: 1.5586\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9597 - mae: 1.4421 - val_loss: 1.0256 - val_mae: 1.4924\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9547 - mae: 1.4356 - val_loss: 1.0490 - val_mae: 1.5286\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9571 - mae: 1.4397 - val_loss: 1.0404 - val_mae: 1.5211\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9468 - mae: 1.4265 - val_loss: 1.0184 - val_mae: 1.4856\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9345 - mae: 1.4098 - val_loss: 1.0158 - val_mae: 1.4889\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9316 - mae: 1.4094 - val_loss: 1.0389 - val_mae: 1.5099\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9253 - mae: 1.3995 - val_loss: 1.0085 - val_mae: 1.4767\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9344 - mae: 1.4094 - val_loss: 0.9900 - val_mae: 1.4592\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9364 - mae: 1.4135 - val_loss: 1.0513 - val_mae: 1.5243\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9300 - mae: 1.4055 - val_loss: 1.0031 - val_mae: 1.4731\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9432 - mae: 1.4217 - val_loss: 1.0156 - val_mae: 1.4797\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9366 - mae: 1.4131 - val_loss: 1.0190 - val_mae: 1.4890\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9321 - mae: 1.4060 - val_loss: 0.9950 - val_mae: 1.4623\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9343 - mae: 1.4112 - val_loss: 1.0325 - val_mae: 1.4918\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9215 - mae: 1.3981 - val_loss: 0.9839 - val_mae: 1.4490\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9150 - mae: 1.3928 - val_loss: 0.9942 - val_mae: 1.4553\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9237 - mae: 1.4012 - val_loss: 1.0008 - val_mae: 1.4625\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9252 - mae: 1.3979 - val_loss: 0.9854 - val_mae: 1.4466\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9151 - mae: 1.3884 - val_loss: 0.9966 - val_mae: 1.4661\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9116 - mae: 1.3858 - val_loss: 1.0070 - val_mae: 1.4612\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9252 - mae: 1.3994 - val_loss: 0.9845 - val_mae: 1.4446\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9030 - mae: 1.3754 - val_loss: 0.9829 - val_mae: 1.4467\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9055 - mae: 1.3783 - val_loss: 0.9833 - val_mae: 1.4470\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9128 - mae: 1.3875 - val_loss: 1.0037 - val_mae: 1.4790\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9141 - mae: 1.3856 - val_loss: 1.0408 - val_mae: 1.5092\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9143 - mae: 1.3872 - val_loss: 0.9711 - val_mae: 1.4313\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9174 - mae: 1.3885 - val_loss: 0.9932 - val_mae: 1.4517\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9028 - mae: 1.3752 - val_loss: 0.9844 - val_mae: 1.4383\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9199 - mae: 1.3884 - val_loss: 0.9357 - val_mae: 1.4021\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9176 - mae: 1.3879 - val_loss: 0.9976 - val_mae: 1.4643\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8957 - mae: 1.3697 - val_loss: 0.9297 - val_mae: 1.3918\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9123 - mae: 1.3854 - val_loss: 0.9408 - val_mae: 1.4029\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9018 - mae: 1.3738 - val_loss: 0.8980 - val_mae: 1.3562\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9113 - mae: 1.3836 - val_loss: 0.9271 - val_mae: 1.3928\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9075 - mae: 1.3772 - val_loss: 0.9490 - val_mae: 1.4102\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9012 - mae: 1.3752 - val_loss: 0.9229 - val_mae: 1.3882\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9082 - mae: 1.3805 - val_loss: 0.9531 - val_mae: 1.4120\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8884 - mae: 1.3556 - val_loss: 0.9307 - val_mae: 1.3871\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9068 - mae: 1.3746 - val_loss: 0.9462 - val_mae: 1.4042\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9030 - mae: 1.3712 - val_loss: 0.9375 - val_mae: 1.4012\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8854 - mae: 1.3519 - val_loss: 0.9338 - val_mae: 1.4032\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8775 - mae: 1.3443 - val_loss: 0.9391 - val_mae: 1.3995\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9028 - mae: 1.3739 - val_loss: 0.9859 - val_mae: 1.4536\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8908 - mae: 1.3616 - val_loss: 0.9535 - val_mae: 1.4119\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8842 - mae: 1.3536 - val_loss: 0.9470 - val_mae: 1.4105\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8990 - mae: 1.3697 - val_loss: 0.9249 - val_mae: 1.3836\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8938 - mae: 1.3626 - val_loss: 0.9436 - val_mae: 1.4043\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8861 - mae: 1.3575 - val_loss: 0.9398 - val_mae: 1.4022\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8860 - mae: 1.3549 - val_loss: 0.9450 - val_mae: 1.4074\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8795 - mae: 1.3526 - val_loss: 0.9115 - val_mae: 1.3742\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8771 - mae: 1.3425 - val_loss: 0.9235 - val_mae: 1.3846\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8705 - mae: 1.3371 - val_loss: 0.9099 - val_mae: 1.3703\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8988 - mae: 1.3714 - val_loss: 0.9284 - val_mae: 1.3850\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8757 - mae: 1.3402 - val_loss: 0.9030 - val_mae: 1.3681\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8714 - mae: 1.3384 - val_loss: 0.9149 - val_mae: 1.3848\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8667 - mae: 1.3358 - val_loss: 0.9033 - val_mae: 1.3634\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8541 - mae: 1.3185 - val_loss: 0.9336 - val_mae: 1.3875\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8699 - mae: 1.3360 - val_loss: 0.9176 - val_mae: 1.3767\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8631 - mae: 1.3271 - val_loss: 0.9123 - val_mae: 1.3759\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8630 - mae: 1.3287 - val_loss: 0.9604 - val_mae: 1.4167\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8757 - mae: 1.3419 - val_loss: 0.9177 - val_mae: 1.3886\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8696 - mae: 1.3380 - val_loss: 0.9292 - val_mae: 1.3888\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8802 - mae: 1.3504 - val_loss: 0.8730 - val_mae: 1.3294\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8655 - mae: 1.3253 - val_loss: 0.9161 - val_mae: 1.3782\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8679 - mae: 1.3320 - val_loss: 0.8980 - val_mae: 1.3594\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8605 - mae: 1.3249 - val_loss: 0.8814 - val_mae: 1.3463\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8568 - mae: 1.3183 - val_loss: 0.9042 - val_mae: 1.3623\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8526 - mae: 1.3183 - val_loss: 0.9177 - val_mae: 1.3777\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8602 - mae: 1.3214 - val_loss: 0.9057 - val_mae: 1.3613\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8535 - mae: 1.3142 - val_loss: 0.9295 - val_mae: 1.3831\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8564 - mae: 1.3240 - val_loss: 0.9048 - val_mae: 1.3592\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8450 - mae: 1.3098 - val_loss: 0.8753 - val_mae: 1.3288\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8657 - mae: 1.3294 - val_loss: 0.9257 - val_mae: 1.3759\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8638 - mae: 1.3316 - val_loss: 0.9401 - val_mae: 1.4055\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8733 - mae: 1.3396 - val_loss: 0.8896 - val_mae: 1.3474\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8409 - mae: 1.3036 - val_loss: 0.8900 - val_mae: 1.3539\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8505 - mae: 1.3137 - val_loss: 0.9681 - val_mae: 1.4332\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8429 - mae: 1.3104 - val_loss: 0.9037 - val_mae: 1.3681\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8672 - mae: 1.3360 - val_loss: 0.9283 - val_mae: 1.4002\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8661 - mae: 1.3307 - val_loss: 0.8813 - val_mae: 1.3374\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8562 - mae: 1.3185 - val_loss: 0.8869 - val_mae: 1.3483\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8528 - mae: 1.3174 - val_loss: 0.8882 - val_mae: 1.3441\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8424 - mae: 1.3047 - val_loss: 0.8570 - val_mae: 1.3125\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8435 - mae: 1.3015 - val_loss: 0.8723 - val_mae: 1.3289\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8653 - mae: 1.3320 - val_loss: 0.8461 - val_mae: 1.3044\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8406 - mae: 1.3027 - val_loss: 0.8635 - val_mae: 1.3201\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8552 - mae: 1.3170 - val_loss: 0.8772 - val_mae: 1.3350\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8455 - mae: 1.3069 - val_loss: 0.8634 - val_mae: 1.3157\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8417 - mae: 1.3049 - val_loss: 0.8555 - val_mae: 1.3071\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8656 - mae: 1.3302 - val_loss: 0.9145 - val_mae: 1.3792\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8433 - mae: 1.3012 - val_loss: 0.8382 - val_mae: 1.2877\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8512 - mae: 1.3143 - val_loss: 0.8550 - val_mae: 1.3122\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8369 - mae: 1.2944 - val_loss: 0.8587 - val_mae: 1.3163\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8238 - mae: 1.2833 - val_loss: 0.8504 - val_mae: 1.3045\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8480 - mae: 1.3126 - val_loss: 0.8566 - val_mae: 1.3076\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8256 - mae: 1.2818 - val_loss: 0.8471 - val_mae: 1.3064\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8351 - mae: 1.2969 - val_loss: 0.8596 - val_mae: 1.3132\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8235 - mae: 1.2806 - val_loss: 0.8498 - val_mae: 1.3026\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8382 - mae: 1.2988 - val_loss: 0.8679 - val_mae: 1.3195\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8397 - mae: 1.3034 - val_loss: 0.8711 - val_mae: 1.3237\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8385 - mae: 1.2979 - val_loss: 0.8891 - val_mae: 1.3477\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8154 - mae: 1.2734 - val_loss: 0.8813 - val_mae: 1.3352\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8181 - mae: 1.2720 - val_loss: 0.8614 - val_mae: 1.3253\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8420 - mae: 1.3037 - val_loss: 0.8429 - val_mae: 1.3005\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8237 - mae: 1.2812 - val_loss: 0.8772 - val_mae: 1.3263\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8147 - mae: 1.2730 - val_loss: 0.8814 - val_mae: 1.3432\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8244 - mae: 1.2831 - val_loss: 0.8560 - val_mae: 1.3151\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8497 - mae: 1.3111 - val_loss: 0.9077 - val_mae: 1.3687\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8254 - mae: 1.2829 - val_loss: 0.8534 - val_mae: 1.3081\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8294 - mae: 1.2863 - val_loss: 0.8701 - val_mae: 1.3253\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8403 - mae: 1.3030 - val_loss: 0.8881 - val_mae: 1.3509\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8299 - mae: 1.2878 - val_loss: 0.8485 - val_mae: 1.2968\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8131 - mae: 1.2698 - val_loss: 0.8625 - val_mae: 1.3161\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8078 - mae: 1.2604 - val_loss: 0.8481 - val_mae: 1.3046\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8199 - mae: 1.2767 - val_loss: 0.8557 - val_mae: 1.3041\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8368 - mae: 1.2954 - val_loss: 0.8738 - val_mae: 1.3294\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.7957 - mae: 1.2534 - val_loss: 0.8753 - val_mae: 1.3323\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8231 - mae: 1.2815 - val_loss: 0.8539 - val_mae: 1.3127\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8095 - mae: 1.2641 - val_loss: 0.8848 - val_mae: 1.3375\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8261 - mae: 1.2872 - val_loss: 0.8565 - val_mae: 1.3050\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8174 - mae: 1.2759 - val_loss: 0.8570 - val_mae: 1.3176\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8338 - mae: 1.2974 - val_loss: 0.8320 - val_mae: 1.2840\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8115 - mae: 1.2644 - val_loss: 0.8300 - val_mae: 1.2841\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8135 - mae: 1.2698 - val_loss: 0.8399 - val_mae: 1.2991\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8211 - mae: 1.2802 - val_loss: 0.8289 - val_mae: 1.2822\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.7948 - mae: 1.2494 - val_loss: 0.8894 - val_mae: 1.3653\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8107 - mae: 1.2682 - val_loss: 0.8431 - val_mae: 1.2940\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8037 - mae: 1.2578 - val_loss: 0.8646 - val_mae: 1.3187\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8207 - mae: 1.2801 - val_loss: 0.8321 - val_mae: 1.2784\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8224 - mae: 1.2809 - val_loss: 0.8356 - val_mae: 1.2841\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8339 - mae: 1.2954 - val_loss: 0.8397 - val_mae: 1.2917\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8159 - mae: 1.2735 - val_loss: 0.8840 - val_mae: 1.3378\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.7937 - mae: 1.2470 - val_loss: 0.8593 - val_mae: 1.3085\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8107 - mae: 1.2685 - val_loss: 0.8642 - val_mae: 1.3097\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8126 - mae: 1.2685 - val_loss: 0.8470 - val_mae: 1.2931\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.7782 - mae: 1.2305 - val_loss: 0.8635 - val_mae: 1.3144\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.7906 - mae: 1.2458 - val_loss: 0.8519 - val_mae: 1.3040\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.7950 - mae: 1.2466 - val_loss: 0.8523 - val_mae: 1.3113\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8254 - mae: 1.2804 - val_loss: 0.8538 - val_mae: 1.3112\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.7831 - mae: 1.2364 - val_loss: 0.8462 - val_mae: 1.2908\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.7968 - mae: 1.2482 - val_loss: 0.8734 - val_mae: 1.3211\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8188 - mae: 1.2752 - val_loss: 0.8364 - val_mae: 1.2866\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.7918 - mae: 1.2477 - val_loss: 0.8466 - val_mae: 1.2990\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.7853 - mae: 1.2355 - val_loss: 0.8231 - val_mae: 1.2735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 12:09:00,921] Trial 42 finished with value: 0.8230664134025574 and parameters: {'dropout_2': 0.14627773253137555, 'dropout_3': 0.3104964013629035, 'dropout_4': 0.4514332549580662, 'dropout_5': 0.43080782555839275, 'learning_rate': 0.0005315067319714526, 'epochs': 200, 'batch_size': 32}. Best is trial 42 with value: 0.8230664134025574.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 2s 7ms/step - loss: 1.6777 - mae: 2.2438 - val_loss: 1.3676 - val_mae: 1.8868\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.6130 - mae: 2.1765 - val_loss: 1.3606 - val_mae: 1.8865\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.5225 - mae: 2.0776 - val_loss: 1.3118 - val_mae: 1.8245\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.4819 - mae: 2.0370 - val_loss: 1.2605 - val_mae: 1.7628\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.4415 - mae: 1.9906 - val_loss: 1.2315 - val_mae: 1.7334\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.4342 - mae: 1.9845 - val_loss: 1.2078 - val_mae: 1.7071\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.4121 - mae: 1.9561 - val_loss: 1.1963 - val_mae: 1.6916\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.3609 - mae: 1.9012 - val_loss: 1.1800 - val_mae: 1.6737\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3370 - mae: 1.8754 - val_loss: 1.1750 - val_mae: 1.6664\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.3191 - mae: 1.8548 - val_loss: 1.1564 - val_mae: 1.6449\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3271 - mae: 1.8650 - val_loss: 1.1513 - val_mae: 1.6381\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.2928 - mae: 1.8255 - val_loss: 1.1523 - val_mae: 1.6365\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2831 - mae: 1.8116 - val_loss: 1.1553 - val_mae: 1.6397\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2498 - mae: 1.7747 - val_loss: 1.1474 - val_mae: 1.6289\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2614 - mae: 1.7887 - val_loss: 1.1380 - val_mae: 1.6160\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2338 - mae: 1.7573 - val_loss: 1.1366 - val_mae: 1.6140\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2289 - mae: 1.7515 - val_loss: 1.1308 - val_mae: 1.6082\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2087 - mae: 1.7287 - val_loss: 1.1283 - val_mae: 1.6055\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2137 - mae: 1.7337 - val_loss: 1.1253 - val_mae: 1.6028\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1953 - mae: 1.7123 - val_loss: 1.1168 - val_mae: 1.5907\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1959 - mae: 1.7096 - val_loss: 1.1153 - val_mae: 1.5880\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1747 - mae: 1.6884 - val_loss: 1.1101 - val_mae: 1.5817\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1702 - mae: 1.6813 - val_loss: 1.1100 - val_mae: 1.5811\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1757 - mae: 1.6891 - val_loss: 1.1051 - val_mae: 1.5735\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1831 - mae: 1.6990 - val_loss: 1.1058 - val_mae: 1.5731\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1469 - mae: 1.6543 - val_loss: 1.1070 - val_mae: 1.5754\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1540 - mae: 1.6625 - val_loss: 1.1033 - val_mae: 1.5714\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1715 - mae: 1.6856 - val_loss: 1.1036 - val_mae: 1.5725\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1591 - mae: 1.6670 - val_loss: 1.1028 - val_mae: 1.5727\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1428 - mae: 1.6479 - val_loss: 1.0919 - val_mae: 1.5613\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1457 - mae: 1.6508 - val_loss: 1.0981 - val_mae: 1.5678\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1440 - mae: 1.6490 - val_loss: 1.0932 - val_mae: 1.5640\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1395 - mae: 1.6428 - val_loss: 1.0911 - val_mae: 1.5622\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1126 - mae: 1.6139 - val_loss: 1.0929 - val_mae: 1.5626\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1224 - mae: 1.6252 - val_loss: 1.0891 - val_mae: 1.5592\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1266 - mae: 1.6281 - val_loss: 1.0879 - val_mae: 1.5591\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1038 - mae: 1.6047 - val_loss: 1.0875 - val_mae: 1.5583\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1111 - mae: 1.6110 - val_loss: 1.0904 - val_mae: 1.5620\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1128 - mae: 1.6154 - val_loss: 1.0873 - val_mae: 1.5601\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0802 - mae: 1.5755 - val_loss: 1.0816 - val_mae: 1.5528\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0856 - mae: 1.5818 - val_loss: 1.0838 - val_mae: 1.5558\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0891 - mae: 1.5832 - val_loss: 1.0834 - val_mae: 1.5560\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0986 - mae: 1.5975 - val_loss: 1.0787 - val_mae: 1.5475\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0953 - mae: 1.5923 - val_loss: 1.0775 - val_mae: 1.5471\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0673 - mae: 1.5616 - val_loss: 1.0765 - val_mae: 1.5468\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0689 - mae: 1.5621 - val_loss: 1.0693 - val_mae: 1.5386\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0848 - mae: 1.5787 - val_loss: 1.0758 - val_mae: 1.5449\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0804 - mae: 1.5757 - val_loss: 1.0776 - val_mae: 1.5481\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0660 - mae: 1.5597 - val_loss: 1.0718 - val_mae: 1.5415\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0606 - mae: 1.5541 - val_loss: 1.0641 - val_mae: 1.5329\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0831 - mae: 1.5805 - val_loss: 1.0547 - val_mae: 1.5217\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0697 - mae: 1.5646 - val_loss: 1.0573 - val_mae: 1.5247\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0557 - mae: 1.5480 - val_loss: 1.0597 - val_mae: 1.5290\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0638 - mae: 1.5581 - val_loss: 1.0588 - val_mae: 1.5278\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0732 - mae: 1.5708 - val_loss: 1.0528 - val_mae: 1.5225\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0487 - mae: 1.5389 - val_loss: 1.0670 - val_mae: 1.5390\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0631 - mae: 1.5556 - val_loss: 1.0553 - val_mae: 1.5238\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0402 - mae: 1.5321 - val_loss: 1.0679 - val_mae: 1.5372\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0506 - mae: 1.5424 - val_loss: 1.0607 - val_mae: 1.5280\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0504 - mae: 1.5417 - val_loss: 1.0549 - val_mae: 1.5223\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0407 - mae: 1.5321 - val_loss: 1.0488 - val_mae: 1.5165\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0567 - mae: 1.5447 - val_loss: 1.0495 - val_mae: 1.5176\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0371 - mae: 1.5268 - val_loss: 1.0466 - val_mae: 1.5159\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0260 - mae: 1.5143 - val_loss: 1.0440 - val_mae: 1.5151\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0515 - mae: 1.5470 - val_loss: 1.0414 - val_mae: 1.5122\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0623 - mae: 1.5558 - val_loss: 1.0443 - val_mae: 1.5146\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0447 - mae: 1.5359 - val_loss: 1.0461 - val_mae: 1.5162\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0506 - mae: 1.5406 - val_loss: 1.0492 - val_mae: 1.5183\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0338 - mae: 1.5219 - val_loss: 1.0517 - val_mae: 1.5215\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0388 - mae: 1.5324 - val_loss: 1.0510 - val_mae: 1.5205\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0235 - mae: 1.5129 - val_loss: 1.0482 - val_mae: 1.5216\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0245 - mae: 1.5153 - val_loss: 1.0397 - val_mae: 1.5108\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0436 - mae: 1.5366 - val_loss: 1.0377 - val_mae: 1.5088\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0341 - mae: 1.5243 - val_loss: 1.0363 - val_mae: 1.5057\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0304 - mae: 1.5207 - val_loss: 1.0378 - val_mae: 1.5092\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0297 - mae: 1.5196 - val_loss: 1.0344 - val_mae: 1.5038\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0220 - mae: 1.5120 - val_loss: 1.0353 - val_mae: 1.5058\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0238 - mae: 1.5150 - val_loss: 1.0378 - val_mae: 1.5069\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0117 - mae: 1.5024 - val_loss: 1.0461 - val_mae: 1.5171\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0183 - mae: 1.5076 - val_loss: 1.0346 - val_mae: 1.5044\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0003 - mae: 1.4853 - val_loss: 1.0295 - val_mae: 1.5009\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0175 - mae: 1.5044 - val_loss: 1.0249 - val_mae: 1.4964\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0237 - mae: 1.5172 - val_loss: 1.0387 - val_mae: 1.5118\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0036 - mae: 1.4886 - val_loss: 1.0321 - val_mae: 1.5018\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0192 - mae: 1.5051 - val_loss: 1.0355 - val_mae: 1.5069\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0112 - mae: 1.4996 - val_loss: 1.0331 - val_mae: 1.5022\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0084 - mae: 1.4975 - val_loss: 1.0298 - val_mae: 1.4982\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0134 - mae: 1.5011 - val_loss: 1.0224 - val_mae: 1.4918\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9866 - mae: 1.4696 - val_loss: 1.0266 - val_mae: 1.4980\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0135 - mae: 1.5020 - val_loss: 1.0206 - val_mae: 1.4897\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0019 - mae: 1.4914 - val_loss: 1.0170 - val_mae: 1.4864\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0020 - mae: 1.4884 - val_loss: 1.0186 - val_mae: 1.4895\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0008 - mae: 1.4852 - val_loss: 1.0217 - val_mae: 1.4936\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9903 - mae: 1.4766 - val_loss: 1.0157 - val_mae: 1.4865\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0077 - mae: 1.4961 - val_loss: 1.0101 - val_mae: 1.4809\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9910 - mae: 1.4773 - val_loss: 1.0151 - val_mae: 1.4857\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9995 - mae: 1.4892 - val_loss: 1.0050 - val_mae: 1.4738\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0041 - mae: 1.4942 - val_loss: 1.0094 - val_mae: 1.4787\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9891 - mae: 1.4761 - val_loss: 1.0131 - val_mae: 1.4813\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9818 - mae: 1.4668 - val_loss: 1.0108 - val_mae: 1.4799\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9813 - mae: 1.4674 - val_loss: 1.0127 - val_mae: 1.4820\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9878 - mae: 1.4716 - val_loss: 1.0176 - val_mae: 1.4878\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9910 - mae: 1.4754 - val_loss: 1.0132 - val_mae: 1.4833\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0088 - mae: 1.4953 - val_loss: 1.0144 - val_mae: 1.4853\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9925 - mae: 1.4787 - val_loss: 1.0162 - val_mae: 1.4870\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9735 - mae: 1.4578 - val_loss: 1.0126 - val_mae: 1.4835\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9755 - mae: 1.4586 - val_loss: 1.0111 - val_mae: 1.4818\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9926 - mae: 1.4791 - val_loss: 1.0102 - val_mae: 1.4796\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9816 - mae: 1.4680 - val_loss: 1.0074 - val_mae: 1.4773\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9793 - mae: 1.4635 - val_loss: 1.0108 - val_mae: 1.4824\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9834 - mae: 1.4685 - val_loss: 1.0095 - val_mae: 1.4794\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9859 - mae: 1.4723 - val_loss: 0.9973 - val_mae: 1.4656\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9814 - mae: 1.4706 - val_loss: 1.0039 - val_mae: 1.4749\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9746 - mae: 1.4589 - val_loss: 1.0057 - val_mae: 1.4753\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9807 - mae: 1.4646 - val_loss: 1.0083 - val_mae: 1.4784\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9794 - mae: 1.4667 - val_loss: 1.0004 - val_mae: 1.4681\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9911 - mae: 1.4782 - val_loss: 0.9990 - val_mae: 1.4677\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9806 - mae: 1.4646 - val_loss: 1.0020 - val_mae: 1.4690\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9741 - mae: 1.4572 - val_loss: 1.0030 - val_mae: 1.4727\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9751 - mae: 1.4572 - val_loss: 1.0024 - val_mae: 1.4703\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9677 - mae: 1.4512 - val_loss: 0.9997 - val_mae: 1.4697\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9645 - mae: 1.4457 - val_loss: 0.9954 - val_mae: 1.4646\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9798 - mae: 1.4650 - val_loss: 0.9974 - val_mae: 1.4684\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9661 - mae: 1.4495 - val_loss: 0.9962 - val_mae: 1.4652\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9747 - mae: 1.4605 - val_loss: 0.9915 - val_mae: 1.4602\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9511 - mae: 1.4292 - val_loss: 0.9911 - val_mae: 1.4580\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9668 - mae: 1.4484 - val_loss: 0.9944 - val_mae: 1.4601\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9791 - mae: 1.4635 - val_loss: 0.9940 - val_mae: 1.4622\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9676 - mae: 1.4523 - val_loss: 0.9926 - val_mae: 1.4600\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9642 - mae: 1.4478 - val_loss: 0.9890 - val_mae: 1.4545\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9610 - mae: 1.4435 - val_loss: 0.9931 - val_mae: 1.4607\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9615 - mae: 1.4423 - val_loss: 0.9942 - val_mae: 1.4607\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9569 - mae: 1.4375 - val_loss: 0.9970 - val_mae: 1.4661\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9733 - mae: 1.4585 - val_loss: 0.9896 - val_mae: 1.4580\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9759 - mae: 1.4629 - val_loss: 0.9878 - val_mae: 1.4552\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9820 - mae: 1.4657 - val_loss: 0.9891 - val_mae: 1.4559\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9538 - mae: 1.4302 - val_loss: 0.9867 - val_mae: 1.4541\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9640 - mae: 1.4458 - val_loss: 0.9893 - val_mae: 1.4558\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9629 - mae: 1.4467 - val_loss: 0.9867 - val_mae: 1.4556\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9734 - mae: 1.4525 - val_loss: 0.9900 - val_mae: 1.4568\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9617 - mae: 1.4461 - val_loss: 0.9907 - val_mae: 1.4596\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9614 - mae: 1.4422 - val_loss: 0.9867 - val_mae: 1.4544\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9554 - mae: 1.4361 - val_loss: 0.9827 - val_mae: 1.4493\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9581 - mae: 1.4382 - val_loss: 0.9841 - val_mae: 1.4504\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9537 - mae: 1.4322 - val_loss: 0.9842 - val_mae: 1.4487\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9549 - mae: 1.4360 - val_loss: 0.9845 - val_mae: 1.4508\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9507 - mae: 1.4311 - val_loss: 0.9861 - val_mae: 1.4499\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9462 - mae: 1.4269 - val_loss: 0.9866 - val_mae: 1.4499\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9591 - mae: 1.4433 - val_loss: 0.9839 - val_mae: 1.4494\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9555 - mae: 1.4378 - val_loss: 0.9854 - val_mae: 1.4485\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9530 - mae: 1.4354 - val_loss: 0.9779 - val_mae: 1.4419\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9523 - mae: 1.4330 - val_loss: 0.9783 - val_mae: 1.4433\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9466 - mae: 1.4272 - val_loss: 0.9875 - val_mae: 1.4499\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9403 - mae: 1.4177 - val_loss: 0.9786 - val_mae: 1.4433\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9526 - mae: 1.4341 - val_loss: 0.9793 - val_mae: 1.4435\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9377 - mae: 1.4155 - val_loss: 0.9789 - val_mae: 1.4423\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9445 - mae: 1.4253 - val_loss: 0.9851 - val_mae: 1.4506\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9564 - mae: 1.4410 - val_loss: 0.9780 - val_mae: 1.4405\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9575 - mae: 1.4404 - val_loss: 0.9693 - val_mae: 1.4346\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9493 - mae: 1.4297 - val_loss: 0.9700 - val_mae: 1.4340\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9504 - mae: 1.4306 - val_loss: 0.9710 - val_mae: 1.4328\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9327 - mae: 1.4101 - val_loss: 0.9806 - val_mae: 1.4423\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9362 - mae: 1.4162 - val_loss: 0.9782 - val_mae: 1.4413\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9488 - mae: 1.4284 - val_loss: 0.9735 - val_mae: 1.4361\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9446 - mae: 1.4253 - val_loss: 0.9707 - val_mae: 1.4345\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9514 - mae: 1.4317 - val_loss: 0.9773 - val_mae: 1.4384\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9401 - mae: 1.4170 - val_loss: 0.9752 - val_mae: 1.4372\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9626 - mae: 1.4430 - val_loss: 0.9701 - val_mae: 1.4313\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9444 - mae: 1.4226 - val_loss: 0.9653 - val_mae: 1.4271\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9254 - mae: 1.3997 - val_loss: 0.9690 - val_mae: 1.4303\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9440 - mae: 1.4254 - val_loss: 0.9739 - val_mae: 1.4329\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9417 - mae: 1.4231 - val_loss: 0.9685 - val_mae: 1.4304\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9351 - mae: 1.4143 - val_loss: 0.9706 - val_mae: 1.4294\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9284 - mae: 1.4047 - val_loss: 0.9679 - val_mae: 1.4247\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9315 - mae: 1.4052 - val_loss: 0.9688 - val_mae: 1.4291\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9356 - mae: 1.4158 - val_loss: 0.9715 - val_mae: 1.4311\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9483 - mae: 1.4287 - val_loss: 0.9694 - val_mae: 1.4305\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9379 - mae: 1.4143 - val_loss: 0.9652 - val_mae: 1.4240\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9398 - mae: 1.4172 - val_loss: 0.9697 - val_mae: 1.4299\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9415 - mae: 1.4195 - val_loss: 0.9739 - val_mae: 1.4334\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9371 - mae: 1.4130 - val_loss: 0.9702 - val_mae: 1.4302\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9204 - mae: 1.3952 - val_loss: 0.9691 - val_mae: 1.4287\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9293 - mae: 1.4077 - val_loss: 0.9648 - val_mae: 1.4263\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9325 - mae: 1.4109 - val_loss: 0.9696 - val_mae: 1.4277\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9323 - mae: 1.4096 - val_loss: 0.9623 - val_mae: 1.4238\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9222 - mae: 1.3993 - val_loss: 0.9596 - val_mae: 1.4179\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9125 - mae: 1.3883 - val_loss: 0.9576 - val_mae: 1.4190\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9368 - mae: 1.4164 - val_loss: 0.9528 - val_mae: 1.4143\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9223 - mae: 1.3992 - val_loss: 0.9566 - val_mae: 1.4170\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9108 - mae: 1.3875 - val_loss: 0.9556 - val_mae: 1.4154\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9401 - mae: 1.4206 - val_loss: 0.9534 - val_mae: 1.4133\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9213 - mae: 1.3985 - val_loss: 0.9559 - val_mae: 1.4167\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9063 - mae: 1.3816 - val_loss: 0.9559 - val_mae: 1.4160\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9177 - mae: 1.3923 - val_loss: 0.9523 - val_mae: 1.4147\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9371 - mae: 1.4158 - val_loss: 0.9543 - val_mae: 1.4122\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9379 - mae: 1.4160 - val_loss: 0.9543 - val_mae: 1.4137\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9258 - mae: 1.3997 - val_loss: 0.9565 - val_mae: 1.4178\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9201 - mae: 1.3951 - val_loss: 0.9534 - val_mae: 1.4126\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9384 - mae: 1.4162 - val_loss: 0.9528 - val_mae: 1.4108\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9377 - mae: 1.4180 - val_loss: 0.9553 - val_mae: 1.4133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 12:11:24,953] Trial 43 finished with value: 0.9553387761116028 and parameters: {'dropout_2': 0.14829137249555352, 'dropout_3': 0.5434342780031164, 'dropout_4': 0.3472014615132998, 'dropout_5': 0.3656291862865827, 'learning_rate': 0.00012084239491251463, 'epochs': 200, 'batch_size': 32}. Best is trial 42 with value: 0.8230664134025574.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 3s 9ms/step - loss: 1.5593 - mae: 2.1171 - val_loss: 1.3383 - val_mae: 1.8655\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.4384 - mae: 1.9865 - val_loss: 1.3078 - val_mae: 1.8232\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3147 - mae: 1.8437 - val_loss: 1.2497 - val_mae: 1.7464\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2865 - mae: 1.8128 - val_loss: 1.2178 - val_mae: 1.6973\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2314 - mae: 1.7491 - val_loss: 1.1911 - val_mae: 1.6710\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1971 - mae: 1.7083 - val_loss: 1.1722 - val_mae: 1.6497\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1733 - mae: 1.6808 - val_loss: 1.1558 - val_mae: 1.6306\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1449 - mae: 1.6470 - val_loss: 1.1381 - val_mae: 1.6161\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1510 - mae: 1.6521 - val_loss: 1.1310 - val_mae: 1.6077\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1174 - mae: 1.6142 - val_loss: 1.1229 - val_mae: 1.6007\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1125 - mae: 1.6060 - val_loss: 1.1207 - val_mae: 1.6004\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1013 - mae: 1.5960 - val_loss: 1.1058 - val_mae: 1.5808\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0968 - mae: 1.5881 - val_loss: 1.1102 - val_mae: 1.5830\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0771 - mae: 1.5673 - val_loss: 1.0964 - val_mae: 1.5663\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0545 - mae: 1.5428 - val_loss: 1.0900 - val_mae: 1.5657\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0524 - mae: 1.5432 - val_loss: 1.0670 - val_mae: 1.5403\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0462 - mae: 1.5345 - val_loss: 1.0649 - val_mae: 1.5343\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0384 - mae: 1.5242 - val_loss: 1.0592 - val_mae: 1.5361\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0500 - mae: 1.5397 - val_loss: 1.0595 - val_mae: 1.5347\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0167 - mae: 1.5021 - val_loss: 1.0604 - val_mae: 1.5321\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0172 - mae: 1.5010 - val_loss: 1.0482 - val_mae: 1.5241\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0409 - mae: 1.5329 - val_loss: 1.0704 - val_mae: 1.5453\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0184 - mae: 1.5079 - val_loss: 1.0465 - val_mae: 1.5207\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0280 - mae: 1.5169 - val_loss: 1.0557 - val_mae: 1.5275\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0113 - mae: 1.4990 - val_loss: 1.0707 - val_mae: 1.5525\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0163 - mae: 1.4999 - val_loss: 1.0310 - val_mae: 1.5027\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0252 - mae: 1.5108 - val_loss: 1.0415 - val_mae: 1.5118\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0157 - mae: 1.5020 - val_loss: 1.0498 - val_mae: 1.5228\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0027 - mae: 1.4849 - val_loss: 1.0204 - val_mae: 1.4972\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9965 - mae: 1.4841 - val_loss: 1.0183 - val_mae: 1.4848\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0156 - mae: 1.4987 - val_loss: 1.0289 - val_mae: 1.5082\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9950 - mae: 1.4819 - val_loss: 1.0886 - val_mae: 1.5794\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9993 - mae: 1.4832 - val_loss: 1.0190 - val_mae: 1.4857\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9853 - mae: 1.4673 - val_loss: 1.0274 - val_mae: 1.5040\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9955 - mae: 1.4810 - val_loss: 1.0240 - val_mae: 1.4987\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9996 - mae: 1.4868 - val_loss: 1.0432 - val_mae: 1.5173\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9825 - mae: 1.4648 - val_loss: 1.0192 - val_mae: 1.4872\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9780 - mae: 1.4607 - val_loss: 1.0356 - val_mae: 1.5000\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9744 - mae: 1.4557 - val_loss: 1.0185 - val_mae: 1.4902\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9748 - mae: 1.4609 - val_loss: 1.0165 - val_mae: 1.4882\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9913 - mae: 1.4723 - val_loss: 1.1012 - val_mae: 1.5995\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9973 - mae: 1.4776 - val_loss: 1.0249 - val_mae: 1.4940\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9737 - mae: 1.4584 - val_loss: 1.0355 - val_mae: 1.5078\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9522 - mae: 1.4302 - val_loss: 1.0318 - val_mae: 1.5079\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9813 - mae: 1.4615 - val_loss: 1.0156 - val_mae: 1.4794\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9547 - mae: 1.4341 - val_loss: 0.9883 - val_mae: 1.4572\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9434 - mae: 1.4193 - val_loss: 1.0355 - val_mae: 1.5139\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9621 - mae: 1.4400 - val_loss: 0.9914 - val_mae: 1.4608\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9631 - mae: 1.4454 - val_loss: 1.0093 - val_mae: 1.4866\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9722 - mae: 1.4573 - val_loss: 1.0225 - val_mae: 1.5000\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9468 - mae: 1.4235 - val_loss: 0.9896 - val_mae: 1.4558\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9575 - mae: 1.4344 - val_loss: 0.9892 - val_mae: 1.4554\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9560 - mae: 1.4365 - val_loss: 0.9949 - val_mae: 1.4601\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9472 - mae: 1.4217 - val_loss: 0.9924 - val_mae: 1.4588\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9280 - mae: 1.4013 - val_loss: 0.9926 - val_mae: 1.4630\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9486 - mae: 1.4244 - val_loss: 0.9863 - val_mae: 1.4546\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9381 - mae: 1.4127 - val_loss: 0.9779 - val_mae: 1.4351\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9569 - mae: 1.4344 - val_loss: 0.9852 - val_mae: 1.4505\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9402 - mae: 1.4158 - val_loss: 0.9874 - val_mae: 1.4610\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9467 - mae: 1.4223 - val_loss: 0.9697 - val_mae: 1.4379\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9373 - mae: 1.4136 - val_loss: 0.9767 - val_mae: 1.4502\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9364 - mae: 1.4069 - val_loss: 0.9835 - val_mae: 1.4481\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9231 - mae: 1.3959 - val_loss: 0.9989 - val_mae: 1.4701\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9359 - mae: 1.4133 - val_loss: 0.9977 - val_mae: 1.4603\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9371 - mae: 1.4107 - val_loss: 0.9733 - val_mae: 1.4334\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9215 - mae: 1.3962 - val_loss: 0.9689 - val_mae: 1.4290\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9353 - mae: 1.4079 - val_loss: 0.9881 - val_mae: 1.4504\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9187 - mae: 1.3891 - val_loss: 0.9868 - val_mae: 1.4501\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9413 - mae: 1.4171 - val_loss: 0.9892 - val_mae: 1.4476\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9273 - mae: 1.4003 - val_loss: 0.9779 - val_mae: 1.4359\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9102 - mae: 1.3785 - val_loss: 0.9577 - val_mae: 1.4131\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9218 - mae: 1.3959 - val_loss: 0.9745 - val_mae: 1.4284\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9169 - mae: 1.3903 - val_loss: 0.9390 - val_mae: 1.3991\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9072 - mae: 1.3774 - val_loss: 0.9497 - val_mae: 1.4084\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9242 - mae: 1.3953 - val_loss: 0.9485 - val_mae: 1.4098\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9230 - mae: 1.3965 - val_loss: 0.9640 - val_mae: 1.4240\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9025 - mae: 1.3729 - val_loss: 0.9887 - val_mae: 1.4571\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9225 - mae: 1.3948 - val_loss: 0.9414 - val_mae: 1.4096\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9104 - mae: 1.3843 - val_loss: 0.9429 - val_mae: 1.4083\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9130 - mae: 1.3821 - val_loss: 0.9590 - val_mae: 1.4277\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8978 - mae: 1.3651 - val_loss: 0.9551 - val_mae: 1.4164\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9159 - mae: 1.3895 - val_loss: 0.9554 - val_mae: 1.4096\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9017 - mae: 1.3707 - val_loss: 0.9583 - val_mae: 1.4223\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9024 - mae: 1.3712 - val_loss: 0.9291 - val_mae: 1.3783\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8947 - mae: 1.3617 - val_loss: 0.9445 - val_mae: 1.4076\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9126 - mae: 1.3825 - val_loss: 0.9295 - val_mae: 1.3938\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9019 - mae: 1.3742 - val_loss: 0.9433 - val_mae: 1.3998\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9054 - mae: 1.3762 - val_loss: 0.9422 - val_mae: 1.4020\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8971 - mae: 1.3668 - val_loss: 0.9905 - val_mae: 1.4521\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8983 - mae: 1.3669 - val_loss: 0.9591 - val_mae: 1.4254\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8958 - mae: 1.3641 - val_loss: 0.9169 - val_mae: 1.3715\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9027 - mae: 1.3747 - val_loss: 0.9372 - val_mae: 1.4053\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8974 - mae: 1.3679 - val_loss: 0.9169 - val_mae: 1.3712\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8768 - mae: 1.3458 - val_loss: 0.9361 - val_mae: 1.3941\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9196 - mae: 1.3884 - val_loss: 0.8935 - val_mae: 1.3523\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8898 - mae: 1.3581 - val_loss: 0.8929 - val_mae: 1.3515\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8964 - mae: 1.3635 - val_loss: 0.9115 - val_mae: 1.3622\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8976 - mae: 1.3682 - val_loss: 0.9266 - val_mae: 1.3778\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8912 - mae: 1.3566 - val_loss: 0.9211 - val_mae: 1.3768\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8940 - mae: 1.3595 - val_loss: 0.9542 - val_mae: 1.4163\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9101 - mae: 1.3819 - val_loss: 0.9261 - val_mae: 1.3814\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8874 - mae: 1.3537 - val_loss: 0.9261 - val_mae: 1.3807\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8748 - mae: 1.3414 - val_loss: 0.9487 - val_mae: 1.4070\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9004 - mae: 1.3707 - val_loss: 0.9156 - val_mae: 1.3634\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8808 - mae: 1.3444 - val_loss: 0.8924 - val_mae: 1.3402\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8778 - mae: 1.3453 - val_loss: 0.9077 - val_mae: 1.3620\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8711 - mae: 1.3346 - val_loss: 0.9443 - val_mae: 1.4059\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8599 - mae: 1.3184 - val_loss: 0.8900 - val_mae: 1.3347\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8891 - mae: 1.3542 - val_loss: 0.9391 - val_mae: 1.3952\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8808 - mae: 1.3454 - val_loss: 0.9341 - val_mae: 1.3801\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8664 - mae: 1.3285 - val_loss: 0.8979 - val_mae: 1.3414\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8685 - mae: 1.3313 - val_loss: 0.9127 - val_mae: 1.3572\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8621 - mae: 1.3276 - val_loss: 0.8851 - val_mae: 1.3265\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8811 - mae: 1.3425 - val_loss: 0.9348 - val_mae: 1.3870\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8784 - mae: 1.3421 - val_loss: 0.9070 - val_mae: 1.3585\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8875 - mae: 1.3563 - val_loss: 0.9295 - val_mae: 1.3852\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8732 - mae: 1.3391 - val_loss: 0.9001 - val_mae: 1.3458\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8650 - mae: 1.3315 - val_loss: 0.9393 - val_mae: 1.3970\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8953 - mae: 1.3625 - val_loss: 0.9048 - val_mae: 1.3582\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8569 - mae: 1.3208 - val_loss: 0.9146 - val_mae: 1.3604\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8718 - mae: 1.3357 - val_loss: 0.9039 - val_mae: 1.3567\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8381 - mae: 1.3004 - val_loss: 0.9425 - val_mae: 1.3924\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8612 - mae: 1.3269 - val_loss: 0.9394 - val_mae: 1.3915\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8710 - mae: 1.3403 - val_loss: 0.9462 - val_mae: 1.4069\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8763 - mae: 1.3398 - val_loss: 0.8953 - val_mae: 1.3456\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8690 - mae: 1.3354 - val_loss: 0.9082 - val_mae: 1.3661\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8502 - mae: 1.3136 - val_loss: 0.9348 - val_mae: 1.3877\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8725 - mae: 1.3346 - val_loss: 0.9666 - val_mae: 1.4280\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8578 - mae: 1.3183 - val_loss: 0.9062 - val_mae: 1.3530\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8505 - mae: 1.3106 - val_loss: 0.9432 - val_mae: 1.3935\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8559 - mae: 1.3209 - val_loss: 0.9059 - val_mae: 1.3583\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8477 - mae: 1.3075 - val_loss: 0.9016 - val_mae: 1.3527\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8549 - mae: 1.3180 - val_loss: 0.9438 - val_mae: 1.3958\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8648 - mae: 1.3256 - val_loss: 0.9292 - val_mae: 1.3782\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8584 - mae: 1.3223 - val_loss: 0.9268 - val_mae: 1.3724\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8454 - mae: 1.3056 - val_loss: 0.9042 - val_mae: 1.3451\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8691 - mae: 1.3280 - val_loss: 0.9657 - val_mae: 1.4100\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8569 - mae: 1.3185 - val_loss: 0.9057 - val_mae: 1.3430\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8647 - mae: 1.3255 - val_loss: 0.8786 - val_mae: 1.3239\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8445 - mae: 1.3042 - val_loss: 0.9231 - val_mae: 1.3817\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8589 - mae: 1.3162 - val_loss: 0.9304 - val_mae: 1.3766\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8419 - mae: 1.2995 - val_loss: 0.8947 - val_mae: 1.3432\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8779 - mae: 1.3416 - val_loss: 0.8977 - val_mae: 1.3407\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8627 - mae: 1.3248 - val_loss: 0.8801 - val_mae: 1.3219\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8513 - mae: 1.3105 - val_loss: 0.8997 - val_mae: 1.3467\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8570 - mae: 1.3184 - val_loss: 0.8960 - val_mae: 1.3412\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8244 - mae: 1.2794 - val_loss: 0.8939 - val_mae: 1.3381\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8505 - mae: 1.3070 - val_loss: 0.8580 - val_mae: 1.3078\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8414 - mae: 1.3008 - val_loss: 0.9325 - val_mae: 1.3803\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8586 - mae: 1.3223 - val_loss: 0.8697 - val_mae: 1.3100\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8762 - mae: 1.3409 - val_loss: 0.9054 - val_mae: 1.3518\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8484 - mae: 1.3093 - val_loss: 0.9322 - val_mae: 1.3725\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8454 - mae: 1.3095 - val_loss: 0.9675 - val_mae: 1.4228\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8571 - mae: 1.3222 - val_loss: 0.9145 - val_mae: 1.3604\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8318 - mae: 1.2902 - val_loss: 0.9431 - val_mae: 1.3964\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8319 - mae: 1.2906 - val_loss: 0.9258 - val_mae: 1.3744\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8495 - mae: 1.3083 - val_loss: 0.8958 - val_mae: 1.3407\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8554 - mae: 1.3189 - val_loss: 0.8928 - val_mae: 1.3384\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8338 - mae: 1.2910 - val_loss: 0.9269 - val_mae: 1.3725\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8482 - mae: 1.3097 - val_loss: 0.9139 - val_mae: 1.3609\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8516 - mae: 1.3128 - val_loss: 0.9290 - val_mae: 1.3688\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8430 - mae: 1.3008 - val_loss: 0.9132 - val_mae: 1.3508\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8385 - mae: 1.2986 - val_loss: 0.9008 - val_mae: 1.3390\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8261 - mae: 1.2814 - val_loss: 0.9077 - val_mae: 1.3453\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8326 - mae: 1.2890 - val_loss: 0.9342 - val_mae: 1.3695\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8418 - mae: 1.3015 - val_loss: 0.9145 - val_mae: 1.3470\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8307 - mae: 1.2881 - val_loss: 0.8859 - val_mae: 1.3207\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8486 - mae: 1.3111 - val_loss: 0.8995 - val_mae: 1.3471\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8343 - mae: 1.2920 - val_loss: 0.9378 - val_mae: 1.3891\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8386 - mae: 1.2961 - val_loss: 0.9313 - val_mae: 1.3760\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8471 - mae: 1.3107 - val_loss: 0.9233 - val_mae: 1.3666\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8276 - mae: 1.2827 - val_loss: 0.9391 - val_mae: 1.3852\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8518 - mae: 1.3137 - val_loss: 0.9061 - val_mae: 1.3451\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8470 - mae: 1.3068 - val_loss: 0.9235 - val_mae: 1.3763\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8228 - mae: 1.2770 - val_loss: 0.9108 - val_mae: 1.3420\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8277 - mae: 1.2857 - val_loss: 0.9170 - val_mae: 1.3587\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8399 - mae: 1.3001 - val_loss: 0.9117 - val_mae: 1.3581\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8251 - mae: 1.2854 - val_loss: 0.8998 - val_mae: 1.3399\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8256 - mae: 1.2830 - val_loss: 0.9235 - val_mae: 1.3640\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8255 - mae: 1.2818 - val_loss: 0.8945 - val_mae: 1.3322\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8374 - mae: 1.2947 - val_loss: 0.9412 - val_mae: 1.3729\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8262 - mae: 1.2850 - val_loss: 0.9227 - val_mae: 1.3533\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8259 - mae: 1.2834 - val_loss: 0.8964 - val_mae: 1.3380\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8261 - mae: 1.2817 - val_loss: 0.8541 - val_mae: 1.2947\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8186 - mae: 1.2790 - val_loss: 0.9172 - val_mae: 1.3577\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8263 - mae: 1.2848 - val_loss: 0.8782 - val_mae: 1.3164\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8029 - mae: 1.2561 - val_loss: 0.8633 - val_mae: 1.3031\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8100 - mae: 1.2676 - val_loss: 0.8886 - val_mae: 1.3346\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8118 - mae: 1.2649 - val_loss: 0.9072 - val_mae: 1.3386\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8147 - mae: 1.2670 - val_loss: 0.8968 - val_mae: 1.3370\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8172 - mae: 1.2720 - val_loss: 0.8573 - val_mae: 1.2951\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8061 - mae: 1.2621 - val_loss: 0.8984 - val_mae: 1.3352\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8105 - mae: 1.2644 - val_loss: 0.9164 - val_mae: 1.3510\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8263 - mae: 1.2807 - val_loss: 0.9122 - val_mae: 1.3584\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8172 - mae: 1.2692 - val_loss: 0.9144 - val_mae: 1.3515\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8081 - mae: 1.2623 - val_loss: 0.9096 - val_mae: 1.3553\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8151 - mae: 1.2755 - val_loss: 0.8907 - val_mae: 1.3244\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8146 - mae: 1.2692 - val_loss: 0.9161 - val_mae: 1.3579\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8031 - mae: 1.2575 - val_loss: 0.9560 - val_mae: 1.3900\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8098 - mae: 1.2644 - val_loss: 0.8815 - val_mae: 1.3132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 12:13:49,243] Trial 44 finished with value: 0.8815274238586426 and parameters: {'dropout_2': 0.18819229739245663, 'dropout_3': 0.2976182560484271, 'dropout_4': 0.43826267025517646, 'dropout_5': 0.42835190724261285, 'learning_rate': 0.000560820778005713, 'epochs': 200, 'batch_size': 32}. Best is trial 42 with value: 0.8230664134025574.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "14/14 [==============================] - 2s 29ms/step - loss: 1.6150 - mae: 2.1658 - val_loss: 1.3863 - val_mae: 1.8603\n",
            "Epoch 2/50\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.4826 - mae: 2.0214 - val_loss: 1.3265 - val_mae: 1.7889\n",
            "Epoch 3/50\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.4325 - mae: 1.9662 - val_loss: 1.2982 - val_mae: 1.7594\n",
            "Epoch 4/50\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3873 - mae: 1.9136 - val_loss: 1.2847 - val_mae: 1.7450\n",
            "Epoch 5/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.3309 - mae: 1.8500 - val_loss: 1.2807 - val_mae: 1.7416\n",
            "Epoch 6/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.3112 - mae: 1.8206 - val_loss: 1.2696 - val_mae: 1.7341\n",
            "Epoch 7/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2868 - mae: 1.7966 - val_loss: 1.2666 - val_mae: 1.7319\n",
            "Epoch 8/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2487 - mae: 1.7552 - val_loss: 1.2526 - val_mae: 1.7177\n",
            "Epoch 9/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2320 - mae: 1.7353 - val_loss: 1.2423 - val_mae: 1.7053\n",
            "Epoch 10/50\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.2560 - mae: 1.7596 - val_loss: 1.2484 - val_mae: 1.7090\n",
            "Epoch 11/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.2114 - mae: 1.7112 - val_loss: 1.2317 - val_mae: 1.6903\n",
            "Epoch 12/50\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1953 - mae: 1.6933 - val_loss: 1.2254 - val_mae: 1.6825\n",
            "Epoch 13/50\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1849 - mae: 1.6815 - val_loss: 1.2243 - val_mae: 1.6811\n",
            "Epoch 14/50\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1754 - mae: 1.6715 - val_loss: 1.2174 - val_mae: 1.6739\n",
            "Epoch 15/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1713 - mae: 1.6669 - val_loss: 1.2111 - val_mae: 1.6689\n",
            "Epoch 16/50\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1582 - mae: 1.6521 - val_loss: 1.2007 - val_mae: 1.6569\n",
            "Epoch 17/50\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.1491 - mae: 1.6416 - val_loss: 1.1935 - val_mae: 1.6511\n",
            "Epoch 18/50\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1506 - mae: 1.6413 - val_loss: 1.1850 - val_mae: 1.6431\n",
            "Epoch 19/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1322 - mae: 1.6261 - val_loss: 1.1893 - val_mae: 1.6484\n",
            "Epoch 20/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1293 - mae: 1.6222 - val_loss: 1.1720 - val_mae: 1.6317\n",
            "Epoch 21/50\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1323 - mae: 1.6234 - val_loss: 1.1674 - val_mae: 1.6288\n",
            "Epoch 22/50\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.1078 - mae: 1.5954 - val_loss: 1.1720 - val_mae: 1.6356\n",
            "Epoch 23/50\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.1160 - mae: 1.6097 - val_loss: 1.1612 - val_mae: 1.6235\n",
            "Epoch 24/50\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.1159 - mae: 1.6032 - val_loss: 1.1561 - val_mae: 1.6197\n",
            "Epoch 25/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.1207 - mae: 1.6085 - val_loss: 1.1537 - val_mae: 1.6178\n",
            "Epoch 26/50\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1051 - mae: 1.5920 - val_loss: 1.1520 - val_mae: 1.6196\n",
            "Epoch 27/50\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0862 - mae: 1.5727 - val_loss: 1.1453 - val_mae: 1.6132\n",
            "Epoch 28/50\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.0756 - mae: 1.5608 - val_loss: 1.1265 - val_mae: 1.5935\n",
            "Epoch 29/50\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.0693 - mae: 1.5536 - val_loss: 1.1129 - val_mae: 1.5799\n",
            "Epoch 30/50\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.0688 - mae: 1.5542 - val_loss: 1.1031 - val_mae: 1.5719\n",
            "Epoch 31/50\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.0768 - mae: 1.5627 - val_loss: 1.1009 - val_mae: 1.5701\n",
            "Epoch 32/50\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.0747 - mae: 1.5600 - val_loss: 1.0942 - val_mae: 1.5643\n",
            "Epoch 33/50\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.0573 - mae: 1.5421 - val_loss: 1.0828 - val_mae: 1.5501\n",
            "Epoch 34/50\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.0512 - mae: 1.5352 - val_loss: 1.0872 - val_mae: 1.5584\n",
            "Epoch 35/50\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.0460 - mae: 1.5284 - val_loss: 1.0785 - val_mae: 1.5502\n",
            "Epoch 36/50\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.0414 - mae: 1.5233 - val_loss: 1.0742 - val_mae: 1.5453\n",
            "Epoch 37/50\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.0367 - mae: 1.5188 - val_loss: 1.0649 - val_mae: 1.5358\n",
            "Epoch 38/50\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.0543 - mae: 1.5378 - val_loss: 1.0644 - val_mae: 1.5354\n",
            "Epoch 39/50\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.0380 - mae: 1.5223 - val_loss: 1.0571 - val_mae: 1.5296\n",
            "Epoch 40/50\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.0371 - mae: 1.5213 - val_loss: 1.0508 - val_mae: 1.5210\n",
            "Epoch 41/50\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.0281 - mae: 1.5097 - val_loss: 1.0428 - val_mae: 1.5112\n",
            "Epoch 42/50\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.0450 - mae: 1.5270 - val_loss: 1.0498 - val_mae: 1.5210\n",
            "Epoch 43/50\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.0263 - mae: 1.5058 - val_loss: 1.0380 - val_mae: 1.5079\n",
            "Epoch 44/50\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 1.0192 - mae: 1.4985 - val_loss: 1.0483 - val_mae: 1.5270\n",
            "Epoch 45/50\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.0182 - mae: 1.4985 - val_loss: 1.0313 - val_mae: 1.5053\n",
            "Epoch 46/50\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.0121 - mae: 1.4900 - val_loss: 1.0290 - val_mae: 1.5006\n",
            "Epoch 47/50\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.0041 - mae: 1.4848 - val_loss: 1.0233 - val_mae: 1.4960\n",
            "Epoch 48/50\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 1.0070 - mae: 1.4898 - val_loss: 1.0117 - val_mae: 1.4788\n",
            "Epoch 49/50\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.0132 - mae: 1.4949 - val_loss: 1.0127 - val_mae: 1.4858\n",
            "Epoch 50/50\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.0167 - mae: 1.5022 - val_loss: 1.0172 - val_mae: 1.4910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 12:14:01,211] Trial 45 finished with value: 1.0171716213226318 and parameters: {'dropout_2': 0.18617535077053035, 'dropout_3': 0.2906543741541861, 'dropout_4': 0.44680369908838663, 'dropout_5': 0.43053201617382586, 'learning_rate': 0.0007257100842976479, 'epochs': 50, 'batch_size': 256}. Best is trial 42 with value: 0.8230664134025574.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 2s 7ms/step - loss: 1.7345 - mae: 2.3033 - val_loss: 1.3576 - val_mae: 1.8850\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.7382 - mae: 2.3082 - val_loss: 1.3668 - val_mae: 1.9123\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.6731 - mae: 2.2394 - val_loss: 1.3545 - val_mae: 1.8954\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.6873 - mae: 2.2531 - val_loss: 1.3405 - val_mae: 1.8755\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.6161 - mae: 2.1745 - val_loss: 1.3269 - val_mae: 1.8608\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5827 - mae: 2.1432 - val_loss: 1.3108 - val_mae: 1.8435\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5643 - mae: 2.1237 - val_loss: 1.2911 - val_mae: 1.8191\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5554 - mae: 2.1131 - val_loss: 1.2863 - val_mae: 1.8165\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.5233 - mae: 2.0777 - val_loss: 1.2831 - val_mae: 1.8116\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4990 - mae: 2.0509 - val_loss: 1.2730 - val_mae: 1.8014\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4816 - mae: 2.0321 - val_loss: 1.2641 - val_mae: 1.7908\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4756 - mae: 2.0264 - val_loss: 1.2562 - val_mae: 1.7804\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4212 - mae: 1.9693 - val_loss: 1.2424 - val_mae: 1.7648\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4316 - mae: 1.9776 - val_loss: 1.2407 - val_mae: 1.7613\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.4242 - mae: 1.9721 - val_loss: 1.2317 - val_mae: 1.7495\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.4043 - mae: 1.9453 - val_loss: 1.2275 - val_mae: 1.7451\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3925 - mae: 1.9310 - val_loss: 1.2223 - val_mae: 1.7386\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3777 - mae: 1.9192 - val_loss: 1.2161 - val_mae: 1.7306\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3712 - mae: 1.9116 - val_loss: 1.2182 - val_mae: 1.7331\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.3686 - mae: 1.9099 - val_loss: 1.2116 - val_mae: 1.7237\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3522 - mae: 1.8869 - val_loss: 1.2067 - val_mae: 1.7195\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.3429 - mae: 1.8769 - val_loss: 1.2003 - val_mae: 1.7115\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.3253 - mae: 1.8624 - val_loss: 1.1939 - val_mae: 1.7043\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3235 - mae: 1.8599 - val_loss: 1.1937 - val_mae: 1.7037\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2874 - mae: 1.8163 - val_loss: 1.1872 - val_mae: 1.6947\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.2876 - mae: 1.8170 - val_loss: 1.1855 - val_mae: 1.6922\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2832 - mae: 1.8109 - val_loss: 1.1829 - val_mae: 1.6874\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2796 - mae: 1.8088 - val_loss: 1.1788 - val_mae: 1.6827\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2543 - mae: 1.7792 - val_loss: 1.1803 - val_mae: 1.6819\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2659 - mae: 1.7904 - val_loss: 1.1769 - val_mae: 1.6757\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2534 - mae: 1.7772 - val_loss: 1.1739 - val_mae: 1.6707\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2713 - mae: 1.7996 - val_loss: 1.1698 - val_mae: 1.6663\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2620 - mae: 1.7847 - val_loss: 1.1707 - val_mae: 1.6664\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2526 - mae: 1.7758 - val_loss: 1.1672 - val_mae: 1.6610\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2244 - mae: 1.7434 - val_loss: 1.1647 - val_mae: 1.6589\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2305 - mae: 1.7501 - val_loss: 1.1614 - val_mae: 1.6548\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2190 - mae: 1.7390 - val_loss: 1.1591 - val_mae: 1.6525\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2376 - mae: 1.7582 - val_loss: 1.1584 - val_mae: 1.6510\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2315 - mae: 1.7513 - val_loss: 1.1569 - val_mae: 1.6483\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2226 - mae: 1.7384 - val_loss: 1.1533 - val_mae: 1.6437\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2234 - mae: 1.7425 - val_loss: 1.1514 - val_mae: 1.6424\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2091 - mae: 1.7248 - val_loss: 1.1516 - val_mae: 1.6412\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2126 - mae: 1.7287 - val_loss: 1.1514 - val_mae: 1.6395\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2182 - mae: 1.7379 - val_loss: 1.1470 - val_mae: 1.6340\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1982 - mae: 1.7102 - val_loss: 1.1490 - val_mae: 1.6357\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1958 - mae: 1.7096 - val_loss: 1.1458 - val_mae: 1.6324\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1961 - mae: 1.7095 - val_loss: 1.1407 - val_mae: 1.6272\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1787 - mae: 1.6884 - val_loss: 1.1441 - val_mae: 1.6318\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1819 - mae: 1.6902 - val_loss: 1.1442 - val_mae: 1.6315\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1971 - mae: 1.7093 - val_loss: 1.1424 - val_mae: 1.6289\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1820 - mae: 1.6927 - val_loss: 1.1406 - val_mae: 1.6264\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1766 - mae: 1.6878 - val_loss: 1.1416 - val_mae: 1.6277\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1883 - mae: 1.6966 - val_loss: 1.1406 - val_mae: 1.6289\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1588 - mae: 1.6686 - val_loss: 1.1413 - val_mae: 1.6281\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1659 - mae: 1.6768 - val_loss: 1.1362 - val_mae: 1.6233\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1559 - mae: 1.6615 - val_loss: 1.1379 - val_mae: 1.6240\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1531 - mae: 1.6573 - val_loss: 1.1305 - val_mae: 1.6160\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1684 - mae: 1.6762 - val_loss: 1.1299 - val_mae: 1.6158\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1560 - mae: 1.6616 - val_loss: 1.1329 - val_mae: 1.6183\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1538 - mae: 1.6597 - val_loss: 1.1306 - val_mae: 1.6159\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1391 - mae: 1.6439 - val_loss: 1.1298 - val_mae: 1.6155\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1399 - mae: 1.6435 - val_loss: 1.1313 - val_mae: 1.6172\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1333 - mae: 1.6347 - val_loss: 1.1289 - val_mae: 1.6147\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1398 - mae: 1.6449 - val_loss: 1.1288 - val_mae: 1.6146\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1360 - mae: 1.6408 - val_loss: 1.1271 - val_mae: 1.6123\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1464 - mae: 1.6506 - val_loss: 1.1291 - val_mae: 1.6150\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1342 - mae: 1.6395 - val_loss: 1.1231 - val_mae: 1.6104\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1494 - mae: 1.6538 - val_loss: 1.1274 - val_mae: 1.6141\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1305 - mae: 1.6318 - val_loss: 1.1273 - val_mae: 1.6127\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1230 - mae: 1.6227 - val_loss: 1.1242 - val_mae: 1.6105\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1177 - mae: 1.6172 - val_loss: 1.1219 - val_mae: 1.6083\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1274 - mae: 1.6273 - val_loss: 1.1233 - val_mae: 1.6105\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1209 - mae: 1.6182 - val_loss: 1.1192 - val_mae: 1.6058\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1137 - mae: 1.6138 - val_loss: 1.1182 - val_mae: 1.6048\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1189 - mae: 1.6195 - val_loss: 1.1176 - val_mae: 1.6026\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1185 - mae: 1.6186 - val_loss: 1.1230 - val_mae: 1.6082\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1084 - mae: 1.6050 - val_loss: 1.1190 - val_mae: 1.6032\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1137 - mae: 1.6117 - val_loss: 1.1203 - val_mae: 1.6034\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1202 - mae: 1.6198 - val_loss: 1.1156 - val_mae: 1.5997\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1051 - mae: 1.6017 - val_loss: 1.1150 - val_mae: 1.5988\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1171 - mae: 1.6170 - val_loss: 1.1160 - val_mae: 1.6019\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1096 - mae: 1.6056 - val_loss: 1.1132 - val_mae: 1.6001\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1087 - mae: 1.6037 - val_loss: 1.1155 - val_mae: 1.6024\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1089 - mae: 1.6068 - val_loss: 1.1140 - val_mae: 1.6008\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1093 - mae: 1.6069 - val_loss: 1.1152 - val_mae: 1.6021\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1027 - mae: 1.5982 - val_loss: 1.1200 - val_mae: 1.6076\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0965 - mae: 1.5912 - val_loss: 1.1202 - val_mae: 1.6082\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0889 - mae: 1.5842 - val_loss: 1.1115 - val_mae: 1.5978\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0879 - mae: 1.5827 - val_loss: 1.1091 - val_mae: 1.5947\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0835 - mae: 1.5791 - val_loss: 1.1097 - val_mae: 1.5963\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0964 - mae: 1.5922 - val_loss: 1.1076 - val_mae: 1.5938\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1006 - mae: 1.6008 - val_loss: 1.1071 - val_mae: 1.5946\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0833 - mae: 1.5801 - val_loss: 1.1067 - val_mae: 1.5941\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0909 - mae: 1.5860 - val_loss: 1.1075 - val_mae: 1.5957\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0813 - mae: 1.5774 - val_loss: 1.1060 - val_mae: 1.5934\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0913 - mae: 1.5867 - val_loss: 1.1040 - val_mae: 1.5907\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0822 - mae: 1.5803 - val_loss: 1.1049 - val_mae: 1.5917\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0862 - mae: 1.5836 - val_loss: 1.1039 - val_mae: 1.5911\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0930 - mae: 1.5885 - val_loss: 1.1024 - val_mae: 1.5899\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0891 - mae: 1.5858 - val_loss: 1.0935 - val_mae: 1.5794\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0836 - mae: 1.5804 - val_loss: 1.1004 - val_mae: 1.5863\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0878 - mae: 1.5803 - val_loss: 1.1002 - val_mae: 1.5874\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0741 - mae: 1.5664 - val_loss: 1.0992 - val_mae: 1.5826\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0872 - mae: 1.5797 - val_loss: 1.0969 - val_mae: 1.5832\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0754 - mae: 1.5686 - val_loss: 1.0971 - val_mae: 1.5825\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0704 - mae: 1.5595 - val_loss: 1.0974 - val_mae: 1.5837\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0920 - mae: 1.5882 - val_loss: 1.0978 - val_mae: 1.5828\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0721 - mae: 1.5648 - val_loss: 1.0949 - val_mae: 1.5828\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0848 - mae: 1.5829 - val_loss: 1.0932 - val_mae: 1.5816\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0591 - mae: 1.5494 - val_loss: 1.0932 - val_mae: 1.5804\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0708 - mae: 1.5620 - val_loss: 1.0925 - val_mae: 1.5772\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0559 - mae: 1.5465 - val_loss: 1.0926 - val_mae: 1.5791\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0599 - mae: 1.5552 - val_loss: 1.0896 - val_mae: 1.5752\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0832 - mae: 1.5789 - val_loss: 1.0898 - val_mae: 1.5762\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0642 - mae: 1.5546 - val_loss: 1.0882 - val_mae: 1.5738\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0673 - mae: 1.5586 - val_loss: 1.0864 - val_mae: 1.5730\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0727 - mae: 1.5710 - val_loss: 1.0866 - val_mae: 1.5741\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0743 - mae: 1.5699 - val_loss: 1.0810 - val_mae: 1.5685\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0648 - mae: 1.5586 - val_loss: 1.0847 - val_mae: 1.5734\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0686 - mae: 1.5658 - val_loss: 1.0803 - val_mae: 1.5685\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0738 - mae: 1.5693 - val_loss: 1.0828 - val_mae: 1.5705\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0495 - mae: 1.5401 - val_loss: 1.0859 - val_mae: 1.5725\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0601 - mae: 1.5548 - val_loss: 1.0837 - val_mae: 1.5700\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0762 - mae: 1.5692 - val_loss: 1.0788 - val_mae: 1.5649\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0624 - mae: 1.5565 - val_loss: 1.0837 - val_mae: 1.5697\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0628 - mae: 1.5551 - val_loss: 1.0772 - val_mae: 1.5616\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0668 - mae: 1.5596 - val_loss: 1.0795 - val_mae: 1.5636\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0654 - mae: 1.5608 - val_loss: 1.0763 - val_mae: 1.5611\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0676 - mae: 1.5620 - val_loss: 1.0804 - val_mae: 1.5665\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0628 - mae: 1.5578 - val_loss: 1.0786 - val_mae: 1.5637\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0440 - mae: 1.5324 - val_loss: 1.0785 - val_mae: 1.5643\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0501 - mae: 1.5423 - val_loss: 1.0788 - val_mae: 1.5645\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0550 - mae: 1.5460 - val_loss: 1.0767 - val_mae: 1.5606\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0665 - mae: 1.5611 - val_loss: 1.0784 - val_mae: 1.5632\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0398 - mae: 1.5283 - val_loss: 1.0819 - val_mae: 1.5695\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0611 - mae: 1.5536 - val_loss: 1.0747 - val_mae: 1.5612\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0436 - mae: 1.5332 - val_loss: 1.0734 - val_mae: 1.5583\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0465 - mae: 1.5388 - val_loss: 1.0760 - val_mae: 1.5618\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0592 - mae: 1.5536 - val_loss: 1.0725 - val_mae: 1.5579\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0452 - mae: 1.5377 - val_loss: 1.0714 - val_mae: 1.5563\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0614 - mae: 1.5508 - val_loss: 1.0714 - val_mae: 1.5555\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0551 - mae: 1.5452 - val_loss: 1.0752 - val_mae: 1.5613\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0547 - mae: 1.5472 - val_loss: 1.0723 - val_mae: 1.5576\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0549 - mae: 1.5454 - val_loss: 1.0705 - val_mae: 1.5538\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0488 - mae: 1.5438 - val_loss: 1.0719 - val_mae: 1.5550\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0500 - mae: 1.5392 - val_loss: 1.0674 - val_mae: 1.5499\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0595 - mae: 1.5550 - val_loss: 1.0725 - val_mae: 1.5560\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0450 - mae: 1.5401 - val_loss: 1.0688 - val_mae: 1.5533\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0537 - mae: 1.5481 - val_loss: 1.0690 - val_mae: 1.5547\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0517 - mae: 1.5427 - val_loss: 1.0693 - val_mae: 1.5551\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0399 - mae: 1.5331 - val_loss: 1.0656 - val_mae: 1.5508\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0440 - mae: 1.5324 - val_loss: 1.0693 - val_mae: 1.5557\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0353 - mae: 1.5277 - val_loss: 1.0696 - val_mae: 1.5557\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0522 - mae: 1.5456 - val_loss: 1.0673 - val_mae: 1.5544\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0370 - mae: 1.5286 - val_loss: 1.0630 - val_mae: 1.5484\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0428 - mae: 1.5349 - val_loss: 1.0650 - val_mae: 1.5485\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0431 - mae: 1.5326 - val_loss: 1.0635 - val_mae: 1.5464\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0221 - mae: 1.5100 - val_loss: 1.0612 - val_mae: 1.5438\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0409 - mae: 1.5348 - val_loss: 1.0610 - val_mae: 1.5436\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0275 - mae: 1.5190 - val_loss: 1.0633 - val_mae: 1.5468\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0450 - mae: 1.5357 - val_loss: 1.0627 - val_mae: 1.5472\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0319 - mae: 1.5211 - val_loss: 1.0641 - val_mae: 1.5475\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0303 - mae: 1.5240 - val_loss: 1.0586 - val_mae: 1.5433\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0400 - mae: 1.5321 - val_loss: 1.0595 - val_mae: 1.5428\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0294 - mae: 1.5164 - val_loss: 1.0661 - val_mae: 1.5500\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0419 - mae: 1.5375 - val_loss: 1.0640 - val_mae: 1.5477\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0303 - mae: 1.5194 - val_loss: 1.0615 - val_mae: 1.5439\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0390 - mae: 1.5318 - val_loss: 1.0615 - val_mae: 1.5454\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0411 - mae: 1.5337 - val_loss: 1.0594 - val_mae: 1.5430\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0276 - mae: 1.5168 - val_loss: 1.0551 - val_mae: 1.5386\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0266 - mae: 1.5154 - val_loss: 1.0552 - val_mae: 1.5377\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0382 - mae: 1.5301 - val_loss: 1.0554 - val_mae: 1.5370\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0194 - mae: 1.5092 - val_loss: 1.0520 - val_mae: 1.5347\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0371 - mae: 1.5295 - val_loss: 1.0517 - val_mae: 1.5340\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0252 - mae: 1.5189 - val_loss: 1.0529 - val_mae: 1.5357\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0235 - mae: 1.5135 - val_loss: 1.0531 - val_mae: 1.5349\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0231 - mae: 1.5138 - val_loss: 1.0534 - val_mae: 1.5358\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0274 - mae: 1.5154 - val_loss: 1.0530 - val_mae: 1.5349\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0222 - mae: 1.5119 - val_loss: 1.0504 - val_mae: 1.5323\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0521 - mae: 1.5502 - val_loss: 1.0572 - val_mae: 1.5397\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0176 - mae: 1.5062 - val_loss: 1.0506 - val_mae: 1.5324\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0327 - mae: 1.5268 - val_loss: 1.0543 - val_mae: 1.5353\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0423 - mae: 1.5352 - val_loss: 1.0540 - val_mae: 1.5364\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0247 - mae: 1.5132 - val_loss: 1.0550 - val_mae: 1.5369\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0299 - mae: 1.5221 - val_loss: 1.0534 - val_mae: 1.5360\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0330 - mae: 1.5218 - val_loss: 1.0510 - val_mae: 1.5334\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0313 - mae: 1.5233 - val_loss: 1.0521 - val_mae: 1.5366\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0209 - mae: 1.5087 - val_loss: 1.0471 - val_mae: 1.5290\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0253 - mae: 1.5145 - val_loss: 1.0474 - val_mae: 1.5289\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0176 - mae: 1.5053 - val_loss: 1.0489 - val_mae: 1.5307\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0234 - mae: 1.5116 - val_loss: 1.0465 - val_mae: 1.5290\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0274 - mae: 1.5195 - val_loss: 1.0460 - val_mae: 1.5287\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0184 - mae: 1.5032 - val_loss: 1.0504 - val_mae: 1.5323\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0237 - mae: 1.5133 - val_loss: 1.0504 - val_mae: 1.5343\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0173 - mae: 1.5084 - val_loss: 1.0494 - val_mae: 1.5324\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0254 - mae: 1.5179 - val_loss: 1.0459 - val_mae: 1.5288\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0217 - mae: 1.5130 - val_loss: 1.0426 - val_mae: 1.5257\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0179 - mae: 1.5064 - val_loss: 1.0384 - val_mae: 1.5200\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0196 - mae: 1.5070 - val_loss: 1.0421 - val_mae: 1.5233\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0221 - mae: 1.5123 - val_loss: 1.0416 - val_mae: 1.5235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 12:16:25,400] Trial 46 finished with value: 1.0416218042373657 and parameters: {'dropout_2': 0.3130220380157738, 'dropout_3': 0.18930574009238138, 'dropout_4': 0.4543500539050472, 'dropout_5': 0.46339342316358545, 'learning_rate': 6.9880715648073e-05, 'epochs': 200, 'batch_size': 32}. Best is trial 42 with value: 0.8230664134025574.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27/27 [==============================] - 3s 18ms/step - loss: 1.6876 - mae: 2.2547 - val_loss: 1.3359 - val_mae: 1.8280\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.6459 - mae: 2.2099 - val_loss: 1.3365 - val_mae: 1.8614\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5823 - mae: 2.1396 - val_loss: 1.3446 - val_mae: 1.8913\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5572 - mae: 2.1142 - val_loss: 1.3556 - val_mae: 1.9151\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.5198 - mae: 2.0736 - val_loss: 1.3635 - val_mae: 1.9292\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.4945 - mae: 2.0469 - val_loss: 1.3584 - val_mae: 1.9233\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.4627 - mae: 2.0093 - val_loss: 1.3497 - val_mae: 1.9131\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.4354 - mae: 1.9823 - val_loss: 1.3367 - val_mae: 1.8974\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.4128 - mae: 1.9615 - val_loss: 1.3163 - val_mae: 1.8696\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.3811 - mae: 1.9230 - val_loss: 1.2955 - val_mae: 1.8456\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.3572 - mae: 1.8955 - val_loss: 1.2611 - val_mae: 1.7938\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.3542 - mae: 1.8899 - val_loss: 1.2347 - val_mae: 1.7550\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.3248 - mae: 1.8588 - val_loss: 1.2040 - val_mae: 1.7053\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.2911 - mae: 1.8218 - val_loss: 1.1940 - val_mae: 1.6926\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.2917 - mae: 1.8270 - val_loss: 1.1754 - val_mae: 1.6679\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.2732 - mae: 1.8022 - val_loss: 1.1650 - val_mae: 1.6541\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.2811 - mae: 1.8115 - val_loss: 1.1452 - val_mae: 1.6336\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.2945 - mae: 1.8242 - val_loss: 1.1297 - val_mae: 1.6172\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.2497 - mae: 1.7734 - val_loss: 1.1161 - val_mae: 1.6040\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.2437 - mae: 1.7649 - val_loss: 1.1125 - val_mae: 1.5982\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.2108 - mae: 1.7329 - val_loss: 1.1058 - val_mae: 1.5920\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.2116 - mae: 1.7293 - val_loss: 1.0984 - val_mae: 1.5834\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.2017 - mae: 1.7176 - val_loss: 1.0923 - val_mae: 1.5739\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.2049 - mae: 1.7223 - val_loss: 1.0860 - val_mae: 1.5682\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1938 - mae: 1.7118 - val_loss: 1.0845 - val_mae: 1.5657\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.2034 - mae: 1.7202 - val_loss: 1.0803 - val_mae: 1.5617\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1884 - mae: 1.7029 - val_loss: 1.0813 - val_mae: 1.5659\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1523 - mae: 1.6624 - val_loss: 1.0751 - val_mae: 1.5596\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1709 - mae: 1.6845 - val_loss: 1.0757 - val_mae: 1.5614\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.1471 - mae: 1.6563 - val_loss: 1.0677 - val_mae: 1.5504\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1660 - mae: 1.6761 - val_loss: 1.0702 - val_mae: 1.5488\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1411 - mae: 1.6469 - val_loss: 1.0656 - val_mae: 1.5474\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1424 - mae: 1.6496 - val_loss: 1.0648 - val_mae: 1.5485\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1233 - mae: 1.6266 - val_loss: 1.0670 - val_mae: 1.5486\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1403 - mae: 1.6473 - val_loss: 1.0645 - val_mae: 1.5457\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.1448 - mae: 1.6522 - val_loss: 1.0627 - val_mae: 1.5418\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1368 - mae: 1.6449 - val_loss: 1.0643 - val_mae: 1.5460\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1319 - mae: 1.6359 - val_loss: 1.0650 - val_mae: 1.5456\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1320 - mae: 1.6368 - val_loss: 1.0642 - val_mae: 1.5444\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1186 - mae: 1.6222 - val_loss: 1.0583 - val_mae: 1.5374\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1018 - mae: 1.6028 - val_loss: 1.0558 - val_mae: 1.5347\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0932 - mae: 1.5929 - val_loss: 1.0569 - val_mae: 1.5356\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.1052 - mae: 1.6043 - val_loss: 1.0591 - val_mae: 1.5398\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0982 - mae: 1.5968 - val_loss: 1.0543 - val_mae: 1.5338\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0871 - mae: 1.5843 - val_loss: 1.0504 - val_mae: 1.5306\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0822 - mae: 1.5797 - val_loss: 1.0523 - val_mae: 1.5310\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0947 - mae: 1.5953 - val_loss: 1.0438 - val_mae: 1.5198\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0806 - mae: 1.5798 - val_loss: 1.0437 - val_mae: 1.5199\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0844 - mae: 1.5794 - val_loss: 1.0365 - val_mae: 1.5113\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0906 - mae: 1.5869 - val_loss: 1.0351 - val_mae: 1.5082\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0756 - mae: 1.5714 - val_loss: 1.0406 - val_mae: 1.5124\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0845 - mae: 1.5832 - val_loss: 1.0395 - val_mae: 1.5144\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0728 - mae: 1.5687 - val_loss: 1.0433 - val_mae: 1.5206\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0717 - mae: 1.5675 - val_loss: 1.0307 - val_mae: 1.5036\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0696 - mae: 1.5662 - val_loss: 1.0422 - val_mae: 1.5182\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0706 - mae: 1.5624 - val_loss: 1.0329 - val_mae: 1.5049\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0690 - mae: 1.5626 - val_loss: 1.0341 - val_mae: 1.5061\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0706 - mae: 1.5675 - val_loss: 1.0346 - val_mae: 1.5080\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0577 - mae: 1.5506 - val_loss: 1.0340 - val_mae: 1.5095\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0563 - mae: 1.5516 - val_loss: 1.0298 - val_mae: 1.5047\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0640 - mae: 1.5573 - val_loss: 1.0313 - val_mae: 1.5060\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0635 - mae: 1.5574 - val_loss: 1.0232 - val_mae: 1.4972\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0619 - mae: 1.5567 - val_loss: 1.0230 - val_mae: 1.4955\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0366 - mae: 1.5308 - val_loss: 1.0242 - val_mae: 1.4991\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0556 - mae: 1.5483 - val_loss: 1.0186 - val_mae: 1.4928\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0415 - mae: 1.5368 - val_loss: 1.0140 - val_mae: 1.4887\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0317 - mae: 1.5284 - val_loss: 1.0132 - val_mae: 1.4877\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0363 - mae: 1.5306 - val_loss: 1.0171 - val_mae: 1.4919\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0277 - mae: 1.5173 - val_loss: 1.0144 - val_mae: 1.4896\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0407 - mae: 1.5342 - val_loss: 1.0106 - val_mae: 1.4833\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0296 - mae: 1.5230 - val_loss: 1.0157 - val_mae: 1.4886\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0359 - mae: 1.5257 - val_loss: 1.0133 - val_mae: 1.4853\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0342 - mae: 1.5268 - val_loss: 1.0129 - val_mae: 1.4856\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0152 - mae: 1.5029 - val_loss: 1.0115 - val_mae: 1.4842\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0295 - mae: 1.5260 - val_loss: 1.0045 - val_mae: 1.4782\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0247 - mae: 1.5164 - val_loss: 1.0049 - val_mae: 1.4784\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0212 - mae: 1.5087 - val_loss: 1.0043 - val_mae: 1.4776\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0267 - mae: 1.5170 - val_loss: 1.0038 - val_mae: 1.4779\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0167 - mae: 1.5102 - val_loss: 0.9986 - val_mae: 1.4729\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0053 - mae: 1.4930 - val_loss: 0.9926 - val_mae: 1.4677\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0158 - mae: 1.5066 - val_loss: 0.9935 - val_mae: 1.4675\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0105 - mae: 1.4983 - val_loss: 0.9928 - val_mae: 1.4661\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0266 - mae: 1.5195 - val_loss: 0.9955 - val_mae: 1.4684\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 1.0072 - mae: 1.4964 - val_loss: 0.9889 - val_mae: 1.4616\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0126 - mae: 1.5027 - val_loss: 0.9839 - val_mae: 1.4552\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0061 - mae: 1.4931 - val_loss: 0.9838 - val_mae: 1.4557\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0117 - mae: 1.5036 - val_loss: 0.9795 - val_mae: 1.4531\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 1.0053 - mae: 1.4917 - val_loss: 0.9949 - val_mae: 1.4688\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0057 - mae: 1.4932 - val_loss: 0.9809 - val_mae: 1.4538\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.9899 - mae: 1.4785 - val_loss: 0.9792 - val_mae: 1.4522\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0059 - mae: 1.4930 - val_loss: 0.9791 - val_mae: 1.4500\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9981 - mae: 1.4853 - val_loss: 0.9747 - val_mae: 1.4471\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9817 - mae: 1.4654 - val_loss: 0.9797 - val_mae: 1.4537\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9890 - mae: 1.4797 - val_loss: 0.9751 - val_mae: 1.4462\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9892 - mae: 1.4742 - val_loss: 0.9750 - val_mae: 1.4469\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 1.0018 - mae: 1.4911 - val_loss: 0.9774 - val_mae: 1.4507\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9889 - mae: 1.4744 - val_loss: 0.9855 - val_mae: 1.4584\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9774 - mae: 1.4663 - val_loss: 0.9719 - val_mae: 1.4464\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9718 - mae: 1.4582 - val_loss: 0.9829 - val_mae: 1.4559\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.9908 - mae: 1.4797 - val_loss: 0.9728 - val_mae: 1.4438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 12:16:49,570] Trial 47 finished with value: 0.972792387008667 and parameters: {'dropout_2': 0.2439164333579673, 'dropout_3': 0.13732352166520273, 'dropout_4': 0.5795411114067678, 'dropout_5': 0.4205106626305688, 'learning_rate': 0.0002799301721087888, 'epochs': 100, 'batch_size': 128}. Best is trial 42 with value: 0.8230664134025574.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "108/108 [==============================] - 2s 7ms/step - loss: 1.6151 - mae: 2.1742 - val_loss: 1.3513 - val_mae: 1.8370\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.4586 - mae: 1.9998 - val_loss: 1.3380 - val_mae: 1.8326\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.3629 - mae: 1.8861 - val_loss: 1.3145 - val_mae: 1.8171\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.3166 - mae: 1.8332 - val_loss: 1.2739 - val_mae: 1.7836\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.2532 - mae: 1.7568 - val_loss: 1.2350 - val_mae: 1.7368\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.2281 - mae: 1.7213 - val_loss: 1.2025 - val_mae: 1.6995\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1944 - mae: 1.6846 - val_loss: 1.2008 - val_mae: 1.6924\n",
            "Epoch 8/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1916 - mae: 1.6816 - val_loss: 1.2035 - val_mae: 1.6899\n",
            "Epoch 9/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1854 - mae: 1.6703 - val_loss: 1.1891 - val_mae: 1.6761\n",
            "Epoch 10/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1627 - mae: 1.6448 - val_loss: 1.1841 - val_mae: 1.6719\n",
            "Epoch 11/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1702 - mae: 1.6523 - val_loss: 1.1835 - val_mae: 1.6663\n",
            "Epoch 12/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1641 - mae: 1.6419 - val_loss: 1.1954 - val_mae: 1.6765\n",
            "Epoch 13/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1388 - mae: 1.6169 - val_loss: 1.1722 - val_mae: 1.6563\n",
            "Epoch 14/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1532 - mae: 1.6392 - val_loss: 1.1614 - val_mae: 1.6442\n",
            "Epoch 15/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1294 - mae: 1.6140 - val_loss: 1.1727 - val_mae: 1.6549\n",
            "Epoch 16/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1387 - mae: 1.6231 - val_loss: 1.1763 - val_mae: 1.6701\n",
            "Epoch 17/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1332 - mae: 1.6173 - val_loss: 1.1778 - val_mae: 1.6824\n",
            "Epoch 18/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1257 - mae: 1.6115 - val_loss: 1.1603 - val_mae: 1.6609\n",
            "Epoch 19/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.1153 - mae: 1.6043 - val_loss: 1.1656 - val_mae: 1.6638\n",
            "Epoch 20/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1063 - mae: 1.5897 - val_loss: 1.1538 - val_mae: 1.6532\n",
            "Epoch 21/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1242 - mae: 1.6142 - val_loss: 1.1572 - val_mae: 1.6576\n",
            "Epoch 22/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1099 - mae: 1.5969 - val_loss: 1.1587 - val_mae: 1.6598\n",
            "Epoch 23/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1069 - mae: 1.5979 - val_loss: 1.1505 - val_mae: 1.6501\n",
            "Epoch 24/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0936 - mae: 1.5827 - val_loss: 1.1476 - val_mae: 1.6462\n",
            "Epoch 25/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1161 - mae: 1.6028 - val_loss: 1.1532 - val_mae: 1.6530\n",
            "Epoch 26/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1098 - mae: 1.5954 - val_loss: 1.1406 - val_mae: 1.6422\n",
            "Epoch 27/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0951 - mae: 1.5849 - val_loss: 1.1415 - val_mae: 1.6413\n",
            "Epoch 28/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0998 - mae: 1.5844 - val_loss: 1.1405 - val_mae: 1.6395\n",
            "Epoch 29/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1022 - mae: 1.5904 - val_loss: 1.1473 - val_mae: 1.6436\n",
            "Epoch 30/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1111 - mae: 1.5992 - val_loss: 1.1431 - val_mae: 1.6422\n",
            "Epoch 31/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0993 - mae: 1.5858 - val_loss: 1.1600 - val_mae: 1.6610\n",
            "Epoch 32/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1015 - mae: 1.5903 - val_loss: 1.1522 - val_mae: 1.6495\n",
            "Epoch 33/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0932 - mae: 1.5789 - val_loss: 1.1244 - val_mae: 1.6230\n",
            "Epoch 34/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0862 - mae: 1.5713 - val_loss: 1.1484 - val_mae: 1.6495\n",
            "Epoch 35/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0926 - mae: 1.5825 - val_loss: 1.1370 - val_mae: 1.6351\n",
            "Epoch 36/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.1100 - mae: 1.5969 - val_loss: 1.1522 - val_mae: 1.6493\n",
            "Epoch 37/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0936 - mae: 1.5814 - val_loss: 1.1448 - val_mae: 1.6444\n",
            "Epoch 38/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.1011 - mae: 1.5848 - val_loss: 1.1330 - val_mae: 1.6267\n",
            "Epoch 39/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0797 - mae: 1.5621 - val_loss: 1.1208 - val_mae: 1.6146\n",
            "Epoch 40/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0918 - mae: 1.5811 - val_loss: 1.1326 - val_mae: 1.6273\n",
            "Epoch 41/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0855 - mae: 1.5735 - val_loss: 1.1113 - val_mae: 1.6010\n",
            "Epoch 42/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0732 - mae: 1.5599 - val_loss: 1.1107 - val_mae: 1.6042\n",
            "Epoch 43/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0759 - mae: 1.5605 - val_loss: 1.1181 - val_mae: 1.6138\n",
            "Epoch 44/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0955 - mae: 1.5820 - val_loss: 1.1302 - val_mae: 1.6261\n",
            "Epoch 45/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0902 - mae: 1.5742 - val_loss: 1.1245 - val_mae: 1.6202\n",
            "Epoch 46/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0774 - mae: 1.5631 - val_loss: 1.1170 - val_mae: 1.6086\n",
            "Epoch 47/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0704 - mae: 1.5557 - val_loss: 1.1150 - val_mae: 1.6100\n",
            "Epoch 48/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0796 - mae: 1.5651 - val_loss: 1.1131 - val_mae: 1.6071\n",
            "Epoch 49/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0754 - mae: 1.5605 - val_loss: 1.1168 - val_mae: 1.6102\n",
            "Epoch 50/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0850 - mae: 1.5737 - val_loss: 1.1171 - val_mae: 1.6132\n",
            "Epoch 51/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0797 - mae: 1.5688 - val_loss: 1.0983 - val_mae: 1.5889\n",
            "Epoch 52/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0697 - mae: 1.5569 - val_loss: 1.0992 - val_mae: 1.5879\n",
            "Epoch 53/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0569 - mae: 1.5420 - val_loss: 1.1043 - val_mae: 1.5978\n",
            "Epoch 54/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0811 - mae: 1.5688 - val_loss: 1.0883 - val_mae: 1.5764\n",
            "Epoch 55/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0640 - mae: 1.5503 - val_loss: 1.1026 - val_mae: 1.5974\n",
            "Epoch 56/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0723 - mae: 1.5598 - val_loss: 1.1041 - val_mae: 1.5943\n",
            "Epoch 57/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0622 - mae: 1.5489 - val_loss: 1.1022 - val_mae: 1.5887\n",
            "Epoch 58/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0805 - mae: 1.5679 - val_loss: 1.1147 - val_mae: 1.6051\n",
            "Epoch 59/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0764 - mae: 1.5607 - val_loss: 1.0895 - val_mae: 1.5754\n",
            "Epoch 60/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0829 - mae: 1.5707 - val_loss: 1.1157 - val_mae: 1.6073\n",
            "Epoch 61/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0643 - mae: 1.5513 - val_loss: 1.1069 - val_mae: 1.5975\n",
            "Epoch 62/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0612 - mae: 1.5459 - val_loss: 1.1026 - val_mae: 1.5893\n",
            "Epoch 63/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0680 - mae: 1.5553 - val_loss: 1.1058 - val_mae: 1.5958\n",
            "Epoch 64/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0546 - mae: 1.5386 - val_loss: 1.1110 - val_mae: 1.6002\n",
            "Epoch 65/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0675 - mae: 1.5535 - val_loss: 1.1059 - val_mae: 1.5939\n",
            "Epoch 66/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0740 - mae: 1.5565 - val_loss: 1.1145 - val_mae: 1.6069\n",
            "Epoch 67/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0679 - mae: 1.5547 - val_loss: 1.1252 - val_mae: 1.6126\n",
            "Epoch 68/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0465 - mae: 1.5266 - val_loss: 1.1086 - val_mae: 1.5979\n",
            "Epoch 69/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0721 - mae: 1.5547 - val_loss: 1.1241 - val_mae: 1.6141\n",
            "Epoch 70/200\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0770 - mae: 1.5610 - val_loss: 1.1042 - val_mae: 1.5924\n",
            "Epoch 71/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0455 - mae: 1.5313 - val_loss: 1.1047 - val_mae: 1.5956\n",
            "Epoch 72/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0626 - mae: 1.5474 - val_loss: 1.1113 - val_mae: 1.6040\n",
            "Epoch 73/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0594 - mae: 1.5426 - val_loss: 1.1095 - val_mae: 1.5988\n",
            "Epoch 74/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0669 - mae: 1.5489 - val_loss: 1.1236 - val_mae: 1.6170\n",
            "Epoch 75/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0601 - mae: 1.5439 - val_loss: 1.1081 - val_mae: 1.6030\n",
            "Epoch 76/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0654 - mae: 1.5533 - val_loss: 1.1110 - val_mae: 1.6027\n",
            "Epoch 77/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0483 - mae: 1.5306 - val_loss: 1.1206 - val_mae: 1.6224\n",
            "Epoch 78/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0511 - mae: 1.5364 - val_loss: 1.1135 - val_mae: 1.6080\n",
            "Epoch 79/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0628 - mae: 1.5477 - val_loss: 1.1119 - val_mae: 1.6077\n",
            "Epoch 80/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0465 - mae: 1.5289 - val_loss: 1.1181 - val_mae: 1.6108\n",
            "Epoch 81/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0510 - mae: 1.5348 - val_loss: 1.1009 - val_mae: 1.5966\n",
            "Epoch 82/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0668 - mae: 1.5461 - val_loss: 1.1105 - val_mae: 1.6030\n",
            "Epoch 83/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0670 - mae: 1.5511 - val_loss: 1.1095 - val_mae: 1.6037\n",
            "Epoch 84/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0569 - mae: 1.5405 - val_loss: 1.1367 - val_mae: 1.6339\n",
            "Epoch 85/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0693 - mae: 1.5495 - val_loss: 1.1114 - val_mae: 1.6015\n",
            "Epoch 86/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0637 - mae: 1.5475 - val_loss: 1.1171 - val_mae: 1.6069\n",
            "Epoch 87/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0692 - mae: 1.5509 - val_loss: 1.1188 - val_mae: 1.6114\n",
            "Epoch 88/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0603 - mae: 1.5432 - val_loss: 1.1198 - val_mae: 1.6116\n",
            "Epoch 89/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0642 - mae: 1.5461 - val_loss: 1.1135 - val_mae: 1.6076\n",
            "Epoch 90/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0655 - mae: 1.5509 - val_loss: 1.1096 - val_mae: 1.5976\n",
            "Epoch 91/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0676 - mae: 1.5537 - val_loss: 1.0972 - val_mae: 1.5808\n",
            "Epoch 92/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0621 - mae: 1.5437 - val_loss: 1.0928 - val_mae: 1.5844\n",
            "Epoch 93/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0601 - mae: 1.5441 - val_loss: 1.1086 - val_mae: 1.6004\n",
            "Epoch 94/200\n",
            "108/108 [==============================] - 1s 12ms/step - loss: 1.0637 - mae: 1.5487 - val_loss: 1.1033 - val_mae: 1.5879\n",
            "Epoch 95/200\n",
            "108/108 [==============================] - 1s 14ms/step - loss: 1.0468 - mae: 1.5289 - val_loss: 1.0848 - val_mae: 1.5721\n",
            "Epoch 96/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0465 - mae: 1.5321 - val_loss: 1.0933 - val_mae: 1.5815\n",
            "Epoch 97/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0505 - mae: 1.5326 - val_loss: 1.0934 - val_mae: 1.5881\n",
            "Epoch 98/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0539 - mae: 1.5361 - val_loss: 1.0937 - val_mae: 1.5875\n",
            "Epoch 99/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0619 - mae: 1.5492 - val_loss: 1.0942 - val_mae: 1.5810\n",
            "Epoch 100/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0447 - mae: 1.5273 - val_loss: 1.0972 - val_mae: 1.5938\n",
            "Epoch 101/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0510 - mae: 1.5314 - val_loss: 1.1075 - val_mae: 1.6018\n",
            "Epoch 102/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0380 - mae: 1.5200 - val_loss: 1.0934 - val_mae: 1.5869\n",
            "Epoch 103/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0340 - mae: 1.5164 - val_loss: 1.0934 - val_mae: 1.5870\n",
            "Epoch 104/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0623 - mae: 1.5470 - val_loss: 1.1008 - val_mae: 1.5901\n",
            "Epoch 105/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0618 - mae: 1.5445 - val_loss: 1.0836 - val_mae: 1.5724\n",
            "Epoch 106/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0575 - mae: 1.5372 - val_loss: 1.1015 - val_mae: 1.5926\n",
            "Epoch 107/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0462 - mae: 1.5299 - val_loss: 1.0974 - val_mae: 1.5904\n",
            "Epoch 108/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0477 - mae: 1.5277 - val_loss: 1.0823 - val_mae: 1.5672\n",
            "Epoch 109/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0592 - mae: 1.5430 - val_loss: 1.1063 - val_mae: 1.5961\n",
            "Epoch 110/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0452 - mae: 1.5264 - val_loss: 1.0793 - val_mae: 1.5689\n",
            "Epoch 111/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0398 - mae: 1.5191 - val_loss: 1.0893 - val_mae: 1.5829\n",
            "Epoch 112/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0503 - mae: 1.5324 - val_loss: 1.0937 - val_mae: 1.5852\n",
            "Epoch 113/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0423 - mae: 1.5269 - val_loss: 1.1002 - val_mae: 1.5907\n",
            "Epoch 114/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0510 - mae: 1.5332 - val_loss: 1.0956 - val_mae: 1.5871\n",
            "Epoch 115/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0442 - mae: 1.5315 - val_loss: 1.0844 - val_mae: 1.5731\n",
            "Epoch 116/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0555 - mae: 1.5372 - val_loss: 1.1001 - val_mae: 1.5915\n",
            "Epoch 117/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0500 - mae: 1.5286 - val_loss: 1.0890 - val_mae: 1.5762\n",
            "Epoch 118/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0569 - mae: 1.5374 - val_loss: 1.0796 - val_mae: 1.5650\n",
            "Epoch 119/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0406 - mae: 1.5219 - val_loss: 1.1036 - val_mae: 1.5957\n",
            "Epoch 120/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0419 - mae: 1.5232 - val_loss: 1.0840 - val_mae: 1.5689\n",
            "Epoch 121/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0422 - mae: 1.5250 - val_loss: 1.0780 - val_mae: 1.5628\n",
            "Epoch 122/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0453 - mae: 1.5309 - val_loss: 1.0874 - val_mae: 1.5728\n",
            "Epoch 123/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0464 - mae: 1.5295 - val_loss: 1.0804 - val_mae: 1.5679\n",
            "Epoch 124/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0448 - mae: 1.5255 - val_loss: 1.0868 - val_mae: 1.5738\n",
            "Epoch 125/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0377 - mae: 1.5163 - val_loss: 1.0895 - val_mae: 1.5768\n",
            "Epoch 126/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0423 - mae: 1.5224 - val_loss: 1.0762 - val_mae: 1.5623\n",
            "Epoch 127/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0442 - mae: 1.5262 - val_loss: 1.0849 - val_mae: 1.5690\n",
            "Epoch 128/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0428 - mae: 1.5234 - val_loss: 1.0994 - val_mae: 1.5900\n",
            "Epoch 129/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0380 - mae: 1.5174 - val_loss: 1.0820 - val_mae: 1.5777\n",
            "Epoch 130/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0481 - mae: 1.5316 - val_loss: 1.1031 - val_mae: 1.5968\n",
            "Epoch 131/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0425 - mae: 1.5263 - val_loss: 1.0928 - val_mae: 1.5822\n",
            "Epoch 132/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0499 - mae: 1.5320 - val_loss: 1.0790 - val_mae: 1.5716\n",
            "Epoch 133/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0364 - mae: 1.5209 - val_loss: 1.1022 - val_mae: 1.5890\n",
            "Epoch 134/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0431 - mae: 1.5264 - val_loss: 1.0849 - val_mae: 1.5725\n",
            "Epoch 135/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0273 - mae: 1.5080 - val_loss: 1.0712 - val_mae: 1.5547\n",
            "Epoch 136/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0542 - mae: 1.5416 - val_loss: 1.0819 - val_mae: 1.5686\n",
            "Epoch 137/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0395 - mae: 1.5276 - val_loss: 1.0801 - val_mae: 1.5670\n",
            "Epoch 138/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0397 - mae: 1.5216 - val_loss: 1.0884 - val_mae: 1.5781\n",
            "Epoch 139/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0422 - mae: 1.5276 - val_loss: 1.0852 - val_mae: 1.5733\n",
            "Epoch 140/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0456 - mae: 1.5292 - val_loss: 1.0730 - val_mae: 1.5561\n",
            "Epoch 141/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0420 - mae: 1.5285 - val_loss: 1.0697 - val_mae: 1.5588\n",
            "Epoch 142/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0423 - mae: 1.5273 - val_loss: 1.0739 - val_mae: 1.5636\n",
            "Epoch 143/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0504 - mae: 1.5337 - val_loss: 1.0908 - val_mae: 1.5794\n",
            "Epoch 144/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0303 - mae: 1.5091 - val_loss: 1.0745 - val_mae: 1.5612\n",
            "Epoch 145/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0221 - mae: 1.5030 - val_loss: 1.0635 - val_mae: 1.5531\n",
            "Epoch 146/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0541 - mae: 1.5373 - val_loss: 1.0788 - val_mae: 1.5601\n",
            "Epoch 147/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0392 - mae: 1.5166 - val_loss: 1.0741 - val_mae: 1.5625\n",
            "Epoch 148/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0376 - mae: 1.5211 - val_loss: 1.0612 - val_mae: 1.5497\n",
            "Epoch 149/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0564 - mae: 1.5406 - val_loss: 1.0884 - val_mae: 1.5652\n",
            "Epoch 150/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0384 - mae: 1.5196 - val_loss: 1.0723 - val_mae: 1.5532\n",
            "Epoch 151/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0341 - mae: 1.5128 - val_loss: 1.0672 - val_mae: 1.5491\n",
            "Epoch 152/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0590 - mae: 1.5405 - val_loss: 1.0807 - val_mae: 1.5547\n",
            "Epoch 153/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0329 - mae: 1.5091 - val_loss: 1.0792 - val_mae: 1.5525\n",
            "Epoch 154/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0451 - mae: 1.5236 - val_loss: 1.0771 - val_mae: 1.5621\n",
            "Epoch 155/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0363 - mae: 1.5185 - val_loss: 1.0619 - val_mae: 1.5404\n",
            "Epoch 156/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0276 - mae: 1.5067 - val_loss: 1.0665 - val_mae: 1.5465\n",
            "Epoch 157/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0339 - mae: 1.5152 - val_loss: 1.0848 - val_mae: 1.5699\n",
            "Epoch 158/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0300 - mae: 1.5093 - val_loss: 1.0851 - val_mae: 1.5697\n",
            "Epoch 159/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0365 - mae: 1.5170 - val_loss: 1.0723 - val_mae: 1.5502\n",
            "Epoch 160/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0242 - mae: 1.5013 - val_loss: 1.0678 - val_mae: 1.5490\n",
            "Epoch 161/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0510 - mae: 1.5320 - val_loss: 1.0842 - val_mae: 1.5706\n",
            "Epoch 162/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0148 - mae: 1.4913 - val_loss: 1.0775 - val_mae: 1.5599\n",
            "Epoch 163/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0228 - mae: 1.5033 - val_loss: 1.0687 - val_mae: 1.5532\n",
            "Epoch 164/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0371 - mae: 1.5198 - val_loss: 1.0667 - val_mae: 1.5487\n",
            "Epoch 165/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0334 - mae: 1.5153 - val_loss: 1.0785 - val_mae: 1.5648\n",
            "Epoch 166/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0289 - mae: 1.5085 - val_loss: 1.0756 - val_mae: 1.5626\n",
            "Epoch 167/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0269 - mae: 1.5056 - val_loss: 1.0698 - val_mae: 1.5483\n",
            "Epoch 168/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0351 - mae: 1.5132 - val_loss: 1.0707 - val_mae: 1.5567\n",
            "Epoch 169/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0339 - mae: 1.5123 - val_loss: 1.0805 - val_mae: 1.5668\n",
            "Epoch 170/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0283 - mae: 1.5051 - val_loss: 1.0680 - val_mae: 1.5508\n",
            "Epoch 171/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0370 - mae: 1.5171 - val_loss: 1.0630 - val_mae: 1.5426\n",
            "Epoch 172/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0261 - mae: 1.5091 - val_loss: 1.0708 - val_mae: 1.5553\n",
            "Epoch 173/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0360 - mae: 1.5156 - val_loss: 1.0785 - val_mae: 1.5590\n",
            "Epoch 174/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0257 - mae: 1.5016 - val_loss: 1.0610 - val_mae: 1.5439\n",
            "Epoch 175/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0214 - mae: 1.5005 - val_loss: 1.0812 - val_mae: 1.5716\n",
            "Epoch 176/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0314 - mae: 1.5143 - val_loss: 1.0655 - val_mae: 1.5488\n",
            "Epoch 177/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0371 - mae: 1.5203 - val_loss: 1.0782 - val_mae: 1.5611\n",
            "Epoch 178/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0289 - mae: 1.5080 - val_loss: 1.0885 - val_mae: 1.5703\n",
            "Epoch 179/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0283 - mae: 1.5058 - val_loss: 1.0679 - val_mae: 1.5525\n",
            "Epoch 180/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0267 - mae: 1.5102 - val_loss: 1.0562 - val_mae: 1.5333\n",
            "Epoch 181/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0259 - mae: 1.5022 - val_loss: 1.0734 - val_mae: 1.5591\n",
            "Epoch 182/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0367 - mae: 1.5153 - val_loss: 1.0709 - val_mae: 1.5567\n",
            "Epoch 183/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0392 - mae: 1.5210 - val_loss: 1.0825 - val_mae: 1.5680\n",
            "Epoch 184/200\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0291 - mae: 1.5087 - val_loss: 1.0850 - val_mae: 1.5687\n",
            "Epoch 185/200\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0404 - mae: 1.5235 - val_loss: 1.0854 - val_mae: 1.5742\n",
            "Epoch 186/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0444 - mae: 1.5240 - val_loss: 1.0879 - val_mae: 1.5732\n",
            "Epoch 187/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0351 - mae: 1.5128 - val_loss: 1.0631 - val_mae: 1.5450\n",
            "Epoch 188/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0288 - mae: 1.5103 - val_loss: 1.0754 - val_mae: 1.5629\n",
            "Epoch 189/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0304 - mae: 1.5111 - val_loss: 1.0681 - val_mae: 1.5437\n",
            "Epoch 190/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0265 - mae: 1.5057 - val_loss: 1.0641 - val_mae: 1.5435\n",
            "Epoch 191/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0161 - mae: 1.4959 - val_loss: 1.0617 - val_mae: 1.5435\n",
            "Epoch 192/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0319 - mae: 1.5124 - val_loss: 1.0717 - val_mae: 1.5562\n",
            "Epoch 193/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0257 - mae: 1.5064 - val_loss: 1.0642 - val_mae: 1.5475\n",
            "Epoch 194/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0192 - mae: 1.4998 - val_loss: 1.0622 - val_mae: 1.5473\n",
            "Epoch 195/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0120 - mae: 1.4901 - val_loss: 1.0730 - val_mae: 1.5597\n",
            "Epoch 196/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0076 - mae: 1.4850 - val_loss: 1.0850 - val_mae: 1.5715\n",
            "Epoch 197/200\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 1.0296 - mae: 1.5110 - val_loss: 1.0793 - val_mae: 1.5637\n",
            "Epoch 198/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0334 - mae: 1.5149 - val_loss: 1.0568 - val_mae: 1.5351\n",
            "Epoch 199/200\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0324 - mae: 1.5135 - val_loss: 1.0650 - val_mae: 1.5434\n",
            "Epoch 200/200\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0232 - mae: 1.5042 - val_loss: 1.0701 - val_mae: 1.5498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 12:19:13,565] Trial 48 finished with value: 1.070059895515442 and parameters: {'dropout_2': 0.7744491734237383, 'dropout_3': 0.30896836105832165, 'dropout_4': 0.6650463022348037, 'dropout_5': 0.4824878000968012, 'learning_rate': 0.0010762310568082187, 'epochs': 200, 'batch_size': 32}. Best is trial 42 with value: 0.8230664134025574.\n",
            "<ipython-input-29-50718b8ee584>:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_2', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_3', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:18: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_4', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:21: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  model.add(layers.Dropout(trial.suggest_uniform('dropout_5', 0.1, 0.9)))\n",
            "<ipython-input-29-50718b8ee584>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "108/108 [==============================] - 2s 7ms/step - loss: 1.5117 - mae: 2.0683 - val_loss: 1.3223 - val_mae: 1.8164\n",
            "Epoch 2/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.3336 - mae: 1.8705 - val_loss: 1.3044 - val_mae: 1.8059\n",
            "Epoch 3/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2796 - mae: 1.8032 - val_loss: 1.2489 - val_mae: 1.7492\n",
            "Epoch 4/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.2355 - mae: 1.7564 - val_loss: 1.2313 - val_mae: 1.7246\n",
            "Epoch 5/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.2026 - mae: 1.7137 - val_loss: 1.1825 - val_mae: 1.6772\n",
            "Epoch 6/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.1740 - mae: 1.6832 - val_loss: 1.1712 - val_mae: 1.6618\n",
            "Epoch 7/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1400 - mae: 1.6439 - val_loss: 1.1805 - val_mae: 1.6779\n",
            "Epoch 8/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1122 - mae: 1.6109 - val_loss: 1.1341 - val_mae: 1.6251\n",
            "Epoch 9/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.1037 - mae: 1.5991 - val_loss: 1.1172 - val_mae: 1.6053\n",
            "Epoch 10/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0779 - mae: 1.5701 - val_loss: 1.1237 - val_mae: 1.6154\n",
            "Epoch 11/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0805 - mae: 1.5754 - val_loss: 1.1049 - val_mae: 1.5876\n",
            "Epoch 12/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0696 - mae: 1.5636 - val_loss: 1.1188 - val_mae: 1.6059\n",
            "Epoch 13/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0754 - mae: 1.5663 - val_loss: 1.1295 - val_mae: 1.6190\n",
            "Epoch 14/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 1.0687 - mae: 1.5608 - val_loss: 1.1486 - val_mae: 1.6479\n",
            "Epoch 15/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 1.0390 - mae: 1.5267 - val_loss: 1.1027 - val_mae: 1.5927\n",
            "Epoch 16/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0409 - mae: 1.5288 - val_loss: 1.0822 - val_mae: 1.5722\n",
            "Epoch 17/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0392 - mae: 1.5278 - val_loss: 1.0725 - val_mae: 1.5576\n",
            "Epoch 18/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0416 - mae: 1.5338 - val_loss: 1.0719 - val_mae: 1.5631\n",
            "Epoch 19/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0335 - mae: 1.5262 - val_loss: 1.0547 - val_mae: 1.5382\n",
            "Epoch 20/150\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 1.0227 - mae: 1.5129 - val_loss: 1.0547 - val_mae: 1.5385\n",
            "Epoch 21/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0057 - mae: 1.4903 - val_loss: 1.0266 - val_mae: 1.4993\n",
            "Epoch 22/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 1.0048 - mae: 1.4894 - val_loss: 1.0445 - val_mae: 1.5205\n",
            "Epoch 23/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9982 - mae: 1.4834 - val_loss: 1.0192 - val_mae: 1.4988\n",
            "Epoch 24/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 1.0065 - mae: 1.4951 - val_loss: 1.0223 - val_mae: 1.4962\n",
            "Epoch 25/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9939 - mae: 1.4784 - val_loss: 1.0362 - val_mae: 1.5076\n",
            "Epoch 26/150\n",
            "108/108 [==============================] - 1s 8ms/step - loss: 0.9984 - mae: 1.4845 - val_loss: 1.0188 - val_mae: 1.4876\n",
            "Epoch 27/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.9895 - mae: 1.4754 - val_loss: 1.0276 - val_mae: 1.5042\n",
            "Epoch 28/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9771 - mae: 1.4607 - val_loss: 1.0217 - val_mae: 1.4921\n",
            "Epoch 29/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9962 - mae: 1.4825 - val_loss: 1.0012 - val_mae: 1.4687\n",
            "Epoch 30/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9901 - mae: 1.4757 - val_loss: 1.0205 - val_mae: 1.4966\n",
            "Epoch 31/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9762 - mae: 1.4592 - val_loss: 0.9889 - val_mae: 1.4542\n",
            "Epoch 32/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9844 - mae: 1.4657 - val_loss: 1.0036 - val_mae: 1.4851\n",
            "Epoch 33/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9786 - mae: 1.4620 - val_loss: 1.0070 - val_mae: 1.4784\n",
            "Epoch 34/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9665 - mae: 1.4485 - val_loss: 0.9984 - val_mae: 1.4622\n",
            "Epoch 35/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9554 - mae: 1.4343 - val_loss: 0.9946 - val_mae: 1.4565\n",
            "Epoch 36/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9854 - mae: 1.4684 - val_loss: 0.9875 - val_mae: 1.4507\n",
            "Epoch 37/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9726 - mae: 1.4519 - val_loss: 0.9875 - val_mae: 1.4478\n",
            "Epoch 38/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9547 - mae: 1.4316 - val_loss: 0.9979 - val_mae: 1.4697\n",
            "Epoch 39/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9684 - mae: 1.4507 - val_loss: 0.9807 - val_mae: 1.4453\n",
            "Epoch 40/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9584 - mae: 1.4380 - val_loss: 0.9849 - val_mae: 1.4592\n",
            "Epoch 41/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9649 - mae: 1.4451 - val_loss: 0.9713 - val_mae: 1.4377\n",
            "Epoch 42/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9678 - mae: 1.4512 - val_loss: 0.9807 - val_mae: 1.4486\n",
            "Epoch 43/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9618 - mae: 1.4432 - val_loss: 0.9732 - val_mae: 1.4459\n",
            "Epoch 44/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9482 - mae: 1.4275 - val_loss: 1.0005 - val_mae: 1.4786\n",
            "Epoch 45/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9515 - mae: 1.4332 - val_loss: 0.9839 - val_mae: 1.4544\n",
            "Epoch 46/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9557 - mae: 1.4361 - val_loss: 0.9746 - val_mae: 1.4356\n",
            "Epoch 47/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9545 - mae: 1.4332 - val_loss: 0.9718 - val_mae: 1.4316\n",
            "Epoch 48/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9494 - mae: 1.4307 - val_loss: 0.9520 - val_mae: 1.4152\n",
            "Epoch 49/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9349 - mae: 1.4132 - val_loss: 0.9566 - val_mae: 1.4210\n",
            "Epoch 50/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9494 - mae: 1.4305 - val_loss: 0.9675 - val_mae: 1.4322\n",
            "Epoch 51/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9697 - mae: 1.4507 - val_loss: 0.9585 - val_mae: 1.4172\n",
            "Epoch 52/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9468 - mae: 1.4235 - val_loss: 0.9724 - val_mae: 1.4354\n",
            "Epoch 53/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9354 - mae: 1.4115 - val_loss: 0.9996 - val_mae: 1.4669\n",
            "Epoch 54/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9309 - mae: 1.4074 - val_loss: 0.9726 - val_mae: 1.4314\n",
            "Epoch 55/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9272 - mae: 1.4028 - val_loss: 0.9788 - val_mae: 1.4456\n",
            "Epoch 56/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9286 - mae: 1.4057 - val_loss: 0.9579 - val_mae: 1.4178\n",
            "Epoch 57/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9349 - mae: 1.4116 - val_loss: 0.9564 - val_mae: 1.4204\n",
            "Epoch 58/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9347 - mae: 1.4127 - val_loss: 0.9685 - val_mae: 1.4312\n",
            "Epoch 59/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9416 - mae: 1.4200 - val_loss: 0.9541 - val_mae: 1.4116\n",
            "Epoch 60/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9329 - mae: 1.4089 - val_loss: 0.9668 - val_mae: 1.4266\n",
            "Epoch 61/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9220 - mae: 1.3968 - val_loss: 0.9457 - val_mae: 1.4033\n",
            "Epoch 62/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9090 - mae: 1.3790 - val_loss: 0.9514 - val_mae: 1.4158\n",
            "Epoch 63/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9082 - mae: 1.3849 - val_loss: 0.9558 - val_mae: 1.4151\n",
            "Epoch 64/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9197 - mae: 1.3942 - val_loss: 0.9629 - val_mae: 1.4287\n",
            "Epoch 65/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9302 - mae: 1.4080 - val_loss: 0.9553 - val_mae: 1.4149\n",
            "Epoch 66/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9079 - mae: 1.3812 - val_loss: 0.9541 - val_mae: 1.4129\n",
            "Epoch 67/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9170 - mae: 1.3911 - val_loss: 0.9216 - val_mae: 1.3762\n",
            "Epoch 68/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9135 - mae: 1.3881 - val_loss: 0.9422 - val_mae: 1.4081\n",
            "Epoch 69/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9178 - mae: 1.3906 - val_loss: 0.9346 - val_mae: 1.3941\n",
            "Epoch 70/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9079 - mae: 1.3801 - val_loss: 0.9324 - val_mae: 1.3827\n",
            "Epoch 71/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8921 - mae: 1.3661 - val_loss: 0.9246 - val_mae: 1.3925\n",
            "Epoch 72/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8919 - mae: 1.3643 - val_loss: 0.9339 - val_mae: 1.3877\n",
            "Epoch 73/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9022 - mae: 1.3768 - val_loss: 0.9419 - val_mae: 1.4024\n",
            "Epoch 74/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9037 - mae: 1.3801 - val_loss: 0.9222 - val_mae: 1.3753\n",
            "Epoch 75/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.9144 - mae: 1.3878 - val_loss: 0.9207 - val_mae: 1.3672\n",
            "Epoch 76/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8934 - mae: 1.3654 - val_loss: 0.9154 - val_mae: 1.3773\n",
            "Epoch 77/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8963 - mae: 1.3700 - val_loss: 0.9176 - val_mae: 1.3737\n",
            "Epoch 78/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.9211 - mae: 1.3976 - val_loss: 0.9210 - val_mae: 1.3760\n",
            "Epoch 79/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8905 - mae: 1.3629 - val_loss: 0.9348 - val_mae: 1.3972\n",
            "Epoch 80/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8972 - mae: 1.3700 - val_loss: 0.9216 - val_mae: 1.3760\n",
            "Epoch 81/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8971 - mae: 1.3675 - val_loss: 0.9251 - val_mae: 1.3980\n",
            "Epoch 82/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.9129 - mae: 1.3889 - val_loss: 0.9116 - val_mae: 1.3761\n",
            "Epoch 83/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8890 - mae: 1.3613 - val_loss: 0.9010 - val_mae: 1.3583\n",
            "Epoch 84/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8846 - mae: 1.3558 - val_loss: 0.9235 - val_mae: 1.3896\n",
            "Epoch 85/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8938 - mae: 1.3688 - val_loss: 0.9244 - val_mae: 1.3808\n",
            "Epoch 86/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8864 - mae: 1.3587 - val_loss: 0.9240 - val_mae: 1.3768\n",
            "Epoch 87/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8777 - mae: 1.3459 - val_loss: 0.9100 - val_mae: 1.3610\n",
            "Epoch 88/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8905 - mae: 1.3623 - val_loss: 0.9109 - val_mae: 1.3563\n",
            "Epoch 89/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.9021 - mae: 1.3770 - val_loss: 0.9223 - val_mae: 1.3815\n",
            "Epoch 90/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8955 - mae: 1.3676 - val_loss: 0.9018 - val_mae: 1.3582\n",
            "Epoch 91/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8885 - mae: 1.3583 - val_loss: 0.9147 - val_mae: 1.3709\n",
            "Epoch 92/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8633 - mae: 1.3328 - val_loss: 0.9179 - val_mae: 1.3765\n",
            "Epoch 93/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8833 - mae: 1.3513 - val_loss: 0.9192 - val_mae: 1.3696\n",
            "Epoch 94/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8769 - mae: 1.3447 - val_loss: 0.9299 - val_mae: 1.3895\n",
            "Epoch 95/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8695 - mae: 1.3350 - val_loss: 0.9153 - val_mae: 1.3695\n",
            "Epoch 96/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8848 - mae: 1.3514 - val_loss: 0.9136 - val_mae: 1.3717\n",
            "Epoch 97/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8808 - mae: 1.3493 - val_loss: 0.9142 - val_mae: 1.3699\n",
            "Epoch 98/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8795 - mae: 1.3553 - val_loss: 0.9241 - val_mae: 1.3818\n",
            "Epoch 99/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8640 - mae: 1.3341 - val_loss: 0.9564 - val_mae: 1.4228\n",
            "Epoch 100/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8620 - mae: 1.3326 - val_loss: 0.9149 - val_mae: 1.3661\n",
            "Epoch 101/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8771 - mae: 1.3445 - val_loss: 0.9242 - val_mae: 1.3904\n",
            "Epoch 102/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8685 - mae: 1.3364 - val_loss: 0.9226 - val_mae: 1.3838\n",
            "Epoch 103/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8771 - mae: 1.3447 - val_loss: 0.8999 - val_mae: 1.3544\n",
            "Epoch 104/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8868 - mae: 1.3581 - val_loss: 0.8974 - val_mae: 1.3539\n",
            "Epoch 105/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8659 - mae: 1.3371 - val_loss: 0.9013 - val_mae: 1.3581\n",
            "Epoch 106/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8560 - mae: 1.3224 - val_loss: 0.9174 - val_mae: 1.3801\n",
            "Epoch 107/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8705 - mae: 1.3414 - val_loss: 0.9100 - val_mae: 1.3773\n",
            "Epoch 108/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8774 - mae: 1.3453 - val_loss: 0.9553 - val_mae: 1.4313\n",
            "Epoch 109/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8677 - mae: 1.3385 - val_loss: 0.8926 - val_mae: 1.3520\n",
            "Epoch 110/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8518 - mae: 1.3170 - val_loss: 0.9082 - val_mae: 1.3695\n",
            "Epoch 111/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8636 - mae: 1.3344 - val_loss: 0.8890 - val_mae: 1.3483\n",
            "Epoch 112/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8772 - mae: 1.3462 - val_loss: 0.9104 - val_mae: 1.3670\n",
            "Epoch 113/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8487 - mae: 1.3116 - val_loss: 0.9057 - val_mae: 1.3646\n",
            "Epoch 114/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8550 - mae: 1.3216 - val_loss: 0.8905 - val_mae: 1.3414\n",
            "Epoch 115/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8681 - mae: 1.3372 - val_loss: 0.9041 - val_mae: 1.3613\n",
            "Epoch 116/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8523 - mae: 1.3180 - val_loss: 0.9051 - val_mae: 1.3560\n",
            "Epoch 117/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8577 - mae: 1.3235 - val_loss: 0.9101 - val_mae: 1.3667\n",
            "Epoch 118/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8689 - mae: 1.3399 - val_loss: 0.8768 - val_mae: 1.3350\n",
            "Epoch 119/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8542 - mae: 1.3231 - val_loss: 0.8871 - val_mae: 1.3516\n",
            "Epoch 120/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8422 - mae: 1.3070 - val_loss: 0.8668 - val_mae: 1.3283\n",
            "Epoch 121/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8423 - mae: 1.3047 - val_loss: 0.8870 - val_mae: 1.3546\n",
            "Epoch 122/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8575 - mae: 1.3258 - val_loss: 0.9063 - val_mae: 1.3695\n",
            "Epoch 123/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8508 - mae: 1.3224 - val_loss: 0.9225 - val_mae: 1.3879\n",
            "Epoch 124/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8493 - mae: 1.3147 - val_loss: 0.8880 - val_mae: 1.3430\n",
            "Epoch 125/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8507 - mae: 1.3149 - val_loss: 0.8941 - val_mae: 1.3517\n",
            "Epoch 126/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8355 - mae: 1.3010 - val_loss: 0.8912 - val_mae: 1.3376\n",
            "Epoch 127/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8423 - mae: 1.3089 - val_loss: 0.8986 - val_mae: 1.3598\n",
            "Epoch 128/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8519 - mae: 1.3167 - val_loss: 0.8837 - val_mae: 1.3442\n",
            "Epoch 129/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8468 - mae: 1.3106 - val_loss: 0.8852 - val_mae: 1.3490\n",
            "Epoch 130/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8692 - mae: 1.3372 - val_loss: 0.8997 - val_mae: 1.3589\n",
            "Epoch 131/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8383 - mae: 1.3003 - val_loss: 0.9149 - val_mae: 1.3783\n",
            "Epoch 132/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8441 - mae: 1.3129 - val_loss: 0.8889 - val_mae: 1.3440\n",
            "Epoch 133/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8480 - mae: 1.3118 - val_loss: 0.8843 - val_mae: 1.3406\n",
            "Epoch 134/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8289 - mae: 1.2883 - val_loss: 0.8956 - val_mae: 1.3429\n",
            "Epoch 135/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8512 - mae: 1.3184 - val_loss: 0.8673 - val_mae: 1.3138\n",
            "Epoch 136/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8375 - mae: 1.3065 - val_loss: 0.8953 - val_mae: 1.3489\n",
            "Epoch 137/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8382 - mae: 1.3031 - val_loss: 0.8499 - val_mae: 1.3069\n",
            "Epoch 138/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8223 - mae: 1.2833 - val_loss: 0.8797 - val_mae: 1.3292\n",
            "Epoch 139/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8431 - mae: 1.3064 - val_loss: 0.8816 - val_mae: 1.3334\n",
            "Epoch 140/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8530 - mae: 1.3210 - val_loss: 0.9038 - val_mae: 1.3513\n",
            "Epoch 141/150\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.8396 - mae: 1.3038 - val_loss: 0.9113 - val_mae: 1.3633\n",
            "Epoch 142/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8400 - mae: 1.3039 - val_loss: 0.9047 - val_mae: 1.3543\n",
            "Epoch 143/150\n",
            "108/108 [==============================] - 1s 6ms/step - loss: 0.8485 - mae: 1.3116 - val_loss: 0.9363 - val_mae: 1.4065\n",
            "Epoch 144/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8333 - mae: 1.2938 - val_loss: 0.8768 - val_mae: 1.3330\n",
            "Epoch 145/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8456 - mae: 1.3054 - val_loss: 0.8779 - val_mae: 1.3338\n",
            "Epoch 146/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8212 - mae: 1.2789 - val_loss: 0.8701 - val_mae: 1.3185\n",
            "Epoch 147/150\n",
            "108/108 [==============================] - 1s 5ms/step - loss: 0.8384 - mae: 1.3023 - val_loss: 0.8886 - val_mae: 1.3385\n",
            "Epoch 148/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8124 - mae: 1.2718 - val_loss: 0.8516 - val_mae: 1.3124\n",
            "Epoch 149/150\n",
            "108/108 [==============================] - 0s 5ms/step - loss: 0.8229 - mae: 1.2818 - val_loss: 0.8637 - val_mae: 1.3269\n",
            "Epoch 150/150\n",
            "108/108 [==============================] - 0s 4ms/step - loss: 0.8194 - mae: 1.2797 - val_loss: 0.8853 - val_mae: 1.3422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-01-27 12:20:38,783] Trial 49 finished with value: 0.8852753043174744 and parameters: {'dropout_2': 0.2032936401078711, 'dropout_3': 0.35358279444253243, 'dropout_4': 0.3187886017506223, 'dropout_5': 0.37323694009726666, 'learning_rate': 0.0005910543805674803, 'epochs': 150, 'batch_size': 32}. Best is trial 42 with value: 0.8230664134025574.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial:\n",
            "Value: 0.8230664134025574\n",
            "Params: \n",
            "    dropout_2: 0.14627773253137555\n",
            "    dropout_3: 0.3104964013629035\n",
            "    dropout_4: 0.4514332549580662\n",
            "    dropout_5: 0.43080782555839275\n",
            "    learning_rate: 0.0005315067319714526\n",
            "    epochs: 200\n",
            "    batch_size: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = pd.get_dummies(new_aus_df.drop(['image_path', 'arousal', 'valence'], axis=1))\n",
        "y = new_aus_df[['arousal', 'valence']]"
      ],
      "metadata": {
        "id": "Uk6oLy__uI_l"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "0ETsUZBJqQcT",
        "outputId": "6c651a9a-1ffc-46fa-e648-7d288a4dc4a3"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          AU01      AU02      AU04      AU05      AU06  AU07      AU09  \\\n",
              "0     0.311357  0.135927  0.798931  0.328358  0.113616   1.0  0.493366   \n",
              "1     0.291428  0.148411  0.881824  0.365231  0.104657   1.0  0.360925   \n",
              "2     0.340948  0.113658  0.831665  0.340745  0.084021   0.0  0.353916   \n",
              "3     0.291201  0.127205  0.756636  0.326717  0.118116   0.0  0.433923   \n",
              "4     0.360045  0.199449  0.859479  0.252883  0.122171   1.0  0.447879   \n",
              "...        ...       ...       ...       ...       ...   ...       ...   \n",
              "5995  0.422657  0.050986  0.963368  0.253873  0.370320   1.0  0.270748   \n",
              "5996  0.421654  0.109469  0.965105  0.249666  0.180396   1.0  0.282195   \n",
              "5997  0.451566  0.175831  0.896503  0.529105  0.116464   0.0  0.126099   \n",
              "5998  0.608986  0.194725  0.704085  0.423611  0.285823   0.0  0.158707   \n",
              "5999  0.420602  0.128949  0.802174  0.316815  0.494270   1.0  0.225843   \n",
              "\n",
              "          AU10  AU11      AU12  ...      AU25      AU26      AU28      AU43  \\\n",
              "0     0.029253   0.0  0.068135  ...  0.993881  0.885669  0.501640  0.786895   \n",
              "1     0.062726   0.0  0.067747  ...  0.990813  0.953209  0.437312  0.711133   \n",
              "2     0.018393   0.0  0.030130  ...  0.978948  0.869907  0.266384  0.741665   \n",
              "3     0.005935   0.0  0.059412  ...  0.999072  0.740485  0.712587  0.701903   \n",
              "4     0.063832   0.0  0.104002  ...  0.994853  0.659481  0.780542  0.535933   \n",
              "...        ...   ...       ...  ...       ...       ...       ...       ...   \n",
              "5995  0.697472   1.0  0.208657  ...  0.998532  0.746642  0.063520  0.549602   \n",
              "5996  0.023396   1.0  0.065852  ...  0.986948  0.566380  0.103323  0.467108   \n",
              "5997  0.014512   1.0  0.046841  ...  0.965488  0.837143  0.633322  0.098899   \n",
              "5998  0.843249   1.0  0.096647  ...  0.992853  0.892109  0.421667  0.059766   \n",
              "5999  0.466679   1.0  0.173511  ...  0.998882  0.789521  0.164114  0.124826   \n",
              "\n",
              "      emotion_anger  emotion_fear  emotion_happiness  emotion_neutral  \\\n",
              "0                 0             1                  0                0   \n",
              "1                 0             0                  1                0   \n",
              "2                 0             0                  0                1   \n",
              "3                 0             0                  0                0   \n",
              "4                 0             0                  0                0   \n",
              "...             ...           ...                ...              ...   \n",
              "5995              0             0                  1                0   \n",
              "5996              0             0                  1                0   \n",
              "5997              0             0                  0                0   \n",
              "5998              0             0                  0                1   \n",
              "5999              0             0                  0                0   \n",
              "\n",
              "      emotion_sadness  emotion_surprise  \n",
              "0                   0                 0  \n",
              "1                   0                 0  \n",
              "2                   0                 0  \n",
              "3                   0                 1  \n",
              "4                   0                 1  \n",
              "...               ...               ...  \n",
              "5995                0                 0  \n",
              "5996                0                 0  \n",
              "5997                1                 0  \n",
              "5998                0                 0  \n",
              "5999                0                 1  \n",
              "\n",
              "[6000 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bc493408-c23e-4ab8-97de-3f86731ba906\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AU01</th>\n",
              "      <th>AU02</th>\n",
              "      <th>AU04</th>\n",
              "      <th>AU05</th>\n",
              "      <th>AU06</th>\n",
              "      <th>AU07</th>\n",
              "      <th>AU09</th>\n",
              "      <th>AU10</th>\n",
              "      <th>AU11</th>\n",
              "      <th>AU12</th>\n",
              "      <th>...</th>\n",
              "      <th>AU25</th>\n",
              "      <th>AU26</th>\n",
              "      <th>AU28</th>\n",
              "      <th>AU43</th>\n",
              "      <th>emotion_anger</th>\n",
              "      <th>emotion_fear</th>\n",
              "      <th>emotion_happiness</th>\n",
              "      <th>emotion_neutral</th>\n",
              "      <th>emotion_sadness</th>\n",
              "      <th>emotion_surprise</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.311357</td>\n",
              "      <td>0.135927</td>\n",
              "      <td>0.798931</td>\n",
              "      <td>0.328358</td>\n",
              "      <td>0.113616</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.493366</td>\n",
              "      <td>0.029253</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.068135</td>\n",
              "      <td>...</td>\n",
              "      <td>0.993881</td>\n",
              "      <td>0.885669</td>\n",
              "      <td>0.501640</td>\n",
              "      <td>0.786895</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.291428</td>\n",
              "      <td>0.148411</td>\n",
              "      <td>0.881824</td>\n",
              "      <td>0.365231</td>\n",
              "      <td>0.104657</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.360925</td>\n",
              "      <td>0.062726</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.067747</td>\n",
              "      <td>...</td>\n",
              "      <td>0.990813</td>\n",
              "      <td>0.953209</td>\n",
              "      <td>0.437312</td>\n",
              "      <td>0.711133</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.340948</td>\n",
              "      <td>0.113658</td>\n",
              "      <td>0.831665</td>\n",
              "      <td>0.340745</td>\n",
              "      <td>0.084021</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.353916</td>\n",
              "      <td>0.018393</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.030130</td>\n",
              "      <td>...</td>\n",
              "      <td>0.978948</td>\n",
              "      <td>0.869907</td>\n",
              "      <td>0.266384</td>\n",
              "      <td>0.741665</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.291201</td>\n",
              "      <td>0.127205</td>\n",
              "      <td>0.756636</td>\n",
              "      <td>0.326717</td>\n",
              "      <td>0.118116</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.433923</td>\n",
              "      <td>0.005935</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.059412</td>\n",
              "      <td>...</td>\n",
              "      <td>0.999072</td>\n",
              "      <td>0.740485</td>\n",
              "      <td>0.712587</td>\n",
              "      <td>0.701903</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.360045</td>\n",
              "      <td>0.199449</td>\n",
              "      <td>0.859479</td>\n",
              "      <td>0.252883</td>\n",
              "      <td>0.122171</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.447879</td>\n",
              "      <td>0.063832</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.104002</td>\n",
              "      <td>...</td>\n",
              "      <td>0.994853</td>\n",
              "      <td>0.659481</td>\n",
              "      <td>0.780542</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>0.422657</td>\n",
              "      <td>0.050986</td>\n",
              "      <td>0.963368</td>\n",
              "      <td>0.253873</td>\n",
              "      <td>0.370320</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.270748</td>\n",
              "      <td>0.697472</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.208657</td>\n",
              "      <td>...</td>\n",
              "      <td>0.998532</td>\n",
              "      <td>0.746642</td>\n",
              "      <td>0.063520</td>\n",
              "      <td>0.549602</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>0.421654</td>\n",
              "      <td>0.109469</td>\n",
              "      <td>0.965105</td>\n",
              "      <td>0.249666</td>\n",
              "      <td>0.180396</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.282195</td>\n",
              "      <td>0.023396</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.065852</td>\n",
              "      <td>...</td>\n",
              "      <td>0.986948</td>\n",
              "      <td>0.566380</td>\n",
              "      <td>0.103323</td>\n",
              "      <td>0.467108</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>0.451566</td>\n",
              "      <td>0.175831</td>\n",
              "      <td>0.896503</td>\n",
              "      <td>0.529105</td>\n",
              "      <td>0.116464</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.126099</td>\n",
              "      <td>0.014512</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.046841</td>\n",
              "      <td>...</td>\n",
              "      <td>0.965488</td>\n",
              "      <td>0.837143</td>\n",
              "      <td>0.633322</td>\n",
              "      <td>0.098899</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>0.608986</td>\n",
              "      <td>0.194725</td>\n",
              "      <td>0.704085</td>\n",
              "      <td>0.423611</td>\n",
              "      <td>0.285823</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.158707</td>\n",
              "      <td>0.843249</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.096647</td>\n",
              "      <td>...</td>\n",
              "      <td>0.992853</td>\n",
              "      <td>0.892109</td>\n",
              "      <td>0.421667</td>\n",
              "      <td>0.059766</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>0.420602</td>\n",
              "      <td>0.128949</td>\n",
              "      <td>0.802174</td>\n",
              "      <td>0.316815</td>\n",
              "      <td>0.494270</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.225843</td>\n",
              "      <td>0.466679</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.173511</td>\n",
              "      <td>...</td>\n",
              "      <td>0.998882</td>\n",
              "      <td>0.789521</td>\n",
              "      <td>0.164114</td>\n",
              "      <td>0.124826</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 26 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bc493408-c23e-4ab8-97de-3f86731ba906')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bc493408-c23e-4ab8-97de-3f86731ba906 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bc493408-c23e-4ab8-97de-3f86731ba906');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-658343ce-96b6-4638-9b8e-311fde7c408d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-658343ce-96b6-4638-9b8e-311fde7c408d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-658343ce-96b6-4638-9b8e-311fde7c408d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Assuming X_train and X_test are your input features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Addestramento del modello\n",
        "model.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Dopo l'addestramento, puoi utilizzare il modello per fare predizioni su nuovi dati\n",
        "# Supponendo che tu abbia dati di test: X_test\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Ora 'predictions' conterrà le previsioni del tuo modello per i dati di test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QWslQQ7t6dH",
        "outputId": "120b9d3a-669a-4fca-a2b2-7f3cba187735"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "120/120 [==============================] - 3s 9ms/step - loss: 2.2719 - mae: 2.8671 - val_loss: 1.7614 - val_mae: 2.3241\n",
            "Epoch 2/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 2.0289 - mae: 2.6117 - val_loss: 1.6163 - val_mae: 2.1767\n",
            "Epoch 3/200\n",
            "120/120 [==============================] - 0s 3ms/step - loss: 1.8724 - mae: 2.4488 - val_loss: 1.5178 - val_mae: 2.0755\n",
            "Epoch 4/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.7773 - mae: 2.3530 - val_loss: 1.4666 - val_mae: 2.0280\n",
            "Epoch 5/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.6636 - mae: 2.2298 - val_loss: 1.3732 - val_mae: 1.9242\n",
            "Epoch 6/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.6004 - mae: 2.1612 - val_loss: 1.3389 - val_mae: 1.8869\n",
            "Epoch 7/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.5543 - mae: 2.1137 - val_loss: 1.3094 - val_mae: 1.8533\n",
            "Epoch 8/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.5243 - mae: 2.0781 - val_loss: 1.3002 - val_mae: 1.8441\n",
            "Epoch 9/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.4872 - mae: 2.0406 - val_loss: 1.2885 - val_mae: 1.8329\n",
            "Epoch 10/200\n",
            "120/120 [==============================] - 0s 3ms/step - loss: 1.4727 - mae: 2.0263 - val_loss: 1.2799 - val_mae: 1.8229\n",
            "Epoch 11/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.4475 - mae: 1.9972 - val_loss: 1.2690 - val_mae: 1.8117\n",
            "Epoch 12/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.4417 - mae: 1.9905 - val_loss: 1.2831 - val_mae: 1.8252\n",
            "Epoch 13/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.4152 - mae: 1.9629 - val_loss: 1.2634 - val_mae: 1.8029\n",
            "Epoch 14/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.4230 - mae: 1.9712 - val_loss: 1.2516 - val_mae: 1.7911\n",
            "Epoch 15/200\n",
            "120/120 [==============================] - 0s 3ms/step - loss: 1.3819 - mae: 1.9289 - val_loss: 1.2435 - val_mae: 1.7835\n",
            "Epoch 16/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.3814 - mae: 1.9285 - val_loss: 1.2305 - val_mae: 1.7689\n",
            "Epoch 17/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.3809 - mae: 1.9267 - val_loss: 1.2332 - val_mae: 1.7743\n",
            "Epoch 18/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.3778 - mae: 1.9249 - val_loss: 1.2250 - val_mae: 1.7632\n",
            "Epoch 19/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.3695 - mae: 1.9172 - val_loss: 1.2385 - val_mae: 1.7782\n",
            "Epoch 20/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.3506 - mae: 1.8907 - val_loss: 1.2151 - val_mae: 1.7520\n",
            "Epoch 21/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.3614 - mae: 1.9055 - val_loss: 1.2205 - val_mae: 1.7589\n",
            "Epoch 22/200\n",
            "120/120 [==============================] - 0s 3ms/step - loss: 1.3355 - mae: 1.8794 - val_loss: 1.2272 - val_mae: 1.7669\n",
            "Epoch 23/200\n",
            "120/120 [==============================] - 1s 4ms/step - loss: 1.3444 - mae: 1.8868 - val_loss: 1.2240 - val_mae: 1.7632\n",
            "Epoch 24/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 1.3507 - mae: 1.8945 - val_loss: 1.2195 - val_mae: 1.7578\n",
            "Epoch 25/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 1.3343 - mae: 1.8774 - val_loss: 1.2274 - val_mae: 1.7678\n",
            "Epoch 26/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 1.3434 - mae: 1.8850 - val_loss: 1.2142 - val_mae: 1.7507\n",
            "Epoch 27/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 1.3390 - mae: 1.8811 - val_loss: 1.2241 - val_mae: 1.7624\n",
            "Epoch 28/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 1.3326 - mae: 1.8742 - val_loss: 1.2199 - val_mae: 1.7587\n",
            "Epoch 29/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.3350 - mae: 1.8750 - val_loss: 1.2198 - val_mae: 1.7572\n",
            "Epoch 30/200\n",
            "120/120 [==============================] - 0s 3ms/step - loss: 1.3288 - mae: 1.8682 - val_loss: 1.2173 - val_mae: 1.7552\n",
            "Epoch 31/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.3048 - mae: 1.8436 - val_loss: 1.2089 - val_mae: 1.7491\n",
            "Epoch 32/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.3109 - mae: 1.8511 - val_loss: 1.2123 - val_mae: 1.7488\n",
            "Epoch 33/200\n",
            "120/120 [==============================] - 1s 4ms/step - loss: 1.3118 - mae: 1.8509 - val_loss: 1.2151 - val_mae: 1.7545\n",
            "Epoch 34/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.3048 - mae: 1.8440 - val_loss: 1.2087 - val_mae: 1.7458\n",
            "Epoch 35/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2974 - mae: 1.8355 - val_loss: 1.2157 - val_mae: 1.7541\n",
            "Epoch 36/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2989 - mae: 1.8374 - val_loss: 1.2116 - val_mae: 1.7483\n",
            "Epoch 37/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2964 - mae: 1.8359 - val_loss: 1.2144 - val_mae: 1.7499\n",
            "Epoch 38/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2968 - mae: 1.8370 - val_loss: 1.2108 - val_mae: 1.7448\n",
            "Epoch 39/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2818 - mae: 1.8203 - val_loss: 1.2026 - val_mae: 1.7376\n",
            "Epoch 40/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2972 - mae: 1.8365 - val_loss: 1.1984 - val_mae: 1.7313\n",
            "Epoch 41/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2738 - mae: 1.8103 - val_loss: 1.1981 - val_mae: 1.7295\n",
            "Epoch 42/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2827 - mae: 1.8205 - val_loss: 1.2242 - val_mae: 1.7596\n",
            "Epoch 43/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2719 - mae: 1.8076 - val_loss: 1.1880 - val_mae: 1.7208\n",
            "Epoch 44/200\n",
            "120/120 [==============================] - 0s 3ms/step - loss: 1.2698 - mae: 1.8071 - val_loss: 1.1948 - val_mae: 1.7261\n",
            "Epoch 45/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2596 - mae: 1.7939 - val_loss: 1.1948 - val_mae: 1.7260\n",
            "Epoch 46/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2745 - mae: 1.8115 - val_loss: 1.2033 - val_mae: 1.7359\n",
            "Epoch 47/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2700 - mae: 1.8072 - val_loss: 1.2046 - val_mae: 1.7385\n",
            "Epoch 48/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2851 - mae: 1.8220 - val_loss: 1.1949 - val_mae: 1.7278\n",
            "Epoch 49/200\n",
            "120/120 [==============================] - 1s 4ms/step - loss: 1.2774 - mae: 1.8139 - val_loss: 1.1927 - val_mae: 1.7235\n",
            "Epoch 50/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 1.2540 - mae: 1.7878 - val_loss: 1.1950 - val_mae: 1.7256\n",
            "Epoch 51/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 1.2720 - mae: 1.8092 - val_loss: 1.1910 - val_mae: 1.7206\n",
            "Epoch 52/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 1.2672 - mae: 1.8020 - val_loss: 1.1780 - val_mae: 1.7058\n",
            "Epoch 53/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 1.2545 - mae: 1.7878 - val_loss: 1.1821 - val_mae: 1.7125\n",
            "Epoch 54/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 1.2582 - mae: 1.7914 - val_loss: 1.1790 - val_mae: 1.7085\n",
            "Epoch 55/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2582 - mae: 1.7930 - val_loss: 1.1943 - val_mae: 1.7280\n",
            "Epoch 56/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2681 - mae: 1.8061 - val_loss: 1.1813 - val_mae: 1.7120\n",
            "Epoch 57/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2490 - mae: 1.7837 - val_loss: 1.1881 - val_mae: 1.7179\n",
            "Epoch 58/200\n",
            "120/120 [==============================] - 0s 3ms/step - loss: 1.2510 - mae: 1.7852 - val_loss: 1.1699 - val_mae: 1.6998\n",
            "Epoch 59/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2431 - mae: 1.7752 - val_loss: 1.1809 - val_mae: 1.7093\n",
            "Epoch 60/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2498 - mae: 1.7854 - val_loss: 1.1687 - val_mae: 1.6952\n",
            "Epoch 61/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2404 - mae: 1.7714 - val_loss: 1.1777 - val_mae: 1.7069\n",
            "Epoch 62/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2503 - mae: 1.7841 - val_loss: 1.1577 - val_mae: 1.6857\n",
            "Epoch 63/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2447 - mae: 1.7784 - val_loss: 1.1770 - val_mae: 1.7066\n",
            "Epoch 64/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2414 - mae: 1.7751 - val_loss: 1.1835 - val_mae: 1.7139\n",
            "Epoch 65/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2302 - mae: 1.7617 - val_loss: 1.1655 - val_mae: 1.6949\n",
            "Epoch 66/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2315 - mae: 1.7635 - val_loss: 1.1771 - val_mae: 1.7073\n",
            "Epoch 67/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2206 - mae: 1.7501 - val_loss: 1.1860 - val_mae: 1.7158\n",
            "Epoch 68/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2285 - mae: 1.7582 - val_loss: 1.1775 - val_mae: 1.7077\n",
            "Epoch 69/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2205 - mae: 1.7520 - val_loss: 1.1600 - val_mae: 1.6889\n",
            "Epoch 70/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2353 - mae: 1.7668 - val_loss: 1.1513 - val_mae: 1.6802\n",
            "Epoch 71/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2304 - mae: 1.7618 - val_loss: 1.1735 - val_mae: 1.7044\n",
            "Epoch 72/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2079 - mae: 1.7359 - val_loss: 1.1793 - val_mae: 1.7125\n",
            "Epoch 73/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2218 - mae: 1.7550 - val_loss: 1.1596 - val_mae: 1.6893\n",
            "Epoch 74/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2167 - mae: 1.7484 - val_loss: 1.1501 - val_mae: 1.6776\n",
            "Epoch 75/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2155 - mae: 1.7451 - val_loss: 1.1699 - val_mae: 1.7001\n",
            "Epoch 76/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 1.2145 - mae: 1.7421 - val_loss: 1.2038 - val_mae: 1.7382\n",
            "Epoch 77/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 1.2045 - mae: 1.7317 - val_loss: 1.1549 - val_mae: 1.6839\n",
            "Epoch 78/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 1.1928 - mae: 1.7201 - val_loss: 1.1639 - val_mae: 1.6935\n",
            "Epoch 79/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 1.2152 - mae: 1.7453 - val_loss: 1.1405 - val_mae: 1.6667\n",
            "Epoch 80/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 1.2032 - mae: 1.7323 - val_loss: 1.1612 - val_mae: 1.6882\n",
            "Epoch 81/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 1.2195 - mae: 1.7520 - val_loss: 1.1723 - val_mae: 1.7030\n",
            "Epoch 82/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1963 - mae: 1.7229 - val_loss: 1.1460 - val_mae: 1.6737\n",
            "Epoch 83/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1967 - mae: 1.7255 - val_loss: 1.1465 - val_mae: 1.6745\n",
            "Epoch 84/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2111 - mae: 1.7419 - val_loss: 1.1632 - val_mae: 1.6919\n",
            "Epoch 85/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2053 - mae: 1.7332 - val_loss: 1.1512 - val_mae: 1.6798\n",
            "Epoch 86/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1987 - mae: 1.7288 - val_loss: 1.1422 - val_mae: 1.6685\n",
            "Epoch 87/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1857 - mae: 1.7134 - val_loss: 1.1675 - val_mae: 1.6969\n",
            "Epoch 88/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1939 - mae: 1.7199 - val_loss: 1.1787 - val_mae: 1.7122\n",
            "Epoch 89/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1844 - mae: 1.7123 - val_loss: 1.1589 - val_mae: 1.6896\n",
            "Epoch 90/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2031 - mae: 1.7326 - val_loss: 1.1495 - val_mae: 1.6771\n",
            "Epoch 91/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.2006 - mae: 1.7292 - val_loss: 1.1620 - val_mae: 1.6909\n",
            "Epoch 92/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1924 - mae: 1.7196 - val_loss: 1.1600 - val_mae: 1.6878\n",
            "Epoch 93/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1790 - mae: 1.7055 - val_loss: 1.1617 - val_mae: 1.6910\n",
            "Epoch 94/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1922 - mae: 1.7176 - val_loss: 1.1631 - val_mae: 1.6924\n",
            "Epoch 95/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1846 - mae: 1.7119 - val_loss: 1.1422 - val_mae: 1.6704\n",
            "Epoch 96/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1885 - mae: 1.7176 - val_loss: 1.1662 - val_mae: 1.6954\n",
            "Epoch 97/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1662 - mae: 1.6911 - val_loss: 1.1717 - val_mae: 1.7026\n",
            "Epoch 98/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1890 - mae: 1.7145 - val_loss: 1.1483 - val_mae: 1.6752\n",
            "Epoch 99/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1652 - mae: 1.6895 - val_loss: 1.1741 - val_mae: 1.7017\n",
            "Epoch 100/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1739 - mae: 1.6997 - val_loss: 1.1442 - val_mae: 1.6717\n",
            "Epoch 101/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1743 - mae: 1.6985 - val_loss: 1.1497 - val_mae: 1.6768\n",
            "Epoch 102/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 1.1831 - mae: 1.7105 - val_loss: 1.1503 - val_mae: 1.6778\n",
            "Epoch 103/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 1.1686 - mae: 1.6951 - val_loss: 1.1536 - val_mae: 1.6830\n",
            "Epoch 104/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 1.1602 - mae: 1.6825 - val_loss: 1.1447 - val_mae: 1.6700\n",
            "Epoch 105/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 1.1632 - mae: 1.6876 - val_loss: 1.1480 - val_mae: 1.6756\n",
            "Epoch 106/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 1.1814 - mae: 1.7093 - val_loss: 1.1684 - val_mae: 1.6976\n",
            "Epoch 107/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 1.1684 - mae: 1.6938 - val_loss: 1.1451 - val_mae: 1.6696\n",
            "Epoch 108/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1582 - mae: 1.6824 - val_loss: 1.1311 - val_mae: 1.6575\n",
            "Epoch 109/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1550 - mae: 1.6754 - val_loss: 1.1476 - val_mae: 1.6754\n",
            "Epoch 110/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1639 - mae: 1.6887 - val_loss: 1.1532 - val_mae: 1.6821\n",
            "Epoch 111/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1619 - mae: 1.6867 - val_loss: 1.1216 - val_mae: 1.6460\n",
            "Epoch 112/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1457 - mae: 1.6693 - val_loss: 1.1393 - val_mae: 1.6623\n",
            "Epoch 113/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1530 - mae: 1.6768 - val_loss: 1.1238 - val_mae: 1.6490\n",
            "Epoch 114/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1667 - mae: 1.6941 - val_loss: 1.1538 - val_mae: 1.6822\n",
            "Epoch 115/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1515 - mae: 1.6744 - val_loss: 1.1301 - val_mae: 1.6520\n",
            "Epoch 116/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1577 - mae: 1.6810 - val_loss: 1.1271 - val_mae: 1.6526\n",
            "Epoch 117/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1444 - mae: 1.6682 - val_loss: 1.1557 - val_mae: 1.6834\n",
            "Epoch 118/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1597 - mae: 1.6817 - val_loss: 1.1512 - val_mae: 1.6802\n",
            "Epoch 119/200\n",
            "120/120 [==============================] - 0s 3ms/step - loss: 1.1729 - mae: 1.6984 - val_loss: 1.1247 - val_mae: 1.6508\n",
            "Epoch 120/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1348 - mae: 1.6582 - val_loss: 1.1414 - val_mae: 1.6694\n",
            "Epoch 121/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1348 - mae: 1.6562 - val_loss: 1.1506 - val_mae: 1.6775\n",
            "Epoch 122/200\n",
            "120/120 [==============================] - 0s 3ms/step - loss: 1.1426 - mae: 1.6639 - val_loss: 1.1472 - val_mae: 1.6727\n",
            "Epoch 123/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1507 - mae: 1.6735 - val_loss: 1.1467 - val_mae: 1.6739\n",
            "Epoch 124/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1540 - mae: 1.6782 - val_loss: 1.1301 - val_mae: 1.6566\n",
            "Epoch 125/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1461 - mae: 1.6692 - val_loss: 1.1138 - val_mae: 1.6380\n",
            "Epoch 126/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1487 - mae: 1.6718 - val_loss: 1.1375 - val_mae: 1.6629\n",
            "Epoch 127/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1460 - mae: 1.6672 - val_loss: 1.1289 - val_mae: 1.6550\n",
            "Epoch 128/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1269 - mae: 1.6469 - val_loss: 1.1359 - val_mae: 1.6642\n",
            "Epoch 129/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 1.1403 - mae: 1.6654 - val_loss: 1.1347 - val_mae: 1.6618\n",
            "Epoch 130/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 1.1292 - mae: 1.6491 - val_loss: 1.1370 - val_mae: 1.6640\n",
            "Epoch 131/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 1.1495 - mae: 1.6734 - val_loss: 1.1375 - val_mae: 1.6628\n",
            "Epoch 132/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 1.1363 - mae: 1.6572 - val_loss: 1.1276 - val_mae: 1.6497\n",
            "Epoch 133/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 1.1475 - mae: 1.6691 - val_loss: 1.1383 - val_mae: 1.6634\n",
            "Epoch 134/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1250 - mae: 1.6455 - val_loss: 1.1442 - val_mae: 1.6713\n",
            "Epoch 135/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1256 - mae: 1.6447 - val_loss: 1.1258 - val_mae: 1.6499\n",
            "Epoch 136/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1193 - mae: 1.6406 - val_loss: 1.1118 - val_mae: 1.6344\n",
            "Epoch 137/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1336 - mae: 1.6542 - val_loss: 1.1257 - val_mae: 1.6492\n",
            "Epoch 138/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1186 - mae: 1.6385 - val_loss: 1.1174 - val_mae: 1.6406\n",
            "Epoch 139/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1309 - mae: 1.6514 - val_loss: 1.1031 - val_mae: 1.6255\n",
            "Epoch 140/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1121 - mae: 1.6332 - val_loss: 1.1257 - val_mae: 1.6510\n",
            "Epoch 141/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1310 - mae: 1.6504 - val_loss: 1.1368 - val_mae: 1.6617\n",
            "Epoch 142/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1240 - mae: 1.6442 - val_loss: 1.1091 - val_mae: 1.6326\n",
            "Epoch 143/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1429 - mae: 1.6636 - val_loss: 1.1339 - val_mae: 1.6557\n",
            "Epoch 144/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1164 - mae: 1.6363 - val_loss: 1.1269 - val_mae: 1.6499\n",
            "Epoch 145/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1071 - mae: 1.6256 - val_loss: 1.1249 - val_mae: 1.6495\n",
            "Epoch 146/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1148 - mae: 1.6346 - val_loss: 1.1200 - val_mae: 1.6432\n",
            "Epoch 147/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1264 - mae: 1.6459 - val_loss: 1.1287 - val_mae: 1.6537\n",
            "Epoch 148/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1117 - mae: 1.6324 - val_loss: 1.1225 - val_mae: 1.6434\n",
            "Epoch 149/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1168 - mae: 1.6358 - val_loss: 1.1136 - val_mae: 1.6374\n",
            "Epoch 150/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1167 - mae: 1.6334 - val_loss: 1.1137 - val_mae: 1.6371\n",
            "Epoch 151/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1179 - mae: 1.6383 - val_loss: 1.1249 - val_mae: 1.6475\n",
            "Epoch 152/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1172 - mae: 1.6378 - val_loss: 1.1276 - val_mae: 1.6499\n",
            "Epoch 153/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1206 - mae: 1.6395 - val_loss: 1.1237 - val_mae: 1.6457\n",
            "Epoch 154/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1038 - mae: 1.6253 - val_loss: 1.1067 - val_mae: 1.6266\n",
            "Epoch 155/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 1.1133 - mae: 1.6332 - val_loss: 1.1192 - val_mae: 1.6388\n",
            "Epoch 156/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 1.0891 - mae: 1.6046 - val_loss: 1.1298 - val_mae: 1.6544\n",
            "Epoch 157/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 1.1122 - mae: 1.6300 - val_loss: 1.1020 - val_mae: 1.6214\n",
            "Epoch 158/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 1.1106 - mae: 1.6292 - val_loss: 1.1186 - val_mae: 1.6410\n",
            "Epoch 159/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 1.1034 - mae: 1.6190 - val_loss: 1.1168 - val_mae: 1.6401\n",
            "Epoch 160/200\n",
            "120/120 [==============================] - 1s 4ms/step - loss: 1.0997 - mae: 1.6182 - val_loss: 1.1445 - val_mae: 1.6658\n",
            "Epoch 161/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1122 - mae: 1.6294 - val_loss: 1.1221 - val_mae: 1.6470\n",
            "Epoch 162/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1138 - mae: 1.6335 - val_loss: 1.1149 - val_mae: 1.6364\n",
            "Epoch 163/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0867 - mae: 1.6039 - val_loss: 1.1167 - val_mae: 1.6393\n",
            "Epoch 164/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0899 - mae: 1.6079 - val_loss: 1.1164 - val_mae: 1.6372\n",
            "Epoch 165/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1078 - mae: 1.6263 - val_loss: 1.1089 - val_mae: 1.6308\n",
            "Epoch 166/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0900 - mae: 1.6096 - val_loss: 1.1134 - val_mae: 1.6341\n",
            "Epoch 167/200\n",
            "120/120 [==============================] - 1s 4ms/step - loss: 1.0963 - mae: 1.6144 - val_loss: 1.1098 - val_mae: 1.6315\n",
            "Epoch 168/200\n",
            "120/120 [==============================] - 1s 5ms/step - loss: 1.0897 - mae: 1.6055 - val_loss: 1.1099 - val_mae: 1.6290\n",
            "Epoch 169/200\n",
            "120/120 [==============================] - 1s 4ms/step - loss: 1.0952 - mae: 1.6129 - val_loss: 1.0937 - val_mae: 1.6132\n",
            "Epoch 170/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1017 - mae: 1.6189 - val_loss: 1.0912 - val_mae: 1.6129\n",
            "Epoch 171/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1019 - mae: 1.6199 - val_loss: 1.0949 - val_mae: 1.6137\n",
            "Epoch 172/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0782 - mae: 1.5919 - val_loss: 1.0949 - val_mae: 1.6182\n",
            "Epoch 173/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0937 - mae: 1.6087 - val_loss: 1.1110 - val_mae: 1.6330\n",
            "Epoch 174/200\n",
            "120/120 [==============================] - 1s 9ms/step - loss: 1.0753 - mae: 1.5879 - val_loss: 1.1300 - val_mae: 1.6516\n",
            "Epoch 175/200\n",
            "120/120 [==============================] - 2s 15ms/step - loss: 1.0818 - mae: 1.5977 - val_loss: 1.1171 - val_mae: 1.6393\n",
            "Epoch 176/200\n",
            "120/120 [==============================] - 1s 8ms/step - loss: 1.0915 - mae: 1.6050 - val_loss: 1.0999 - val_mae: 1.6210\n",
            "Epoch 177/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 1.0880 - mae: 1.6048 - val_loss: 1.1276 - val_mae: 1.6494\n",
            "Epoch 178/200\n",
            "120/120 [==============================] - 1s 7ms/step - loss: 1.0882 - mae: 1.6012 - val_loss: 1.0919 - val_mae: 1.6078\n",
            "Epoch 179/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 1.0955 - mae: 1.6130 - val_loss: 1.1050 - val_mae: 1.6254\n",
            "Epoch 180/200\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 1.0864 - mae: 1.6025 - val_loss: 1.1123 - val_mae: 1.6298\n",
            "Epoch 181/200\n",
            "120/120 [==============================] - 1s 4ms/step - loss: 1.0832 - mae: 1.5999 - val_loss: 1.1192 - val_mae: 1.6429\n",
            "Epoch 182/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0774 - mae: 1.5937 - val_loss: 1.1042 - val_mae: 1.6258\n",
            "Epoch 183/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0835 - mae: 1.5978 - val_loss: 1.1100 - val_mae: 1.6279\n",
            "Epoch 184/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0818 - mae: 1.5960 - val_loss: 1.0951 - val_mae: 1.6118\n",
            "Epoch 185/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0625 - mae: 1.5758 - val_loss: 1.0943 - val_mae: 1.6129\n",
            "Epoch 186/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.1012 - mae: 1.6181 - val_loss: 1.1240 - val_mae: 1.6439\n",
            "Epoch 187/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0703 - mae: 1.5848 - val_loss: 1.1142 - val_mae: 1.6368\n",
            "Epoch 188/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0749 - mae: 1.5900 - val_loss: 1.1102 - val_mae: 1.6321\n",
            "Epoch 189/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0797 - mae: 1.5950 - val_loss: 1.1118 - val_mae: 1.6350\n",
            "Epoch 190/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0893 - mae: 1.6077 - val_loss: 1.1042 - val_mae: 1.6266\n",
            "Epoch 191/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0893 - mae: 1.6037 - val_loss: 1.1169 - val_mae: 1.6383\n",
            "Epoch 192/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0794 - mae: 1.5965 - val_loss: 1.1221 - val_mae: 1.6438\n",
            "Epoch 193/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0565 - mae: 1.5684 - val_loss: 1.1274 - val_mae: 1.6490\n",
            "Epoch 194/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0770 - mae: 1.5927 - val_loss: 1.1048 - val_mae: 1.6232\n",
            "Epoch 195/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0849 - mae: 1.5970 - val_loss: 1.1273 - val_mae: 1.6461\n",
            "Epoch 196/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0671 - mae: 1.5814 - val_loss: 1.1051 - val_mae: 1.6198\n",
            "Epoch 197/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0746 - mae: 1.5925 - val_loss: 1.0985 - val_mae: 1.6155\n",
            "Epoch 198/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0652 - mae: 1.5807 - val_loss: 1.1146 - val_mae: 1.6370\n",
            "Epoch 199/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0655 - mae: 1.5779 - val_loss: 1.1129 - val_mae: 1.6304\n",
            "Epoch 200/200\n",
            "120/120 [==============================] - 0s 4ms/step - loss: 1.0674 - mae: 1.5803 - val_loss: 1.1043 - val_mae: 1.6245\n",
            "38/38 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Mean Squared Error (MSE): {loss}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCDOXBWwlYzw",
        "outputId": "bd710c28-022f-4ee0-9dee-6367cbb2d4b0"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - 0s 2ms/step - loss: 1.1309 - mae: 1.6448\n",
            "Mean Squared Error (MSE): 1.1308751106262207\n",
            "Mean Absolute Error (MAE): 1.6447924375534058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sq3O8ZOZFA7N"
      },
      "outputs": [],
      "source": [
        "model_path = 'arousal_valence_pred_model.h5'\n",
        "\n",
        "model.save(model_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}